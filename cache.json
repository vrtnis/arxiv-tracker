{"2024-08-01T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2407.21740v2","updated":"2024-08-01T03:16:43Z","published":"2024-07-31T16:52:00Z","title":"Contrastive Factor Analysis","summary":"  Factor analysis, often regarded as a Bayesian variant of matrix\nfactorization, offers superior capabilities in capturing uncertainty, modeling\ncomplex dependencies, and ensuring robustness. As the deep learning era\narrives, factor analysis is receiving less and less attention due to their\nlimited expressive ability. On the contrary, contrastive learning has emerged\nas a potent technique with demonstrated efficacy in unsupervised\nrepresentational learning. While the two methods are different paradigms,\nrecent theoretical analysis has revealed the mathematical equivalence between\ncontrastive learning and matrix factorization, providing a potential\npossibility for factor analysis combined with contrastive learning. Motivated\nby the interconnectedness of contrastive learning, matrix factorization, and\nfactor analysis, this paper introduces a novel Contrastive Factor Analysis\nframework, aiming to leverage factor analysis's advantageous properties within\nthe realm of contrastive learning. To further leverage the interpretability\nproperties of non-negative factor analysis, which can learn disentangled\nrepresentations, contrastive factor analysis is extended to a non-negative\nversion. Finally, extensive experimental validation showcases the efficacy of\nthe proposed contrastive (non-negative) factor analysis methodology across\nmultiple key properties, including expressiveness, robustness,\ninterpretability, and accurate uncertainty estimation.\n","authors":["Zhibin Duan","Tiansheng Wen","Yifei Wang","Chen Zhu","Bo Chen","Mingyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.21740v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.05846v2","updated":"2024-08-01T17:46:38Z","published":"2023-08-10T19:56:15Z","title":"Seed Kernel Counting using Domain Randomization and Object Tracking\n  Neural Networks","summary":"  High-throughput phenotyping (HTP) of seeds, also known as seed phenotyping,\nis the comprehensive assessment of complex seed traits such as growth,\ndevelopment, tolerance, resistance, ecology, yield, and the measurement of\nparameters that form more complex traits. One of the key aspects of seed\nphenotyping is cereal yield estimation that the seed production industry relies\nupon to conduct their business. While mechanized seed kernel counters are\navailable in the market currently, they are often priced high and sometimes\noutside the range of small scale seed production firms' affordability. The\ndevelopment of object tracking neural network models such as You Only Look Once\n(YOLO) enables computer scientists to design algorithms that can estimate\ncereal yield inexpensively. The key bottleneck with neural network models is\nthat they require a plethora of labelled training data before they can be put\nto task. We demonstrate that the use of synthetic imagery serves as a feasible\nsubstitute to train neural networks for object tracking that includes the tasks\nof object classification and detection. Furthermore, we propose a seed kernel\ncounter that uses a low-cost mechanical hopper, trained YOLOv8 neural network\nmodel, and object tracking algorithms on StrongSORT and ByteTrack to estimate\ncereal yield from videos. The experiment yields a seed kernel count with an\naccuracy of 95.2\\% and 93.2\\% for Soy and Wheat respectively using the\nStrongSORT algorithm, and an accuray of 96.8\\% and 92.4\\% for Soy and Wheat\nrespectively using the ByteTrack algorithm.\n","authors":["Venkat Margapuri","Prapti Thapaliya","Mitchell Neilsen"],"pdf_url":"https://arxiv.org/pdf/2308.05846v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18320v2","updated":"2024-08-01T17:43:19Z","published":"2024-05-28T16:11:11Z","title":"Self-Supervised Learning Based Handwriting Verification","summary":"  We present SSL-HV: Self-Supervised Learning approaches applied to the task of\nHandwriting Verification. This task involves determining whether a given pair\nof handwritten images originate from the same or different writer distribution.\nWe have compared the performance of multiple generative, contrastive SSL\napproaches against handcrafted feature extractors and supervised learning on\nCEDAR AND dataset. We show that ResNet based Variational Auto-Encoder (VAE)\noutperforms other generative approaches achieving 76.3% accuracy, while\nResNet-18 fine-tuned using Variance-Invariance-Covariance Regularization\n(VICReg) outperforms other contrastive approaches achieving 78% accuracy. Using\na pre-trained VAE and VICReg for the downstream task of writer verification we\nobserved a relative improvement in accuracy of 6.7% and 9% over ResNet-18\nsupervised baseline with 10% writer labels.\n","authors":["Mihir Chauhan","Mohammad Abuzar Hashemi","Abhishek Satbhai","Mir Basheer Ali","Bina Ramamurthy","Mingchen Gao","Siwei Lyu","Sargur Srihari"],"pdf_url":"https://arxiv.org/pdf/2405.18320v2.pdf","comment":"8 pages, 2 figures, 2 tables, Accepted at Irish Machine Vision and\n  Image Processing Conference 2024"},{"id":"http://arxiv.org/abs/2402.10344v2","updated":"2024-08-01T17:34:51Z","published":"2024-02-15T22:17:17Z","title":"Evaluating Neural Radiance Fields (NeRFs) for 3D Plant Geometry\n  Reconstruction in Field Conditions","summary":"  We evaluate different Neural Radiance Fields (NeRFs) techniques for\nreconstructing (3D) plants in varied environments, from indoor settings to\noutdoor fields. Traditional techniques often struggle to capture the complex\ndetails of plants, which is crucial for botanical and agricultural\nunderstanding. We evaluate three scenarios with increasing complexity and\ncompare the results with the point cloud obtained using LiDAR as ground truth\ndata. In the most realistic field scenario, the NeRF models achieve a 74.65% F1\nscore with 30 minutes of training on the GPU, highlighting the efficiency and\naccuracy of NeRFs in challenging environments. These findings not only\ndemonstrate the potential of NeRF in detailed and realistic 3D plant modeling\nbut also suggest practical approaches for enhancing the speed and efficiency of\nthe 3D reconstruction process.\n","authors":["Muhammad Arbab Arshad","Talukder Jubery","James Afful","Anushrut Jignasu","Aditya Balu","Baskar Ganapathysubramanian","Soumik Sarkar","Adarsh Krishnamurthy"],"pdf_url":"https://arxiv.org/pdf/2402.10344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14360v2","updated":"2024-08-01T17:14:17Z","published":"2024-06-20T14:33:51Z","title":"Deblurring Neural Radiance Fields with Event-driven Bundle Adjustment","summary":"  Neural Radiance Fields (NeRF) achieves impressive 3D representation learning\nand novel view synthesis results with high-quality multi-view images as input.\nHowever, motion blur in images often occurs in low-light and high-speed motion\nscenes, which significantly degrades the reconstruction quality of NeRF.\nPrevious deblurring NeRF methods struggle to estimate pose and lighting changes\nduring the exposure time, making them unable to accurately model the motion\nblur. The bio-inspired event camera measuring intensity changes with high\ntemporal resolution makes up this information deficiency. In this paper, we\npropose Event-driven Bundle Adjustment for Deblurring Neural Radiance Fields\n(EBAD-NeRF) to jointly optimize the learnable poses and NeRF parameters by\nleveraging the hybrid event-RGB data. An intensity-change-metric event loss and\na photo-metric blur loss are introduced to strengthen the explicit modeling of\ncamera motion blur. Experiments on both synthetic and real-captured data\ndemonstrate that EBAD-NeRF can obtain accurate camera trajectory during the\nexposure time and learn a sharper 3D representations compared to prior works.\n","authors":["Yunshan Qi","Lin Zhu","Yifan Zhao","Nan Bao","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2406.14360v2.pdf","comment":"Accepted by 32nd ACM International Conference on Multimedia (MM 2024)"},{"id":"http://arxiv.org/abs/2407.20021v3","updated":"2024-08-01T16:13:45Z","published":"2024-07-29T13:57:40Z","title":"MimiQ: Low-Bit Data-Free Quantization of Vision Transformers with\n  Encouraging Inter-Head Attention Similarity","summary":"  Data-free quantization (DFQ) is a technique that creates a lightweight\nnetwork from its full-precision counterpart without the original training data,\noften through a synthetic dataset. Although several DFQ methods have been\nproposed for vision transformer (ViT) architectures, they fail to achieve\nefficacy in low-bit settings. Examining the existing methods, we identify that\ntheir synthetic data produce misaligned attention maps, while those of the real\nsamples are highly aligned. From the observation of aligned attention, we find\nthat aligning attention maps of synthetic data helps to improve the overall\nperformance of quantized ViTs. Motivated by this finding, we devise MimiQ, a\nnovel DFQ method designed for ViTs that focuses on inter-head attention\nsimilarity. First, we generate synthetic data by aligning head-wise attention\nresponses in relation to spatial query patches. Then, we apply head-wise\nstructural attention distillation to align the attention maps of the quantized\nnetwork to those of the full-precision teacher. The experimental results show\nthat the proposed method significantly outperforms baselines, setting a new\nstate-of-the-art performance for data-free ViT quantization.\n","authors":["Kanghyun Choi","Hye Yoon Lee","Dain Kwon","SunJong Park","Kyuyeun Kim","Noseong Park","Jinho Lee"],"pdf_url":"https://arxiv.org/pdf/2407.20021v3.pdf","comment":"Author Preprint"},{"id":"http://arxiv.org/abs/2307.09067v2","updated":"2024-08-01T16:09:50Z","published":"2023-07-18T08:37:58Z","title":"Evaluate Fine-tuning Strategies for Fetal Head Ultrasound Image\n  Segmentation with U-Net","summary":"  Fetal head segmentation is a crucial step in measuring the fetal head\ncircumference (HC) during gestation, an important biometric in obstetrics for\nmonitoring fetal growth. However, manual biometry generation is time-consuming\nand results in inconsistent accuracy. To address this issue, convolutional\nneural network (CNN) models have been utilized to improve the efficiency of\nmedical biometry. But training a CNN network from scratch is a challenging\ntask, we proposed a Transfer Learning (TL) method. Our approach involves\nfine-tuning (FT) a U-Net network with a lightweight MobileNet as the encoder to\nperform segmentation on a set of fetal head ultrasound (US) images with limited\neffort. This method addresses the challenges associated with training a CNN\nnetwork from scratch. It suggests that our proposed FT strategy yields\nsegmentation performance that is comparable when trained with a reduced number\nof parameters by 85.8%. And our proposed FT strategy outperforms other\nstrategies with smaller trainable parameter sizes below 4.4 million. Thus, we\ncontend that it can serve as a dependable FT approach for reducing the size of\nmodels in medical image analysis. Our key findings highlight the importance of\nthe balance between model performance and size in developing Artificial\nIntelligence (AI) applications by TL methods. Code is available at\nhttps://github.com/13204942/FT_Methods_for_Fetal_Head_Segmentation.\n","authors":["Fangyijie Wang","Guénolé Silvestre","Kathleen M. Curran"],"pdf_url":"https://arxiv.org/pdf/2307.09067v2.pdf","comment":"Irish Machine Vision and Image Processing Conference Proceedings 2023"},{"id":"http://arxiv.org/abs/2406.11551v3","updated":"2024-08-01T16:00:04Z","published":"2024-06-17T13:49:12Z","title":"Towards Self-Supervised FG-SBIR with Unified Sample Feature Alignment\n  and Multi-Scale Token Recycling","summary":"  Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) aims to minimize the\ndistance between sketches and corresponding images in the embedding space.\nHowever, scalability is hindered by the growing complexity of solutions, mainly\ndue to the abstract nature of fine-grained sketches. In this paper, we propose\nan effective approach to narrow the gap between the two domains. It mainly\nfacilitates unified mutual information sharing both intra- and inter-samples,\nrather than treating them as a single feature alignment problem between\nmodalities. Specifically, our approach includes: (i) Employing dual\nweight-sharing networks to optimize alignment within the sketch and image\ndomain, which also effectively mitigates model learning saturation issues. (ii)\nIntroducing an objective optimization function based on contrastive loss to\nenhance the model's ability to align features in both intra- and inter-samples.\n(iii) Presenting a self-supervised Multi-Scale Token Recycling (MSTR) Module\nfeatured by recycling discarded patch tokens in multi-scale features, further\nenhancing representation capability and retrieval performance. Our framework\nachieves excellent results on CNN- and ViT-based backbones. Extensive\nexperiments demonstrate its superiority over existing methods. We also\nintroduce Cloths-V1, the first professional fashion sketch-image dataset,\nutilized to validate our method and will be beneficial for other applications\n","authors":["Jianan Jiang","Hao Tang","Zhilin Jiang","Weiren Yu","Di Wu"],"pdf_url":"https://arxiv.org/pdf/2406.11551v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.14168v4","updated":"2024-08-01T15:56:43Z","published":"2024-01-25T13:27:03Z","title":"Vivim: a Video Vision Mamba for Medical Video Segmentation","summary":"  Medical video segmentation gains increasing attention in clinical practice\ndue to the redundant dynamic references in video frames. However, traditional\nconvolutional neural networks have a limited receptive field and\ntransformer-based networks are mediocre in constructing long-term dependency\nfrom the perspective of computational complexity. This bottleneck poses a\nsignificant challenge when processing longer sequences in medical video\nanalysis tasks using available devices with limited memory. Recently, state\nspace models (SSMs), famous by Mamba, have exhibited impressive achievements in\nefficient long sequence modeling, which develops deep neural networks by\nexpanding the receptive field on many vision tasks significantly.\nUnfortunately, vanilla SSMs failed to simultaneously capture causal temporal\ncues and preserve non-casual spatial information. To this end, this paper\npresents a Video Vision Mamba-based framework, dubbed as Vivim, for medical\nvideo segmentation tasks. Our Vivim can effectively compress the long-term\nspatiotemporal representation into sequences at varying scales with our\ndesigned Temporal Mamba Block. We also introduce an improved boundary-aware\naffine constraint across frames to enhance the discriminative ability of Vivim\non ambiguous lesions. Extensive experiments on thyroid segmentation, breast\nlesion segmentation in ultrasound videos, and polyp segmentation in colonoscopy\nvideos demonstrate the effectiveness and efficiency of our Vivim, superior to\nexisting methods. The code is available at:\nhttps://github.com/scott-yjyang/Vivim. The dataset will be released once\naccepted.\n","authors":["Yijun Yang","Zhaohu Xing","Lequan Yu","Chunwang Huang","Huazhu Fu","Lei Zhu"],"pdf_url":"https://arxiv.org/pdf/2401.14168v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.13312v2","updated":"2024-08-01T15:54:29Z","published":"2023-04-26T06:33:31Z","title":"Technical Note: Defining and Quantifying AND-OR Interactions for\n  Faithful and Concise Explanation of DNNs","summary":"  In this technical note, we aim to explain a deep neural network (DNN) by\nquantifying the encoded interactions between input variables, which reflects\nthe DNN's inference logic. Specifically, we first rethink the definition of\ninteractions, and then formally define faithfulness and conciseness for\ninteraction-based explanation. To this end, we propose two kinds of\ninteractions, i.e., the AND interaction and the OR interaction. For\nfaithfulness, we prove the uniqueness of the AND (OR) interaction in\nquantifying the effect of the AND (OR) relationship between input variables.\nBesides, based on AND-OR interactions, we design techniques to boost the\nconciseness of the explanation, while not hurting the faithfulness. In this\nway, the inference logic of a DNN can be faithfully and concisely explained by\na set of symbolic concepts.\n","authors":["Mingjie Li","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.13312v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2111.06206"},{"id":"http://arxiv.org/abs/2401.14832v3","updated":"2024-08-01T15:46:43Z","published":"2024-01-26T13:01:28Z","title":"Text Image Inpainting via Global Structure-Guided Diffusion Models","summary":"  Real-world text can be damaged by corrosion issues caused by environmental or\nhuman factors, which hinder the preservation of the complete styles of texts,\ne.g., texture and structure. These corrosion issues, such as graffiti signs and\nincomplete signatures, bring difficulties in understanding the texts, thereby\nposing significant challenges to downstream applications, e.g., scene text\nrecognition and signature identification. Notably, current inpainting\ntechniques often fail to adequately address this problem and have difficulties\nrestoring accurate text images along with reasonable and consistent styles.\nFormulating this as an open problem of text image inpainting, this paper aims\nto build a benchmark to facilitate its study. In doing so, we establish two\nspecific text inpainting datasets which contain scene text images and\nhandwritten text images, respectively. Each of them includes images revamped by\nreal-life and synthetic datasets, featuring pairs of original images, corrupted\nimages, and other assistant information. On top of the datasets, we further\ndevelop a novel neural framework, Global Structure-guided Diffusion Model\n(GSDM), as a potential solution. Leveraging the global structure of the text as\na prior, the proposed GSDM develops an efficient diffusion model to recover\nclean texts. The efficacy of our approach is demonstrated by thorough empirical\nstudy, including a substantial boost in both recognition accuracy and image\nquality. These findings not only highlight the effectiveness of our method but\nalso underscore its potential to enhance the broader field of text image\nunderstanding and processing. Code and datasets are available at:\nhttps://github.com/blackprotoss/GSDM.\n","authors":["Shipeng Zhu","Pengfei Fang","Chenjie Zhu","Zuoyan Zhao","Qiang Xu","Hui Xue"],"pdf_url":"https://arxiv.org/pdf/2401.14832v3.pdf","comment":"Accepted by AAAI-24"},{"id":"http://arxiv.org/abs/2311.00048v2","updated":"2024-08-01T15:37:52Z","published":"2023-10-31T18:01:41Z","title":"SC-MIL: Sparsely Coded Multiple Instance Learning for Whole Slide Image\n  Classification","summary":"  Multiple Instance Learning (MIL) has been widely used in weakly supervised\nwhole slide image (WSI) classification. Typical MIL methods include a feature\nembedding part, which embeds the instances into features via a pre-trained\nfeature extractor, and an MIL aggregator that combines instance embeddings into\npredictions. Most efforts have typically focused on improving these parts. This\ninvolves refining the feature embeddings through self-supervised pre-training\nas well as modeling the correlations between instances separately.\n  In this paper, we proposed a sparsely coding MIL (SC-MIL) method that\naddresses those two aspects at the same time by leveraging sparse dictionary\nlearning. The sparse dictionary learning captures the similarities of instances\nby expressing them as sparse linear combinations of atoms in an over-complete\ndictionary. In addition, imposing sparsity improves instance feature embeddings\nby suppressing irrelevant instances while retaining the most relevant ones. To\nmake the conventional sparse coding algorithm compatible with deep learning, we\nunrolled it into a sparsely coded module leveraging deep unrolling. The\nproposed SC module can be incorporated into any existing MIL framework in a\nplug-and-play manner with an acceptable computational cost. The experimental\nresults on multiple datasets demonstrated that the proposed SC module could\nsubstantially boost the performance of state-of-the-art MIL methods. The codes\nare available at\n\\href{https://github.com/sotiraslab/SCMIL.git}{https://github.com/sotiraslab/SCMIL.git}.\n","authors":["Peijie Qiu","Pan Xiao","Wenhui Zhu","Yalin Wang","Aristeidis Sotiras"],"pdf_url":"https://arxiv.org/pdf/2311.00048v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02111v3","updated":"2024-08-01T14:01:56Z","published":"2023-12-04T18:43:45Z","title":"TriDeNT: Triple Deep Network Training for Privileged Knowledge\n  Distillation in Histopathology","summary":"  Computational pathology models rarely utilise data that will not be available\nfor inference. This means most models cannot learn from highly informative data\nsuch as additional immunohistochemical (IHC) stains and spatial\ntranscriptomics. We present TriDeNT, a novel self-supervised method for\nutilising privileged data that is not available during inference to improve\nperformance. We demonstrate the efficacy of this method for a range of\ndifferent paired data including immunohistochemistry, spatial transcriptomics\nand expert nuclei annotations. In all settings, TriDeNT outperforms other\nstate-of-the-art methods in downstream tasks, with observed improvements of up\nto 101%. Furthermore, we provide qualitative and quantitative measurements of\nthe features learned by these models and how they differ from baselines.\nTriDeNT offers a novel method to distil knowledge from scarce or costly data\nduring training, to create significantly better models for routine inputs.\n","authors":["Lucas Farndale","Robert Insall","Ke Yuan"],"pdf_url":"https://arxiv.org/pdf/2312.02111v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.12715v2","updated":"2024-08-01T13:53:43Z","published":"2022-09-26T14:11:05Z","title":"Enhancing convolutional neural network generalizability via low-rank\n  weight approximation","summary":"  Noise is ubiquitous during image acquisition. Sufficient denoising is often\nan important first step for image processing. In recent decades, deep neural\nnetworks (DNNs) have been widely used for image denoising. Most DNN-based image\ndenoising methods require a large-scale dataset or focus on supervised\nsettings, in which single/pairs of clean images or a set of noisy images are\nrequired. This poses a significant burden on the image acquisition process.\nMoreover, denoisers trained on datasets of limited scale may incur\nover-fitting. To mitigate these issues, we introduce a new self-supervised\nframework for image denoising based on the Tucker low-rank tensor\napproximation. With the proposed design, we are able to characterize our\ndenoiser with fewer parameters and train it based on a single image, which\nconsiderably improves the model's generalizability and reduces the cost of data\nacquisition. Extensive experiments on both synthetic and real-world noisy\nimages have been conducted. Empirical results show that our proposed method\noutperforms existing non-learning-based methods (e.g., low-pass filter,\nnon-local mean), single-image unsupervised denoisers (e.g., DIP, NN+BM3D)\nevaluated on both in-sample and out-sample datasets. The proposed method even\nachieves comparable performances with some supervised methods (e.g., DnCNN).\n","authors":["Chenyin Gao","Shu Yang","Anru R. Zhang"],"pdf_url":"https://arxiv.org/pdf/2209.12715v2.pdf","comment":"accepted by IET Image Processing"},{"id":"http://arxiv.org/abs/2407.12927v2","updated":"2024-08-01T13:51:26Z","published":"2024-07-17T18:01:25Z","title":"Textualized and Feature-based Models for Compound Multimodal Emotion\n  Recognition in the Wild","summary":"  Systems for multimodal emotion recognition (ER) are commonly trained to\nextract features from different modalities (e.g., visual, audio, and textual)\nthat are combined to predict individual basic emotions. However, compound\nemotions often occur in real-world scenarios, and the uncertainty of\nrecognizing such complex emotions over diverse modalities is challenging for\nfeature-based models As an alternative, emerging multimodal large language\nmodels (LLMs) like BERT and LLaMA rely on explicit non-verbal cues that may be\ntranslated from different non-textual modalities (e.g., audio and visual) into\ntext. Textualization of modalities augments data with emotional cues to help\nthe LLM encode the interconnections between all modalities in a shared text\nspace. In such text-based models, prior knowledge of ER tasks is leveraged to\ntextualize relevant nonverbal cues such as audio tone from vocal expressions,\nand action unit intensity from facial expressions. Since the pre-trained\nweights are publicly available for many LLMs, training on large-scale datasets\nis unnecessary, allowing fine-tuning for downstream tasks such as compound ER\n(CER). This paper compares the potential of text- and feature-based approaches\nfor compound multimodal ER in videos. Experiments were conducted on the\nchallenging C-EXPR-DB dataset in the wild for CER, and contrasted with results\non the MELD dataset for basic ER. Our results indicate that multimodal\ntextualization provides lower accuracy than feature-based models on C-EXPR-DB,\nwhere text transcripts are captured in the wild. However, higher accuracy can\nbe achieved when the video data has rich transcripts. Our code is available.\n","authors":["Nicolas Richet","Soufiane Belharbi","Haseeb Aslam","Meike Emilie Schadt","Manuela González-González","Gustave Cortal","Alessandro Lameiras Koerich","Marco Pedersoli","Alain Finkel","Simon Bacon","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/2407.12927v2.pdf","comment":"13 pages, 3 figures"},{"id":"http://arxiv.org/abs/2407.11356v2","updated":"2024-08-01T12:58:29Z","published":"2024-07-16T03:41:48Z","title":"The Devil is in the Statistics: Mitigating and Exploiting Statistics\n  Difference for Generalizable Semi-supervised Medical Image Segmentation","summary":"  Despite the recent success of domain generalization in medical image\nsegmentation, voxel-wise annotation for all source domains remains a huge\nburden. Semi-supervised domain generalization has been proposed very recently\nto combat this challenge by leveraging limited labeled data along with abundant\nunlabeled data collected from multiple medical institutions, depending on\nprecisely harnessing unlabeled data while improving generalization\nsimultaneously. In this work, we observe that domain shifts between medical\ninstitutions cause disparate feature statistics, which significantly\ndeteriorates pseudo-label quality due to an unexpected normalization process.\nNevertheless, this phenomenon could be exploited to facilitate unseen domain\ngeneralization. Therefore, we propose 1) multiple statistics-individual\nbranches to mitigate the impact of domain shifts for reliable pseudo-labels and\n2) one statistics-aggregated branch for domain-invariant feature learning.\nFurthermore, to simulate unseen domains with statistics difference, we approach\nthis from two aspects, i.e., a perturbation with histogram matching at image\nlevel and a random batch normalization selection strategy at feature level,\nproducing diverse statistics to expand the training distribution. Evaluation\nresults on three medical image datasets demonstrate the effectiveness of our\nmethod compared with recent SOTA methods. The code is available at\nhttps://github.com/qiumuyang/SIAB.\n","authors":["Muyang Qiu","Jian Zhang","Lei Qi","Qian Yu","Yinghuan Shi","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2407.11356v2.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2404.04526v2","updated":"2024-08-01T11:17:28Z","published":"2024-04-06T06:48:16Z","title":"DATENeRF: Depth-Aware Text-based Editing of NeRFs","summary":"  Recent advancements in diffusion models have shown remarkable proficiency in\nediting 2D images based on text prompts. However, extending these techniques to\nedit scenes in Neural Radiance Fields (NeRF) is complex, as editing individual\n2D frames can result in inconsistencies across multiple views. Our crucial\ninsight is that a NeRF scene's geometry can serve as a bridge to integrate\nthese 2D edits. Utilizing this geometry, we employ a depth-conditioned\nControlNet to enhance the coherence of each 2D image modification. Moreover, we\nintroduce an inpainting approach that leverages the depth information of NeRF\nscenes to distribute 2D edits across different images, ensuring robustness\nagainst errors and resampling challenges. Our results reveal that this\nmethodology achieves more consistent, lifelike, and detailed edits than\nexisting leading methods for text-driven NeRF scene editing.\n","authors":["Sara Rojas","Julien Philip","Kai Zhang","Sai Bi","Fujun Luan","Bernard Ghanem","Kalyan Sunkavall"],"pdf_url":"https://arxiv.org/pdf/2404.04526v2.pdf","comment":"3D Scene Editing, Neural Rendering, Diffusion Models, Accepted to\n  ECCV24"},{"id":"http://arxiv.org/abs/2404.10572v2","updated":"2024-08-01T10:34:47Z","published":"2024-04-16T13:47:27Z","title":"Label merge-and-split: A graph-colouring approach for memory-efficient\n  brain parcellation","summary":"  Whole brain parcellation requires inferring hundreds of segmentation labels\nin large image volumes and thus presents significant practical challenges for\ndeep learning approaches. We introduce label merge-and-split, a method that\nfirst greatly reduces the effective number of labels required for\nlearning-based whole brain parcellation and then recovers original labels.\nUsing a greedy graph colouring algorithm, our method automatically groups and\nmerges multiple spatially separate labels prior to model training and\ninference. The merged labels may be semantically unrelated. A deep learning\nmodel is trained to predict merged labels. At inference time, original labels\nare restored using atlas-based influence regions. In our experiments, the\nproposed approach reduces the number of labels by up to 68% while achieving\nsegmentation accuracy comparable to the baseline method without label merging\nand splitting. Moreover, model training and inference times as well as GPU\nmemory requirements were reduced significantly. The proposed method can be\napplied to all semantic segmentation tasks with a large number of spatially\nseparate classes within an atlas-based prior.\n","authors":["Aaron Kujawa","Reuben Dorent","Sebastien Ourselin","Tom Vercauteren"],"pdf_url":"https://arxiv.org/pdf/2404.10572v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07558v2","updated":"2024-08-01T09:57:28Z","published":"2024-04-21T04:37:24Z","title":"An AI-Enabled Framework Within Reach for Enhancing Healthcare\n  Sustainability and Fairness","summary":"  Good health and well-being is among key issues in the United Nations 2030\nSustainable Development Goals. The rising prevalence of large-scale infectious\ndiseases and the accelerated aging of the global population are driving the\ntransformation of healthcare technologies. In this context, establishing\nlarge-scale public health datasets, developing medical models, and creating\ndecision-making systems with a human-centric approach are of strategic\nsignificance. Recently, by leveraging the extraordinary number of accessible\ncameras, groundbreaking advancements have emerged in AI methods for\nphysiological signal monitoring and disease diagnosis using camera sensors.\nThese approaches, requiring no specialized medical equipment, offer convenient\nmanners of collecting large-scale medical data in response to public health\nevents. Therefore, we outline a prospective framework and heuristic vision for\na camera-based public health (CBPH) framework utilizing visual physiological\nmonitoring technology. The CBPH can be considered as a convenient and universal\nframework for public health, advancing the United Nations Sustainable\nDevelopment Goals, particularly in promoting the universality, sustainability,\nand equity of healthcare in low- and middle-income countries or regions.\nFurthermore, CBPH provides a comprehensive solution for building a large-scale\nand human-centric medical database, and a multi-task large medical model for\npublic health and medical scientific discoveries. It has a significant\npotential to revolutionize personal monitoring technologies, digital medicine,\ntelemedicine, and primary health care in public health. Therefore, it can be\ndeemed that the outcomes of this paper will contribute to the establishment of\na sustainable and fair framework for public health, which serves as a crucial\nbridge for advancing scientific discoveries in the realm of AI for medicine\n(AI4Medicine).\n","authors":["Bin Huang","Changchen Zhao","Zimeng Liu","Shenda Hong","Baochang Zhang","Hao Lu","Zhijun Liu","Wenjin Wang","Hui Liu"],"pdf_url":"https://arxiv.org/pdf/2406.07558v2.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.20660v2","updated":"2024-08-01T09:56:15Z","published":"2024-07-30T08:52:51Z","title":"What makes for good morphology representations for spatial omics?","summary":"  Spatial omics has transformed our understanding of tissue architecture by\npreserving spatial context of gene expression patterns. Simultaneously,\nadvances in imaging AI have enabled extraction of morphological features\ndescribing the tissue. The intersection of spatial omics and imaging AI\npresents opportunities for a more holistic understanding. In this review we\nintroduce a framework for categorizing spatial omics-morphology combination\nmethods, focusing on how morphological features can be translated or integrated\ninto spatial omics analyses. By translation we mean finding morphological\nfeatures that spatially correlate with gene expression patterns with the\npurpose of predicting gene expression. Such features can be used to generate\nsuper-resolution gene expression maps or infer genetic information from\nclinical H&E-stained samples. By integration we mean finding morphological\nfeatures that spatially complement gene expression patterns with the purpose of\nenriching information. Such features can be used to define spatial domains,\nespecially where gene expression has preceded morphological changes and where\nmorphology remains after gene expression. We discuss learning strategies and\ndirections for further development of the field.\n","authors":["Eduard Chelebian","Christophe Avenel","Carolina Wählby"],"pdf_url":"https://arxiv.org/pdf/2407.20660v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18401v3","updated":"2024-08-01T09:04:39Z","published":"2024-04-29T03:36:05Z","title":"Spectral-Spatial Mamba for Hyperspectral Image Classification","summary":"  Recently, deep learning models have achieved excellent performance in\nhyperspectral image (HSI) classification. Among the many deep models,\nTransformer has gradually attracted interest for its excellence in modeling the\nlong-range dependencies of spatial-spectral features in HSI. However,\nTransformer has the problem of quadratic computational complexity due to the\nself-attention mechanism, which is heavier than other models and thus has\nlimited adoption in HSI processing. Fortunately, the recently emerging state\nspace model-based Mamba shows great computational efficiency while achieving\nthe modeling power of Transformers. Therefore, in this paper, we make a\npreliminary attempt to apply the Mamba to HSI classification, leading to the\nproposed spectral-spatial Mamba (SS-Mamba). Specifically, the proposed SS-Mamba\nmainly consists of spectral-spatial token generation module and several stacked\nspectral-spatial Mamba blocks. Firstly, the token generation module converts\nany given HSI cube to spatial and spectral tokens as sequences. And then these\ntokens are sent to stacked spectral-spatial mamba blocks (SS-MB). Each SS-MB\nblock consists of two basic mamba blocks and a spectral-spatial feature\nenhancement module. The spatial and spectral tokens are processed separately by\nthe two basic mamba blocks, respectively. Besides, the feature enhancement\nmodule modulates spatial and spectral tokens using HSI sample's center region\ninformation. In this way, the spectral and spatial tokens cooperate with each\nother and achieve information fusion within each block. The experimental\nresults conducted on widely used HSI datasets reveal that the proposed model\nachieves competitive results compared with the state-of-the-art methods. The\nMamba-based method opens a new window for HSI classification.\n","authors":["Lingbo Huang","Yushi Chen","Xin He"],"pdf_url":"https://arxiv.org/pdf/2404.18401v3.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2405.01409v3","updated":"2024-08-01T08:58:40Z","published":"2024-05-02T16:01:58Z","title":"Goal-conditioned reinforcement learning for ultrasound navigation\n  guidance","summary":"  Transesophageal echocardiography (TEE) plays a pivotal role in cardiology for\ndiagnostic and interventional procedures. However, using it effectively\nrequires extensive training due to the intricate nature of image acquisition\nand interpretation. To enhance the efficiency of novice sonographers and reduce\nvariability in scan acquisitions, we propose a novel ultrasound (US) navigation\nassistance method based on contrastive learning as goal-conditioned\nreinforcement learning (GCRL). We augment the previous framework using a novel\ncontrastive patient batching method (CPB) and a data-augmented contrastive\nloss, both of which we demonstrate are essential to ensure generalization to\nanatomical variations across patients. The proposed framework enables\nnavigation to both standard diagnostic as well as intricate interventional\nviews with a single model. Our method was developed with a large dataset of 789\npatients and obtained an average error of 6.56 mm in position and 9.36 degrees\nin angle on a testing dataset of 140 patients, which is competitive or superior\nto models trained on individual views. Furthermore, we quantitatively validate\nour method's ability to navigate to interventional views such as the Left\nAtrial Appendage (LAA) view used in LAA closure. Our approach holds promise in\nproviding valuable guidance during transesophageal ultrasound examinations,\ncontributing to the advancement of skill acquisition for cardiac ultrasound\npractitioners.\n","authors":["Abdoul Aziz Amadou","Vivek Singh","Florin C. Ghesu","Young-Ho Kim","Laura Stanciulescu","Harshitha P. Sai","Puneet Sharma","Alistair Young","Ronak Rajani","Kawal Rhode"],"pdf_url":"https://arxiv.org/pdf/2405.01409v3.pdf","comment":"Accepted in MICCAI 2024; 11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.04664v2","updated":"2024-08-01T08:39:23Z","published":"2023-03-08T15:34:57Z","title":"Centroid-centered Modeling for Efficient Vision Transformer Pre-training","summary":"  Masked Image Modeling (MIM) is a new self-supervised vision pre-training\nparadigm using a Vision Transformer (ViT). Previous works can be pixel-based or\ntoken-based, using original pixels or discrete visual tokens from parametric\ntokenizer models, respectively. Our proposed centroid-based approach, CCViT,\nleverages k-means clustering to obtain centroids for image modeling without\nsupervised training of the tokenizer model, which only takes seconds to create.\nThis non-parametric centroid tokenizer only takes seconds to create and is\nfaster for token inference. The centroids can represent both patch pixels and\nindex tokens with the property of local invariance. Specifically, we adopt\npatch masking and centroid replacing strategies to construct corrupted inputs,\nand two stacked encoder blocks to predict corrupted patch tokens and\nreconstruct original patch pixels. Experiments show that our CCViT achieves\n84.4% top-1 accuracy on ImageNet-1K classification with ViT-B and 86.0% with\nViT-L. We also transfer our pre-trained model to other downstream tasks. Our\napproach achieves competitive results with recent baselines without external\nsupervision and distillation training from other models.\n","authors":["Xin Yan","Zuchao Li","Lefei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.04664v2.pdf","comment":"Codes are available at https://github.com/Cakeyan/CCViT_Public"},{"id":"http://arxiv.org/abs/2407.03104v2","updated":"2024-08-01T08:08:43Z","published":"2024-07-03T13:41:44Z","title":"KeyVideoLLM: Towards Large-scale Video Keyframe Selection","summary":"  Recently, with the rise of web videos, managing and understanding large-scale\nvideo datasets has become increasingly important. Video Large Language Models\n(VideoLLMs) have emerged in recent years due to their strong video\nunderstanding capabilities. However, training and inference processes for\nVideoLLMs demand vast amounts of data, presenting significant challenges to\ndata management, particularly regarding efficiency, robustness, and\neffectiveness. In this work, we present KeyVideoLLM, a text-video frame\nsimilarity-based keyframe selection method designed to manage VideoLLM data\nefficiently, robustly, and effectively. Specifically, KeyVideoLLM achieves a\nremarkable data compression rate of up to 60.9 times, substantially lowering\ndisk space requirements, which proves its high efficiency. Additionally, it\nmaintains a 100% selection success rate across all video formats and scales,\nenhances processing speed by up to 200 times compared to existing keyframe\nselection methods, and does not require hyperparameter tuning. Beyond its\noutstanding efficiency and robustness, KeyVideoLLM further improves model\nperformance in video question-answering tasks during both training and\ninference stages. Notably, it consistently achieved the state-of-the-art (SoTA)\nexperimental results on diverse datasets.\n","authors":["Hao Liang","Jiapeng Li","Tianyi Bai","Xijie Huang","Linzhuang Sun","Zhengren Wang","Conghui He","Bin Cui","Chong Chen","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.03104v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08604v2","updated":"2024-08-01T07:55:49Z","published":"2024-06-12T19:17:17Z","title":"GRU-Net: Gaussian Attention Aided Dense Skip Connection Based\n  MultiResUNet for Breast Histopathology Image Segmentation","summary":"  Breast cancer is a major global health concern. Pathologists face challenges\nin analyzing complex features from pathological images, which is a\ntime-consuming and labor-intensive task. Therefore, efficient computer-based\ndiagnostic tools are needed for early detection and treatment planning. This\npaper presents a modified version of MultiResU-Net for histopathology image\nsegmentation, which is selected as the backbone for its ability to analyze and\nsegment complex features at multiple scales and ensure effective feature flow\nvia skip connections. The modified version also utilizes the Gaussian\ndistribution-based Attention Module (GdAM) to incorporate\nhistopathology-relevant text information in a Gaussian distribution. The\nsampled features from the Gaussian text feature-guided distribution highlight\nspecific spatial regions based on prior knowledge. Finally, using the\nControlled Dense Residual Block (CDRB) on skip connections of MultiResU-Net,\nthe information is transferred from the encoder layers to the decoder layers in\na controlled manner using a scaling parameter derived from the extracted\nspatial features. We validate our approach on two diverse breast cancer\nhistopathology image datasets: TNBC and MonuSeg, demonstrating superior\nsegmentation performance compared to state-of-the-art methods. The code for our\nproposed model is available on https://github.com/AyushRoy2001/GRU-Net.\n","authors":["Ayush Roy","Payel Pramanik","Sohom Ghosal","Daria Valenkova","Dmitrii Kaplun","Ram Sarkar"],"pdf_url":"https://arxiv.org/pdf/2406.08604v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20843v2","updated":"2024-08-01T07:43:11Z","published":"2024-07-30T14:16:09Z","title":"DFE-IANet: A Method for Polyp Image Classification Based on Dual-domain\n  Feature Extraction and Interaction Attention","summary":"  It is helpful in preventing colorectal cancer to detect and treat polyps in\nthe gastrointestinal tract early. However, there have been few studies to date\non designing polyp image classification networks that balance efficiency and\naccuracy. This challenge is mainly attributed to the fact that polyps are\nsimilar to other pathologies and have complex features influenced by texture,\ncolor, and morphology. In this paper, we propose a novel network DFE-IANet\nbased on both spectral transformation and feature interaction. Firstly, to\nextract detailed features and multi-scale features, the features are\ntransformed by the multi-scale frequency domain feature extraction (MSFD) block\nto extract texture details at the fine-grained level in the frequency domain.\nSecondly, the multi-scale interaction attention (MSIA) block is designed to\nenhance the network's capability of extracting critical features. This block\nintroduces multi-scale features into self-attention, aiming to adaptively guide\nthe network to concentrate on vital regions. Finally, with a compact parameter\nof only 4M, DFE-IANet outperforms the latest and classical networks in terms of\nefficiency. Furthermore, DFE-IANet achieves state-of-the-art (SOTA) results on\nthe challenging Kvasir dataset, demonstrating a remarkable Top-1 accuracy of\n93.94%. This outstanding accuracy surpasses ViT by 8.94%, ResNet50 by 1.69%,\nand VMamba by 1.88%. Our code is publicly available at\nhttps://github.com/PURSUETHESUN/DFE-IANet.\n","authors":["Wei Wang","Jixing He","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2407.20843v2.pdf","comment":"This paper has been accepted by 2024 International Conference on\n  Intelligent Computing (ICIC 2024). It can be accessed at\n  http://poster-openaccess.com"},{"id":"http://arxiv.org/abs/2312.03781v4","updated":"2024-08-01T07:29:47Z","published":"2023-12-06T09:39:38Z","title":"Lite-Mind: Towards Efficient and Robust Brain Representation Network","summary":"  The limited data availability and the low signal-to-noise ratio of fMRI\nsignals lead to the challenging task of fMRI-to-image retrieval.\nState-of-the-art MindEye remarkably improves fMRI-to-image retrieval\nperformance by leveraging a large model, i.e., a 996M MLP Backbone per subject,\nto align fMRI embeddings to the final hidden layer of CLIP's Vision Transformer\n(ViT). However, significant individual variations exist among subjects, even\nunder identical experimental setups, mandating the training of large\nsubject-specific models. The substantial parameters pose significant challenges\nin deploying fMRI decoding on practical devices. To this end, we propose\nLite-Mind, a lightweight, efficient, and robust brain representation learning\nparadigm based on Discrete Fourier Transform (DFT), which efficiently aligns\nfMRI voxels to fine-grained information of CLIP. We elaborately design a DFT\nbackbone with Spectrum Compression and Frequency Projector modules to learn\ninformative and robust voxel embeddings. Our experiments demonstrate that\nLite-Mind achieves an impressive 94.6% fMRI-to-image retrieval accuracy on the\nNSD dataset for Subject 1, with 98.7% fewer parameters than MindEye. Lite-Mind\nis also proven to be able to be migrated to smaller fMRI datasets and\nestablishes a new state-of-the-art for zero-shot classification on the GOD\ndataset.\n","authors":["Zixuan Gong","Qi Zhang","Guangyin Bao","Lei Zhu","Ke Liu","Liang Hu","Duoqian Miao","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.03781v4.pdf","comment":"17 pages, ACM MM 2024 Oral"},{"id":"http://arxiv.org/abs/2404.01065v2","updated":"2024-08-01T07:01:49Z","published":"2024-04-01T11:57:40Z","title":"T-Mamba: A unified framework with Long-Range Dependency in dual-domain\n  for 2D & 3D Tooth Segmentation","summary":"  Tooth segmentation is a pivotal step in modern digital dentistry, essential\nfor applications across orthodontic diagnosis and treatment planning. Despite\nits importance, this process is fraught with challenges due to the high noise\nand low contrast inherent in 2D and 3D tooth data. Both Convolutional Neural\nNetworks (CNNs) and Transformers has shown promise in medical image\nsegmentation, yet each method has limitations in handling long-range\ndependencies and computational complexity. To address this issue, this paper\nintroduces T-Mamba, integrating frequency-based features and shared\nbi-positional encoding into vision mamba to address limitations in efficient\nglobal feature modeling. Besides, we design a gate selection unit to integrate\ntwo features in spatial domain and one feature in frequency domain adaptively.\nT-Mamba is the first work to introduce frequency-based features into vision\nmamba, and its flexibility allows it to process both 2D and 3D tooth data\nwithout the need for separate modules. Also, the TED3, a large-scale public\ntooth 2D dental X-ray dataset, has been presented in this paper. Extensive\nexperiments demonstrate that T-Mamba achieves new SOTA results on a public\ntooth CBCT dataset and outperforms previous SOTA methods on TED3 dataset. The\ncode and models are publicly available at: https://github.com/isbrycee/T-Mamba.\n","authors":["Jing Hao","Yonghui Zhu","Lei He","Moyun Liu","James Kit Hon Tsoi","Kuo Feng Hung"],"pdf_url":"https://arxiv.org/pdf/2404.01065v2.pdf","comment":"25 pages, 10 figures, 7 tables"},{"id":"http://arxiv.org/abs/2404.17100v2","updated":"2024-08-01T06:46:04Z","published":"2024-04-26T01:21:08Z","title":"Open-Set Video-based Facial Expression Recognition with Human\n  Expression-sensitive Prompting","summary":"  In Video-based Facial Expression Recognition (V-FER), models are typically\ntrained on closed-set datasets with a fixed number of known classes. However,\nthese models struggle with unknown classes common in real-world scenarios. In\nthis paper, we introduce a challenging Open-set Video-based Facial Expression\nRecognition (OV-FER) task, aiming to identify both known and new, unseen facial\nexpressions. While existing approaches use large-scale vision-language models\nlike CLIP to identify unseen classes, we argue that these methods may not\nadequately capture the subtle human expressions needed for OV-FER. To address\nthis limitation, we propose a novel Human Expression-Sensitive Prompting (HESP)\nmechanism to significantly enhance CLIP's ability to model video-based facial\nexpression details effectively. Our proposed HESP comprises three components:\n1) a textual prompting module with learnable prompts to enhance CLIP's textual\nrepresentation of both known and unknown emotions, 2) a visual prompting module\nthat encodes temporal emotional information from video frames using\nexpression-sensitive attention, equipping CLIP with a new visual modeling\nability to extract emotion-rich information, and 3) an open-set multi-task\nlearning scheme that promotes interaction between the textual and visual\nmodules, improving the understanding of novel human emotions in video\nsequences. Extensive experiments conducted on four OV-FER task settings\ndemonstrate that HESP can significantly boost CLIP's performance (a relative\nimprovement of 17.93% on AUROC and 106.18% on OSCR) and outperform other\nstate-of-the-art open-set video understanding methods by a large margin. Code\nis available at https://github.com/cosinehuang/HESP.\n","authors":["Yuanyuan Liu","Yuxuan Huang","Shuyang Liu","Yibing Zhan","Zijing Chen","Zhe Chen"],"pdf_url":"https://arxiv.org/pdf/2404.17100v2.pdf","comment":"Accepted by ACM MM2024"},{"id":"http://arxiv.org/abs/2210.03437v2","updated":"2024-08-01T06:02:27Z","published":"2022-10-07T10:13:30Z","title":"KRF: Keypoint Refinement with Fusion Network for 6D Pose Estimation","summary":"  Some robust point cloud registration approaches with controllable pose\nrefinement magnitude, such as ICP and its variants, are commonly used to\nimprove 6D pose estimation accuracy. However, the effectiveness of these\nmethods gradually diminishes with the advancement of deep learning techniques\nand the enhancement of initial pose accuracy, primarily due to their lack of\nspecific design for pose refinement. In this paper, we propose Point Cloud\nCompletion and Keypoint Refinement with Fusion Data (PCKRF), a new pose\nrefinement pipeline for 6D pose estimation. The pipeline consists of two steps.\nFirst, it completes the input point clouds via a novel pose-sensitive point\ncompletion network. The network uses both local and global features with pose\ninformation during point completion. Then, it registers the completed object\npoint cloud with the corresponding target point cloud by our proposed Color\nsupported Iterative KeyPoint (CIKP) method. The CIKP method introduces color\ninformation into registration and registers a point cloud around each keypoint\nto increase stability. The PCKRF pipeline can be integrated with existing\npopular 6D pose estimation methods, such as the full flow bidirectional fusion\nnetwork, to further improve their pose estimation accuracy. Experiments\ndemonstrate that our method exhibits superior stability compared to existing\napproaches when optimizing initial poses with relatively high precision.\nNotably, the results indicate that our method effectively complements most\nexisting pose estimation techniques, leading to improved performance in most\ncases. Furthermore, our method achieves promising results even in challenging\nscenarios involving textureless and symmetrical objects. Our source code is\navailable at https://github.com/zhanhz/KRF.\n","authors":["Yiheng Han","Irvin Haozhe Zhan","Long Zeng","Yu-Ping Wang","Ran Yi","Minjing Yu","Matthieu Gaetan Lin","Jenny Sheng","Yong-Jin Liu"],"pdf_url":"https://arxiv.org/pdf/2210.03437v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.10224v3","updated":"2024-08-01T05:18:48Z","published":"2024-01-18T18:59:09Z","title":"The Manga Whisperer: Automatically Generating Transcriptions for Comics","summary":"  In the past few decades, Japanese comics, commonly referred to as Manga, have\ntranscended both cultural and linguistic boundaries to become a true worldwide\nsensation. Yet, the inherent reliance on visual cues and illustration within\nmanga renders it largely inaccessible to individuals with visual impairments.\nIn this work, we seek to address this substantial barrier, with the aim of\nensuring that manga can be appreciated and actively engaged by everyone.\nSpecifically, we tackle the problem of diarisation i.e. generating a\ntranscription of who said what and when, in a fully automatic way.\n  To this end, we make the following contributions: (1) we present a unified\nmodel, Magi, that is able to (a) detect panels, text boxes and character boxes,\n(b) cluster characters by identity (without knowing the number of clusters\napriori), and (c) associate dialogues to their speakers; (2) we propose a novel\napproach that is able to sort the detected text boxes in their reading order\nand generate a dialogue transcript; (3) we annotate an evaluation benchmark for\nthis task using publicly available [English] manga pages. The code, evaluation\ndatasets and the pre-trained model can be found at:\nhttps://github.com/ragavsachdeva/magi.\n","authors":["Ragav Sachdeva","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2401.10224v3.pdf","comment":"Accepted at CVPR'24"},{"id":"http://arxiv.org/abs/2303.08046v2","updated":"2024-08-01T05:15:43Z","published":"2023-03-07T09:20:39Z","title":"Ultra-High-Resolution Detector Simulation with Intra-Event Aware GAN and\n  Self-Supervised Relational Reasoning","summary":"  Simulating high-resolution detector responses is a computationally intensive\nprocess that has long been challenging in Particle Physics. Despite the ability\nof generative models to streamline it, full ultra-high-granularity detector\nsimulation still proves to be difficult as it contains correlated and\nfine-grained information. To overcome these limitations, we propose Intra-Event\nAware Generative Adversarial Network (IEA-GAN). IEA-GAN presents a Relational\nReasoning Module that approximates an event in detector simulation, generating\ncontextualized high-resolution full detector responses with a proper relational\ninductive bias. IEA-GAN also introduces a Self-Supervised intra-event aware\nloss and Uniformity loss, significantly enhancing sample fidelity and\ndiversity. We demonstrate IEA-GAN's application in generating sensor-dependent\nimages for the ultra-high-granularity Pixel Vertex Detector (PXD), with more\nthan 7.5 M information channels at the Belle II Experiment. Applications of\nthis work span from Foundation Models for high-granularity detector simulation,\nsuch as at the HL-LHC (High Luminosity LHC), to simulation-based inference and\nfine-grained density estimation. To our knowledge, IEA-GAN is the first\nalgorithm for faithful ultra-high-granularity full detector simulation with\nevent-based reasoning.\n","authors":["Baran Hashemi","Nikolai Hartmann","Sahand Sharifzadeh","James Kahn","Thomas Kuhr"],"pdf_url":"https://arxiv.org/pdf/2303.08046v2.pdf","comment":"Published at Nature Communications"},{"id":"http://arxiv.org/abs/2406.13642v5","updated":"2024-08-01T04:46:58Z","published":"2024-06-19T15:41:30Z","title":"SpatialBot: Precise Spatial Understanding with Vision Language Models","summary":"  Vision Language Models (VLMs) have achieved impressive performance in 2D\nimage understanding, however they are still struggling with spatial\nunderstanding which is the foundation of Embodied AI. In this paper, we propose\nSpatialBot for better spatial understanding by feeding both RGB and depth\nimages. Additionally, we have constructed the SpatialQA dataset, which involves\nmulti-level depth-related questions to train VLMs for depth understanding.\nFinally, we present SpatialBench to comprehensively evaluate VLMs' capabilities\nin spatial understanding at different levels. Extensive experiments on our\nspatial-understanding benchmark, general VLM benchmarks and Embodied AI tasks,\ndemonstrate the remarkable improvements of SpatialBot trained on SpatialQA. The\nmodel, code and data are available at https://github.com/BAAI-DCAI/SpatialBot.\n","authors":["Wenxiao Cai","Yaroslav Ponomarenko","Jianhao Yuan","Xiaoqi Li","Wankou Yang","Hao Dong","Bo Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.13642v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19394v2","updated":"2024-08-01T04:22:29Z","published":"2024-07-28T04:23:40Z","title":"Depth-Wise Convolutions in Vision Transformers for Efficient Training on\n  Small Datasets","summary":"  The Vision Transformer (ViT) leverages the Transformer's encoder to capture\nglobal information by dividing images into patches and achieves superior\nperformance across various computer vision tasks. However, the self-attention\nmechanism of ViT captures the global context from the outset, overlooking the\ninherent relationships between neighboring pixels in images or videos.\nTransformers mainly focus on global information while ignoring the fine-grained\nlocal details. Consequently, ViT lacks inductive bias during image or video\ndataset training. In contrast, convolutional neural networks (CNNs), with their\nreliance on local filters, possess an inherent inductive bias, making them more\nefficient and quicker to converge than ViT with less data. In this paper, we\npresent a lightweight Depth-Wise Convolution module as a shortcut in ViT\nmodels, bypassing entire Transformer blocks to ensure the models capture both\nlocal and global information with minimal overhead. Additionally, we introduce\ntwo architecture variants, allowing the Depth-Wise Convolution modules to be\napplied to multiple Transformer blocks for parameter savings, and incorporating\nindependent parallel Depth-Wise Convolution modules with different kernels to\nenhance the acquisition of local information. The proposed approach\nsignificantly boosts the performance of ViT models on image classification,\nobject detection and instance segmentation by a large margin, especially on\nsmall datasets, as evaluated on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet\nfor image classification, and COCO for object detection and instance\nsegmentation. The source code can be accessed at\nhttps://github.com/ZTX-100/Efficient_ViT_with_DW.\n","authors":["Tianxiao Zhang","Wenju Xu","Bo Luo","Guanghui Wang"],"pdf_url":"https://arxiv.org/pdf/2407.19394v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20756v2","updated":"2024-08-01T04:01:39Z","published":"2024-07-30T11:57:40Z","title":"SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision\n  Language Models","summary":"  Recently, with the rise of web images, managing and understanding large-scale\nimage datasets has become increasingly important. Vision Large Language Models\n(VLLMs) have recently emerged due to their robust vision-understanding\ncapabilities. However, training these models requires vast amounts of data,\nposing challenges to efficiency, effectiveness, data quality, and privacy. In\nthis paper, we introduce SynthVLM, a novel data synthesis pipeline for VLLMs.\nUnlike existing methods that generate captions from images, SynthVLM employs\nadvanced diffusion models and high-quality captions to automatically generate\nand select high-resolution images from captions, creating precisely aligned\nimage-text pairs. Leveraging these pairs, we achieve state-of-the-art (SoTA)\nperformance on various vision question answering tasks, maintaining high\nalignment quality and preserving advanced language abilities. Moreover,\nSynthVLM surpasses traditional GPT-4 Vision-based caption generation methods in\nperformance while significantly reducing computational overhead. Crucially, our\nmethod's reliance on purely generated data ensures the preservation of privacy,\nachieving SoTA performance with just 100k data points (only 18% of the official\ndataset size).\n","authors":["Zheng Liu","Hao Liang","Xijie Huang","Wentao Xiong","Qinhan Yu","Linzhuang Sun","Chong Chen","Conghui He","Bin Cui","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.20756v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.09786v4","updated":"2024-08-01T03:57:24Z","published":"2024-01-18T08:10:34Z","title":"Adaptive Self-training Framework for Fine-grained Scene Graph Generation","summary":"  Scene graph generation (SGG) models have suffered from inherent problems\nregarding the benchmark datasets such as the long-tailed predicate distribution\nand missing annotation problems. In this work, we aim to alleviate the\nlong-tailed problem of SGG by utilizing unannotated triplets. To this end, we\nintroduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels\nfor unannotated triplets based on which the SGG models are trained. While there\nhas been significant progress in self-training for image recognition, designing\na self-training framework for the SGG task is more challenging due to its\ninherent nature such as the semantic ambiguity and the long-tailed distribution\nof predicate classes. Hence, we propose a novel pseudo-labeling technique for\nSGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is\na model-agnostic framework that can be applied to any existing SGG models.\nFurthermore, we devise a graph structure learner (GSL) that is beneficial when\nadopting our proposed self-training framework to the state-of-the-art\nmessage-passing neural network (MPNN)-based SGG models. Our extensive\nexperiments verify the effectiveness of ST-SGG on various SGG models,\nparticularly in enhancing the performance on fine-grained predicate classes.\n","authors":["Kibum Kim","Kanghoon Yoon","Yeonjun In","Jinyoung Moon","Donghyun Kim","Chanyoung Park"],"pdf_url":"https://arxiv.org/pdf/2401.09786v4.pdf","comment":"9 pages; ICLR 2024"},{"id":"http://arxiv.org/abs/2407.13211v2","updated":"2024-08-01T03:07:49Z","published":"2024-07-18T06:50:39Z","title":"Research on Image Super-Resolution Reconstruction Mechanism based on\n  Convolutional Neural Network","summary":"  Super-resolution reconstruction techniques entail the utilization of software\nalgorithms to transform one or more sets of low-resolution images captured from\nthe same scene into high-resolution images. In recent years, considerable\nadvancement has been observed in the domain of single-image super-resolution\nalgorithms, particularly those based on deep learning techniques. Nevertheless,\nthe extraction of image features and nonlinear mapping methods in the\nreconstruction process remain challenging for existing algorithms. These issues\nresult in the network architecture being unable to effectively utilize the\ndiverse range of information at different levels. The loss of high-frequency\ndetails is significant, and the final reconstructed image features are overly\nsmooth, with a lack of fine texture details. This negatively impacts the\nsubjective visual quality of the image. The objective is to recover\nhigh-quality, high-resolution images from low-resolution images. In this work,\nan enhanced deep convolutional neural network model is employed, comprising\nmultiple convolutional layers, each of which is configured with specific\nfilters and activation functions to effectively capture the diverse features of\nthe image. Furthermore, a residual learning strategy is employed to accelerate\ntraining and enhance the convergence of the network, while sub-pixel\nconvolutional layers are utilized to refine the high-frequency details and\ntextures of the image. The experimental analysis demonstrates the superior\nperformance of the proposed model on multiple public datasets when compared\nwith the traditional bicubic interpolation method and several other\nlearning-based super-resolution methods. Furthermore, it proves the model's\nefficacy in maintaining image edges and textures.\n","authors":["Hao Yan","Zixiang Wang","Zhengjia Xu","Zhuoyue Wang","Zhizhong Wu","Ranran Lyu"],"pdf_url":"https://arxiv.org/pdf/2407.13211v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11816v2","updated":"2024-08-01T02:49:00Z","published":"2023-12-19T03:15:50Z","title":"A Dual-way Enhanced Framework from Text Matching Point of View for\n  Multimodal Entity Linking","summary":"  Multimodal Entity Linking (MEL) aims at linking ambiguous mentions with\nmultimodal information to entity in Knowledge Graph (KG) such as Wikipedia,\nwhich plays a key role in many applications. However, existing methods suffer\nfrom shortcomings, including modality impurity such as noise in raw image and\nambiguous textual entity representation, which puts obstacles to MEL. We\nformulate multimodal entity linking as a neural text matching problem where\neach multimodal information (text and image) is treated as a query, and the\nmodel learns the mapping from each query to the relevant entity from candidate\nentities. This paper introduces a dual-way enhanced (DWE) framework for MEL:\n(1) our model refines queries with multimodal data and addresses semantic gaps\nusing cross-modal enhancers between text and image information. Besides, DWE\ninnovatively leverages fine-grained image attributes, including facial\ncharacteristic and scene feature, to enhance and refine visual features. (2)By\nusing Wikipedia descriptions, DWE enriches entity semantics and obtains more\ncomprehensive textual representation, which reduces between textual\nrepresentation and the entities in KG. Extensive experiments on three public\nbenchmarks demonstrate that our method achieves state-of-the-art (SOTA)\nperformance, indicating the superiority of our model. The code is released on\nhttps://github.com/season1blue/DWE\n","authors":["Shezheng Song","Shan Zhao","Chengyu Wang","Tianwei Yan","Shasha Li","Xiaoguang Mao","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2312.11816v2.pdf","comment":"AAAI23 Accept"},{"id":"http://arxiv.org/abs/2407.14055v2","updated":"2024-08-01T02:19:08Z","published":"2024-07-19T06:31:22Z","title":"Quantum Hamiltonian Embedding of Images for Data Reuploading Classifiers","summary":"  When applying quantum computing to machine learning tasks, one of the first\nconsiderations is the design of the quantum machine learning model itself.\nConventionally, the design of quantum machine learning algorithms relies on the\n``quantisation\" of classical learning algorithms, such as using quantum linear\nalgebra to implement important subroutines of classical algorithms, if not the\nentire algorithm, seeking to achieve quantum advantage through possible\nrun-time accelerations brought by quantum computing. However, recent research\nhas started questioning whether quantum advantage via speedup is the right goal\nfor quantum machine learning [1]. Research also has been undertaken to exploit\nproperties that are unique to quantum systems, such as quantum contextuality,\nto better design quantum machine learning models [2]. In this paper, we take an\nalternative approach by incorporating the heuristics and empirical evidences\nfrom the design of classical deep learning algorithms to the design of quantum\nneural networks. We first construct a model based on the data reuploading\ncircuit [3] with the quantum Hamiltonian data embedding unitary [4]. Through\nnumerical experiments on images datasets, including the famous MNIST and\nFashionMNIST datasets, we demonstrate that our model outperforms the quantum\nconvolutional neural network (QCNN)[5] by a large margin (up to over 40% on\nMNIST test set). Based on the model design process and numerical results, we\nthen laid out six principles for designing quantum machine learning models,\nespecially quantum neural networks.\n","authors":["Peiyong Wang","Casey R. Myers","Lloyd C. L. Hollenberg","Udaya Parampalli"],"pdf_url":"https://arxiv.org/pdf/2407.14055v2.pdf","comment":"11 figures, 31 pages. Code available on\n  https://github.com/peiyong-addwater/HamEmbedding. Author affiliation updated\n  for v2. Acknowledgements and funding information added for v2"},{"id":"http://arxiv.org/abs/2407.21266v2","updated":"2024-08-01T01:59:58Z","published":"2024-07-31T01:07:21Z","title":"DDU-Net: A Domain Decomposition-based CNN for High-Resolution Image\n  Segmentation on Multiple GPUs","summary":"  The segmentation of ultra-high resolution images poses challenges such as\nloss of spatial information or computational inefficiency. In this work, a\nnovel approach that combines encoder-decoder architectures with domain\ndecomposition strategies to address these challenges is proposed. Specifically,\na domain decomposition-based U-Net (DDU-Net) architecture is introduced, which\npartitions input images into non-overlapping patches that can be processed\nindependently on separate devices. A communication network is added to\nfacilitate inter-patch information exchange to enhance the understanding of\nspatial context. Experimental validation is performed on a synthetic dataset\nthat is designed to measure the effectiveness of the communication network.\nThen, the performance is tested on the DeepGlobe land cover classification\ndataset as a real-world benchmark data set. The results demonstrate that the\napproach, which includes inter-patch communication for images divided into\n$16\\times16$ non-overlapping subimages, achieves a $2-3\\,\\%$ higher\nintersection over union (IoU) score compared to the same network without\ninter-patch communication. The performance of the network which includes\ncommunication is equivalent to that of a baseline U-Net trained on the full\nimage, showing that our model provides an effective solution for segmenting\nultra-high-resolution images while preserving spatial context. The code is\navailable at https://github.com/corne00/HiRes-Seg-CNN.\n","authors":["Corné Verburg","Alexander Heinlein","Eric C. Cyr"],"pdf_url":"https://arxiv.org/pdf/2407.21266v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21263v2","updated":"2024-08-01T01:59:39Z","published":"2024-07-31T00:56:06Z","title":"Outlier Detection in Large Radiological Datasets using UMAP","summary":"  The success of machine learning algorithms heavily relies on the quality of\nsamples and the accuracy of their corresponding labels. However, building and\nmaintaining large, high-quality datasets is an enormous task. This is\nespecially true for biomedical data and for meta-sets that are compiled from\nsmaller ones, as variations in image quality, labeling, reports, and archiving\ncan lead to errors, inconsistencies, and repeated samples. Here, we show that\nthe uniform manifold approximation and projection (UMAP) algorithm can find\nthese anomalies essentially by forming independent clusters that are distinct\nfrom the main (good) data but similar to other points with the same error type.\nAs a representative example, we apply UMAP to discover outliers in the publicly\navailable ChestX-ray14, CheXpert, and MURA datasets. While the results are\narchival and retrospective and focus on radiological images, the graph-based\nmethods work for any data type and will prove equally beneficial for curation\nat the time of dataset creation.\n","authors":["Mohammad Tariqul Islam","Jason W. Fleischer"],"pdf_url":"https://arxiv.org/pdf/2407.21263v2.pdf","comment":"Accepted in MICCAI-2024 Workshop on Topology- and Graph-Informed\n  Imaging Informatics (TGI3)"},{"id":"http://arxiv.org/abs/2408.00766v1","updated":"2024-08-01T17:59:59Z","published":"2024-08-01T17:59:59Z","title":"Optimizing Diffusion Models for Joint Trajectory Prediction and\n  Controllable Generation","summary":"  Diffusion models are promising for joint trajectory prediction and\ncontrollable generation in autonomous driving, but they face challenges of\ninefficient inference steps and high computational demands. To tackle these\nchallenges, we introduce Optimal Gaussian Diffusion (OGD) and Estimated Clean\nManifold (ECM) Guidance. OGD optimizes the prior distribution for a small\ndiffusion time $T$ and starts the reverse diffusion process from it. ECM\ndirectly injects guidance gradients to the estimated clean manifold,\neliminating extensive gradient backpropagation throughout the network. Our\nmethodology streamlines the generative process, enabling practical applications\nwith reduced computational overhead. Experimental validation on the large-scale\nArgoverse 2 dataset demonstrates our approach's superior performance, offering\na viable solution for computationally efficient, high-quality joint trajectory\nprediction and controllable generation for autonomous driving. Our project\nwebpage is at https://yixiaowang7.github.io/OptTrajDiff_Page/.\n","authors":["Yixiao Wang","Chen Tang","Lingfeng Sun","Simone Rossi","Yichen Xie","Chensheng Peng","Thomas Hannagan","Stefano Sabatini","Nicola Poerio","Masayoshi Tomizuka","Wei Zhan"],"pdf_url":"https://arxiv.org/pdf/2408.00766v1.pdf","comment":"30 pages, 20 figures, Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2408.00765v1","updated":"2024-08-01T17:59:54Z","published":"2024-08-01T17:59:54Z","title":"MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models\n  for Integrated Capabilities","summary":"  MM-Vet, with open-ended vision-language questions targeting at evaluating\nintegrated capabilities, has become one of the most popular benchmarks for\nlarge multimodal model evaluation. MM-Vet assesses six core vision-language\n(VL) capabilities: recognition, knowledge, spatial awareness, language\ngeneration, OCR, and math. However, its question format is restricted to single\nimage-text pairs, lacking the interleaved image and text sequences prevalent in\nreal-world scenarios. To address this limitation, we introduce MM-Vet v2, which\nincludes a new VL capability called \"image-text sequence understanding\",\nevaluating models' ability to process VL sequences. Furthermore, we maintain\nthe high quality of evaluation samples while further expanding the evaluation\nset size. Using MM-Vet v2 to benchmark large multimodal models, we found that\nClaude 3.5 Sonnet is the best model with a score of 71.8, slightly\noutperforming GPT-4o which scored 71.0. Among open-weight models,\nInternVL2-Llama3-76B leads with a score of 68.4.\n","authors":["Weihao Yu","Zhengyuan Yang","Linfeng Ren","Linjie Li","Jianfeng Wang","Kevin Lin","Chung-Ching Lin","Zicheng Liu","Lijuan Wang","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2408.00765v1.pdf","comment":"Extension of MM-Vet: arXiv:2308.02490"},{"id":"http://arxiv.org/abs/2408.00762v1","updated":"2024-08-01T17:59:27Z","published":"2024-08-01T17:59:27Z","title":"UniTalker: Scaling up Audio-Driven 3D Facial Animation through A Unified\n  Model","summary":"  Audio-driven 3D facial animation aims to map input audio to realistic facial\nmotion. Despite significant progress, limitations arise from inconsistent 3D\nannotations, restricting previous models to training on specific annotations\nand thereby constraining the training scale. In this work, we present\nUniTalker, a unified model featuring a multi-head architecture designed to\neffectively leverage datasets with varied annotations. To enhance training\nstability and ensure consistency among multi-head outputs, we employ three\ntraining strategies, namely, PCA, model warm-up, and pivot identity embedding.\nTo expand the training scale and diversity, we assemble A2F-Bench, comprising\nfive publicly available datasets and three newly curated datasets. These\ndatasets contain a wide range of audio domains, covering multilingual speech\nvoices and songs, thereby scaling the training data from commonly employed\ndatasets, typically less than 1 hour, to 18.5 hours. With a single trained\nUniTalker model, we achieve substantial lip vertex error reductions of 9.2% for\nBIWI dataset and 13.7% for Vocaset. Additionally, the pre-trained UniTalker\nexhibits promise as the foundation model for audio-driven facial animation\ntasks. Fine-tuning the pre-trained UniTalker on seen datasets further enhances\nperformance on each dataset, with an average error reduction of 6.3% on\nA2F-Bench. Moreover, fine-tuning UniTalker on an unseen dataset with only half\nthe data surpasses prior state-of-the-art models trained on the full dataset.\nThe code and dataset are available at the project page\nhttps://github.com/X-niper/UniTalker.\n","authors":["Xiangyu Fan","Jiaqi Li","Zhiqian Lin","Weiye Xiao","Lei Yang"],"pdf_url":"https://arxiv.org/pdf/2408.00762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00760v1","updated":"2024-08-01T17:59:09Z","published":"2024-08-01T17:59:09Z","title":"Smoothed Energy Guidance: Guiding Diffusion Models with Reduced Energy\n  Curvature of Attention","summary":"  Conditional diffusion models have shown remarkable success in visual content\ngeneration, producing high-quality samples across various domains, largely due\nto classifier-free guidance (CFG). Recent attempts to extend guidance to\nunconditional models have relied on heuristic techniques, resulting in\nsuboptimal generation quality and unintended effects. In this work, we propose\nSmoothed Energy Guidance (SEG), a novel training- and condition-free approach\nthat leverages the energy-based perspective of the self-attention mechanism to\nenhance image generation. By defining the energy of self-attention, we\nintroduce a method to reduce the curvature of the energy landscape of attention\nand use the output as the unconditional prediction. Practically, we control the\ncurvature of the energy landscape by adjusting the Gaussian kernel parameter\nwhile keeping the guidance scale parameter fixed. Additionally, we present a\nquery blurring method that is equivalent to blurring the entire attention\nweights without incurring quadratic complexity in the number of tokens. In our\nexperiments, SEG achieves a Pareto improvement in both quality and the\nreduction of side effects. The code is available at\n\\url{https://github.com/SusungHong/SEG-SDXL}.\n","authors":["Susung Hong"],"pdf_url":"https://arxiv.org/pdf/2408.00760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00759v1","updated":"2024-08-01T17:58:19Z","published":"2024-08-01T17:58:19Z","title":"Text-Guided Video Masked Autoencoder","summary":"  Recent video masked autoencoder (MAE) works have designed improved masking\nalgorithms focused on saliency. These works leverage visual cues such as motion\nto mask the most salient regions. However, the robustness of such visual cues\ndepends on how often input videos match underlying assumptions. On the other\nhand, natural language description is an information dense representation of\nvideo that implicitly captures saliency without requiring modality-specific\nassumptions, and has not been explored yet for video MAE. To this end, we\nintroduce a novel text-guided masking algorithm (TGM) that masks the video\nregions with highest correspondence to paired captions. Without leveraging any\nexplicit visual cues for saliency, our TGM is competitive with state-of-the-art\nmasking algorithms such as motion-guided masking. To further benefit from the\nsemantics of natural language for masked reconstruction, we next introduce a\nunified framework for joint MAE and masked video-text contrastive learning. We\nshow that across existing masking algorithms, unifying MAE and masked\nvideo-text contrastive learning improves downstream performance compared to\npure MAE on a variety of video recognition tasks, especially for linear probe.\nWithin this unified framework, our TGM achieves the best relative performance\non five action recognition and one egocentric datasets, highlighting the\ncomplementary nature of natural language for masked video modeling.\n","authors":["David Fan","Jue Wang","Shuai Liao","Zhikang Zhang","Vimal Bhat","Xinyu Li"],"pdf_url":"https://arxiv.org/pdf/2408.00759v1.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2408.00756v1","updated":"2024-08-01T17:57:25Z","published":"2024-08-01T17:57:25Z","title":"Segment anything model 2: an application to 2D and 3D medical images","summary":"  Segment Anything Model (SAM) has gained significant attention because of its\nability to segment a variety of objects in images given a prompt. The recently\ndeveloped SAM 2 has extended this ability to video inputs. This opens an\nopportunity to apply SAM to 3D images, one of the fundamental tasks in the\nmedical imaging field. In this paper, we provide an extensive evaluation of SAM\n2's ability to segment both 2D and 3D medical images. We collect 18 medical\nimaging datasets, including common 3D modalities such as computed tomography\n(CT), magnetic resonance imaging (MRI), and positron emission tomography (PET)\nas well as 2D modalities such as X-ray and ultrasound. We consider two\nevaluation pipelines of SAM 2: (1) multi-frame 3D segmentation, where prompts\nare provided to one or multiple slice(s) selected from the volume, and (2)\nsingle-frame 2D segmentation, where prompts are provided to each slice. The\nformer is only applicable to 3D modalities, while the latter applies to both 2D\nand 3D modalities. We learn that SAM 2 exhibits similar performance as SAM\nunder single-frame 2D segmentation, and has variable performance under\nmulti-frame 3D segmentation depending on the choices of slices to annotate, the\ndirection of the propagation, the predictions utilized during the propagation,\netc.\n","authors":["Haoyu Dong","Hanxue Gu","Yaqian Chen","Jichen Yang","Maciej A. Mazurowski"],"pdf_url":"https://arxiv.org/pdf/2408.00756v1.pdf","comment":"11 pages, 7 figures. A first attempt on evaluating SAM 2 on medical\n  images"},{"id":"http://arxiv.org/abs/2408.00754v1","updated":"2024-08-01T17:57:12Z","published":"2024-08-01T17:57:12Z","title":"Coarse Correspondence Elicit 3D Spacetime Understanding in Multimodal\n  Language Model","summary":"  Multimodal language models (MLLMs) are increasingly being implemented in\nreal-world environments, necessitating their ability to interpret 3D spaces and\ncomprehend temporal dynamics. Despite their potential, current top models\nwithin our community still fall short in adequately understanding spatial and\ntemporal dimensions. We introduce Coarse Correspondence, a simple,\ntraining-free, effective, and general-purpose visual prompting method to elicit\n3D and temporal understanding in multimodal LLMs. Our method uses a lightweight\ntracking model to find object correspondences between frames in a video or\nbetween sets of image viewpoints. It selects the most frequent object instances\nand visualizes them with markers with unique IDs in the image. With this simple\napproach, we achieve state-of-the-art results on 3D understanding benchmarks\nincluding ScanQA (+20.5\\%) and a subset of OpenEQA (+9.7\\%), and on long-form\nvideo benchmarks such as EgoSchema (+6.0\\%). We also curate a small diagnostic\ndataset to evaluate whether MLLMs can reason about space from a described\nviewpoint other than the camera viewpoint. Again, Coarse Correspondence\nimproves spatial perspective-taking abilities but we highlight that MLLMs\nstruggle with this task. Together, we demonstrate that our simple prompting\nmethod can significantly aid downstream tasks that require 3D or temporal\nreasoning.\n","authors":["Benlin Liu","Yuhao Dong","Yiqin Wang","Yongming Rao","Yansong Tang","Wei-Chiu Ma","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2408.00754v1.pdf","comment":"project page: https://coarse-correspondence.github.io"},{"id":"http://arxiv.org/abs/2408.00749v1","updated":"2024-08-01T17:52:10Z","published":"2024-08-01T17:52:10Z","title":"Leaf Angle Estimation using Mask R-CNN and LETR Vision Transformer","summary":"  Modern day studies show a high degree of correlation between high yielding\ncrop varieties and plants with upright leaf angles. It is observed that plants\nwith upright leaf angles intercept more light than those without upright leaf\nangles, leading to a higher rate of photosynthesis. Plant scientists and\nbreeders benefit from tools that can directly measure plant parameters in the\nfield i.e. on-site phenotyping. The estimation of leaf angles by manual means\nin a field setting is tedious and cumbersome. We mitigate the tedium using a\ncombination of the Mask R-CNN instance segmentation neural network, and Line\nSegment Transformer (LETR), a vision transformer. The proposed Computer Vision\n(CV) pipeline is applied on two image datasets, Summer 2015-Ames ULA and Summer\n2015- Ames MLA, with a combined total of 1,827 plant images collected in the\nfield using FieldBook, an Android application aimed at on-site phenotyping. The\nleaf angles estimated by the proposed pipeline on the image datasets are\ncompared to two independent manual measurements using ImageJ, a Java-based\nimage processing program developed at the National Institutes of Health and the\nLaboratory for Optical and Computational Instrumentation. The results, when\ncompared for similarity using the Cosine Similarity measure, exhibit 0.98\nsimilarity scores on both independent measurements of Summer 2015-Ames ULA and\nSummer 2015-Ames MLA image datasets, demonstrating the feasibility of the\nproposed pipeline for on-site measurement of leaf angles.\n","authors":["Venkat Margapuri","Prapti Thapaliya","Trevor Rife"],"pdf_url":"https://arxiv.org/pdf/2408.00749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00744v1","updated":"2024-08-01T17:48:08Z","published":"2024-08-01T17:48:08Z","title":"Collaborative Vision-Text Representation Optimizing for Open-Vocabulary\n  Segmentation","summary":"  Pre-trained vision-language models, e.g. CLIP, have been increasingly used to\naddress the challenging Open-Vocabulary Segmentation (OVS) task, benefiting\nfrom their well-aligned vision-text embedding space. Typical solutions involve\neither freezing CLIP during training to unilaterally maintain its zero-shot\ncapability, or fine-tuning CLIP vision encoder to achieve perceptual\nsensitivity to local regions. However, few of them incorporate vision-text\ncollaborative optimization. Based on this, we propose the Content-Dependent\nTransfer to adaptively enhance each text embedding by interacting with the\ninput image, which presents a parameter-efficient way to optimize the text\nrepresentation. Besides, we additionally introduce a Representation\nCompensation strategy, reviewing the original CLIP-V representation as\ncompensation to maintain the zero-shot capability of CLIP. In this way, the\nvision and text representation of CLIP are optimized collaboratively, enhancing\nthe alignment of the vision-text feature space. To the best of our knowledge,\nwe are the first to establish the collaborative vision-text optimizing\nmechanism within the OVS field. Extensive experiments demonstrate our method\nachieves superior performance on popular OVS benchmarks. In open-vocabulary\nsemantic segmentation, our method outperforms the previous state-of-the-art\napproaches by +0.5, +2.3, +3.4, +0.4 and +1.1 mIoU, respectively on A-847,\nA-150, PC-459, PC-59 and PAS-20. Furthermore, in a panoptic setting on ADE20K,\nwe achieve the performance of 27.1 PQ, 73.5 SQ, and 32.9 RQ. Code will be\navailable at https://github.com/jiaosiyu1999/MAFT-Plus.git .\n","authors":["Siyu Jiao","Hongguang Zhu","Jiannan Huang","Yao Zhao","Yunchao Wei","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2408.00744v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2408.00738v1","updated":"2024-08-01T17:35:58Z","published":"2024-08-01T17:35:58Z","title":"Virchow 2: Scaling Self-Supervised Mixed Magnification Models in\n  Pathology","summary":"  Foundation models are rapidly being developed for computational pathology\napplications. However, it remains an open question which factors are most\nimportant for downstream performance with data scale and diversity, model size,\nand training algorithm all playing a role. In this work, we present the result\nof scaling both data and model size, surpassing previous studies in both\ndimensions, and introduce two new models: Virchow 2, a 632M parameter vision\ntransformer, and Virchow 2G, a 1.85B parameter vision transformer, each trained\nwith 3.1M histopathology whole slide images. To support this scale, we propose\ndomain-inspired adaptations to the DINOv2 training algorithm, which is quickly\nbecoming the default method in self-supervised learning for computational\npathology. We achieve state of the art performance on twelve tile-level tasks,\nas compared to the top performing competing models. Our results suggest that\ndata diversity and domain-specific training can outperform models that only\nscale in the number of parameters, but, on average, performance benefits from\ndomain-tailoring, data scale, and model scale.\n","authors":["Eric Zimmermann","Eugene Vorontsov","Julian Viret","Adam Casson","Michal Zelechowski","George Shaikovski","Neil Tenenholtz","James Hall","Thomas Fuchs","Nicolo Fusi","Siqi Liu","Kristen Severson"],"pdf_url":"https://arxiv.org/pdf/2408.00738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00735v1","updated":"2024-08-01T17:27:28Z","published":"2024-08-01T17:27:28Z","title":"TurboEdit: Text-Based Image Editing Using Few-Step Diffusion Models","summary":"  Diffusion models have opened the path to a wide range of text-based image\nediting frameworks. However, these typically build on the multi-step nature of\nthe diffusion backwards process, and adapting them to distilled, fast-sampling\nmethods has proven surprisingly challenging. Here, we focus on a popular line\nof text-based editing frameworks - the ``edit-friendly'' DDPM-noise inversion\napproach. We analyze its application to fast sampling methods and categorize\nits failures into two classes: the appearance of visual artifacts, and\ninsufficient editing strength. We trace the artifacts to mismatched noise\nstatistics between inverted noises and the expected noise schedule, and suggest\na shifted noise schedule which corrects for this offset. To increase editing\nstrength, we propose a pseudo-guidance approach that efficiently increases the\nmagnitude of edits without introducing new artifacts. All in all, our method\nenables text-based image editing with as few as three diffusion steps, while\nproviding novel insights into the mechanisms behind popular text-based editing\napproaches.\n","authors":["Gilad Deutch","Rinon Gal","Daniel Garibi","Or Patashnik","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2408.00735v1.pdf","comment":"Project page: https://turboedit-paper.github.io/"},{"id":"http://arxiv.org/abs/2408.00714v1","updated":"2024-08-01T17:00:08Z","published":"2024-08-01T17:00:08Z","title":"SAM 2: Segment Anything in Images and Videos","summary":"  We present Segment Anything Model 2 (SAM 2), a foundation model towards\nsolving promptable visual segmentation in images and videos. We build a data\nengine, which improves model and data via user interaction, to collect the\nlargest video segmentation dataset to date. Our model is a simple transformer\narchitecture with streaming memory for real-time video processing. SAM 2\ntrained on our data provides strong performance across a wide range of tasks.\nIn video segmentation, we observe better accuracy, using 3x fewer interactions\nthan prior approaches. In image segmentation, our model is more accurate and 6x\nfaster than the Segment Anything Model (SAM). We believe that our data, model,\nand insights will serve as a significant milestone for video segmentation and\nrelated perception tasks. We are releasing a version of our model, the dataset\nand an interactive demo.\n","authors":["Nikhila Ravi","Valentin Gabeur","Yuan-Ting Hu","Ronghang Hu","Chaitanya Ryali","Tengyu Ma","Haitham Khedr","Roman Rädle","Chloe Rolland","Laura Gustafson","Eric Mintun","Junting Pan","Kalyan Vasudev Alwala","Nicolas Carion","Chao-Yuan Wu","Ross Girshick","Piotr Dollár","Christoph Feichtenhofer"],"pdf_url":"https://arxiv.org/pdf/2408.00714v1.pdf","comment":"Website: https://ai.meta.com/sam2"},{"id":"http://arxiv.org/abs/2408.00712v1","updated":"2024-08-01T16:58:50Z","published":"2024-08-01T16:58:50Z","title":"MotionFix: Text-Driven 3D Human Motion Editing","summary":"  The focus of this paper is 3D motion editing. Given a 3D human motion and a\ntextual description of the desired modification, our goal is to generate an\nedited motion as described by the text. The challenges include the lack of\ntraining data and the design of a model that faithfully edits the source\nmotion. In this paper, we address both these challenges. We build a methodology\nto semi-automatically collect a dataset of triplets in the form of (i) a source\nmotion, (ii) a target motion, and (iii) an edit text, and create the new\nMotionFix dataset. Having access to such data allows us to train a conditional\ndiffusion model, TMED, that takes both the source motion and the edit text as\ninput. We further build various baselines trained only on text-motion pairs\ndatasets, and show superior performance of our model trained on triplets. We\nintroduce new retrieval-based metrics for motion editing and establish a new\nbenchmark on the evaluation set of MotionFix. Our results are encouraging,\npaving the way for further research on finegrained motion generation. Code and\nmodels will be made publicly available.\n","authors":["Nikos Athanasiou","Alpár Ceske","Markos Diomataris","Michael J. Black","Gül Varol"],"pdf_url":"https://arxiv.org/pdf/2408.00712v1.pdf","comment":"arXiv v1"},{"id":"http://arxiv.org/abs/2408.00707v1","updated":"2024-08-01T16:54:11Z","published":"2024-08-01T16:54:11Z","title":"Synthetic dual image generation for reduction of labeling efforts in\n  semantic segmentation of micrographs with a customized metric function","summary":"  Training of semantic segmentation models for material analysis requires\nmicrographs and their corresponding masks. It is quite unlikely that perfect\nmasks will be drawn, especially at the edges of objects, and sometimes the\namount of data that can be obtained is small, since only a few samples are\navailable. These aspects make it very problematic to train a robust model. We\ndemonstrate a workflow for the improvement of semantic segmentation models of\nmicrographs through the generation of synthetic microstructural images in\nconjunction with masks. The workflow only requires joining a few micrographs\nwith their respective masks to create the input for a Vector\nQuantised-Variational AutoEncoder model that includes an embedding space, which\nis trained such that a generative model (PixelCNN) learns the distribution of\neach input, transformed into discrete codes, and can be used to sample new\ncodes. The latter will eventually be decoded by VQ-VAE to generate images\nalongside corresponding masks for semantic segmentation. To evaluate the\nsynthetic data, we have trained U-Net models with different amounts of these\nsynthetic data in conjunction with real data. These models were then evaluated\nusing non-synthetic images only. Additionally, we introduce a customized metric\nderived from the mean Intersection over Union (mIoU). The proposed metric\nprevents a few falsely predicted pixels from greatly reducing the value of the\nmIoU. We have achieved a reduction in sample preparation and acquisition times,\nas well as the efforts, needed for image processing and labeling tasks, are\nless when it comes to training semantic segmentation model. The approach could\nbe generalized to various types of image data such that it serves as a\nuser-friendly solution for training models with a small number of real images.\n","authors":["Matias Oscar Volman Stern","Dominic Hohs","Andreas Jansche","Timo Bernthaler","Gerhard Schneider"],"pdf_url":"https://arxiv.org/pdf/2408.00707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00706v1","updated":"2024-08-01T16:52:39Z","published":"2024-08-01T16:52:39Z","title":"Point-supervised Brain Tumor Segmentation with Box-prompted MedSAM","summary":"  Delineating lesions and anatomical structure is important for image-guided\ninterventions. Point-supervised medical image segmentation (PSS) has great\npotential to alleviate costly expert delineation labeling. However, due to the\nlack of precise size and boundary guidance, the effectiveness of PSS often\nfalls short of expectations. Although recent vision foundational models, such\nas the medical segment anything model (MedSAM), have made significant\nadvancements in bounding-box-prompted segmentation, it is not straightforward\nto utilize point annotation, and is prone to semantic ambiguity. In this\npreliminary study, we introduce an iterative framework to facilitate\nsemantic-aware point-supervised MedSAM. Specifically, the semantic box-prompt\ngenerator (SBPG) module has the capacity to convert the point input into\npotential pseudo bounding box suggestions, which are explicitly refined by the\nprototype-based semantic similarity. This is then succeeded by a prompt-guided\nspatial refinement (PGSR) module that harnesses the exceptional\ngeneralizability of MedSAM to infer the segmentation mask, which also updates\nthe box proposal seed in SBPG. Performance can be progressively improved with\nadequate iterations. We conducted an evaluation on BraTS2018 for the\nsegmentation of whole brain tumors and demonstrated its superior performance\ncompared to traditional PSS methods and on par with box-supervised methods.\n","authors":["Xiaofeng Liu","Jonghye Woo","Chao Ma","Jinsong Ouyang","Georges El Fakhri"],"pdf_url":"https://arxiv.org/pdf/2408.00706v1.pdf","comment":"2024 IEEE Nuclear Science Symposium and Medical Imaging Conference"},{"id":"http://arxiv.org/abs/2408.00701v1","updated":"2024-08-01T16:48:03Z","published":"2024-08-01T16:48:03Z","title":"Joint Neural Networks for One-shot Object Recognition and Detection","summary":"  This paper presents a novel joint neural networks approach to address the\nchallenging one-shot object recognition and detection tasks. Inspired by\nSiamese neural networks and state-of-art multi-box detection approaches, the\njoint neural networks are able to perform object recognition and detection for\ncategories that remain unseen during the training process. Following the\none-shot object recognition/detection constraints, the training and testing\ndatasets do not contain overlapped classes, in other words, all the test\nclasses remain unseen during training. The joint networks architecture is able\nto effectively compare pairs of images via stacked convolutional layers of the\nquery and target inputs, recognising patterns of the same input query category\nwithout relying on previous training around this category. The proposed\napproach achieves 61.41% accuracy for one-shot object recognition on the\nMiniImageNet dataset and 47.1% mAP for one-shot object detection when trained\non the COCO dataset and tested using the Pascal VOC dataset. Code available at\nhttps://github.com/cjvargasc/JNN recog and https://github.com/cjvargasc/JNN\ndetection/\n","authors":["Camilo J. Vargas","Qianni Zhang","Ebroul Izquierdo"],"pdf_url":"https://arxiv.org/pdf/2408.00701v1.pdf","comment":"published as part of the PhD thesis:\n  https://qmro.qmul.ac.uk/xmlui/handle/123456789/72758"},{"id":"http://arxiv.org/abs/2408.00677v1","updated":"2024-08-01T16:20:02Z","published":"2024-08-01T16:20:02Z","title":"Scaling Backwards: Minimal Synthetic Pre-training?","summary":"  Pre-training and transfer learning are an important building block of current\ncomputer vision systems. While pre-training is usually performed on large\nreal-world image datasets, in this paper we ask whether this is truly\nnecessary. To this end, we search for a minimal, purely synthetic pre-training\ndataset that allows us to achieve performance similar to the 1 million images\nof ImageNet-1k. We construct such a dataset from a single fractal with\nperturbations. With this, we contribute three main findings. (i) We show that\npre-training is effective even with minimal synthetic images, with performance\non par with large-scale pre-training datasets like ImageNet-1k for full\nfine-tuning. (ii) We investigate the single parameter with which we construct\nartificial categories for our dataset. We find that while the shape differences\ncan be indistinguishable to humans, they are crucial for obtaining strong\nperformances. (iii) Finally, we investigate the minimal requirements for\nsuccessful pre-training. Surprisingly, we find that a substantial reduction of\nsynthetic images from 1k to 1 can even lead to an increase in pre-training\nperformance, a motivation to further investigate ``scaling backwards''.\nFinally, we extend our method from synthetic images to real images to see if a\nsingle real image can show similar pre-training effect through shape\naugmentation. We find that the use of grayscale images and affine\ntransformations allows even real images to ``scale backwards''.\n","authors":["Ryo Nakamura","Ryu Tadokoro","Ryosuke Yamada","Yuki M. Asano","Iro Laina","Christian Rupprecht","Nakamasa Inoue","Rio Yokota","Hirokatsu Kataoka"],"pdf_url":"https://arxiv.org/pdf/2408.00677v1.pdf","comment":"Accepted to ECCV2024"},{"id":"http://arxiv.org/abs/2408.00672v1","updated":"2024-08-01T16:13:07Z","published":"2024-08-01T16:13:07Z","title":"ExpertAF: Expert Actionable Feedback from Video","summary":"  Feedback is essential for learning a new skill or improving one's current\nskill-level. However, current methods for skill-assessment from video only\nprovide scores or compare demonstrations, leaving the burden of knowing what to\ndo differently on the user. We introduce a novel method to generate actionable\nfeedback from video of a person doing a physical activity, such as basketball\nor soccer. Our method takes a video demonstration and its accompanying 3D body\npose and generates (1) free-form expert commentary describing what the person\nis doing well and what they could improve, and (2) a visual expert\ndemonstration that incorporates the required corrections. We show how to\nleverage Ego-Exo4D's videos of skilled activity and expert commentary together\nwith a strong language model to create a weakly-supervised training dataset for\nthis task, and we devise a multimodal video-language model to infer coaching\nfeedback. Our method is able to reason across multi-modal input combinations to\noutput full-spectrum, actionable coaching -- expert commentary, expert video\nretrieval, and the first-of-its-kind expert pose generation -- outperforming\nstrong vision-language models on both established metrics and human preference\nstudies.\n","authors":["Kumar Ashutosh","Tushar Nagarajan","Georgios Pavlakos","Kris Kitani","Kristen Grauman"],"pdf_url":"https://arxiv.org/pdf/2408.00672v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2408.00653v1","updated":"2024-08-01T15:41:57Z","published":"2024-08-01T15:41:57Z","title":"SF3D: Stable Fast 3D Mesh Reconstruction with UV-unwrapping and\n  Illumination Disentanglement","summary":"  We present SF3D, a novel method for rapid and high-quality textured object\nmesh reconstruction from a single image in just 0.5 seconds. Unlike most\nexisting approaches, SF3D is explicitly trained for mesh generation,\nincorporating a fast UV unwrapping technique that enables swift texture\ngeneration rather than relying on vertex colors. The method also learns to\npredict material parameters and normal maps to enhance the visual quality of\nthe reconstructed 3D meshes. Furthermore, SF3D integrates a delighting step to\neffectively remove low-frequency illumination effects, ensuring that the\nreconstructed meshes can be easily used in novel illumination conditions.\nExperiments demonstrate the superior performance of SF3D over the existing\ntechniques. Project page: https://stable-fast-3d.github.io\n","authors":["Mark Boss","Zixuan Huang","Aaryaman Vasishta","Varun Jampani"],"pdf_url":"https://arxiv.org/pdf/2408.00653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00644v1","updated":"2024-08-01T15:35:44Z","published":"2024-08-01T15:35:44Z","title":"Towards End-to-End Explainable Facial Action Unit Recognition via\n  Vision-Language Joint Learning","summary":"  Facial action units (AUs), as defined in the Facial Action Coding System\n(FACS), have received significant research interest owing to their diverse\nrange of applications in facial state analysis. Current mainstream FAU\nrecognition models have a notable limitation, i.e., focusing only on the\naccuracy of AU recognition and overlooking explanations of corresponding AU\nstates. In this paper, we propose an end-to-end Vision-Language joint learning\nnetwork for explainable FAU recognition (termed VL-FAU), which aims to\nreinforce AU representation capability and language interpretability through\nthe integration of joint multimodal tasks. Specifically, VL-FAU brings together\nlanguage models to generate fine-grained local muscle descriptions and\ndistinguishable global face description when optimising FAU recognition.\nThrough this, the global facial representation and its local AU representations\nwill achieve higher distinguishability among different AUs and different\nsubjects. In addition, multi-level AU representation learning is utilised to\nimprove AU individual attention-aware representation capabilities based on\nmulti-scale combined facial stem feature. Extensive experiments on DISFA and\nBP4D AU datasets show that the proposed approach achieves superior performance\nover the state-of-the-art methods on most of the metrics. In addition, compared\nwith mainstream FAU recognition methods, VL-FAU can provide local- and\nglobal-level interpretability language descriptions with the AUs' predictions.\n","authors":["Xuri Ge","Junchen Fu","Fuhai Chen","Shan An","Nicu Sebe","Joemon M. Jose"],"pdf_url":"https://arxiv.org/pdf/2408.00644v1.pdf","comment":"10 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2408.00640v1","updated":"2024-08-01T15:27:48Z","published":"2024-08-01T15:27:48Z","title":"AMAES: Augmented Masked Autoencoder Pretraining on Public Brain MRI Data\n  for 3D-Native Segmentation","summary":"  This study investigates the impact of self-supervised pretraining of 3D\nsemantic segmentation models on a large-scale, domain-specific dataset. We\nintroduce BRAINS-45K, a dataset of 44,756 brain MRI volumes from public\nsources, the largest public dataset available, and revisit a number of design\nchoices for pretraining modern segmentation architectures by simplifying and\noptimizing state-of-the-art methods, and combining them with a novel\naugmentation strategy. The resulting AMAES framework is based on\nmasked-image-modeling and intensity-based augmentation reversal and balances\nmemory usage, runtime, and finetuning performance. Using the popular U-Net and\nthe recent MedNeXt architecture as backbones, we evaluate the effect of\npretraining on three challenging downstream tasks, covering single-sequence,\nlow-resource settings, and out-of-domain generalization. The results highlight\nthat pretraining on the proposed dataset with AMAES significantly improves\nsegmentation performance in the majority of evaluated cases, and that it is\nbeneficial to pretrain the model with augmentations, despite pretraing on a\nlarge-scale dataset. Code and model checkpoints for reproducing results, as\nwell as the BRAINS-45K dataset are available at\n\\url{https://github.com/asbjrnmunk/amaes}.\n","authors":["Asbjørn Munk","Jakob Ambsdorf","Sebastian Llambias","Mads Nielsen"],"pdf_url":"https://arxiv.org/pdf/2408.00640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00639v1","updated":"2024-08-01T15:26:24Z","published":"2024-08-01T15:26:24Z","title":"Privacy-preserving datasets by capturing feature distributions with\n  Conditional VAEs","summary":"  Large and well-annotated datasets are essential for advancing deep learning\napplications, however often costly or impossible to obtain by a single entity.\nIn many areas, including the medical domain, approaches relying on data sharing\nhave become critical to address those challenges. While effective in increasing\ndataset size and diversity, data sharing raises significant privacy concerns.\nCommonly employed anonymization methods based on the k-anonymity paradigm often\nfail to preserve data diversity, affecting model robustness. This work\nintroduces a novel approach using Conditional Variational Autoencoders (CVAEs)\ntrained on feature vectors extracted from large pre-trained vision foundation\nmodels. Foundation models effectively detect and represent complex patterns\nacross diverse domains, allowing the CVAE to faithfully capture the embedding\nspace of a given data distribution to generate (sample) a diverse,\nprivacy-respecting, and potentially unbounded set of synthetic feature vectors.\nOur method notably outperforms traditional approaches in both medical and\nnatural image domains, exhibiting greater dataset diversity and higher\nrobustness against perturbations while preserving sample privacy. These results\nunderscore the potential of generative models to significantly impact deep\nlearning applications in data-scarce and privacy-sensitive environments. The\nsource code is available at\nhttps://github.com/francescodisalvo05/cvae-anonymization .\n","authors":["Francesco Di Salvo","David Tafler","Sebastian Doerrich","Christian Ledig"],"pdf_url":"https://arxiv.org/pdf/2408.00639v1.pdf","comment":"Accepted at BMVC 2024"},{"id":"http://arxiv.org/abs/2408.00636v1","updated":"2024-08-01T15:20:20Z","published":"2024-08-01T15:20:20Z","title":"Deep Learning in Medical Image Classification from MRI-based Brain Tumor\n  Images","summary":"  Brain tumors are among the deadliest diseases in the world. Magnetic\nResonance Imaging (MRI) is one of the most effective ways to detect brain\ntumors. Accurate detection of brain tumors based on MRI scans is critical, as\nit can potentially save many lives and facilitate better decision-making at the\nearly stages of the disease. Within our paper, four different types of\nMRI-based images have been collected from the database: glioma tumor, no tumor,\npituitary tumor, and meningioma tumor. Our study focuses on making predictions\nfor brain tumor classification. Five models, including four pre-trained models\n(MobileNet, EfficientNet-B0, ResNet-18, and VGG16) and one new model,\nMobileNet-BT, have been proposed for this study.\n","authors":["Xiaoyi Liu","Zhuoyue Wang"],"pdf_url":"https://arxiv.org/pdf/2408.00636v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00629v1","updated":"2024-08-01T15:14:10Z","published":"2024-08-01T15:14:10Z","title":"Empowering Snapshot Compressive Imaging: Spatial-Spectral State Space\n  Model with Across-Scanning and Local Enhancement","summary":"  Snapshot Compressive Imaging (SCI) relies on decoding algorithms such as CNN\nor Transformer to reconstruct the hyperspectral image (HSI) from its compressed\nmeasurement. Although existing CNN and Transformer-based methods have proven\neffective, CNNs are limited by their inadequate modeling of long-range\ndependencies, while Transformer ones face high computational costs due to\nquadratic complexity. Recent Mamba models have demonstrated superior\nperformance over CNN and Transformer-based architectures in some visual tasks,\nbut these models have not fully utilized the local similarities in both spatial\nand spectral dimensions. Moreover, the long-sequence modeling capability of SSM\nmay offer an advantage in processing the numerous spectral bands for HSI\nreconstruction, which has not yet been explored. In this paper, we introduce a\nState Space Model with Across-Scanning and Local Enhancement, named ASLE-SSM,\nthat employs a Spatial-Spectral SSM for global-local balanced context encoding\nand cross-channel interaction promoting. Specifically, we introduce local\nscanning in the spatial dimension to balance the global and local receptive\nfields, and then propose our across-scanning method based on spatial-spectral\nlocal cubes to leverage local similarities between adjacent spectral bands and\npixels to guide the reconstruction process. These two scanning mechanisms\nextract the HSI's local features while balancing the global perspective without\nany additional costs. Experimental results illustrate ASLE-SSM's superiority\nover existing state-of-the-art methods, with an inference speed 2.4 times\nfaster than Transformer-based MST and saving 0.12 (M) of parameters, achieving\nthe lowest computational cost and parameter count.\n","authors":["Wenzhe Tian","Haijin Zeng","Yin-Ping Zhao","Yongyong Chen","Zhen Wang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.00629v1.pdf","comment":"12 pages,6 figures"},{"id":"http://arxiv.org/abs/2408.00624v1","updated":"2024-08-01T15:09:32Z","published":"2024-08-01T15:09:32Z","title":"SynesLM: A Unified Approach for Audio-visual Speech Recognition and\n  Translation via Language Model and Synthetic Data","summary":"  In this work, we present SynesLM, an unified model which can perform three\nmultimodal language understanding tasks: audio-visual automatic speech\nrecognition(AV-ASR) and visual-aided speech/machine translation(VST/VMT).\nUnlike previous research that focused on lip motion as visual cues for speech\nsignals, our work explores more general visual information within entire\nframes, such as objects and actions. Additionally, we use synthetic image data\nto enhance the correlation between image and speech data. We benchmark SynesLM\nagainst the How2 dataset, demonstrating performance on par with\nstate-of-the-art (SOTA) models dedicated to AV-ASR while maintaining our\nmultitasking framework. Remarkably, for zero-shot AV-ASR, SynesLM achieved SOTA\nperformance by lowering the Word Error Rate (WER) from 43.4% to 39.4% on the\nVisSpeech Dataset. Furthermore, our results in VST and VMT outperform the\nprevious results, improving the BLEU score to 43.5 from 37.2 for VST, and to\n54.8 from 54.4 for VMT.\n","authors":["Yichen Lu","Jiaqi Song","Xuankai Chang","Hengwei Bian","Soumi Maiti","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2408.00624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00620v1","updated":"2024-08-01T15:05:42Z","published":"2024-08-01T15:05:42Z","title":"Are Bigger Encoders Always Better in Vision Large Models?","summary":"  In recent years, multimodal large language models (MLLMs) have shown strong\npotential in real-world applications. They are developing rapidly due to their\nremarkable ability to comprehend multimodal information and their inherent\npowerful cognitive and reasoning capabilities. Among MLLMs, vision language\nmodels (VLM) stand out for their ability to understand vision information.\nHowever, the scaling trend of VLMs under the current mainstream paradigm has\nnot been extensively studied. Whether we can achieve better performance by\ntraining even larger models is still unclear. To address this issue, we\nconducted experiments on the pretraining stage of MLLMs. We conduct our\nexperiment using different encoder sizes and large language model (LLM) sizes.\nOur findings indicate that merely increasing the size of encoders does not\nnecessarily enhance the performance of VLMs. Moreover, we analyzed the effects\nof LLM backbone parameter size and data quality on the pretraining outcomes.\nAdditionally, we explored the differences in scaling laws between LLMs and\nVLMs.\n","authors":["Bozhou Li","Hao Liang","Zimo Meng","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00619v1","updated":"2024-08-01T15:01:07Z","published":"2024-08-01T15:01:07Z","title":"Harnessing Uncertainty-aware Bounding Boxes for Unsupervised 3D Object\n  Detection","summary":"  Unsupervised 3D object detection aims to identify objects of interest from\nunlabeled raw data, such as LiDAR points. Recent approaches usually adopt\npseudo 3D bounding boxes (3D bboxes) from clustering algorithm to initialize\nthe model training, and then iteratively updating both pseudo labels and the\ntrained model. However, pseudo bboxes inevitably contain noises, and such\ninaccurate annotation accumulates to the final model, compromising the\nperformance. Therefore, in an attempt to mitigate the negative impact of pseudo\nbboxes, we introduce a new uncertainty-aware framework. In particular, Our\nmethod consists of two primary components: uncertainty estimation and\nuncertainty regularization. (1) In the uncertainty estimation phase, we\nincorporate an extra auxiliary detection branch alongside the primary detector.\nThe prediction disparity between the primary and auxiliary detectors is\nleveraged to estimate uncertainty at the box coordinate level, including\nposition, shape, orientation. (2) Based on the assessed uncertainty, we\nregularize the model training via adaptively adjusting every 3D bboxes\ncoordinates. For pseudo bbox coordinates with high uncertainty, we assign a\nrelatively low loss weight. Experiment verifies that the proposed method is\nrobust against the noisy pseudo bboxes, yielding substantial improvements on\nnuScenes and Lyft compared to existing techniques, with increases of 6.9% in\nAP$_{BEV}$ and 2.5% in AP$_{3D}$ on nuScenes, and 2.2% in AP$_{BEV}$ and 1.0%\nin AP$_{3D}$ on Lyft.\n","authors":["Ruiyang Zhang","Hu Zhang","Hang Yu","Zhedong Zheng"],"pdf_url":"https://arxiv.org/pdf/2408.00619v1.pdf","comment":"Preprint, 14 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2408.00599v1","updated":"2024-08-01T14:31:06Z","published":"2024-08-01T14:31:06Z","title":"Learned Compression of Point Cloud Geometry and Attributes in a Single\n  Model through Multimodal Rate-Control","summary":"  Point cloud compression is essential to experience volumetric multimedia as\nit drastically reduces the required streaming data rates. Point attributes,\nspecifically colors, extend the challenge of lossy compression beyond geometric\nrepresentation to achieving joint reconstruction of texture and geometry.\nState-of-the-art methods separate geometry and attributes to compress them\nindividually. This comes at a computational cost, requiring an encoder and a\ndecoder for each modality. Additionally, as attribute compression methods\nrequire the same geometry for encoding and decoding, the encoder emulates the\ndecoder-side geometry reconstruction as an input step to project and compress\nthe attributes. In this work, we propose to learn joint compression of geometry\nand attributes using a single, adaptive autoencoder model, embedding both\nmodalities into a unified latent space which is then entropy encoded. Key to\nthe technique is to replace the search for trade-offs between rate, attribute\nquality and geometry quality, through conditioning the model on the desired\nqualities of both modalities, bypassing the need for training model ensembles.\nTo differentiate important point cloud regions during encoding or to allow\nview-dependent compression for user-centered streaming, conditioning is\npointwise, which allows for local quality and rate variation. Our evaluation\nshows comparable performance to state-of-the-art compression methods for\ngeometry and attributes, while reducing complexity compared to related\ncompression methods.\n","authors":["Michael Rudolph","Aron Riemenschneider","Amr Rizk"],"pdf_url":"https://arxiv.org/pdf/2408.00599v1.pdf","comment":"20 pages, 13 figures"},{"id":"http://arxiv.org/abs/2408.00591v1","updated":"2024-08-01T14:20:47Z","published":"2024-08-01T14:20:47Z","title":"Regional quality estimation for echocardiography using deep learning","summary":"  Automatic estimation of cardiac ultrasound image quality can be beneficial\nfor guiding operators and ensuring the accuracy of clinical measurements.\nPrevious work often fails to distinguish the view correctness of the\nechocardiogram from the image quality. Additionally, previous studies only\nprovide a global image quality value, which limits their practical utility. In\nthis work, we developed and compared three methods to estimate image quality:\n1) classic pixel-based metrics like the generalized contrast-to-noise ratio\n(gCNR) on myocardial segments as region of interest and left ventricle lumen as\nbackground, obtained using a U-Net segmentation 2) local image coherence\nderived from a U-Net model that predicts coherence from B-Mode images 3) a deep\nconvolutional network that predicts the quality of each region directly in an\nend-to-end fashion. We evaluate each method against manual regional image\nquality annotations by three experienced cardiologists. The results indicate\npoor performance of the gCNR metric, with Spearman correlation to the\nannotations of \\r{ho} = 0.24. The end-to-end learning model obtains the best\nresult, \\r{ho} = 0.69, comparable to the inter-observer correlation, \\r{ho} =\n0.63. Finally, the coherence-based method, with \\r{ho} = 0.58, outperformed the\nclassical metrics and is more generic than the end-to-end approach.\n","authors":["Gilles Van De Vyver","Svein-Erik Måsøy","Håvard Dalen","Bjørnar Leangen Grenne","Espen Holte","Sindre Hellum Olaisen","John Nyberg","Andreas Østvik","Lasse Løvstakken","Erik Smistad"],"pdf_url":"https://arxiv.org/pdf/2408.00591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00565v1","updated":"2024-08-01T13:52:18Z","published":"2024-08-01T13:52:18Z","title":"MUFASA: Multi-View Fusion and Adaptation Network with Spatial Awareness\n  for Radar Object Detection","summary":"  In recent years, approaches based on radar object detection have made\nsignificant progress in autonomous driving systems due to their robustness\nunder adverse weather compared to LiDAR. However, the sparsity of radar point\nclouds poses challenges in achieving precise object detection, highlighting the\nimportance of effective and comprehensive feature extraction technologies. To\naddress this challenge, this paper introduces a comprehensive feature\nextraction method for radar point clouds. This study first enhances the\ncapability of detection networks by using a plug-and-play module, GeoSPA. It\nleverages the Lalonde features to explore local geometric patterns.\nAdditionally, a distributed multi-view attention mechanism, DEMVA, is designed\nto integrate the shared information across the entire dataset with the global\ninformation of each individual frame. By employing the two modules, we present\nour method, MUFASA, which enhances object detection performance through\nimproved feature extraction. The approach is evaluated on the VoD and\nTJ4DRaDSet datasets to demonstrate its effectiveness. In particular, we achieve\nstate-of-the-art results among radar-based methods on the VoD dataset with the\nmAP of 50.24%.\n","authors":["Xiangyuan Peng","Miao Tang","Huawei Sun","Kay Bierzynski","Lorenzo Servadei","Robert Wille"],"pdf_url":"https://arxiv.org/pdf/2408.00565v1.pdf","comment":"Accepted by ICANN 2024"},{"id":"http://arxiv.org/abs/2408.00555v1","updated":"2024-08-01T13:38:58Z","published":"2024-08-01T13:38:58Z","title":"Alleviating Hallucination in Large Vision-Language Models with Active\n  Retrieval Augmentation","summary":"  Despite the remarkable ability of large vision-language models (LVLMs) in\nimage comprehension, these models frequently generate plausible yet factually\nincorrect responses, a phenomenon known as hallucination.Recently, in large\nlanguage models (LLMs), augmenting LLMs by retrieving information from external\nknowledge resources has been proven as a promising solution to mitigate\nhallucinations.However, the retrieval augmentation in LVLM significantly lags\nbehind the widespread applications of LVLM. Moreover, when transferred to\naugmenting LVLMs, sometimes the hallucination degree of the model is even\nexacerbated.Motivated by the research gap and counter-intuitive phenomenon, we\nintroduce a novel framework, the Active Retrieval-Augmented large\nvision-language model (ARA), specifically designed to address hallucinations by\nincorporating three critical dimensions: (i) dissecting the retrieval targets\nbased on the inherent hierarchical structures of images. (ii) pinpointing the\nmost effective retrieval methods and filtering out the reliable retrieval\nresults. (iii) timing the retrieval process to coincide with episodes of low\ncertainty, while circumventing unnecessary retrieval during periods of high\ncertainty. To assess the capability of our proposed ARA model in reducing\nhallucination, we employ three widely used LVLM models (LLaVA-1.5, Qwen-VL, and\nmPLUG-Owl2) across four benchmarks. Our empirical observations suggest that by\nutilizing fitting retrieval mechanisms and timing the retrieval judiciously, we\ncan effectively mitigate the hallucination problem. We hope that this study can\nprovide deeper insights into how to adapt the retrieval augmentation to LVLMs\nfor reducing hallucinations with more effective retrieval and minimal retrieval\noccurrences.\n","authors":["Xiaoye Qu","Qiyuan Chen","Wei Wei","Jishuo Sun","Jianfeng Dong"],"pdf_url":"https://arxiv.org/pdf/2408.00555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00550v1","updated":"2024-08-01T13:34:35Z","published":"2024-08-01T13:34:35Z","title":"Mitigating Multilingual Hallucination in Large Vision-Language Models","summary":"  While Large Vision-Language Models (LVLMs) have exhibited remarkable\ncapabilities across a wide range of tasks, they suffer from hallucination\nproblems, where models generate plausible yet incorrect answers given the input\nimage-query pair. This hallucination phenomenon is even more severe when\nquerying the image in non-English languages, while existing methods for\nmitigating hallucinations in LVLMs only consider the English scenarios. In this\npaper, we make the first attempt to mitigate this important multilingual\nhallucination in LVLMs. With thorough experiment analysis, we found that\nmultilingual hallucination in LVLMs is a systemic problem that could arise from\ndeficiencies in multilingual capabilities or inadequate multimodal abilities.\nTo this end, we propose a two-stage Multilingual Hallucination Removal (MHR)\nframework for LVLMs, aiming to improve resistance to hallucination for both\nhigh-resource and low-resource languages. Instead of relying on the intricate\nmanual annotations of multilingual resources, we fully leverage the inherent\ncapabilities of the LVLM and propose a novel cross-lingual alignment method,\nwhich generates multiple responses for each image-query input and then\nidentifies the hallucination-aware pairs for each language. These data pairs\nare finally used for direct preference optimization to prompt the LVLMs to\nfavor non-hallucinating responses. Experimental results show that our MHR\nachieves a substantial reduction in hallucination generation for LVLMs.\nNotably, on our extended multilingual POPE benchmark, our framework delivers an\naverage increase of 19.0% in accuracy across 13 different languages. Our code\nand model weights are available at https://github.com/ssmisya/MHR\n","authors":["Xiaoye Qu","Mingyang Song","Wei Wei","Jianfeng Dong","Yu Cheng"],"pdf_url":"https://arxiv.org/pdf/2408.00550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00538v1","updated":"2024-08-01T13:21:34Z","published":"2024-08-01T13:21:34Z","title":"High-Quality, ROS Compatible Video Encoding and Decoding for\n  High-Definition Datasets","summary":"  Robotic datasets are important for scientific benchmarking and developing\nalgorithms, for example for Simultaneous Localization and Mapping (SLAM).\nModern robotic datasets feature video data of high resolution and high\nframerates. Storing and sharing those datasets becomes thus very costly,\nespecially if more than one camera is used for the datasets. It is thus\nessential to store this video data in a compressed format. This paper\ninvestigates the use of modern video encoders for robotic datasets. We provide\na software that can replay mp4 videos within ROS 1 and ROS 2 frameworks,\nsupporting the synchronized playback in simulated time. Furthermore, the paper\nevaluates different encoders and their settings to find optimal configurations\nin terms of resulting size, quality and encoding time. Through this work we\nshow that it is possible to store and share even highest quality video datasets\nwithin reasonable storage constraints.\n","authors":["Jian Li","Bowen Xu","Sören Schwertfeger"],"pdf_url":"https://arxiv.org/pdf/2408.00538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00498v1","updated":"2024-08-01T12:08:20Z","published":"2024-08-01T12:08:20Z","title":"How Effective are Self-Supervised Models for Contact Identification in\n  Videos","summary":"  The exploration of video content via Self-Supervised Learning (SSL) models\nhas unveiled a dynamic field of study, emphasizing both the complex challenges\nand unique opportunities inherent in this area. Despite the growing body of\nresearch, the ability of SSL models to detect physical contacts in videos\nremains largely unexplored, particularly the effectiveness of methods such as\ndownstream supervision with linear probing or full fine-tuning. This work aims\nto bridge this gap by employing eight different convolutional neural networks\n(CNNs) based video SSL models to identify instances of physical contact within\nvideo sequences specifically. The Something-Something v2 (SSv2) and\nEpic-Kitchen (EK-100) datasets were chosen for evaluating these approaches due\nto the promising results on UCF101 and HMDB51, coupled with their limited prior\nassessment on SSv2 and EK-100. Additionally, these datasets feature diverse\nenvironments and scenarios, essential for testing the robustness and accuracy\nof video-based models. This approach not only examines the effectiveness of\neach model in recognizing physical contacts but also explores the performance\nin the action recognition downstream task. By doing so, valuable insights into\nthe adaptability of SSL models in interpreting complex, dynamic visual\ninformation are contributed.\n","authors":["Malitha Gunawardhana","Limalka Sadith","Liel David","Daniel Harari","Muhammad Haris Khan"],"pdf_url":"https://arxiv.org/pdf/2408.00498v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.00496v1","updated":"2024-08-01T12:05:02Z","published":"2024-08-01T12:05:02Z","title":"SegStitch: Multidimensional Transformer for Robust and Efficient Medical\n  Imaging Segmentation","summary":"  Medical imaging segmentation plays a significant role in the automatic\nrecognition and analysis of lesions. State-of-the-art methods, particularly\nthose utilizing transformers, have been prominently adopted in 3D semantic\nsegmentation due to their superior performance in scalability and\ngeneralizability. However, plain vision transformers encounter challenges due\nto their neglect of local features and their high computational complexity. To\naddress these challenges, we introduce three key contributions: Firstly, we\nproposed SegStitch, an innovative architecture that integrates transformers\nwith denoising ODE blocks. Instead of taking whole 3D volumes as inputs, we\nadapt axial patches and customize patch-wise queries to ensure semantic\nconsistency. Additionally, we conducted extensive experiments on the BTCV and\nACDC datasets, achieving improvements up to 11.48% and 6.71% respectively in\nmDSC, compared to state-of-the-art methods. Lastly, our proposed method\ndemonstrates outstanding efficiency, reducing the number of parameters by 36.7%\nand the number of FLOPS by 10.7% compared to UNETR. This advancement holds\npromising potential for adapting our method to real-world clinical practice.\nThe code will be available at https://github.com/goblin327/SegStitch\n","authors":["Shengbo Tan","Zeyu Zhang","Ying Cai","Daji Ergu","Lin Wu","Binbin Hu","Pengzhang Yu","Yang Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.00496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00493v1","updated":"2024-08-01T11:53:44Z","published":"2024-08-01T11:53:44Z","title":"Explainable Emotion Decoding for Human and Computer Vision","summary":"  Modern Machine Learning (ML) has significantly advanced various research\nfields, but the opaque nature of ML models hinders their adoption in several\ndomains. Explainable AI (XAI) addresses this challenge by providing additional\ninformation to help users understand the internal decision-making process of ML\nmodels. In the field of neuroscience, enriching a ML model for brain decoding\nwith attribution-based XAI techniques means being able to highlight which brain\nareas correlate with the task at hand, thus offering valuable insights to\ndomain experts. In this paper, we analyze human and Computer Vision (CV)\nsystems in parallel, training and explaining two ML models based respectively\non functional Magnetic Resonance Imaging (fMRI) and movie frames. We do so by\nleveraging the \"StudyForrest\" dataset, which includes functional Magnetic\nResonance Imaging (fMRI) scans of subjects watching the \"Forrest Gump\" movie,\nemotion annotations, and eye-tracking data. For human vision the ML task is to\nlink fMRI data with emotional annotations, and the explanations highlight the\nbrain regions strongly correlated with the label. On the other hand, for\ncomputer vision, the input data is movie frames, and the explanations are\npixel-level heatmaps. We cross-analyzed our results, linking human attention\n(obtained through eye-tracking) with XAI saliency on CV models and brain region\nactivations. We show how a parallel analysis of human and computer vision can\nprovide useful information for both the neuroscience community (allocation\ntheory) and the ML community (biological plausibility of convolutional models).\n","authors":["Alessio Borriero","Martina Milazzo","Matteo Diano","Davide Orsenigo","Maria Chiara Villa","Chiara Di Fazio","Marco Tamietto","Alan Perotti"],"pdf_url":"https://arxiv.org/pdf/2408.00493v1.pdf","comment":"This work has been accepted to be presented to The 2nd World\n  Conference on eXplainable Artificial Intelligence (xAI 2024), July 17-19,\n  2024 - Malta"},{"id":"http://arxiv.org/abs/2408.00491v1","updated":"2024-08-01T11:52:56Z","published":"2024-08-01T11:52:56Z","title":"GalleryGPT: Analyzing Paintings with Large Multimodal Models","summary":"  Artwork analysis is important and fundamental skill for art appreciation,\nwhich could enrich personal aesthetic sensibility and facilitate the critical\nthinking ability. Understanding artworks is challenging due to its subjective\nnature, diverse interpretations, and complex visual elements, requiring\nexpertise in art history, cultural background, and aesthetic theory. However,\nlimited by the data collection and model ability, previous works for\nautomatically analyzing artworks mainly focus on classification, retrieval, and\nother simple tasks, which is far from the goal of AI. To facilitate the\nresearch progress, in this paper, we step further to compose comprehensive\nanalysis inspired by the remarkable perception and generation ability of large\nmultimodal models. Specifically, we first propose a task of composing paragraph\nanalysis for artworks, i.e., painting in this paper, only focusing on visual\ncharacteristics to formulate more comprehensive understanding of artworks. To\nsupport the research on formal analysis, we collect a large dataset\nPaintingForm, with about 19k painting images and 50k analysis paragraphs. We\nfurther introduce a superior large multimodal model for painting analysis\ncomposing, dubbed GalleryGPT, which is slightly modified and fine-tuned based\non LLaVA architecture leveraging our collected data. We conduct formal analysis\ngeneration and zero-shot experiments across several datasets to assess the\ncapacity of our model. The results show remarkable performance improvements\ncomparing with powerful baseline LMMs, demonstrating its superb ability of art\nanalysis and generalization. \\textcolor{blue}{The codes and model are available\nat: https://github.com/steven640pixel/GalleryGPT.\n","authors":["Yi Bin","Wenhao Shi","Yujuan Ding","Zhiqiang Hu","Zheng Wang","Yang Yang","See-Kiong Ng","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2408.00491v1.pdf","comment":"Accepted as Oral Presentation at ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2408.00489v1","updated":"2024-08-01T11:51:50Z","published":"2024-08-01T11:51:50Z","title":"Multi-label Sewer Pipe Defect Recognition with Mask Attention Feature\n  Enhancement and Label Correlation Learning","summary":"  The coexistence of multiple defect categories as well as the substantial\nclass imbalance problem significantly impair the detection of sewer pipeline\ndefects. To solve this problem, a multi-label pipe defect recognition method is\nproposed based on mask attention guided feature enhancement and label\ncorrelation learning. The proposed method can achieve current approximate\nstate-of-the-art classification performance using just 1/16 of the Sewer-ML\ntraining dataset and exceeds the current best method by 11.87\\% in terms of F2\nmetric on the full dataset, while also proving the superiority of the model.\nThe major contribution of this study is the development of a more efficient\nmodel for identifying and locating multiple defects in sewer pipe images for a\nmore accurate sewer pipeline condition assessment. Moreover, by employing class\nactivation maps, our method can accurately pinpoint multiple defect categories\nin the image which demonstrates a strong model interpretability. Our code is\navailable at\n\\href{https://github.com/shengyu27/MA-Q2L}{\\textcolor{black}{https://github.com/shengyu27/MA-Q2L.}\n","authors":["Xin Zuo","Yu Sheng","Jifeng Shen","Yongwei Shan"],"pdf_url":"https://arxiv.org/pdf/2408.00489v1.pdf","comment":"Accepted by the Journal of Computing in Civil Engineering"},{"id":"http://arxiv.org/abs/2408.00483v1","updated":"2024-08-01T11:39:45Z","published":"2024-08-01T11:39:45Z","title":"A Systematic Review on Long-Tailed Learning","summary":"  Long-tailed data is a special type of multi-class imbalanced data with a very\nlarge amount of minority/tail classes that have a very significant combined\ninfluence. Long-tailed learning aims to build high-performance models on\ndatasets with long-tailed distributions, which can identify all the classes\nwith high accuracy, in particular the minority/tail classes. It is a\ncutting-edge research direction that has attracted a remarkable amount of\nresearch effort in the past few years. In this paper, we present a\ncomprehensive survey of latest advances in long-tailed visual learning. We\nfirst propose a new taxonomy for long-tailed learning, which consists of eight\ndifferent dimensions, including data balancing, neural architecture, feature\nenrichment, logits adjustment, loss function, bells and whistles, network\noptimization, and post hoc processing techniques. Based on our proposed\ntaxonomy, we present a systematic review of long-tailed learning methods,\ndiscussing their commonalities and alignable differences. We also analyze the\ndifferences between imbalance learning and long-tailed learning approaches.\nFinally, we discuss prospects and future directions in this field.\n","authors":["Chongsheng Zhang","George Almpanidis","Gaojuan Fan","Binquan Deng","Yanbo Zhang","Ji Liu","Aouaidjia Kamel","Paolo Soda","João Gama"],"pdf_url":"https://arxiv.org/pdf/2408.00483v1.pdf","comment":"Current Under Revision at IEEE TNNLS. [This is the long/Full-length\n  version of our Long-Tailed Learning Survey paper]"},{"id":"http://arxiv.org/abs/2408.00470v1","updated":"2024-08-01T11:16:26Z","published":"2024-08-01T11:16:26Z","title":"Image Super-Resolution with Taylor Expansion Approximation and Large\n  Field Reception","summary":"  Self-similarity techniques are booming in blind super-resolution (SR) due to\naccurate estimation of the degradation types involved in low-resolution images.\nHowever, high-dimensional matrix multiplication within self-similarity\ncomputation prohibitively consumes massive computational costs. We find that\nthe high-dimensional attention map is derived from the matrix multiplication\nbetween Query and Key, followed by a softmax function. This softmax makes the\nmatrix multiplication between Query and Key inseparable, posing a great\nchallenge in simplifying computational complexity. To address this issue, we\nfirst propose a second-order Taylor expansion approximation (STEA) to separate\nthe matrix multiplication of Query and Key, resulting in the complexity\nreduction from $\\mathcal{O}(N^2)$ to $\\mathcal{O}(N)$. Then, we design a\nmulti-scale large field reception (MLFR) to compensate for the performance\ndegradation caused by STEA. Finally, we apply these two core designs to\nlaboratory and real-world scenarios by constructing LabNet and RealNet,\nrespectively. Extensive experimental results tested on five synthetic datasets\ndemonstrate that our LabNet sets a new benchmark in qualitative and\nquantitative evaluations. Tested on the RealWorld38 dataset, our RealNet\nachieves superior visual quality over existing methods. Ablation studies\nfurther verify the contributions of STEA and MLFR towards both LabNet and\nRealNet frameworks.\n","authors":["Jiancong Feng","Yuan-Gen Wang","Mingjie Li","Fengchuang Xing"],"pdf_url":"https://arxiv.org/pdf/2408.00470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00458v1","updated":"2024-08-01T10:55:20Z","published":"2024-08-01T10:55:20Z","title":"Reenact Anything: Semantic Video Motion Transfer Using Motion-Textual\n  Inversion","summary":"  Recent years have seen a tremendous improvement in the quality of video\ngeneration and editing approaches. While several techniques focus on editing\nappearance, few address motion. Current approaches using text, trajectories, or\nbounding boxes are limited to simple motions, so we specify motions with a\nsingle motion reference video instead. We further propose to use a pre-trained\nimage-to-video model rather than a text-to-video model. This approach allows us\nto preserve the exact appearance and position of a target object or scene and\nhelps disentangle appearance from motion. Our method, called motion-textual\ninversion, leverages our observation that image-to-video models extract\nappearance mainly from the (latent) image input, while the text/image embedding\ninjected via cross-attention predominantly controls motion. We thus represent\nmotion using text/image embedding tokens. By operating on an inflated\nmotion-text embedding containing multiple text/image embedding tokens per\nframe, we achieve a high temporal motion granularity. Once optimized on the\nmotion reference video, this embedding can be applied to various target images\nto generate videos with semantically similar motions. Our approach does not\nrequire spatial alignment between the motion reference video and target image,\ngeneralizes across various domains, and can be applied to various tasks such as\nfull-body and face reenactment, as well as controlling the motion of inanimate\nobjects and the camera. We empirically demonstrate the effectiveness of our\nmethod in the semantic video motion transfer task, significantly outperforming\nexisting methods in this context.\n","authors":["Manuel Kansy","Jacek Naruniec","Christopher Schroers","Markus Gross","Romann M. Weber"],"pdf_url":"https://arxiv.org/pdf/2408.00458v1.pdf","comment":"Preprint. All videos in this paper are best viewed as animations with\n  Acrobat Reader by pressing the highlighted frame of each video"},{"id":"http://arxiv.org/abs/2408.00441v1","updated":"2024-08-01T10:25:14Z","published":"2024-08-01T10:25:14Z","title":"Focus, Distinguish, and Prompt: Unleashing CLIP for Efficient and\n  Flexible Scene Text Retrieval","summary":"  Scene text retrieval aims to find all images containing the query text from\nan image gallery. Current efforts tend to adopt an Optical Character\nRecognition (OCR) pipeline, which requires complicated text detection and/or\nrecognition processes, resulting in inefficient and inflexible retrieval.\nDifferent from them, in this work we propose to explore the intrinsic potential\nof Contrastive Language-Image Pre-training (CLIP) for OCR-free scene text\nretrieval. Through empirical analysis, we observe that the main challenges of\nCLIP as a text retriever are: 1) limited text perceptual scale, and 2)\nentangled visual-semantic concepts. To this end, a novel model termed FDP\n(Focus, Distinguish, and Prompt) is developed. FDP first focuses on scene text\nvia shifting the attention to the text area and probing the hidden text\nknowledge, and then divides the query text into content word and function word\nfor processing, in which a semantic-aware prompting scheme and a distracted\nqueries assistance module are utilized. Extensive experiments show that FDP\nsignificantly enhances the inference speed while achieving better or\ncompetitive retrieval accuracy compared to existing methods. Notably, on the\nIIIT-STR benchmark, FDP surpasses the state-of-the-art model by 4.37% with a 4\ntimes faster speed. Furthermore, additional experiments under phrase-level and\nattribute-aware scene text retrieval settings validate FDP's particular\nadvantages in handling diverse forms of query text. The source code will be\npublicly available at https://github.com/Gyann-z/FDP.\n","authors":["Gangyan Zeng","Yuan Zhang","Jin Wei","Dongbao Yang","Peng Zhang","Yiwen Gao","Xugong Qin","Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.00441v1.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2408.00438v1","updated":"2024-08-01T10:16:58Z","published":"2024-08-01T10:16:58Z","title":"MonoMM: A Multi-scale Mamba-Enhanced Network for Real-time Monocular 3D\n  Object Detection","summary":"  Recent advancements in transformer-based monocular 3D object detection\ntechniques have exhibited exceptional performance in inferring 3D attributes\nfrom single 2D images. However, most existing methods rely on\nresource-intensive transformer architectures, which often lead to significant\ndrops in computational efficiency and performance when handling long sequence\ndata. To address these challenges and advance monocular 3D object detection\ntechnology, we propose an innovative network architecture, MonoMM, a\nMulti-scale \\textbf{M}amba-Enhanced network for real-time Monocular 3D object\ndetection. This well-designed architecture primarily includes the following two\ncore modules: Focused Multi-Scale Fusion (FMF) Module, which focuses on\neffectively preserving and fusing image information from different scales with\nlower computational resource consumption. By precisely regulating the\ninformation flow, the FMF module enhances the model adaptability and robustness\nto scale variations while maintaining image details. Depth-Aware Feature\nEnhancement Mamba (DMB) Module: It utilizes the fused features from image\ncharacteristics as input and employs a novel adaptive strategy to globally\nintegrate depth information and visual information. This depth fusion strategy\nnot only improves the accuracy of depth estimation but also enhances the model\nperformance under different viewing angles and environmental conditions.\nMoreover, the modular design of MonoMM provides high flexibility and\nscalability, facilitating adjustments and optimizations according to specific\napplication needs. Extensive experiments conducted on the KITTI dataset show\nthat our method outperforms previous monocular methods and achieves real-time\ndetection.\n","authors":["Youjia Fu","Zihao Xu","Junsong Fu","Huixia Xue","Shuqiu Tan","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2408.00438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00427v1","updated":"2024-08-01T09:59:57Z","published":"2024-08-01T09:59:57Z","title":"CARMIL: Context-Aware Regularization on Multiple Instance Learning\n  models for Whole Slide Images","summary":"  Multiple Instance Learning (MIL) models have proven effective for cancer\nprognosis from Whole Slide Images. However, the original MIL formulation\nincorrectly assumes the patches of the same image to be independent, leading to\na loss of spatial context as information flows through the network.\nIncorporating contextual knowledge into predictions is particularly important\ngiven the inclination for cancerous cells to form clusters and the presence of\nspatial indicators for tumors. State-of-the-art methods often use attention\nmechanisms eventually combined with graphs to capture spatial knowledge. In\nthis paper, we take a novel and transversal approach, addressing this issue\nthrough the lens of regularization. We propose Context-Aware Regularization for\nMultiple Instance Learning (CARMIL), a versatile regularization scheme designed\nto seamlessly integrate spatial knowledge into any MIL model. Additionally, we\npresent a new and generic metric to quantify the Context-Awareness of any MIL\nmodel when applied to Whole Slide Images, resolving a previously unexplored gap\nin the field. The efficacy of our framework is evaluated for two survival\nanalysis tasks on glioblastoma (TCGA GBM) and colon cancer data (TCGA COAD).\n","authors":["Thiziri Nait Saada","Valentina Di-Proietto","Benoit Schmauch","Katharina Von Loga","Lucas Fidon"],"pdf_url":"https://arxiv.org/pdf/2408.00427v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00420v1","updated":"2024-08-01T09:42:44Z","published":"2024-08-01T09:42:44Z","title":"MPT-PAR:Mix-Parameters Transformer for Panoramic Activity Recognition","summary":"  The objective of the panoramic activity recognition task is to identify\nbehaviors at various granularities within crowded and complex environments,\nencompassing individual actions, social group activities, and global\nactivities. Existing methods generally use either parameter-independent modules\nto capture task-specific features or parameter-sharing modules to obtain common\nfeatures across all tasks. However, there is often a strong interrelatedness\nand complementary effect between tasks of different granularities that previous\nmethods have yet to notice. In this paper, we propose a model called MPT-PAR\nthat considers both the unique characteristics of each task and the synergies\nbetween different tasks simultaneously, thereby maximizing the utilization of\nfeatures across multi-granularity activity recognition. Furthermore, we\nemphasize the significance of temporal and spatial information by introducing a\nspatio-temporal relation-enhanced module and a scene representation learning\nmodule, which integrate the the spatio-temporal context of action and global\nscene into the feature map of each granularity. Our method achieved an overall\nF1 score of 47.5\\% on the JRDB-PAR dataset, significantly outperforming all the\nstate-of-the-art methods.\n","authors":["Wenqing Gan","Yan Sun","Feiran Liu","Xiangfeng Luo"],"pdf_url":"https://arxiv.org/pdf/2408.00420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00418v1","updated":"2024-08-01T09:39:27Z","published":"2024-08-01T09:39:27Z","title":"Towards Reliable Advertising Image Generation Using Human Feedback","summary":"  In the e-commerce realm, compelling advertising images are pivotal for\nattracting customer attention. While generative models automate image\ngeneration, they often produce substandard images that may mislead customers\nand require significant labor costs to inspect. This paper delves into\nincreasing the rate of available generated images. We first introduce a\nmulti-modal Reliable Feedback Network (RFNet) to automatically inspect the\ngenerated images. Combining the RFNet into a recurrent process, Recurrent\nGeneration, results in a higher number of available advertising images. To\nfurther enhance production efficiency, we fine-tune diffusion models with an\ninnovative Consistent Condition regularization utilizing the feedback from\nRFNet (RFFT). This results in a remarkable increase in the available rate of\ngenerated images, reducing the number of attempts in Recurrent Generation, and\nproviding a highly efficient production process without sacrificing visual\nappeal. We also construct a Reliable Feedback 1 Million (RF1M) dataset which\ncomprises over one million generated advertising images annotated by human,\nwhich helps to train RFNet to accurately assess the availability of generated\nimages and faithfully reflect the human feedback. Generally speaking, our\napproach offers a reliable solution for advertising image generation.\n","authors":["Zhenbang Du","Wei Feng","Haohan Wang","Yaoyu Li","Jingsen Wang","Jian Li","Zheng Zhang","Jingjing Lv","Xin Zhu","Junsheng Jin","Junjie Shen","Zhangang Lin","Jingping Shao"],"pdf_url":"https://arxiv.org/pdf/2408.00418v1.pdf","comment":"ECCV2024"},{"id":"http://arxiv.org/abs/2408.00415v1","updated":"2024-08-01T09:32:01Z","published":"2024-08-01T09:32:01Z","title":"DriveArena: A Closed-loop Generative Simulation Platform for Autonomous\n  Driving","summary":"  This paper presented DriveArena, the first high-fidelity closed-loop\nsimulation system designed for driving agents navigating in real scenarios.\nDriveArena features a flexible, modular architecture, allowing for the seamless\ninterchange of its core components: Traffic Manager, a traffic simulator\ncapable of generating realistic traffic flow on any worldwide street map, and\nWorld Dreamer, a high-fidelity conditional generative model with infinite\nautoregression. This powerful synergy empowers any driving agent capable of\nprocessing real-world images to navigate in DriveArena's simulated environment.\nThe agent perceives its surroundings through images generated by World Dreamer\nand output trajectories. These trajectories are fed into Traffic Manager,\nachieving realistic interactions with other vehicles and producing a new scene\nlayout. Finally, the latest scene layout is relayed back into World Dreamer,\nperpetuating the simulation cycle. This iterative process fosters closed-loop\nexploration within a highly realistic environment, providing a valuable\nplatform for developing and evaluating driving agents across diverse and\nchallenging scenarios. DriveArena signifies a substantial leap forward in\nleveraging generative image data for the driving simulation platform, opening\ninsights for closed-loop autonomous driving. Code will be available soon on\nGitHub: https://github.com/PJLab-ADG/DriveArena\n","authors":["Xuemeng Yang","Licheng Wen","Yukai Ma","Jianbiao Mei","Xin Li","Tiantian Wei","Wenjie Lei","Daocheng Fu","Pinlong Cai","Min Dou","Botian Shi","Liang He","Yong Liu","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2408.00415v1.pdf","comment":"19 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.00388v1","updated":"2024-08-01T08:57:47Z","published":"2024-08-01T08:57:47Z","title":"Deepfake Media Forensics: State of the Art and Challenges Ahead","summary":"  AI-generated synthetic media, also called Deepfakes, have significantly\ninfluenced so many domains, from entertainment to cybersecurity. Generative\nAdversarial Networks (GANs) and Diffusion Models (DMs) are the main frameworks\nused to create Deepfakes, producing highly realistic yet fabricated content.\nWhile these technologies open up new creative possibilities, they also bring\nsubstantial ethical and security risks due to their potential misuse. The rise\nof such advanced media has led to the development of a cognitive bias known as\nImpostor Bias, where individuals doubt the authenticity of multimedia due to\nthe awareness of AI's capabilities. As a result, Deepfake detection has become\na vital area of research, focusing on identifying subtle inconsistencies and\nartifacts with machine learning techniques, especially Convolutional Neural\nNetworks (CNNs). Research in forensic Deepfake technology encompasses five main\nareas: detection, attribution and recognition, passive authentication,\ndetection in realistic scenarios, and active authentication. Each area tackles\nspecific challenges, from tracing the origins of synthetic media and examining\nits inherent characteristics for authenticity. This paper reviews the primary\nalgorithms that address these challenges, examining their advantages,\nlimitations, and future prospects.\n","authors":["Irene Amerini","Mauro Barni","Sebastiano Battiato","Paolo Bestagini","Giulia Boato","Tania Sari Bonaventura","Vittoria Bruni","Roberto Caldelli","Francesco De Natale","Rocco De Nicola","Luca Guarnera","Sara Mandelli","Gian Luca Marcialis","Marco Micheletto","Andrea Montibeller","Giulia Orru'","Alessandro Ortis","Pericle Perazzo","Davide Salvi","Stefano Tubaro","Claudia Melis Tonti","Massimo Villari","Domenico Vitulano"],"pdf_url":"https://arxiv.org/pdf/2408.00388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00380v1","updated":"2024-08-01T08:41:13Z","published":"2024-08-01T08:41:13Z","title":"Enhancing Whole Slide Pathology Foundation Models through Stain\n  Normalization","summary":"  Recent advancements in digital pathology have led to the development of\nnumerous foundational models that utilize self-supervised learning on patches\nextracted from gigapixel whole slide images (WSIs). While this approach\nleverages vast amounts of unlabeled data, we have discovered a significant\nissue: features extracted from these self-supervised models tend to cluster by\nindividual WSIs, a phenomenon we term WSI-specific feature collapse. This\nproblem can potentially limit the model's generalization ability and\nperformance on various downstream tasks. To address this issue, we introduce\nStain Normalized Pathology Foundational Model, a novel foundational model\ntrained on patches that have undergone stain normalization. Stain normalization\nhelps reduce color variability arising from different laboratories and\nscanners, enabling the model to learn more consistent features. Stain\nNormalized Pathology Foundational Model is trained using 285,153,903 patches\nextracted from a total of 34,795 WSIs, combining data from The Cancer Genome\nAtlas (TCGA) and the Genotype-Tissue Expression (GTEx) project. Our experiments\ndemonstrate that Stain Normalized Pathology Foundational Model significantly\nmitigates the feature collapse problem, indicating that the model has learned\nmore generalized features rather than overfitting to individual WSI\ncharacteristics. We compared Stain Normalized Pathology Foundational Model with\nstate-of-the-art models across six downstream task datasets, and our results\nshow that \\name{} achieves excellent performance relative to the number of WSIs\nused and the model's parameter count. This suggests that the application of\nstain normalization has substantially improved the model's efficiency and\ngeneralization capabilities.\n","authors":["Juseung Yun","Yi Hu","Jinhyung Kim","Jongseong Jang","Soonyoung Lee"],"pdf_url":"https://arxiv.org/pdf/2408.00380v1.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2408.00374v1","updated":"2024-08-01T08:32:03Z","published":"2024-08-01T08:32:03Z","title":"Conformal Trajectory Prediction with Multi-View Data Integration in\n  Cooperative Driving","summary":"  Current research on trajectory prediction primarily relies on data collected\nby onboard sensors of an ego vehicle. With the rapid advancement in connected\ntechnologies, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure\n(V2I) communication, valuable information from alternate views becomes\naccessible via wireless networks. The integration of information from\nalternative views has the potential to overcome the inherent limitations\nassociated with a single viewpoint, such as occlusions and limited field of\nview. In this work, we introduce V2INet, a novel trajectory prediction\nframework designed to model multi-view data by extending existing single-view\nmodels. Unlike previous approaches where the multi-view data is manually fused\nor formulated as a separate training stage, our model supports end-to-end\ntraining, enhancing both flexibility and performance. Moreover, the predicted\nmultimodal trajectories are calibrated by a post-hoc conformal prediction\nmodule to get valid and efficient confidence regions. We evaluated the entire\nframework using the real-world V2I dataset V2X-Seq. Our results demonstrate\nsuperior performance in terms of Final Displacement Error (FDE) and Miss Rate\n(MR) using a single GPU. The code is publicly available at:\n\\url{https://github.com/xichennn/V2I_trajectory_prediction}.\n","authors":["Xi Chen","Rahul Bhadani","Larry Head"],"pdf_url":"https://arxiv.org/pdf/2408.00374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00372v1","updated":"2024-08-01T08:29:42Z","published":"2024-08-01T08:29:42Z","title":"Few-shot Defect Image Generation based on Consistency Modeling","summary":"  Image generation can solve insufficient labeled data issues in defect\ndetection. Most defect generation methods are only trained on a single product\nwithout considering the consistencies among multiple products, leading to poor\nquality and diversity of generated results. To address these issues, we propose\nDefectDiffu, a novel text-guided diffusion method to model both intra-product\nbackground consistency and inter-product defect consistency across multiple\nproducts and modulate the consistency perturbation directions to control\nproduct type and defect strength, achieving diversified defect image\ngeneration. Firstly, we leverage a text encoder to separately provide\nconsistency prompts for background, defect, and fusion parts of the\ndisentangled integrated architecture, thereby disentangling defects and normal\nbackgrounds. Secondly, we propose the double-free strategy to generate defect\nimages through two-stage perturbation of consistency direction, thereby\ncontrolling product type and defect strength by adjusting the perturbation\nscale. Besides, DefectDiffu can generate defect mask annotations utilizing\ncross-attention maps from the defect part. Finally, to improve the generation\nquality of small defects and masks, we propose the adaptive attention-enhance\nloss to increase the attention to defects. Experimental results demonstrate\nthat DefectDiffu surpasses state-of-the-art methods in terms of generation\nquality and diversity, thus effectively improving downstream defection\nperformance. Moreover, defect perturbation directions can be transferred among\nvarious products to achieve zero-shot defect generation, which is highly\nbeneficial for addressing insufficient data issues. The code are available at\nhttps://github.com/FFDD-diffusion/DefectDiffu.\n","authors":["Qingfeng Shi","Jing Wei","Fei Shen","Zhengtao Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00365v1","updated":"2024-08-01T08:10:32Z","published":"2024-08-01T08:10:32Z","title":"Multimodal Fusion and Coherence Modeling for Video Topic Segmentation","summary":"  The video topic segmentation (VTS) task segments videos into intelligible,\nnon-overlapping topics, facilitating efficient comprehension of video content\nand quick access to specific content. VTS is also critical to various\ndownstream video understanding tasks. Traditional VTS methods using shallow\nfeatures or unsupervised approaches struggle to accurately discern the nuances\nof topical transitions. Recently, supervised approaches have achieved superior\nperformance on video action or scene segmentation over unsupervised approaches.\nIn this work, we improve supervised VTS by thoroughly exploring multimodal\nfusion and multimodal coherence modeling. Specifically, (1) we enhance\nmultimodal fusion by exploring different architectures using cross-attention\nand mixture of experts. (2) To generally strengthen multimodality alignment and\nfusion, we pre-train and fine-tune the model with multimodal contrastive\nlearning. (3) We propose a new pre-training task tailored for the VTS task, and\na novel fine-tuning task for enhancing multimodal coherence modeling for VTS.\nWe evaluate the proposed approaches on educational videos, in the form of\nlectures, due to the vital role of topic segmentation of educational videos in\nboosting learning experiences. Additionally, we introduce a large-scale Chinese\nlecture video dataset to augment the existing English corpus, promoting further\nresearch in VTS. Experiments on both English and Chinese lecture datasets\ndemonstrate that our model achieves superior VTS performance compared to\ncompetitive unsupervised and supervised baselines.\n","authors":["Hai Yu","Chong Deng","Qinglin Zhang","Jiaqing Liu","Qian Chen","Wen Wang"],"pdf_url":"https://arxiv.org/pdf/2408.00365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00361v1","updated":"2024-08-01T08:03:13Z","published":"2024-08-01T08:03:13Z","title":"High-Precision Self-Supervised Monocular Depth Estimation with\n  Rich-Resource Prior","summary":"  In the area of self-supervised monocular depth estimation, models that\nutilize rich-resource inputs, such as high-resolution and multi-frame inputs,\ntypically achieve better performance than models that use ordinary single image\ninput. However, these rich-resource inputs may not always be available,\nlimiting the applicability of these methods in general scenarios. In this\npaper, we propose Rich-resource Prior Depth estimator (RPrDepth), which only\nrequires single input image during the inference phase but can still produce\nhighly accurate depth estimations comparable to rich resource based methods.\nSpecifically, we treat rich-resource data as prior information and extract\nfeatures from it as reference features in an offline manner. When estimating\nthe depth for a single-image image, we search for similar pixels from the\nrich-resource features and use them as prior information to estimate the depth.\nExperimental results demonstrate that our model outperform other single-image\nmodel and can achieve comparable or even better performance than models with\nrich-resource inputs, only using low-resolution single-image input.\n","authors":["Wencheng Han","Jianbing Shen"],"pdf_url":"https://arxiv.org/pdf/2408.00361v1.pdf","comment":"ECCV2024"},{"id":"http://arxiv.org/abs/2408.00355v1","updated":"2024-08-01T07:52:07Z","published":"2024-08-01T07:52:07Z","title":"DNTextSpotter: Arbitrary-Shaped Scene Text Spotting via Improved\n  Denoising Training","summary":"  More and more end-to-end text spotting methods based on Transformer\narchitecture have demonstrated superior performance. These methods utilize a\nbipartite graph matching algorithm to perform one-to-one optimal matching\nbetween predicted objects and actual objects. However, the instability of\nbipartite graph matching can lead to inconsistent optimization targets, thereby\naffecting the training performance of the model. Existing literature applies\ndenoising training to solve the problem of bipartite graph matching instability\nin object detection tasks. Unfortunately, this denoising training method cannot\nbe directly applied to text spotting tasks, as these tasks need to perform\nirregular shape detection tasks and more complex text recognition tasks than\nclassification. To address this issue, we propose a novel denoising training\nmethod (DNTextSpotter) for arbitrary-shaped text spotting. Specifically, we\ndecompose the queries of the denoising part into noised positional queries and\nnoised content queries. We use the four Bezier control points of the Bezier\ncenter curve to generate the noised positional queries. For the noised content\nqueries, considering that the output of the text in a fixed positional order is\nnot conducive to aligning position with content, we employ a masked character\nsliding method to initialize noised content queries, thereby assisting in the\nalignment of text content and position. To improve the model's perception of\nthe background, we further utilize an additional loss function for background\ncharacters classification in the denoising training part.Although DNTextSpotter\nis conceptually simple, it outperforms the state-of-the-art methods on four\nbenchmarks (Total-Text, SCUT-CTW1500, ICDAR15, and Inverse-Text), especially\nyielding an improvement of 11.3% against the best approach in Inverse-Text\ndataset.\n","authors":["Yu Xie","Qian Qiao","Jun Gao","Tianxiang Wu","Shaoyao Huang","Jiaqing Fan","Ziqiang Cao","Zili Wang","Yue Zhang","Jielei Zhang","Huyang Sun"],"pdf_url":"https://arxiv.org/pdf/2408.00355v1.pdf","comment":"Accepted by ACMMM2024"},{"id":"http://arxiv.org/abs/2408.00352v1","updated":"2024-08-01T07:44:11Z","published":"2024-08-01T07:44:11Z","title":"Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion","summary":"  Human motion generation driven by deep generative models has enabled\ncompelling applications, but the ability of text-to-motion (T2M) models to\nproduce realistic motions from text prompts raises security concerns if\nexploited maliciously. Despite growing interest in T2M, few methods focus on\nsafeguarding these models against adversarial attacks, with existing work on\ntext-to-image models proving insufficient for the unique motion domain. In the\npaper, we propose ALERT-Motion, an autonomous framework leveraging large\nlanguage models (LLMs) to craft targeted adversarial attacks against black-box\nT2M models. Unlike prior methods modifying prompts through predefined rules,\nALERT-Motion uses LLMs' knowledge of human motion to autonomously generate\nsubtle yet powerful adversarial text descriptions. It comprises two key\nmodules: an adaptive dispatching module that constructs an LLM-based agent to\niteratively refine and search for adversarial prompts; and a multimodal\ninformation contrastive module that extracts semantically relevant motion\ninformation to guide the agent's search. Through this LLM-driven approach,\nALERT-Motion crafts adversarial prompts querying victim models to produce\noutputs closely matching targeted motions, while avoiding obvious\nperturbations. Evaluations across popular T2M models demonstrate ALERT-Motion's\nsuperiority over previous methods, achieving higher attack success rates with\nstealthier adversarial prompts. This pioneering work on T2M adversarial attacks\nhighlights the urgency of developing defensive measures as motion generation\ntechnology advances, urging further research into safe and responsible\ndeployment.\n","authors":["Honglei Miao","Fan Ma","Ruijie Quan","Kun Zhan","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2408.00352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00351v1","updated":"2024-08-01T07:42:45Z","published":"2024-08-01T07:42:45Z","title":"Hierarchically Structured Neural Bones for Reconstructing Animatable\n  Objects from Casual Videos","summary":"  We propose a new framework for creating and easily manipulating 3D models of\narbitrary objects using casually captured videos. Our core ingredient is a\nnovel hierarchy deformation model, which captures motions of objects with a\ntree-structured bones. Our hierarchy system decomposes motions based on the\ngranularity and reveals the correlations between parts without exploiting any\nprior structural knowledge. We further propose to regularize the bones to be\npositioned at the basis of motions, centers of parts, sufficiently covering\nrelated surfaces of the part. This is achieved by our bone occupancy function,\nwhich identifies whether a given 3D point is placed within the bone. Coupling\nthe proposed components, our framework offers several clear advantages: (1)\nusers can obtain animatable 3D models of the arbitrary objects in improved\nquality from their casual videos, (2) users can manipulate 3D models in an\nintuitive manner with minimal costs, and (3) users can interactively add or\ndelete control points as necessary. The experimental results demonstrate the\nefficacy of our framework on diverse instances, in reconstruction quality,\ninterpretability and easier manipulation. Our code is available at\nhttps://github.com/subin6/HSNB.\n","authors":["Subin Jeon","In Cho","Minsu Kim","Woong Oh Cho","Seon Joo Kim"],"pdf_url":"https://arxiv.org/pdf/2408.00351v1.pdf","comment":"ECCV 2024 accepted"},{"id":"http://arxiv.org/abs/2408.00350v1","updated":"2024-08-01T07:40:00Z","published":"2024-08-01T07:40:00Z","title":"A Simple Background Augmentation Method for Object Detection with\n  Diffusion Model","summary":"  In computer vision, it is well-known that a lack of data diversity will\nimpair model performance. In this study, we address the challenges of enhancing\nthe dataset diversity problem in order to benefit various downstream tasks such\nas object detection and instance segmentation. We propose a simple yet\neffective data augmentation approach by leveraging advancements in generative\nmodels, specifically text-to-image synthesis technologies like Stable\nDiffusion. Our method focuses on generating variations of labeled real images,\nutilizing generative object and background augmentation via inpainting to\naugment existing training data without the need for additional annotations. We\nfind that background augmentation, in particular, significantly improves the\nmodels' robustness and generalization capabilities. We also investigate how to\nadjust the prompt and mask to ensure the generated content comply with the\nexisting annotations. The efficacy of our augmentation techniques is validated\nthrough comprehensive evaluations of the COCO dataset and several other key\nobject detection benchmarks, demonstrating notable enhancements in model\nperformance across diverse scenarios. This approach offers a promising solution\nto the challenges of dataset enhancement, contributing to the development of\nmore accurate and robust computer vision models.\n","authors":["Yuhang Li","Xin Dong","Chen Chen","Weiming Zhuang","Lingjuan Lyu"],"pdf_url":"https://arxiv.org/pdf/2408.00350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00347v1","updated":"2024-08-01T07:35:54Z","published":"2024-08-01T07:35:54Z","title":"Advancing Medical Image Segmentation: Morphology-Driven Learning with\n  Diffusion Transformer","summary":"  Understanding the morphological structure of medical images and precisely\nsegmenting the region of interest or abnormality is an important task that can\nassist in diagnosis. However, the unique properties of medical imaging make\nclear segmentation difficult, and the high cost and time-consuming task of\nlabeling leads to a coarse-grained representation of ground truth. Facing with\nthese problems, we propose a novel Diffusion Transformer Segmentation (DTS)\nmodel for robust segmentation in the presence of noise. We propose an\nalternative to the dominant Denoising U-Net encoder through experiments\napplying a transformer architecture, which captures global dependency through\nself-attention. Additionally, we propose k-neighbor label smoothing, reverse\nboundary attention, and self-supervised learning with morphology-driven\nlearning to improve the ability to identify complex structures. Our model,\nwhich analyzes the morphological representation of images, shows better results\nthan the previous models in various medical imaging modalities, including CT,\nMRI, and lesion images.\n","authors":["Sungmin Kang","Jaeha Song","Jihie Kim"],"pdf_url":"https://arxiv.org/pdf/2408.00347v1.pdf","comment":"Accepted in BMVC 2024"},{"id":"http://arxiv.org/abs/2408.00343v1","updated":"2024-08-01T07:27:54Z","published":"2024-08-01T07:27:54Z","title":"IN-Sight: Interactive Navigation through Sight","summary":"  Current visual navigation systems often treat the environment as static,\nlacking the ability to adaptively interact with obstacles. This limitation\nleads to navigation failure when encountering unavoidable obstructions. In\nresponse, we introduce IN-Sight, a novel approach to self-supervised path\nplanning, enabling more effective navigation strategies through interaction\nwith obstacles. Utilizing RGB-D observations, IN-Sight calculates\ntraversability scores and incorporates them into a semantic map, facilitating\nlong-range path planning in complex, maze-like environments. To precisely\nnavigate around obstacles, IN-Sight employs a local planner, trained\nimperatively on a differentiable costmap using representation learning\ntechniques. The entire framework undergoes end-to-end training within the\nstate-of-the-art photorealistic Intel SPEAR Simulator. We validate the\neffectiveness of IN-Sight through extensive benchmarking in a variety of\nsimulated scenarios and ablation studies. Moreover, we demonstrate the system's\nreal-world applicability with zero-shot sim-to-real transfer, deploying our\nplanner on the legged robot platform ANYmal, showcasing its practical potential\nfor interactive navigation in real environments.\n","authors":["Philipp Schoch","Fan Yang","Yuntao Ma","Stefan Leutenegger","Marco Hutter","Quentin Leboute"],"pdf_url":"https://arxiv.org/pdf/2408.00343v1.pdf","comment":"The 2024 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS 2024)"},{"id":"http://arxiv.org/abs/2408.00337v1","updated":"2024-08-01T07:17:10Z","published":"2024-08-01T07:17:10Z","title":"DistillGrasp: Integrating Features Correlation with Knowledge\n  Distillation for Depth Completion of Transparent Objects","summary":"  Due to the visual properties of reflection and refraction, RGB-D cameras\ncannot accurately capture the depth of transparent objects, leading to\nincomplete depth maps. To fill in the missing points, recent studies tend to\nexplore new visual features and design complex networks to reconstruct the\ndepth, however, these approaches tremendously increase computation, and the\ncorrelation of different visual features remains a problem. To this end, we\npropose an efficient depth completion network named DistillGrasp which\ndistillates knowledge from the teacher branch to the student branch.\nSpecifically, in the teacher branch, we design a position correlation block\n(PCB) that leverages RGB images as the query and key to search for the\ncorresponding values, guiding the model to establish correct correspondence\nbetween two features and transfer it to the transparent areas. For the student\nbranch, we propose a consistent feature correlation module (CFCM) that retains\nthe reliable regions of RGB images and depth maps respectively according to the\nconsistency and adopts a CNN to capture the pairwise relationship for depth\ncompletion. To avoid the student branch only learning regional features from\nthe teacher branch, we devise a distillation loss that not only considers the\ndistance loss but also the object structure and edge information. Extensive\nexperiments conducted on the ClearGrasp dataset manifest that our teacher\nnetwork outperforms state-of-the-art methods in terms of accuracy and\ngeneralization, and the student network achieves competitive results with a\nhigher speed of 48 FPS. In addition, the significant improvement in a\nreal-world robotic grasping system illustrates the effectiveness and robustness\nof our proposed system.\n","authors":["Yiheng Huang","Junhong Chen","Nick Michiels","Muhammad Asim","Luc Claesen","Wenyin Liu"],"pdf_url":"https://arxiv.org/pdf/2408.00337v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.00332v1","updated":"2024-08-01T07:10:45Z","published":"2024-08-01T07:10:45Z","title":"Vision-based Wearable Steering Assistance for People with Impaired\n  Vision in Jogging","summary":"  Outdoor sports pose a challenge for people with impaired vision. The demand\nfor higher-speed mobility inspired us to develop a vision-based wearable\nsteering assistance. To ensure broad applicability, we focused on a\nrepresentative sports environment, the athletics track. Our efforts centered on\nimproving the speed and accuracy of perception, enhancing planning adaptability\nfor the real world, and providing swift and safe assistance for people with\nimpaired vision. In perception, we engineered a lightweight multitask network\ncapable of simultaneously detecting track lines and obstacles. Additionally,\ndue to the limitations of existing datasets for supporting multi-task detection\nin athletics tracks, we diligently collected and annotated a new dataset (MAT)\ncontaining 1000 images. In planning, we integrated the methods of sampling and\nspline curves, addressing the planning challenges of curves. Meanwhile, we\nutilized the positions of the track lines and obstacles as constraints to guide\npeople with impaired vision safely along the current track. Our system is\ndeployed on an embedded device, Jetson Orin NX. Through outdoor experiments, it\ndemonstrated adaptability in different sports scenarios, assisting users in\nachieving free movement of 400-meter at an average speed of 1.34 m/s, meeting\nthe level of normal people in jogging. Our MAT dataset is publicly available\nfrom https://github.com/snoopy-l/MAT\n","authors":["Xiaotong Liu","Binglu Wang","Zhijun Li"],"pdf_url":"https://arxiv.org/pdf/2408.00332v1.pdf","comment":"Accepted to ICRA 2024"},{"id":"http://arxiv.org/abs/2408.00331v1","updated":"2024-08-01T07:08:11Z","published":"2024-08-01T07:08:11Z","title":"DECIDER: Leveraging Foundation Model Priors for Improved Model Failure\n  Detection and Explanation","summary":"  Reliably detecting when a deployed machine learning model is likely to fail\non a given input is crucial for ensuring safe operation. In this work, we\npropose DECIDER (Debiasing Classifiers to Identify Errors Reliably), a novel\napproach that leverages priors from large language models (LLMs) and\nvision-language models (VLMs) to detect failures in image classification\nmodels. DECIDER utilizes LLMs to specify task-relevant core attributes and\nconstructs a ``debiased'' version of the classifier by aligning its visual\nfeatures to these core attributes using a VLM, and detects potential failure by\nmeasuring disagreement between the original and debiased models. In addition to\nproactively identifying samples on which the model would fail, DECIDER also\nprovides human-interpretable explanations for failure through a novel\nattribute-ablation strategy. Through extensive experiments across diverse\nbenchmarks spanning subpopulation shifts (spurious correlations, class\nimbalance) and covariate shifts (synthetic corruptions, domain shifts), DECIDER\nconsistently achieves state-of-the-art failure detection performance,\nsignificantly outperforming baselines in terms of the overall Matthews\ncorrelation coefficient as well as failure and success recall. Our codes can be\naccessed at~\\url{https://github.com/kowshikthopalli/DECIDER/}\n","authors":["Rakshith Subramanyam","Kowshik Thopalli","Vivek Narayanaswamy","Jayaraman J. Thiagarajan"],"pdf_url":"https://arxiv.org/pdf/2408.00331v1.pdf","comment":"Accepted at ECCV (European Conference on Computer Vision) 2024"},{"id":"http://arxiv.org/abs/2408.00315v1","updated":"2024-08-01T06:26:05Z","published":"2024-08-01T06:26:05Z","title":"ADBM: Adversarial diffusion bridge model for reliable adversarial\n  purification","summary":"  Recently Diffusion-based Purification (DiffPure) has been recognized as an\neffective defense method against adversarial examples. However, we find\nDiffPure which directly employs the original pre-trained diffusion models for\nadversarial purification, to be suboptimal. This is due to an inherent\ntrade-off between noise purification performance and data recovery quality.\nAdditionally, the reliability of existing evaluations for DiffPure is\nquestionable, as they rely on weak adaptive attacks. In this work, we propose a\nnovel Adversarial Diffusion Bridge Model, termed ADBM. ADBM directly constructs\na reverse bridge from the diffused adversarial data back to its original clean\nexamples, enhancing the purification capabilities of the original diffusion\nmodels. Through theoretical analysis and experimental validation across various\nscenarios, ADBM has proven to be a superior and robust defense mechanism,\noffering significant promise for practical applications.\n","authors":["Xiao Li","Wenxuan Sun","Huanran Chen","Qiongxiu Li","Yining Liu","Yingzhe He","Jie Shi","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2408.00315v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2408.00311v1","updated":"2024-08-01T06:14:37Z","published":"2024-08-01T06:14:37Z","title":"Translating Imaging to Genomics: Leveraging Transformers for Predictive\n  Modeling","summary":"  In this study, we present a novel approach for predicting genomic information\nfrom medical imaging modalities using a transformer-based model. We aim to\nbridge the gap between imaging and genomics data by leveraging transformer\nnetworks, allowing for accurate genomic profile predictions from CT/MRI images.\nPresently most studies rely on the use of whole slide images (WSI) for the\nassociation, which are obtained via invasive methodologies. We propose using\nonly available CT/MRI images to predict genomic sequences. Our transformer\nbased approach is able to efficiently generate associations between multiple\nsequences based on CT/MRI images alone. This work paves the way for the use of\nnon-invasive imaging modalities for precise and personalized healthcare,\nallowing for a better understanding of diseases and treatment.\n","authors":["Aiman Farooq","Deepak Mishra","Santanu Chaudhury"],"pdf_url":"https://arxiv.org/pdf/2408.00311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00303v1","updated":"2024-08-01T06:02:59Z","published":"2024-08-01T06:02:59Z","title":"Neural Octahedral Field: Octahedral prior for simultaneous smoothing and\n  sharp edge regularization","summary":"  Neural implicit representation, the parameterization of distance function as\na coordinate neural field, has emerged as a promising lead in tackling surface\nreconstruction from unoriented point clouds. To enforce consistent orientation,\nexisting methods focus on regularizing the gradient of the distance function,\nsuch as constraining it to be of the unit norm, minimizing its divergence, or\naligning it with the eigenvector of Hessian that corresponds to zero\neigenvalue. However, under the presence of large scanning noise, they tend to\neither overfit the noise input or produce an excessively smooth reconstruction.\nIn this work, we propose to guide the surface reconstruction under a new\nvariant of neural field, the octahedral field, leveraging the spherical\nharmonics representation of octahedral frames originated in the hexahedral\nmeshing. Such field automatically snaps to geometry features when constrained\nto be smooth, and naturally preserves sharp angles when interpolated over\ncreases. By simultaneously fitting and smoothing the octahedral field alongside\nthe implicit geometry, it behaves analogously to bilateral filtering, resulting\nin smooth reconstruction while preserving sharp edges. Despite being operated\npurely pointwise, our method outperforms various traditional and neural\napproaches across extensive experiments, and is very competitive with methods\nthat require normal and data priors. Our full implementation is available at:\nhttps://github.com/Ankbzpx/frame-field.\n","authors":["Ruichen Zheng","Tao Yu"],"pdf_url":"https://arxiv.org/pdf/2408.00303v1.pdf","comment":"project page: https://github.com/Ankbzpx/frame-field"},{"id":"http://arxiv.org/abs/2408.00300v1","updated":"2024-08-01T05:56:34Z","published":"2024-08-01T05:56:34Z","title":"Towards Flexible Evaluation for Generative Visual Question Answering","summary":"  Throughout rapid development of multimodal large language models, a crucial\ningredient is a fair and accurate evaluation of their multimodal comprehension\nabilities. Although Visual Question Answering (VQA) could serve as a developed\ntest field, limitations of VQA evaluation, like the inflexible pattern of Exact\nMatch, have hindered MLLMs from demonstrating their real capability and\ndiscourage rich responses. Therefore, this paper proposes the use of\nsemantics-based evaluators for assessing unconstrained open-ended responses on\nVQA datasets. As characteristics of VQA have made such evaluation significantly\ndifferent than the traditional Semantic Textual Similarity (STS) task, to\nsystematically analyze the behaviour and compare the performance of various\nevaluators including LLM-based ones, we proposes three key properties, i.e.,\nAlignment, Consistency and Generalization, and a corresponding dataset\nAssessing VQA Evaluators (AVE) to facilitate analysis. In addition, this paper\nproposes a Semantically Flexible VQA Evaluator (SFVE) with meticulous design\nbased on the unique features of VQA evaluation. Experimental results verify the\nfeasibility of model-based VQA evaluation and effectiveness of the proposed\nevaluator that surpasses existing semantic evaluators by a large margin. The\nproposed training scheme generalizes to both the BERT-like encoders and\ndecoder-only LLM.\n","authors":["Huishan Ji","Qingyi Si","Zheng Lin","Weiping Wang"],"pdf_url":"https://arxiv.org/pdf/2408.00300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00298v1","updated":"2024-08-01T05:47:04Z","published":"2024-08-01T05:47:04Z","title":"Tails Tell Tales: Chapter-Wide Manga Transcriptions with Character Names","summary":"  Enabling engagement of manga by visually impaired individuals presents a\nsignificant challenge due to its inherently visual nature. With the goal of\nfostering accessibility, this paper aims to generate a dialogue transcript of a\ncomplete manga chapter, entirely automatically, with a particular emphasis on\nensuring narrative consistency. This entails identifying (i) what is being\nsaid, i.e., detecting the texts on each page and classifying them into\nessential vs non-essential, and (ii) who is saying it, i.e., attributing each\ndialogue to its speaker, while ensuring the same characters are named\nconsistently throughout the chapter.\n  To this end, we introduce: (i) Magiv2, a model that is capable of generating\nhigh-quality chapter-wide manga transcripts with named characters and\nsignificantly higher precision in speaker diarisation over prior works; (ii) an\nextension of the PopManga evaluation dataset, which now includes annotations\nfor speech-bubble tail boxes, associations of text to corresponding tails,\nclassifications of text as essential or non-essential, and the identity for\neach character box; and (iii) a new character bank dataset, which comprises\nover 11K characters from 76 manga series, featuring 11.5K exemplar character\nimages in total, as well as a list of chapters in which they appear. The code,\ntrained model, and both datasets can be found at:\nhttps://github.com/ragavsachdeva/magi\n","authors":["Ragav Sachdeva","Gyungin Shin","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2408.00298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00297v1","updated":"2024-08-01T05:46:57Z","published":"2024-08-01T05:46:57Z","title":"EmoTalk3D: High-Fidelity Free-View Synthesis of Emotional 3D Talking\n  Head","summary":"  We present a novel approach for synthesizing 3D talking heads with\ncontrollable emotion, featuring enhanced lip synchronization and rendering\nquality. Despite significant progress in the field, prior methods still suffer\nfrom multi-view consistency and a lack of emotional expressiveness. To address\nthese issues, we collect EmoTalk3D dataset with calibrated multi-view videos,\nemotional annotations, and per-frame 3D geometry. By training on the EmoTalk3D\ndataset, we propose a \\textit{`Speech-to-Geometry-to-Appearance'} mapping\nframework that first predicts faithful 3D geometry sequence from the audio\nfeatures, then the appearance of a 3D talking head represented by 4D Gaussians\nis synthesized from the predicted geometry. The appearance is further\ndisentangled into canonical and dynamic Gaussians, learned from multi-view\nvideos, and fused to render free-view talking head animation. Moreover, our\nmodel enables controllable emotion in the generated talking heads and can be\nrendered in wide-range views. Our method exhibits improved rendering quality\nand stability in lip motion generation while capturing dynamic facial details\nsuch as wrinkles and subtle expressions. Experiments demonstrate the\neffectiveness of our approach in generating high-fidelity and\nemotion-controllable 3D talking heads. The code and EmoTalk3D dataset are\nreleased at https://nju-3dv.github.io/projects/EmoTalk3D.\n","authors":["Qianyun He","Xinya Ji","Yicheng Gong","Yuanxun Lu","Zhengyu Diao","Linjia Huang","Yao Yao","Siyu Zhu","Zhan Ma","Songcen Xu","Xiaofei Wu","Zixiao Zhang","Xun Cao","Hao Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.00297v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2408.00296v1","updated":"2024-08-01T05:46:06Z","published":"2024-08-01T05:46:06Z","title":"Head360: Learning a Parametric 3D Full-Head for Free-View Synthesis in\n  360°","summary":"  Creating a 360{\\deg} parametric model of a human head is a very challenging\ntask. While recent advancements have demonstrated the efficacy of leveraging\nsynthetic data for building such parametric head models, their performance\nremains inadequate in crucial areas such as expression-driven animation,\nhairstyle editing, and text-based modifications. In this paper, we build a\ndataset of artist-designed high-fidelity human heads and propose to create a\nnovel parametric 360{\\deg} renderable parametric head model from it. Our scheme\ndecouples the facial motion/shape and facial appearance, which are represented\nby a classic parametric 3D mesh model and an attached neural texture,\nrespectively. We further propose a training method for decompositing hairstyle\nand facial appearance, allowing free-swapping of the hairstyle. A novel\ninversion fitting method is presented based on single image input with high\ngeneralization and fidelity. To the best of our knowledge, our model is the\nfirst parametric 3D full-head that achieves 360{\\deg} free-view synthesis,\nimage-based fitting, appearance editing, and animation within a single model.\nExperiments show that facial motions and appearances are well disentangled in\nthe parametric space, leading to SOTA performance in rendering and animating\nquality. The code and SynHead100 dataset are released at\nhttps://nju-3dv.github.io/projects/Head360.\n","authors":["Yuxiao He","Yiyu Zhuang","Yanwen Wang","Yao Yao","Siyu Zhu","Xiaoyu Li","Qi Zhang","Xun Cao","Hao Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.00296v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2408.00294v1","updated":"2024-08-01T05:41:59Z","published":"2024-08-01T05:41:59Z","title":"RDP: Ranked Differential Privacy for Facial Feature Protection in\n  Multiscale Sparsified Subspace","summary":"  With the widespread sharing of personal face images in applications' public\ndatabases, face recognition systems faces real threat of being breached by\npotential adversaries who are able to access users' face images and use them to\nintrude the face recognition systems. In this paper, we propose a novel privacy\nprotection method in the multiscale sparsified feature subspaces to protect\nsensitive facial features, by taking care of the influence or weight ranked\nfeature coefficients on the privacy budget, named \"Ranked Differential Privacy\n(RDP)\". After the multiscale feature decomposition, the lightweight Laplacian\nnoise is added to the dimension-reduced sparsified feature coefficients\naccording to the geometric superposition method. Then, we rigorously prove that\nthe RDP satisfies Differential Privacy. After that, the nonlinear Lagrange\nMultiplier (LM) method is formulated for the constraint optimization problem of\nmaximizing the utility of the visualization quality protected face images with\nsanitizing noise, under a given facial features privacy budget. Then, two\nmethods are proposed to solve the nonlinear LM problem and obtain the optimal\nnoise scale parameters: 1) the analytical Normalization Approximation (NA)\nmethod with identical average noise scale parameter for real-time online\napplications; and 2) the LM optimization Gradient Descent (LMGD) numerical\nmethod to obtain the nonlinear solution through iterative updating for more\naccurate offline applications. Experimental results on two real-world datasets\nshow that our proposed RDP outperforms other state-of-the-art methods: at a\nprivacy budget of 0.2, the PSNR (Peak Signal-to-Noise Ratio) of the RDP is\nabout ~10 dB higher than (10 times as high as) the highest PSNR of all compared\nmethods.\n","authors":["Lu Ou","Shaolin Liao","Shihui Gao","Guandong Huang","Zheng Qi"],"pdf_url":"https://arxiv.org/pdf/2408.00294v1.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.00290v1","updated":"2024-08-01T05:24:20Z","published":"2024-08-01T05:24:20Z","title":"Multi-Modal Parameter-Efficient Fine-tuning via Graph Neural Network","summary":"  With the advent of the era of foundation models, pre-training and fine-tuning\nhave become common paradigms. Recently, parameter-efficient fine-tuning has\ngarnered widespread attention due to its better balance between the number of\nlearnable parameters and performance. However, some current parameter-efficient\nfine-tuning methods only model a single modality and lack the utilization of\nstructural knowledge in downstream tasks. To address this issue, this paper\nproposes a multi-modal parameter-efficient fine-tuning method based on graph\nnetworks. Each image is fed into a multi-modal large language model (MLLM) to\ngenerate a text description. The image and its corresponding text description\nare then processed by a frozen image encoder and text encoder to generate image\nfeatures and text features, respectively. A graph is constructed based on the\nsimilarity of the multi-modal feature nodes, and knowledge and relationships\nrelevant to these features are extracted from each node. Additionally, Elastic\nWeight Consolidation (EWC) regularization is incorporated into the loss\nfunction to mitigate the problem of forgetting during task learning. The\nproposed model achieves test accuracies on the OxfordPets, Flowers102, and\nFood101 datasets that improve by 4.45%, 2.92%, and 0.23%, respectively. The\ncode is available at https://github.com/yunche0/GA-Net/tree/master.\n","authors":["Bin Cheng","Jiaxuan Lu"],"pdf_url":"https://arxiv.org/pdf/2408.00290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00950v1","updated":"2024-08-01T23:11:03Z","published":"2024-08-01T23:11:03Z","title":"PrivateGaze: Preserving User Privacy in Black-box Mobile Gaze Tracking\n  Services","summary":"  Eye gaze contains rich information about human attention and cognitive\nprocesses. This capability makes the underlying technology, known as gaze\ntracking, a critical enabler for many ubiquitous applications and has triggered\nthe development of easy-to-use gaze estimation services. Indeed, by utilizing\nthe ubiquitous cameras on tablets and smartphones, users can readily access\nmany gaze estimation services. In using these services, users must provide\ntheir full-face images to the gaze estimator, which is often a black box. This\nposes significant privacy threats to the users, especially when a malicious\nservice provider gathers a large collection of face images to classify\nsensitive user attributes. In this work, we present PrivateGaze, the first\napproach that can effectively preserve users' privacy in black-box gaze\ntracking services without compromising gaze estimation performance.\nSpecifically, we proposed a novel framework to train a privacy preserver that\nconverts full-face images into obfuscated counterparts, which are effective for\ngaze estimation while containing no privacy information. Evaluation on four\ndatasets shows that the obfuscated image can protect users' private\ninformation, such as identity and gender, against unauthorized attribute\nclassification. Meanwhile, when used directly by the black-box gaze estimator\nas inputs, the obfuscated images lead to comparable tracking performance to the\nconventional, unprotected full-face images.\n","authors":["Lingyu Du","Jinyuan Jia","Xucong Zhang","Guohao Lan"],"pdf_url":"https://arxiv.org/pdf/2408.00950v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00943v1","updated":"2024-08-01T22:25:06Z","published":"2024-08-01T22:25:06Z","title":"Data-Driven Traffic Simulation for an Intersection in a Metropolis","summary":"  We present a novel data-driven simulation environment for modeling traffic in\nmetropolitan street intersections. Using real-world tracking data collected\nover an extended period of time, we train trajectory forecasting models to\nlearn agent interactions and environmental constraints that are difficult to\ncapture conventionally. Trajectories of new agents are first coarsely generated\nby sampling from the spatial and temporal generative distributions, then\nrefined using state-of-the-art trajectory forecasting models. The simulation\ncan run either autonomously, or under explicit human control conditioned on the\ngenerative distributions. We present the experiments for a variety of model\nconfigurations. Under an iterative prediction scheme, the way-point-supervised\nTrajNet++ model obtained 0.36 Final Displacement Error (FDE) in 20 FPS on an\nNVIDIA A100 GPU.\n","authors":["Chengbo Zang","Mehmet Kerem Turkcan","Gil Zussman","Javad Ghaderi","Zoran Kostic"],"pdf_url":"https://arxiv.org/pdf/2408.00943v1.pdf","comment":"CVPR 2024 Workshop POETS Oral"},{"id":"http://arxiv.org/abs/2408.00940v1","updated":"2024-08-01T22:08:52Z","published":"2024-08-01T22:08:52Z","title":"A dual-task mutual learning framework for predicting post-thrombectomy\n  cerebral hemorrhage","summary":"  Ischemic stroke is a severe condition caused by the blockage of brain blood\nvessels, and can lead to the death of brain tissue due to oxygen deprivation.\nThrombectomy has become a common treatment choice for ischemic stroke due to\nits immediate effectiveness. But, it carries the risk of postoperative cerebral\nhemorrhage. Clinically, multiple CT scans within 0-72 hours post-surgery are\nused to monitor for hemorrhage. However, this approach exposes radiation dose\nto patients, and may delay the detection of cerebral hemorrhage. To address\nthis dilemma, we propose a novel prediction framework for measuring\npostoperative cerebral hemorrhage using only the patient's initial CT scan.\nSpecifically, we introduce a dual-task mutual learning framework to takes the\ninitial CT scan as input and simultaneously estimates both the follow-up CT\nscan and prognostic label to predict the occurrence of postoperative cerebral\nhemorrhage. Our proposed framework incorporates two attention mechanisms, i.e.,\nself-attention and interactive attention. Specifically, the self-attention\nmechanism allows the model to focus more on high-density areas in the image,\nwhich are critical for diagnosis (i.e., potential hemorrhage areas). The\ninteractive attention mechanism further models the dependencies between the\ninterrelated generation and classification tasks, enabling both tasks to\nperform better than the case when conducted individually. Validated on clinical\ndata, our method can generate follow-up CT scans better than state-of-the-art\nmethods, and achieves an accuracy of 86.37% in predicting follow-up prognostic\nlabels. Thus, our work thus contributes to the timely screening of\npost-thrombectomy cerebral hemorrhage, and could significantly reform the\nclinical process of thrombectomy and other similar operations related to\nstroke.\n","authors":["Caiwen Jiang","Tianyu Wang","Xiaodan Xing","Mianxin Liu","Guang Yang","Zhongxiang Ding","Dinggang Shen"],"pdf_url":"https://arxiv.org/pdf/2408.00940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00938v1","updated":"2024-08-01T22:01:42Z","published":"2024-08-01T22:01:42Z","title":"CIResDiff: A Clinically-Informed Residual Diffusion Model for Predicting\n  Idiopathic Pulmonary Fibrosis Progression","summary":"  The progression of Idiopathic Pulmonary Fibrosis (IPF) significantly\ncorrelates with higher patient mortality rates. Early detection of IPF\nprogression is critical for initiating timely treatment, which can effectively\nslow down the advancement of the disease. However, the current clinical\ncriteria define disease progression requiring two CT scans with a one-year\ninterval, presenting a dilemma: a disease progression is identified only after\nthe disease has already progressed. To this end, in this paper, we develop a\nnovel diffusion model to accurately predict the progression of IPF by\ngenerating patient's follow-up CT scan from the initial CT scan. Specifically,\nfrom the clinical prior knowledge, we tailor improvements to the traditional\ndiffusion model and propose a Clinically-Informed Residual Diffusion model,\ncalled CIResDiff. The key innovations of CIResDiff include 1) performing the\ntarget region pre-registration to align the lung regions of two CT scans at\ndifferent time points for reducing the generation difficulty, 2) adopting the\nresidual diffusion instead of traditional diffusion to enable the model focus\nmore on differences (i.e., lesions) between the two CT scans rather than the\nlargely identical anatomical content, and 3) designing the clinically-informed\nprocess based on CLIP technology to integrate lung function information which\nis highly relevant to diagnosis into the reverse process for assisting\ngeneration. Extensive experiments on clinical data demonstrate that our\napproach can outperform state-of-the-art methods and effectively predict the\nprogression of IPF.\n","authors":["Caiwen Jiang","Xiaodan Xing","Zaixin Ou","Mianxin Liu","Walsh Simon","Guang Yang","Dinggang Shen"],"pdf_url":"https://arxiv.org/pdf/2408.00938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00932v1","updated":"2024-08-01T21:50:23Z","published":"2024-08-01T21:50:23Z","title":"Towards Zero-Shot Annotation of the Built Environment with\n  Vision-Language Models (Vision Paper)","summary":"  Equitable urban transportation applications require high-fidelity digital\nrepresentations of the built environment: not just streets and sidewalks, but\nbike lanes, marked and unmarked crossings, curb ramps and cuts, obstructions,\ntraffic signals, signage, street markings, potholes, and more. Direct\ninspections and manual annotations are prohibitively expensive at scale.\nConventional machine learning methods require substantial annotated training\ndata for adequate performance. In this paper, we consider vision language\nmodels as a mechanism for annotating diverse urban features from satellite\nimages, reducing the dependence on human annotation to produce large training\nsets. While these models have achieved impressive results in describing common\nobjects in images captured from a human perspective, their training sets are\nless likely to include strong signals for esoteric features in the built\nenvironment, and their performance in these settings is therefore unclear. We\ndemonstrate proof-of-concept combining a state-of-the-art vision language model\nand variants of a prompting strategy that asks the model to consider segmented\nelements independently of the original image. Experiments on two urban features\n-- stop lines and raised tables -- show that while direct zero-shot prompting\ncorrectly annotates nearly zero images, the pre-segmentation strategies can\nannotate images with near 40% intersection-over-union accuracy. We describe how\nthese results inform a new research agenda in automatic annotation of the built\nenvironment to improve equity, accessibility, and safety at broad scale and in\ndiverse environments.\n","authors":["Bin Han","Yiwei Yang","Anat Caspi","Bill Howe"],"pdf_url":"https://arxiv.org/pdf/2408.00932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.13651v4","updated":"2024-08-01T21:49:31Z","published":"2023-08-25T19:40:56Z","title":"PCNN: Probable-Class Nearest-Neighbor Explanations Improve Fine-Grained\n  Image Classification Accuracy for AIs and Humans","summary":"  Nearest neighbors (NN) are traditionally used to compute final decisions,\ne.g., in Support Vector Machines or k-NN classifiers, and to provide users with\nexplanations for the model's decision. In this paper, we show a novel utility\nof nearest neighbors: To improve predictions of a frozen, pretrained classifier\nC. We leverage an image comparator S that (1) compares the input image with NN\nimages from the top-K most probable classes; and (2) uses S' output scores to\nweight the confidence scores of C. Our method consistently improves\nfine-grained image classification accuracy on CUB-200, Cars-196, and Dogs-120.\nAlso, a human study finds that showing lay users our probable-class nearest\nneighbors (PCNN) reduces over-reliance on AI, thus improving their decision\naccuracy over prior work which only shows only the top-1 class examples.\n","authors":["Giang Nguyen","Valerie Chen","Mohammad Reza Taesiri","Anh Totti Nguyen"],"pdf_url":"https://arxiv.org/pdf/2308.13651v4.pdf","comment":"Accepted to Transaction of Machine Learning Research"},{"id":"http://arxiv.org/abs/2408.00923v1","updated":"2024-08-01T21:27:31Z","published":"2024-08-01T21:27:31Z","title":"Reclaiming Residual Knowledge: A Novel Paradigm to Low-Bit Quantization","summary":"  This paper explores a novel paradigm in low-bit (i.e. 4-bits or lower)\nquantization, differing from existing state-of-the-art methods, by framing\noptimal quantization as an architecture search problem within convolutional\nneural networks (ConvNets). Our framework, dubbed \\textbf{CoRa} (Optimal\nQuantization Residual \\textbf{Co}nvolutional Operator Low-\\textbf{Ra}nk\nAdaptation), is motivated by two key aspects. Firstly, quantization residual\nknowledge, i.e. the lost information between floating-point weights and\nquantized weights, has long been neglected by the research community.\nReclaiming the critical residual knowledge, with an infinitesimal extra\nparameter cost, can reverse performance degradation without training. Secondly,\nstate-of-the-art quantization frameworks search for optimal quantized weights\nto address the performance degradation. Yet, the vast search spaces in weight\noptimization pose a challenge for the efficient optimization in large models.\nFor example, state-of-the-art BRECQ necessitates $2 \\times 10^4$ iterations to\nquantize models. Fundamentally differing from existing methods, \\textbf{CoRa}\nsearches for the optimal architectures of low-rank adapters, reclaiming\ncritical quantization residual knowledge, within the search spaces smaller\ncompared to the weight spaces, by many orders of magnitude. The low-rank\nadapters approximate the quantization residual weights, discarded in previous\nmethods. We evaluate our approach over multiple pre-trained ConvNets on\nImageNet. \\textbf{CoRa} achieves comparable performance against both\nstate-of-the-art quantization-aware training and post-training quantization\nbaselines, in $4$-bit and $3$-bit quantization, by using less than $250$\niterations on a small calibration set with $1600$ images. Thus, \\textbf{CoRa}\nestablishes a new state-of-the-art in terms of the optimization efficiency in\nlow-bit quantization.\n","authors":["Róisín Luo","Alexandru Drimbarean","James McDermott","Colm O'Riordan"],"pdf_url":"https://arxiv.org/pdf/2408.00923v1.pdf","comment":"Accepted by The 35th British Machine Vision Conference (BMVC 2024)"},{"id":"http://arxiv.org/abs/2406.02529v2","updated":"2024-08-01T20:53:09Z","published":"2024-06-04T17:51:08Z","title":"ReLUs Are Sufficient for Learning Implicit Neural Representations","summary":"  Motivated by the growing theoretical understanding of neural networks that\nemploy the Rectified Linear Unit (ReLU) as their activation function, we\nrevisit the use of ReLU activation functions for learning implicit neural\nrepresentations (INRs). Inspired by second order B-spline wavelets, we\nincorporate a set of simple constraints to the ReLU neurons in each layer of a\ndeep neural network (DNN) to remedy the spectral bias. This in turn enables its\nuse for various INR tasks. Empirically, we demonstrate that, contrary to\npopular belief, one can learn state-of-the-art INRs based on a DNN composed of\nonly ReLU neurons. Next, by leveraging recent theoretical works which\ncharacterize the kinds of functions ReLU neural networks learn, we provide a\nway to quantify the regularity of the learned function. This offers a\nprincipled approach to selecting the hyperparameters in INR architectures. We\nsubstantiate our claims through experiments in signal representation, super\nresolution, and computed tomography, demonstrating the versatility and\neffectiveness of our method. The code for all experiments can be found at\nhttps://github.com/joeshenouda/relu-inrs.\n","authors":["Joseph Shenouda","Yamin Zhou","Robert D. Nowak"],"pdf_url":"https://arxiv.org/pdf/2406.02529v2.pdf","comment":"Accepted to ICML 2024"},{"id":"http://arxiv.org/abs/2403.14837v2","updated":"2024-08-01T20:01:58Z","published":"2024-03-21T21:13:53Z","title":"Osmosis: RGBD Diffusion Prior for Underwater Image Restoration","summary":"  Underwater image restoration is a challenging task because of water effects\nthat increase dramatically with distance. This is worsened by lack of ground\ntruth data of clean scenes without water. Diffusion priors have emerged as\nstrong image restoration priors. However, they are often trained with a dataset\nof the desired restored output, which is not available in our case. We also\nobserve that using only color data is insufficient, and therefore augment the\nprior with a depth channel. We train an unconditional diffusion model prior on\nthe joint space of color and depth, using standard RGBD datasets of natural\noutdoor scenes in air. Using this prior together with a novel guidance method\nbased on the underwater image formation model, we generate posterior samples of\nclean images, removing the water effects. Even though our prior did not see any\nunderwater images during training, our method outperforms state-of-the-art\nbaselines for image restoration on very challenging scenes. Our code, models\nand data are available on the project website.\n","authors":["Opher Bar Nathan","Deborah Levy","Tali Treibitz","Dan Rosenbaum"],"pdf_url":"https://arxiv.org/pdf/2403.14837v2.pdf","comment":"ECCV 2024. Project page with results and code:\n  https://osmosis-diffusion.github.io/"},{"id":"http://arxiv.org/abs/2408.00891v1","updated":"2024-08-01T20:00:18Z","published":"2024-08-01T20:00:18Z","title":"Temporal Evolution of Knee Osteoarthritis: A Diffusion-based Morphing\n  Model for X-ray Medical Image Synthesis","summary":"  Knee Osteoarthritis (KOA) is a common musculoskeletal disorder that\nsignificantly affects the mobility of older adults. In the medical domain,\nimages containing temporal data are frequently utilized to study temporal\ndynamics and statistically monitor disease progression. While deep\nlearning-based generative models for natural images have been widely\nresearched, there are comparatively few methods available for synthesizing\ntemporal knee X-rays. In this work, we introduce a novel deep-learning model\ndesigned to synthesize intermediate X-ray images between a specific patient's\nhealthy knee and severe KOA stages. During the testing phase, based on a\nhealthy knee X-ray, the proposed model can produce a continuous and effective\nsequence of KOA X-ray images with varying degrees of severity. Specifically, we\nintroduce a Diffusion-based Morphing Model by modifying the Denoising Diffusion\nProbabilistic Model. Our approach integrates diffusion and morphing modules,\nenabling the model to capture spatial morphing details between source and\ntarget knee X-ray images and synthesize intermediate frames along a geodesic\npath. A hybrid loss consisting of diffusion loss, morphing loss, and\nsupervision loss was employed. We demonstrate that our proposed approach\nachieves the highest temporal frame synthesis performance, effectively\naugmenting data for classification models and simulating the progression of\nKOA.\n","authors":["Zhe Wang","Aladine Chetouani","Rachid Jennane","Yuhua Ru","Wasim Issa","Mohamed Jarraya"],"pdf_url":"https://arxiv.org/pdf/2408.00891v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04943v2","updated":"2024-08-01T18:55:16Z","published":"2024-03-07T23:18:34Z","title":"AFreeCA: Annotation-Free Counting for All","summary":"  Object counting methods typically rely on manually annotated datasets. The\ncost of creating such datasets has restricted the versatility of these networks\nto count objects from specific classes (such as humans or penguins), and\ncounting objects from diverse categories remains a challenge. The availability\nof robust text-to-image latent diffusion models (LDMs) raises the question of\nwhether these models can be utilized to generate counting datasets. However,\nLDMs struggle to create images with an exact number of objects based solely on\ntext prompts but they can be used to offer a dependable \\textit{sorting} signal\nby adding and removing objects within an image. Leveraging this data, we\ninitially introduce an unsupervised sorting methodology to learn object-related\nfeatures that are subsequently refined and anchored for counting purposes using\ncounting data generated by LDMs. Further, we present a density\nclassifier-guided method for dividing an image into patches containing objects\nthat can be reliably counted. Consequently, we can generate counting data for\nany type of object and count them in an unsupervised manner. Our approach\noutperforms other unsupervised and few-shot alternatives and is not restricted\nto specific object classes for which counting data is available. Code to be\nreleased upon acceptance.\n","authors":["Adriano D'Alessandro","Ali Mahdavi-Amiri","Ghassan Hamarneh"],"pdf_url":"https://arxiv.org/pdf/2403.04943v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.04421v2","updated":"2024-08-01T18:52:31Z","published":"2023-09-08T16:32:56Z","title":"SynthoGestures: A Novel Framework for Synthetic Dynamic Hand Gesture\n  Generation for Driving Scenarios","summary":"  Creating a diverse and comprehensive dataset of hand gestures for dynamic\nhuman-machine interfaces in the automotive domain can be challenging and\ntime-consuming. To overcome this challenge, we propose using synthetic gesture\ndatasets generated by virtual 3D models. Our framework utilizes Unreal Engine\nto synthesize realistic hand gestures, offering customization options and\nreducing the risk of overfitting. Multiple variants, including gesture speed,\nperformance, and hand shape, are generated to improve generalizability. In\naddition, we simulate different camera locations and types, such as RGB,\ninfrared, and depth cameras, without incurring additional time and cost to\nobtain these cameras. Experimental results demonstrate that our proposed\nframework, SynthoGestures (https://github.com/amrgomaaelhady/SynthoGestures),\nimproves gesture recognition accuracy and can replace or augment real-hand\ndatasets. By saving time and effort in the creation of the data set, our tool\naccelerates the development of gesture recognition systems for automotive\napplications.\n","authors":["Amr Gomaa","Robin Zitt","Guillermo Reyes","Antonio Krüger"],"pdf_url":"https://arxiv.org/pdf/2309.04421v2.pdf","comment":"Accepted at IEEE IV'24. Shorter versions were accepted as\n  AutomotiveUI2023 Work in Progress and UIST2023 Poster Papers"},{"id":"http://arxiv.org/abs/2408.00874v1","updated":"2024-08-01T18:49:45Z","published":"2024-08-01T18:49:45Z","title":"Medical SAM 2: Segment medical images as video via Segment Anything\n  Model 2","summary":"  In this paper, we introduce Medical SAM 2 (MedSAM-2), an advanced\nsegmentation model that utilizes the SAM 2 framework to address both 2D and 3D\nmedical image segmentation tasks. By adopting the philosophy of taking medical\nimages as videos, MedSAM-2 not only applies to 3D medical images but also\nunlocks new One-prompt Segmentation capability. That allows users to provide a\nprompt for just one or a specific image targeting an object, after which the\nmodel can autonomously segment the same type of object in all subsequent\nimages, regardless of temporal relationships between the images. We evaluated\nMedSAM-2 across a variety of medical imaging modalities, including abdominal\norgans, optic discs, brain tumors, thyroid nodules, and skin lesions, comparing\nit against state-of-the-art models in both traditional and interactive\nsegmentation settings. Our findings show that MedSAM-2 not only surpasses\nexisting models in performance but also exhibits superior generalization across\na range of medical image segmentation tasks. Our code will be released at:\nhttps://github.com/MedicineToken/Medical-SAM2\n","authors":["Jiayuan Zhu","Yunli Qi","Junde Wu"],"pdf_url":"https://arxiv.org/pdf/2408.00874v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10280v2","updated":"2024-08-01T18:49:11Z","published":"2023-03-17T23:23:55Z","title":"Synthetic-to-Real Domain Adaptation for Action Recognition: A Dataset\n  and Baseline Performances","summary":"  Human action recognition is a challenging problem, particularly when there is\nhigh variability in factors such as subject appearance, backgrounds and\nviewpoint. While deep neural networks (DNNs) have been shown to perform well on\naction recognition tasks, they typically require large amounts of high-quality\nlabeled data to achieve robust performance across a variety of conditions.\nSynthetic data has shown promise as a way to avoid the substantial costs and\npotential ethical concerns associated with collecting and labeling enormous\namounts of data in the real-world. However, synthetic data may differ from real\ndata in important ways. This phenomenon, known as \\textit{domain shift}, can\nlimit the utility of synthetic data in robotics applications. To mitigate the\neffects of domain shift, substantial effort is being dedicated to the\ndevelopment of domain adaptation (DA) techniques. Yet, much remains to be\nunderstood about how best to develop these techniques. In this paper, we\nintroduce a new dataset called Robot Control Gestures (RoCoG-v2). The dataset\nis composed of both real and synthetic videos from seven gesture classes, and\nis intended to support the study of synthetic-to-real domain shift for\nvideo-based action recognition. Our work expands upon existing datasets by\nfocusing the action classes on gestures for human-robot teaming, as well as by\nenabling investigation of domain shift in both ground and aerial views. We\npresent baseline results using state-of-the-art action recognition and domain\nadaptation algorithms and offer initial insight on tackling the\nsynthetic-to-real and ground-to-air domain shifts.\n","authors":["Arun V. Reddy","Ketul Shah","William Paul","Rohita Mocharla","Judy Hoffman","Kapil D. Katyal","Dinesh Manocha","Celso M. de Melo","Rama Chellappa"],"pdf_url":"https://arxiv.org/pdf/2303.10280v2.pdf","comment":"ICRA 2023. The first two authors contributed equally. Dataset\n  available at: https://github.com/reddyav1/RoCoG-v2"}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2407.21740v2","updated":"2024-08-01T03:16:43Z","published":"2024-07-31T16:52:00Z","title":"Contrastive Factor Analysis","summary":"  Factor analysis, often regarded as a Bayesian variant of matrix\nfactorization, offers superior capabilities in capturing uncertainty, modeling\ncomplex dependencies, and ensuring robustness. As the deep learning era\narrives, factor analysis is receiving less and less attention due to their\nlimited expressive ability. On the contrary, contrastive learning has emerged\nas a potent technique with demonstrated efficacy in unsupervised\nrepresentational learning. While the two methods are different paradigms,\nrecent theoretical analysis has revealed the mathematical equivalence between\ncontrastive learning and matrix factorization, providing a potential\npossibility for factor analysis combined with contrastive learning. Motivated\nby the interconnectedness of contrastive learning, matrix factorization, and\nfactor analysis, this paper introduces a novel Contrastive Factor Analysis\nframework, aiming to leverage factor analysis's advantageous properties within\nthe realm of contrastive learning. To further leverage the interpretability\nproperties of non-negative factor analysis, which can learn disentangled\nrepresentations, contrastive factor analysis is extended to a non-negative\nversion. Finally, extensive experimental validation showcases the efficacy of\nthe proposed contrastive (non-negative) factor analysis methodology across\nmultiple key properties, including expressiveness, robustness,\ninterpretability, and accurate uncertainty estimation.\n","authors":["Zhibin Duan","Tiansheng Wen","Yifei Wang","Chen Zhu","Bo Chen","Mingyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.21740v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.03733v3","updated":"2024-08-01T17:47:24Z","published":"2022-12-07T15:55:00Z","title":"Tiered Reward: Designing Rewards for Specification and Fast Learning of\n  Desired Behavior","summary":"  Reinforcement-learning agents seek to maximize a reward signal through\nenvironmental interactions. As humans, our job in the learning process is to\ndesign reward functions to express desired behavior and enable the agent to\nlearn such behavior swiftly. However, designing good reward functions to induce\nthe desired behavior is generally hard, let alone the question of which rewards\nmake learning fast. In this work, we introduce a family of a reward structures\nwe call Tiered Reward that addresses both of these questions. We consider the\nreward-design problem in tasks formulated as reaching desirable states and\navoiding undesirable states. To start, we propose a strict partial ordering of\nthe policy space to resolve trade-offs in behavior preference. We prefer\npolicies that reach the good states faster and with higher probability while\navoiding the bad states longer. Next, we introduce Tiered Reward, a class of\nenvironment-independent reward functions and show it is guaranteed to induce\npolicies that are Pareto-optimal according to our preference relation. Finally,\nwe demonstrate that Tiered Reward leads to fast learning with multiple tabular\nand deep reinforcement-learning algorithms.\n","authors":["Zhiyuan Zhou","Shreyas Sundara Raman","Henry Sowerby","Michael L. Littman"],"pdf_url":"https://arxiv.org/pdf/2212.03733v3.pdf","comment":"For code, see https://github.com/zhouzypaul/tiered-reward"},{"id":"http://arxiv.org/abs/2308.05846v2","updated":"2024-08-01T17:46:38Z","published":"2023-08-10T19:56:15Z","title":"Seed Kernel Counting using Domain Randomization and Object Tracking\n  Neural Networks","summary":"  High-throughput phenotyping (HTP) of seeds, also known as seed phenotyping,\nis the comprehensive assessment of complex seed traits such as growth,\ndevelopment, tolerance, resistance, ecology, yield, and the measurement of\nparameters that form more complex traits. One of the key aspects of seed\nphenotyping is cereal yield estimation that the seed production industry relies\nupon to conduct their business. While mechanized seed kernel counters are\navailable in the market currently, they are often priced high and sometimes\noutside the range of small scale seed production firms' affordability. The\ndevelopment of object tracking neural network models such as You Only Look Once\n(YOLO) enables computer scientists to design algorithms that can estimate\ncereal yield inexpensively. The key bottleneck with neural network models is\nthat they require a plethora of labelled training data before they can be put\nto task. We demonstrate that the use of synthetic imagery serves as a feasible\nsubstitute to train neural networks for object tracking that includes the tasks\nof object classification and detection. Furthermore, we propose a seed kernel\ncounter that uses a low-cost mechanical hopper, trained YOLOv8 neural network\nmodel, and object tracking algorithms on StrongSORT and ByteTrack to estimate\ncereal yield from videos. The experiment yields a seed kernel count with an\naccuracy of 95.2\\% and 93.2\\% for Soy and Wheat respectively using the\nStrongSORT algorithm, and an accuray of 96.8\\% and 92.4\\% for Soy and Wheat\nrespectively using the ByteTrack algorithm.\n","authors":["Venkat Margapuri","Prapti Thapaliya","Mitchell Neilsen"],"pdf_url":"https://arxiv.org/pdf/2308.05846v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18320v2","updated":"2024-08-01T17:43:19Z","published":"2024-05-28T16:11:11Z","title":"Self-Supervised Learning Based Handwriting Verification","summary":"  We present SSL-HV: Self-Supervised Learning approaches applied to the task of\nHandwriting Verification. This task involves determining whether a given pair\nof handwritten images originate from the same or different writer distribution.\nWe have compared the performance of multiple generative, contrastive SSL\napproaches against handcrafted feature extractors and supervised learning on\nCEDAR AND dataset. We show that ResNet based Variational Auto-Encoder (VAE)\noutperforms other generative approaches achieving 76.3% accuracy, while\nResNet-18 fine-tuned using Variance-Invariance-Covariance Regularization\n(VICReg) outperforms other contrastive approaches achieving 78% accuracy. Using\na pre-trained VAE and VICReg for the downstream task of writer verification we\nobserved a relative improvement in accuracy of 6.7% and 9% over ResNet-18\nsupervised baseline with 10% writer labels.\n","authors":["Mihir Chauhan","Mohammad Abuzar Hashemi","Abhishek Satbhai","Mir Basheer Ali","Bina Ramamurthy","Mingchen Gao","Siwei Lyu","Sargur Srihari"],"pdf_url":"https://arxiv.org/pdf/2405.18320v2.pdf","comment":"8 pages, 2 figures, 2 tables, Accepted at Irish Machine Vision and\n  Image Processing Conference 2024"},{"id":"http://arxiv.org/abs/2310.12428v2","updated":"2024-08-01T17:38:27Z","published":"2023-10-19T02:42:20Z","title":"Enhanced Local Explainability and Trust Scores with Random Forest\n  Proximities","summary":"  We initiate a novel approach to explain the predictions and out of sample\nperformance of random forest (RF) regression and classification models by\nexploiting the fact that any RF can be mathematically formulated as an adaptive\nweighted K nearest-neighbors model. Specifically, we employ a recent result\nthat, for both regression and classification tasks, any RF prediction can be\nrewritten exactly as a weighted sum of the training targets, where the weights\nare RF proximities between the corresponding pairs of data points. We show that\nthis linearity facilitates a local notion of explainability of RF predictions\nthat generates attributions for any model prediction across observations in the\ntraining set, and thereby complements established feature-based methods like\nSHAP, which generate attributions for a model prediction across input features.\nWe show how this proximity-based approach to explainability can be used in\nconjunction with SHAP to explain not just the model predictions, but also\nout-of-sample performance, in the sense that proximities furnish a novel means\nof assessing when a given model prediction is more or less likely to be\ncorrect. We demonstrate this approach in the modeling of US corporate bond\nprices and returns in both regression and classification cases.\n","authors":["Joshua Rosaler","Dhruv Desai","Bhaskarjit Sarmah","Dimitrios Vamvourellis","Deran Onay","Dhagash Mehta","Stefano Pasquali"],"pdf_url":"https://arxiv.org/pdf/2310.12428v2.pdf","comment":"5 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.10251v3","updated":"2024-08-01T16:27:20Z","published":"2024-06-10T08:23:52Z","title":"The Impact of Quantization on Retrieval-Augmented Generation: An\n  Analysis of Small LLMs","summary":"  Post-training quantization reduces the computational demand of Large Language\nModels (LLMs) but can weaken some of their capabilities. Since LLM abilities\nemerge with scale, smaller LLMs are more sensitive to quantization. In this\npaper, we explore how quantization affects smaller LLMs' ability to perform\nretrieval-augmented generation (RAG), specifically in longer contexts. We chose\npersonalization for evaluation because it is a challenging domain to perform\nusing RAG as it requires long-context reasoning over multiple documents. We\ncompare the original FP16 and the quantized INT4 performance of multiple 7B and\n8B LLMs on two tasks while progressively increasing the number of retrieved\ndocuments to test how quantized models fare against longer contexts. To better\nunderstand the effect of retrieval, we evaluate three retrieval models in our\nexperiments. Our findings reveal that if a 7B LLM performs the task well,\nquantization does not impair its performance and long-context reasoning\ncapabilities. We conclude that it is possible to utilize RAG with quantized\nsmaller LLMs.\n","authors":["Mert Yazan","Suzan Verberne","Frederik Situmeang"],"pdf_url":"https://arxiv.org/pdf/2406.10251v3.pdf","comment":"Accepted to the IR-RAG Workshop at SIGIR 2024"},{"id":"http://arxiv.org/abs/2303.07865v4","updated":"2024-08-01T16:14:04Z","published":"2023-03-14T12:56:47Z","title":"Predicting the Geolocation of Tweets Using transformer models on\n  Customized Data","summary":"  This research is aimed to solve the tweet/user geolocation prediction task\nand provide a flexible methodology for the geotagging of textual big data. The\nsuggested approach implements neural networks for natural language processing\n(NLP) to estimate the location as coordinate pairs (longitude, latitude) and\ntwo-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models\nhas been finetuned on a Twitter dataset using pretrained Bidirectional Encoder\nRepresentations from Transformers (BERT) as base models. Performance metrics\nshow a median error of fewer than 30 km on a worldwide-level, and fewer than 15\nkm on the US-level datasets for the models trained and evaluated on text\nfeatures of tweets' content and metadata context. Our source code and data are\navailable at https://github.com/K4TEL/geo-twitter.git\n","authors":["Kateryna Lutsai","Christoph H. Lampert"],"pdf_url":"https://arxiv.org/pdf/2303.07865v4.pdf","comment":"31 pages, 5 tables, 9 figures"},{"id":"http://arxiv.org/abs/2407.20021v3","updated":"2024-08-01T16:13:45Z","published":"2024-07-29T13:57:40Z","title":"MimiQ: Low-Bit Data-Free Quantization of Vision Transformers with\n  Encouraging Inter-Head Attention Similarity","summary":"  Data-free quantization (DFQ) is a technique that creates a lightweight\nnetwork from its full-precision counterpart without the original training data,\noften through a synthetic dataset. Although several DFQ methods have been\nproposed for vision transformer (ViT) architectures, they fail to achieve\nefficacy in low-bit settings. Examining the existing methods, we identify that\ntheir synthetic data produce misaligned attention maps, while those of the real\nsamples are highly aligned. From the observation of aligned attention, we find\nthat aligning attention maps of synthetic data helps to improve the overall\nperformance of quantized ViTs. Motivated by this finding, we devise MimiQ, a\nnovel DFQ method designed for ViTs that focuses on inter-head attention\nsimilarity. First, we generate synthetic data by aligning head-wise attention\nresponses in relation to spatial query patches. Then, we apply head-wise\nstructural attention distillation to align the attention maps of the quantized\nnetwork to those of the full-precision teacher. The experimental results show\nthat the proposed method significantly outperforms baselines, setting a new\nstate-of-the-art performance for data-free ViT quantization.\n","authors":["Kanghyun Choi","Hye Yoon Lee","Dain Kwon","SunJong Park","Kyuyeun Kim","Noseong Park","Jinho Lee"],"pdf_url":"https://arxiv.org/pdf/2407.20021v3.pdf","comment":"Author Preprint"},{"id":"http://arxiv.org/abs/2407.02651v2","updated":"2024-08-01T15:56:00Z","published":"2024-07-02T20:33:50Z","title":"Improving Steering and Verification in AI-Assisted Data Analysis with\n  Interactive Task Decomposition","summary":"  LLM-powered tools like ChatGPT Data Analysis, have the potential to help\nusers tackle the challenging task of data analysis programming, which requires\nexpertise in data processing, programming, and statistics. However, our\nformative study (n=15) uncovered serious challenges in verifying AI-generated\nresults and steering the AI (i.e., guiding the AI system to produce the desired\noutput). We developed two contrasting approaches to address these challenges.\nThe first (Stepwise) decomposes the problem into step-by-step subgoals with\npairs of editable assumptions and code until task completion, while the second\n(Phasewise) decomposes the entire problem into three editable, logical phases:\nstructured input/output assumptions, execution plan, and code. A controlled,\nwithin-subjects experiment (n=18) compared these systems against a\nconversational baseline. Users reported significantly greater control with the\nStepwise and Phasewise systems, and found intervention, correction, and\nverification easier, compared to the baseline. The results suggest design\nguidelines and trade-offs for AI-assisted data analysis tools.\n","authors":["Majeed Kazemitabaar","Jack Williams","Ian Drosos","Tovi Grossman","Austin Henley","Carina Negreanu","Advait Sarkar"],"pdf_url":"https://arxiv.org/pdf/2407.02651v2.pdf","comment":"Published at UIST 2024; 19 pages, 9 figures, and 2 tables"},{"id":"http://arxiv.org/abs/2304.13312v2","updated":"2024-08-01T15:54:29Z","published":"2023-04-26T06:33:31Z","title":"Technical Note: Defining and Quantifying AND-OR Interactions for\n  Faithful and Concise Explanation of DNNs","summary":"  In this technical note, we aim to explain a deep neural network (DNN) by\nquantifying the encoded interactions between input variables, which reflects\nthe DNN's inference logic. Specifically, we first rethink the definition of\ninteractions, and then formally define faithfulness and conciseness for\ninteraction-based explanation. To this end, we propose two kinds of\ninteractions, i.e., the AND interaction and the OR interaction. For\nfaithfulness, we prove the uniqueness of the AND (OR) interaction in\nquantifying the effect of the AND (OR) relationship between input variables.\nBesides, based on AND-OR interactions, we design techniques to boost the\nconciseness of the explanation, while not hurting the faithfulness. In this\nway, the inference logic of a DNN can be faithfully and concisely explained by\na set of symbolic concepts.\n","authors":["Mingjie Li","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.13312v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2111.06206"},{"id":"http://arxiv.org/abs/2406.11911v2","updated":"2024-08-01T15:44:19Z","published":"2024-06-16T16:46:55Z","title":"A Notion of Complexity for Theory of Mind via Discrete World Models","summary":"  Theory of Mind (ToM) can be used to assess the capabilities of Large Language\nModels (LLMs) in complex scenarios where social reasoning is required. While\nthe research community has proposed many ToM benchmarks, their hardness varies\ngreatly, and their complexity is not well defined. This work proposes a\nframework to measure the complexity of ToM tasks. We quantify a problem's\ncomplexity as the number of states necessary to solve it correctly. Our\ncomplexity measure also accounts for spurious states of a ToM problem designed\nto make it apparently harder. We use our method to assess the complexity of\nfive widely adopted ToM benchmarks. On top of this framework, we design a\nprompting technique that augments the information available to a model with a\ndescription of how the environment changes with the agents' interactions. We\nname this technique Discrete World Models (DWM) and show how it elicits\nsuperior performance on ToM tasks.\n","authors":["X. Angelo Huang","Emanuele La Malfa","Samuele Marro","Andrea Asperti","Anthony Cohn","Michael Wooldridge"],"pdf_url":"https://arxiv.org/pdf/2406.11911v2.pdf","comment":"https://flecart.github.io/complexity-tom-dwm"},{"id":"http://arxiv.org/abs/2311.00048v2","updated":"2024-08-01T15:37:52Z","published":"2023-10-31T18:01:41Z","title":"SC-MIL: Sparsely Coded Multiple Instance Learning for Whole Slide Image\n  Classification","summary":"  Multiple Instance Learning (MIL) has been widely used in weakly supervised\nwhole slide image (WSI) classification. Typical MIL methods include a feature\nembedding part, which embeds the instances into features via a pre-trained\nfeature extractor, and an MIL aggregator that combines instance embeddings into\npredictions. Most efforts have typically focused on improving these parts. This\ninvolves refining the feature embeddings through self-supervised pre-training\nas well as modeling the correlations between instances separately.\n  In this paper, we proposed a sparsely coding MIL (SC-MIL) method that\naddresses those two aspects at the same time by leveraging sparse dictionary\nlearning. The sparse dictionary learning captures the similarities of instances\nby expressing them as sparse linear combinations of atoms in an over-complete\ndictionary. In addition, imposing sparsity improves instance feature embeddings\nby suppressing irrelevant instances while retaining the most relevant ones. To\nmake the conventional sparse coding algorithm compatible with deep learning, we\nunrolled it into a sparsely coded module leveraging deep unrolling. The\nproposed SC module can be incorporated into any existing MIL framework in a\nplug-and-play manner with an acceptable computational cost. The experimental\nresults on multiple datasets demonstrated that the proposed SC module could\nsubstantially boost the performance of state-of-the-art MIL methods. The codes\nare available at\n\\href{https://github.com/sotiraslab/SCMIL.git}{https://github.com/sotiraslab/SCMIL.git}.\n","authors":["Peijie Qiu","Pan Xiao","Wenhui Zhu","Yalin Wang","Aristeidis Sotiras"],"pdf_url":"https://arxiv.org/pdf/2311.00048v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14814v3","updated":"2024-08-01T15:15:34Z","published":"2024-03-21T19:59:52Z","title":"The opportunities and risks of large language models in mental health","summary":"  Global rates of mental health concerns are rising, and there is increasing\nrealization that existing models of mental health care will not adequately\nexpand to meet the demand. With the emergence of large language models (LLMs)\nhas come great optimism regarding their promise to create novel, large-scale\nsolutions to support mental health. Despite their nascence, LLMs have already\nbeen applied to mental health related tasks. In this paper, we summarize the\nextant literature on efforts to use LLMs to provide mental health education,\nassessment, and intervention and highlight key opportunities for positive\nimpact in each area. We then highlight risks associated with LLMs' application\nto mental health and encourage the adoption of strategies to mitigate these\nrisks. The urgent need for mental health support must be balanced with\nresponsible development, testing, and deployment of mental health LLMs. It is\nespecially critical to ensure that mental health LLMs are fine-tuned for mental\nhealth, enhance mental health equity, and adhere to ethical standards and that\npeople, including those with lived experience with mental health concerns, are\ninvolved in all stages from development through deployment. Prioritizing these\nefforts will minimize potential harms to mental health and maximize the\nlikelihood that LLMs will positively impact mental health globally.\n","authors":["Hannah R. Lawrence","Renee A. Schneider","Susan B. Rubin","Maja J. Mataric","Daniel J. McDuff","Megan Jones Bell"],"pdf_url":"https://arxiv.org/pdf/2403.14814v3.pdf","comment":"15 pages, 2 tables, 4 figures"},{"id":"http://arxiv.org/abs/2407.21483v2","updated":"2024-08-01T14:22:58Z","published":"2024-07-31T09:48:27Z","title":"eSPARQL: Representing and Reconciling Agnostic and Atheistic Beliefs in\n  RDF-star Knowledge Graphs","summary":"  Over the past few years, we have seen the emergence of large knowledge graphs\ncombining information from multiple sources. Sometimes, this information is\nprovided in the form of assertions about other assertions, defining contexts\nwhere assertions are valid. A recent extension to RDF which admits statements\nover statements, called RDF-star, is in revision to become a W3C standard.\nHowever, there is no proposal for a semantics of these RDF-star statements nor\na built-in facility to operate over them. In this paper, we propose a query\nlanguage for epistemic RDF-star metadata based on a four-valued logic, called\neSPARQL. Our proposed query language extends SPARQL-star, the query language\nfor RDF-star, with a new type of FROM clause to facilitate operating with\nmultiple and sometimes conflicting beliefs. We show that the proposed query\nlanguage can express four use case queries, including the following features:\n(i) querying the belief of an individual, (ii) the aggregating of beliefs,\n(iii) querying who is conflicting with somebody, and (iv) beliefs about beliefs\n(i.e., nesting of beliefs).\n","authors":["Xiny Pan","Daniel Hernández","Philipp Seifer","Ralf Lämmel","Steffen Staab"],"pdf_url":"https://arxiv.org/pdf/2407.21483v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10269v3","updated":"2024-08-01T14:22:11Z","published":"2023-12-16T00:02:49Z","title":"The DSA Transparency Database: Auditing Self-reported Moderation Actions\n  by Social Media","summary":"  Since September 2023, the Digital Services Act (DSA) obliges large online\nplatforms to submit detailed data on each moderation action they take within\nthe European Union (EU) to the DSA Transparency Database. From its inception,\nthis centralized database has sparked scholarly interest as an unprecedented\nand potentially unique trove of data on real-world online moderation. Here, we\nthoroughly analyze all 353.12M records submitted by the eight largest social\nmedia platforms in the EU during the first 100 days of the database.\nSpecifically, we conduct a platform-wise comparative study of their: volume of\nmoderation actions, grounds for decision, types of applied restrictions, types\nof moderated content, timeliness in undertaking and submitting moderation\nactions, and use of automation. Furthermore, we systematically cross-check the\ncontents of the database with the platforms' own transparency reports. Our\nanalyses reveal that (i) the platforms adhered only in part to the philosophy\nand structure of the database, (ii) the structure of the database is partially\ninadequate for the platforms' reporting needs, (iii) the platforms exhibited\nsubstantial differences in their moderation actions, (iv) a remarkable fraction\nof the database data is inconsistent, (v) the platform X (formerly Twitter)\npresents the most inconsistencies. Our findings have far-reaching implications\nfor policymakers and scholars across diverse disciplines. They offer guidance\nfor future regulations that cater to the reporting needs of online platforms in\ngeneral, but also highlight opportunities to improve and refine the database\nitself.\n","authors":["Amaury Trujillo","Tiziano Fagni","Stefano Cresci"],"pdf_url":"https://arxiv.org/pdf/2312.10269v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11054v2","updated":"2024-08-01T14:10:22Z","published":"2024-07-09T09:25:27Z","title":"Generative AI for Health Technology Assessment: Opportunities,\n  Challenges, and Policy Considerations","summary":"  This review introduces the transformative potential of generative Artificial\nIntelligence (AI) and foundation models, including large language models\n(LLMs), for health technology assessment (HTA). We explore their applications\nin four critical areas, evidence synthesis, evidence generation, clinical\ntrials and economic modeling: (1) Evidence synthesis: Generative AI has the\npotential to assist in automating literature reviews and meta-analyses by\nproposing search terms, screening abstracts, and extracting data with notable\naccuracy; (2) Evidence generation: These models can potentially facilitate\nautomating the process and analyze the increasingly available large collections\nof real-world data (RWD), including unstructured clinical notes and imaging,\nenhancing the speed and quality of real-world evidence (RWE) generation; (3)\nClinical trials: Generative AI can be used to optimize trial design, improve\npatient matching, and manage trial data more efficiently; and (4) Economic\nmodeling: Generative AI can also aid in the development of health economic\nmodels, from conceptualization to validation, thus streamlining the overall HTA\nprocess. Despite their promise, these technologies, while rapidly improving,\nare still nascent and continued careful evaluation in their applications to HTA\nis required. To ensure their responsible use and implementation, both\ndevelopers and users of research incorporating these tools, should familiarize\nthemselves with their current limitations, including the issues related to\nscientific validity, risk of bias, and consider equity and ethical\nimplications. We also surveyed the current policy landscape and provide\nsuggestions for HTA agencies on responsibly integrating generative AI into\ntheir workflows, emphasizing the importance of human oversight and the\nfast-evolving nature of these tools.\n","authors":["Rachael Fleurence","Jiang Bian","Xiaoyan Wang","Hua Xu","Dalia Dawoud","Mitch Higashi","Jagpreet Chhatwal"],"pdf_url":"https://arxiv.org/pdf/2407.11054v2.pdf","comment":"24 pages, 1 figure, 1 table, 2 boxes, 103 references"},{"id":"http://arxiv.org/abs/2312.02111v3","updated":"2024-08-01T14:01:56Z","published":"2023-12-04T18:43:45Z","title":"TriDeNT: Triple Deep Network Training for Privileged Knowledge\n  Distillation in Histopathology","summary":"  Computational pathology models rarely utilise data that will not be available\nfor inference. This means most models cannot learn from highly informative data\nsuch as additional immunohistochemical (IHC) stains and spatial\ntranscriptomics. We present TriDeNT, a novel self-supervised method for\nutilising privileged data that is not available during inference to improve\nperformance. We demonstrate the efficacy of this method for a range of\ndifferent paired data including immunohistochemistry, spatial transcriptomics\nand expert nuclei annotations. In all settings, TriDeNT outperforms other\nstate-of-the-art methods in downstream tasks, with observed improvements of up\nto 101%. Furthermore, we provide qualitative and quantitative measurements of\nthe features learned by these models and how they differ from baselines.\nTriDeNT offers a novel method to distil knowledge from scarce or costly data\nduring training, to create significantly better models for routine inputs.\n","authors":["Lucas Farndale","Robert Insall","Ke Yuan"],"pdf_url":"https://arxiv.org/pdf/2312.02111v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.10821v2","updated":"2024-08-01T13:53:48Z","published":"2023-08-21T16:13:23Z","title":"Optimized Deep Learning Models for Malware Detection under Concept Drift","summary":"  Despite the promising results of machine learning models in malicious files\ndetection, they face the problem of concept drift due to their constant\nevolution. This leads to declining performance over time, as the data\ndistribution of the new files differs from the training one, requiring frequent\nmodel update. In this work, we propose a model-agnostic protocol to improve a\nbaseline neural network against drift. We show the importance of feature\nreduction and training with the most recent validation set possible, and\npropose a loss function named Drift-Resilient Binary Cross-Entropy, an\nimprovement to the classical Binary Cross-Entropy more effective against drift.\nWe train our model on the EMBER dataset, published in2018, and evaluate it on a\ndataset of recent malicious files, collected between 2020 and 2023. Our\nimproved model shows promising results, detecting 15.2% more malware than a\nbaseline model.\n","authors":["William Maillet","Benjamin Marais"],"pdf_url":"https://arxiv.org/pdf/2308.10821v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14763v3","updated":"2024-08-01T13:35:22Z","published":"2024-04-23T05:56:35Z","title":"Evolutionary Reinforcement Learning via Cooperative Coevolution","summary":"  Recently, evolutionary reinforcement learning has obtained much attention in\nvarious domains. Maintaining a population of actors, evolutionary reinforcement\nlearning utilises the collected experiences to improve the behaviour policy\nthrough efficient exploration. However, the poor scalability of genetic\noperators limits the efficiency of optimising high-dimensional neural\nnetworks.To address this issue, this paper proposes a novel cooperative\ncoevolutionary reinforcement learning (CoERL) algorithm. Inspired by\ncooperative coevolution, CoERL periodically and adaptively decomposes the\npolicy optimisation problem into multiple subproblems and evolves a population\nof neural networks for each of the subproblems. Instead of using genetic\noperators, CoERL directly searches for partial gradients to update the policy.\nUpdating policy with partial gradients maintains consistency between the\nbehaviour spaces of parents and offspring across generations.The experiences\ncollected by the population are then used to improve the entire policy, which\nenhances the sampling efficiency.Experiments on six benchmark locomotion tasks\ndemonstrate that CoERL outperforms seven state-of-the-art algorithms and\nbaselines.Ablation study verifies the unique contribution of CoERL's core\ningredients.\n","authors":["Chengpeng Hu","Jialin Liu","Xin Yao"],"pdf_url":"https://arxiv.org/pdf/2404.14763v3.pdf","comment":"This paper is accepted by 27th European Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2405.01555v3","updated":"2024-08-01T11:54:46Z","published":"2024-03-18T02:28:10Z","title":"Digital Twin-Empowered Task Assignment in Aerial MEC Network: A Resource\n  Coalition Cooperation Approach with Generative Model","summary":"  To meet the demands for ubiquitous communication and temporary edge computing\nin 6G networks, aerial mobile edge computing (MEC) networks have been\nenvisioned as a new paradigm. However, dynamic user requests pose challenges\nfor task assignment strategies. Most of the existing research assumes that the\nstrategy is deployed on ground-based stations or UAVs, which will be\nineffective in an environment lacking infrastructure and continuous energy\nsupply. Moreover, the resource mutual exclusion problem of dynamic task\nassignment has not been effectively solved. Toward this end, we introduce the\ndigital twin (DT) into the aerial MEC network to study the resource coalition\ncooperation approach with the generative model (GM), which provides a\npreliminary coalition structure for the coalition game. Specifically, we\npropose a novel network framework that is composed of an application plane, a\nphysical plane, and a virtual plane. After that, the task assignment problem is\nsimplified to convex optimization programming with linear constraints. And\nthen, we also propose a resource coalition cooperation approach that is based\non a transferable utility (TU) coalition game to obtain an approximate optimal\nsolution. Numerical results confirm the effectiveness of our proposed approach\nin terms of energy consumption and utilization of resources.\n","authors":["Xin Tang","Qian Chen","Rong Yu","Xiaohuan Li"],"pdf_url":"https://arxiv.org/pdf/2405.01555v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16888v2","updated":"2024-08-01T11:46:26Z","published":"2024-06-08T12:46:12Z","title":"A Nested Model for AI Design and Validation","summary":"  The growing AI field faces trust, transparency, fairness, and discrimination\nchallenges. Despite the need for new regulations, there is a mismatch between\nregulatory science and AI, preventing a consistent framework. A five-layer\nnested model for AI design and validation aims to address these issues and\nstreamline AI application design and validation, improving fairness, trust, and\nAI adoption. This model aligns with regulations, addresses AI practitioner's\ndaily challenges, and offers prescriptive guidance for determining appropriate\nevaluation approaches by identifying unique validity threats. We have three\nrecommendations motivated by this model: authors should distinguish between\nlayers when claiming contributions to clarify the specific areas in which the\ncontribution is made and to avoid confusion, authors should explicitly state\nupstream assumptions to ensure that the context and limitations of their AI\nsystem are clearly understood, AI venues should promote thorough testing and\nvalidation of AI systems and their compliance with regulatory requirements.\n","authors":["Akshat Dubey","Zewen Yang","Georges Hattab"],"pdf_url":"https://arxiv.org/pdf/2407.16888v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09713v2","updated":"2024-08-01T11:24:13Z","published":"2024-03-11T15:15:27Z","title":"A Hybrid Intelligence Method for Argument Mining","summary":"  Large-scale survey tools enable the collection of citizen feedback in opinion\ncorpora. Extracting the key arguments from a large and noisy set of opinions\nhelps in understanding the opinions quickly and accurately. Fully automated\nmethods can extract arguments but (1) require large labeled datasets that\ninduce large annotation costs and (2) work well for known viewpoints, but not\nfor novel points of view. We propose HyEnA, a hybrid (human + AI) method for\nextracting arguments from opinionated texts, combining the speed of automated\nprocessing with the understanding and reasoning capabilities of humans. We\nevaluate HyEnA on three citizen feedback corpora. We find that, on the one\nhand, HyEnA achieves higher coverage and precision than a state-of-the-art\nautomated method when compared to a common set of diverse opinions, justifying\nthe need for human insight. On the other hand, HyEnA requires less human effort\nand does not compromise quality compared to (fully manual) expert analysis,\ndemonstrating the benefit of combining human and artificial intelligence.\n","authors":["Michiel van der Meer","Enrico Liscio","Catholijn M. Jonker","Aske Plaat","Piek Vossen","Pradeep K. Murukannaiah"],"pdf_url":"https://arxiv.org/pdf/2403.09713v2.pdf","comment":"Published in JAIR"},{"id":"http://arxiv.org/abs/2407.14364v2","updated":"2024-08-01T11:16:30Z","published":"2024-07-19T14:52:11Z","title":"Towards Assessing Data Replication in Music Generation with Music\n  Similarity Metrics on Raw Audio","summary":"  Recent advancements in music generation are raising multiple concerns about\nthe implications of AI in creative music processes, current business models and\nimpacts related to intellectual property management. A relevant discussion and\nrelated technical challenge is the potential replication and plagiarism of the\ntraining set in AI-generated music, which could lead to misuse of data and\nintellectual property rights violations. To tackle this issue, we present the\nMusic Replication Assessment (MiRA) tool: a model-independent open evaluation\nmethod based on diverse audio music similarity metrics to assess data\nreplication. We evaluate the ability of five metrics to identify exact\nreplication by conducting a controlled replication experiment in different\nmusic genres using synthetic samples. Our results show that the proposed\nmethodology can estimate exact data replication with a proportion higher than\n10%. By introducing the MiRA tool, we intend to encourage the open evaluation\nof music-generative models by researchers, developers, and users concerning\ndata replication, highlighting the importance of the ethical, social, legal,\nand economic consequences. Code and examples are available for reproducibility\npurposes.\n","authors":["Roser Batlle-Roca","Wei-Hisang Liao","Xavier Serra","Yuki Mitsufuji","Emilia Gómez"],"pdf_url":"https://arxiv.org/pdf/2407.14364v2.pdf","comment":"Accepted at ISMIR 2024"},{"id":"http://arxiv.org/abs/2406.12925v2","updated":"2024-08-01T10:09:15Z","published":"2024-06-14T13:54:29Z","title":"GLiNER multi-task: Generalist Lightweight Model for Various Information\n  Extraction Tasks","summary":"  Information extraction tasks require both accurate, efficient, and\ngeneralisable models. Classical supervised deep learning approaches can achieve\nthe required performance, but they need large datasets and are limited in their\nability to adapt to different tasks. On the other hand, large language models\n(LLMs) demonstrate good generalization, meaning that they can adapt to many\ndifferent tasks based on user requests. However, LLMs are computationally\nexpensive and tend to fail to generate structured outputs. In this article, we\nwill introduce a new kind of GLiNER model that can be used for various\ninformation extraction tasks while being a small encoder model. Our model\nachieved SoTA performance on zero-shot NER benchmarks and leading performance\non question-answering, summarization and relation extraction tasks.\nAdditionally, in this article, we will cover experimental results on\nself-learning approaches for named entity recognition using GLiNER models.\n","authors":["Ihor Stepanov","Mykhailo Shtopko"],"pdf_url":"https://arxiv.org/pdf/2406.12925v2.pdf","comment":"11 pages, 1 figure, 6 tables"},{"id":"http://arxiv.org/abs/2406.07558v2","updated":"2024-08-01T09:57:28Z","published":"2024-04-21T04:37:24Z","title":"An AI-Enabled Framework Within Reach for Enhancing Healthcare\n  Sustainability and Fairness","summary":"  Good health and well-being is among key issues in the United Nations 2030\nSustainable Development Goals. The rising prevalence of large-scale infectious\ndiseases and the accelerated aging of the global population are driving the\ntransformation of healthcare technologies. In this context, establishing\nlarge-scale public health datasets, developing medical models, and creating\ndecision-making systems with a human-centric approach are of strategic\nsignificance. Recently, by leveraging the extraordinary number of accessible\ncameras, groundbreaking advancements have emerged in AI methods for\nphysiological signal monitoring and disease diagnosis using camera sensors.\nThese approaches, requiring no specialized medical equipment, offer convenient\nmanners of collecting large-scale medical data in response to public health\nevents. Therefore, we outline a prospective framework and heuristic vision for\na camera-based public health (CBPH) framework utilizing visual physiological\nmonitoring technology. The CBPH can be considered as a convenient and universal\nframework for public health, advancing the United Nations Sustainable\nDevelopment Goals, particularly in promoting the universality, sustainability,\nand equity of healthcare in low- and middle-income countries or regions.\nFurthermore, CBPH provides a comprehensive solution for building a large-scale\nand human-centric medical database, and a multi-task large medical model for\npublic health and medical scientific discoveries. It has a significant\npotential to revolutionize personal monitoring technologies, digital medicine,\ntelemedicine, and primary health care in public health. Therefore, it can be\ndeemed that the outcomes of this paper will contribute to the establishment of\na sustainable and fair framework for public health, which serves as a crucial\nbridge for advancing scientific discoveries in the realm of AI for medicine\n(AI4Medicine).\n","authors":["Bin Huang","Changchen Zhao","Zimeng Liu","Shenda Hong","Baochang Zhang","Hao Lu","Zhijun Liu","Wenjin Wang","Hui Liu"],"pdf_url":"https://arxiv.org/pdf/2406.07558v2.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.00320v2","updated":"2024-08-01T09:07:45Z","published":"2024-03-30T11:13:18Z","title":"Advancing Multimodal Data Fusion in Pain Recognition: A Strategy\n  Leveraging Statistical Correlation and Human-Centered Perspectives","summary":"  This research presents a novel multimodal data fusion methodology for pain\nbehavior recognition, integrating statistical correlation analysis with\nhuman-centered insights. Our approach introduces two key innovations: 1)\nintegrating data-driven statistical relevance weights into the fusion strategy\nto effectively utilize complementary information from heterogeneous modalities,\nand 2) incorporating human-centric movement characteristics into multimodal\nrepresentation learning for detailed modeling of pain behaviors. Validated\nacross various deep learning architectures, our method demonstrates superior\nperformance and broad applicability. We propose a customizable framework that\naligns each modality with a suitable classifier based on statistical\nsignificance, advancing personalized and effective multimodal fusion.\nFurthermore, our methodology provides explainable analysis of multimodal data,\ncontributing to interpretable and explainable AI in healthcare. By highlighting\nthe importance of data diversity and modality-specific representations, we\nenhance traditional fusion techniques and set new standards for recognizing\ncomplex pain behaviors. Our findings have significant implications for\npromoting patient-centered healthcare interventions and supporting explainable\nclinical decision-making.\n","authors":["Xingrui Gu","Zhixuan Wang","Irisa Jin","Zekun Wu"],"pdf_url":"https://arxiv.org/pdf/2404.00320v2.pdf","comment":"Accepted by AHRI 2024"},{"id":"http://arxiv.org/abs/2405.01409v3","updated":"2024-08-01T08:58:40Z","published":"2024-05-02T16:01:58Z","title":"Goal-conditioned reinforcement learning for ultrasound navigation\n  guidance","summary":"  Transesophageal echocardiography (TEE) plays a pivotal role in cardiology for\ndiagnostic and interventional procedures. However, using it effectively\nrequires extensive training due to the intricate nature of image acquisition\nand interpretation. To enhance the efficiency of novice sonographers and reduce\nvariability in scan acquisitions, we propose a novel ultrasound (US) navigation\nassistance method based on contrastive learning as goal-conditioned\nreinforcement learning (GCRL). We augment the previous framework using a novel\ncontrastive patient batching method (CPB) and a data-augmented contrastive\nloss, both of which we demonstrate are essential to ensure generalization to\nanatomical variations across patients. The proposed framework enables\nnavigation to both standard diagnostic as well as intricate interventional\nviews with a single model. Our method was developed with a large dataset of 789\npatients and obtained an average error of 6.56 mm in position and 9.36 degrees\nin angle on a testing dataset of 140 patients, which is competitive or superior\nto models trained on individual views. Furthermore, we quantitatively validate\nour method's ability to navigate to interventional views such as the Left\nAtrial Appendage (LAA) view used in LAA closure. Our approach holds promise in\nproviding valuable guidance during transesophageal ultrasound examinations,\ncontributing to the advancement of skill acquisition for cardiac ultrasound\npractitioners.\n","authors":["Abdoul Aziz Amadou","Vivek Singh","Florin C. Ghesu","Young-Ho Kim","Laura Stanciulescu","Harshitha P. Sai","Puneet Sharma","Alistair Young","Ronak Rajani","Kawal Rhode"],"pdf_url":"https://arxiv.org/pdf/2405.01409v3.pdf","comment":"Accepted in MICCAI 2024; 11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2404.17591v2","updated":"2024-08-01T08:54:15Z","published":"2024-04-19T13:28:36Z","title":"Large Language Models for Next Point-of-Interest Recommendation","summary":"  The next Point of Interest (POI) recommendation task is to predict users'\nimmediate next POI visit given their historical data. Location-Based Social\nNetwork (LBSN) data, which is often used for the next POI recommendation task,\ncomes with challenges. One frequently disregarded challenge is how to\neffectively use the abundant contextual information present in LBSN data.\nPrevious methods are limited by their numerical nature and fail to address this\nchallenge. In this paper, we propose a framework that uses pretrained Large\nLanguage Models (LLMs) to tackle this challenge. Our framework allows us to\npreserve heterogeneous LBSN data in its original format, hence avoiding the\nloss of contextual information. Furthermore, our framework is capable of\ncomprehending the inherent meaning of contextual information due to the\ninclusion of commonsense knowledge. In experiments, we test our framework on\nthree real-world LBSN datasets. Our results show that the proposed framework\noutperforms the state-of-the-art models in all three datasets. Our analysis\ndemonstrates the effectiveness of the proposed framework in using contextual\ninformation as well as alleviating the commonly encountered cold-start and\nshort trajectory problems.\n","authors":["Peibo Li","Maarten de Rijke","Hao Xue","Shuang Ao","Yang Song","Flora D. Salim"],"pdf_url":"https://arxiv.org/pdf/2404.17591v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06925v2","updated":"2024-08-01T08:51:46Z","published":"2024-01-12T23:14:34Z","title":"Modeling Latent Selection with Structural Causal Models","summary":"  Selection bias is ubiquitous in real-world data, and can lead to misleading\nresults if not dealt with properly. We introduce a conditioning operation on\nStructural Causal Models (SCMs) to model latent selection from a causal\nperspective. We show that the conditioning operation transforms an SCM with the\npresence of an explicit latent selection mechanism into an SCM without such\nselection mechanism, which partially encodes the causal semantics of the\nselected subpopulation according to the original SCM. Furthermore, we show that\nthis conditioning operation preserves the simplicity, acyclicity, and linearity\nof SCMs, and commutes with marginalization. Thanks to these properties,\ncombined with marginalization and intervention, the conditioning operation\noffers a valuable tool for conducting causal reasoning tasks within causal\nmodels where latent details have been abstracted away. We demonstrate by\nexample how classical results of causal inference can be generalized to include\nselection bias and how the conditioning operation helps with modeling of\nreal-world problems.\n","authors":["Leihao Chen","Onno Zoeter","Joris M. Mooij"],"pdf_url":"https://arxiv.org/pdf/2401.06925v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17879v2","updated":"2024-08-01T08:18:57Z","published":"2024-07-25T08:47:40Z","title":"HG-PIPE: Vision Transformer Acceleration with Hybrid-Grained Pipeline","summary":"  Vision Transformer (ViT) acceleration with field programmable gate array\n(FPGA) is promising but challenging. Existing FPGA-based ViT accelerators\nmainly rely on temporal architectures, which process different operators by\nreusing the same hardware blocks and suffer from extensive memory access\noverhead. Pipelined architectures, either coarse-grained or fine-grained,\nunroll the ViT computation spatially for memory access efficiency. However,\nthey usually suffer from significant hardware resource constraints and pipeline\nbubbles induced by the global computation dependency of ViT. In this paper, we\nintroduce HG-PIPE, a pipelined FPGA accelerator for high-throughput and\nlow-latency ViT processing. HG-PIPE features a hybrid-grained pipeline\narchitecture to reduce on-chip buffer cost and couples the computation dataflow\nand parallelism design to eliminate the pipeline bubbles. HG-PIPE further\nintroduces careful approximations to implement both linear and non-linear\noperators with abundant Lookup Tables (LUTs), thus alleviating resource\nconstraints. On a ZCU102 FPGA, HG-PIPE achieves 2.78 times better throughput\nand 2.52 times better resource efficiency than the prior-art accelerators,\ne.g., AutoViTAcc. With a VCK190 FPGA, HG-PIPE realizes end-to-end ViT\nacceleration on a single device and achieves 7118 images/s, which is 2.81 times\nfaster than a V100 GPU.\n","authors":["Qingyu Guo","Jiayong Wan","Songqiang Xu","Meng Li","Yuan Wang"],"pdf_url":"https://arxiv.org/pdf/2407.17879v2.pdf","comment":"Accepted by ICCAD 2024"},{"id":"http://arxiv.org/abs/2312.10417v2","updated":"2024-08-01T08:03:48Z","published":"2023-12-16T11:06:11Z","title":"M^2ConceptBase: A Fine-Grained Aligned Concept-Centric Multimodal\n  Knowledge Base","summary":"  Multimodal knowledge bases (MMKBs) provide cross-modal aligned knowledge\ncrucial for multimodal tasks. However, the images in existing MMKBs are\ngenerally collected for entities in encyclopedia knowledge graphs. Therefore,\ndetailed groundings of visual semantics with linguistic concepts are lacking,\nwhich are essential for the visual concept cognition ability of multimodal\nmodels. Addressing this gap, we introduce M^2ConceptBase, the first\nconcept-centric MMKB. M^2ConceptBase models concepts as nodes with associated\nimages and detailed textual descriptions. We propose a context-aware multimodal\nsymbol grounding approach to align concept-image and concept-description pairs\nusing context information from image-text datasets. Comprising 951K images and\n152K concepts, M^2ConceptBase links each concept to an average of 6.27 images\nand a single description, ensuring comprehensive visual and textual semantics.\nHuman studies confirm more than 95% alignment accuracy, underscoring its\nquality. Additionally, our experiments demonstrate that M^2ConceptBase\nsignificantly enhances VQA model performance on the OK-VQA task. M^2ConceptBase\nalso substantially improves the fine-grained concept understanding capabilities\nof multimodal large language models through retrieval augmentation in two\nconcept-related tasks, highlighting its value.\n","authors":["Zhiwei Zha","Jiaan Wang","Zhixu Li","Xiangru Zhu","Wei Song","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2312.10417v2.pdf","comment":"Accepted by CIKM2024"},{"id":"http://arxiv.org/abs/2402.02563v3","updated":"2024-08-01T07:46:54Z","published":"2024-02-04T16:45:01Z","title":"DefInt: A Default-interventionist Framework for Efficient Reasoning with\n  Hybrid Large Language Models","summary":"  Large language models (LLMs) have shown impressive emergent abilities in a\nwide range of tasks, but still face challenges in handling complex reasoning\nproblems. Previous works like chain-of-thought (CoT) and tree-of-thoughts (ToT)\nhave predominately focused on enhancing accuracy, but overlook the rapidly\nincreasing token cost, which could be particularly problematic for open-ended\nreal-world tasks with huge solution spaces. Motivated by the dual process\ntheory of human cognition, we propose a Default-Interventionist framework\n(DefInt) to unleash the synergistic potential of hybrid LLMs. By default,\nDefInt uses smaller-scale language models to generate low-cost reasoning\nthoughts, which resembles the fast intuitions produced by System 1. If the\nintuitions are considered with low confidence, DefInt will invoke the\nreflective reasoning of scaled-up language models as the intervention of System\n2, which can override the default thoughts and rectify the reasoning process.\nExperiments on five representative reasoning tasks show that DefInt\nconsistently achieves state-of-the-art reasoning accuracy and solution\ndiversity. More importantly, it substantially reduces the token cost by 49%-79%\ncompared to the second accurate baselines. Specifically, the open-ended tasks\nhave an average 75% token cost reduction. Code repo with all prompts will be\nreleased upon publication.\n","authors":["Yu Shang","Yu Li","Fengli Xu","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2402.02563v3.pdf","comment":"18 pages, 10 figures, 14 tables"},{"id":"http://arxiv.org/abs/2312.03781v4","updated":"2024-08-01T07:29:47Z","published":"2023-12-06T09:39:38Z","title":"Lite-Mind: Towards Efficient and Robust Brain Representation Network","summary":"  The limited data availability and the low signal-to-noise ratio of fMRI\nsignals lead to the challenging task of fMRI-to-image retrieval.\nState-of-the-art MindEye remarkably improves fMRI-to-image retrieval\nperformance by leveraging a large model, i.e., a 996M MLP Backbone per subject,\nto align fMRI embeddings to the final hidden layer of CLIP's Vision Transformer\n(ViT). However, significant individual variations exist among subjects, even\nunder identical experimental setups, mandating the training of large\nsubject-specific models. The substantial parameters pose significant challenges\nin deploying fMRI decoding on practical devices. To this end, we propose\nLite-Mind, a lightweight, efficient, and robust brain representation learning\nparadigm based on Discrete Fourier Transform (DFT), which efficiently aligns\nfMRI voxels to fine-grained information of CLIP. We elaborately design a DFT\nbackbone with Spectrum Compression and Frequency Projector modules to learn\ninformative and robust voxel embeddings. Our experiments demonstrate that\nLite-Mind achieves an impressive 94.6% fMRI-to-image retrieval accuracy on the\nNSD dataset for Subject 1, with 98.7% fewer parameters than MindEye. Lite-Mind\nis also proven to be able to be migrated to smaller fMRI datasets and\nestablishes a new state-of-the-art for zero-shot classification on the GOD\ndataset.\n","authors":["Zixuan Gong","Qi Zhang","Guangyin Bao","Lei Zhu","Ke Liu","Liang Hu","Duoqian Miao","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.03781v4.pdf","comment":"17 pages, ACM MM 2024 Oral"},{"id":"http://arxiv.org/abs/2209.05467v2","updated":"2024-08-01T07:12:14Z","published":"2022-09-07T10:09:12Z","title":"Modelling Assessment Rubrics through Bayesian Networks: a Pragmatic\n  Approach","summary":"  Automatic assessment of learner competencies is a fundamental task in\nintelligent tutoring systems. An assessment rubric typically and effectively\ndescribes relevant competencies and competence levels. This paper presents an\napproach to deriving a learner model directly from an assessment rubric\ndefining some (partial) ordering of competence levels. The model is based on\nBayesian networks and exploits logical gates with uncertainty (often referred\nto as noisy gates) to reduce the number of parameters of the model, so to\nsimplify their elicitation by experts and allow real-time inference in\nintelligent tutoring systems. We illustrate how the approach can be applied to\nautomatize the human assessment of an activity developed for testing\ncomputational thinking skills. The simple elicitation of the model starting\nfrom the assessment rubric opens up the possibility of quickly automating the\nassessment of several tasks, making them more easily exploitable in the context\nof adaptive assessment tools and intelligent tutoring systems.\n","authors":["Francesca Mangili","Giorgia Adorni","Alberto Piatti","Claudio Bonesana","Alessandro Antonucci"],"pdf_url":"https://arxiv.org/pdf/2209.05467v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11409v4","updated":"2024-08-01T06:42:27Z","published":"2023-10-17T17:15:41Z","title":"LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks","summary":"  Penetration testing, an essential component of software security testing,\nallows organizations to identify and remediate vulnerabilities in their\nsystems, thus bolstering their defense mechanisms against cyberattacks. One\nrecent advancement in the realm of penetration testing is the utilization of\nLanguage Models (LLMs). We explore the intersection of LLMs and penetration\ntesting to gain insight into their capabilities and challenges in the context\nof privilege escalation. We introduce a fully automated privilege-escalation\ntool designed for evaluating the efficacy of LLMs for (ethical) hacking,\nexecuting benchmarks using multiple LLMs, and investigating their respective\nresults.\n  Our results show that GPT-4-turbo is well suited to exploit vulnerabilities\n(33-83% of vulnerabilities). GPT-3.5-turbo can abuse 16-50% of vulnerabilities,\nwhile local models, such as Llama3, can only exploit between 0 and 33% of the\nvulnerabilities.\n  We analyze the impact of different context sizes, in-context learning,\noptional high-level guidance mechanisms, and memory management techniques. We\ndiscuss challenging areas for LLMs, including maintaining focus during testing,\ncoping with errors, and finally comparing LLMs with human hackers.\n  The current version of the LLM-guided privilege-escalation prototype can be\nfound at https://github.com/ipa-labs/hackingBuddyGPT.\n","authors":["Andreas Happe","Aaron Kaplan","Juergen Cito"],"pdf_url":"https://arxiv.org/pdf/2310.11409v4.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2402.17934v2","updated":"2024-08-01T05:52:51Z","published":"2024-02-27T23:12:45Z","title":"Inducing Generalization across Languages and Tasks using Featurized\n  Low-Rank Mixtures","summary":"  Adapting pretrained large language models (LLMs) to various downstream tasks\nin tens or hundreds of human languages is computationally expensive.\nParameter-efficient fine-tuning (PEFT) significantly reduces the adaptation\ncost, by tuning only a small amount of parameters. However, common PEFT methods\nLoRA (Hu et al., 2022) suffer from suboptimal performance on diverse dataset\nmixtures, due to aggressive parameter tying and negative interference among\ndifferent datasets. In this work, we propose Featurized Low-rank Mixtures\n(FLix), a novel PEFT method designed for effective multitask multilingual\nadaptation. FLix associates each unique dataset feature, such as the dataset's\nlanguage or task, with its own low-rank weight update parameters. By composing\nfeature-specific parameters for each dataset, FLix can accommodate diverse\ndataset mixtures and generalize better to unseen datasets. Our experiments show\nthat FLix leads to significant improvements over a variety of tasks for both\nsupervised learning and zero-shot settings with gains of up to $14.2$ inexact\nmatch points in zero-shot semantic parsing.\n","authors":["Chu-Cheng Lin","Xinyi Wang","Jonathan H. Clark","Han Lu","Yun Zhu","Chenxi Whitehouse","Hongkun Yu"],"pdf_url":"https://arxiv.org/pdf/2402.17934v2.pdf","comment":"Revised version"},{"id":"http://arxiv.org/abs/2303.08046v2","updated":"2024-08-01T05:15:43Z","published":"2023-03-07T09:20:39Z","title":"Ultra-High-Resolution Detector Simulation with Intra-Event Aware GAN and\n  Self-Supervised Relational Reasoning","summary":"  Simulating high-resolution detector responses is a computationally intensive\nprocess that has long been challenging in Particle Physics. Despite the ability\nof generative models to streamline it, full ultra-high-granularity detector\nsimulation still proves to be difficult as it contains correlated and\nfine-grained information. To overcome these limitations, we propose Intra-Event\nAware Generative Adversarial Network (IEA-GAN). IEA-GAN presents a Relational\nReasoning Module that approximates an event in detector simulation, generating\ncontextualized high-resolution full detector responses with a proper relational\ninductive bias. IEA-GAN also introduces a Self-Supervised intra-event aware\nloss and Uniformity loss, significantly enhancing sample fidelity and\ndiversity. We demonstrate IEA-GAN's application in generating sensor-dependent\nimages for the ultra-high-granularity Pixel Vertex Detector (PXD), with more\nthan 7.5 M information channels at the Belle II Experiment. Applications of\nthis work span from Foundation Models for high-granularity detector simulation,\nsuch as at the HL-LHC (High Luminosity LHC), to simulation-based inference and\nfine-grained density estimation. To our knowledge, IEA-GAN is the first\nalgorithm for faithful ultra-high-granularity full detector simulation with\nevent-based reasoning.\n","authors":["Baran Hashemi","Nikolai Hartmann","Sahand Sharifzadeh","James Kahn","Thomas Kuhr"],"pdf_url":"https://arxiv.org/pdf/2303.08046v2.pdf","comment":"Published at Nature Communications"},{"id":"http://arxiv.org/abs/2401.11864v5","updated":"2024-08-01T04:03:32Z","published":"2024-01-22T11:37:18Z","title":"Distilling Mathematical Reasoning Capabilities into Small Language\n  Models","summary":"  This work addresses the challenge of democratizing advanced Large Language\nModels (LLMs) by compressing their mathematical reasoning capabilities into\nsub-billion parameter Small Language Models (SLMs) without compromising\nperformance. We introduce Equation-of-Thought Distillation (EoTD), a novel\ntechnique that encapsulates the reasoning process into equation-based\nrepresentations to construct an EoTD dataset for fine-tuning SLMs.\nAdditionally, we propose the Ensemble Thoughts Distillation (ETD) framework to\nenhance the reasoning performance of SLMs. This involves creating a reasoning\ndataset with multiple thought processes, including Chain-of-Thought (CoT),\nProgram-of-Thought (PoT), and Equation-of-Thought (EoT), and using it for\nfine-tuning. Our experimental performance demonstrates that EoTD significantly\nboosts the reasoning abilities of SLMs, while ETD enables these models to\nachieve state-of-the-art reasoning performance.\n","authors":["Xunyu Zhu","Jian Li","Yong Liu","Can Ma","Weiping Wang"],"pdf_url":"https://arxiv.org/pdf/2401.11864v5.pdf","comment":"Accepted for publication in Neural Networks"},{"id":"http://arxiv.org/abs/2401.09786v4","updated":"2024-08-01T03:57:24Z","published":"2024-01-18T08:10:34Z","title":"Adaptive Self-training Framework for Fine-grained Scene Graph Generation","summary":"  Scene graph generation (SGG) models have suffered from inherent problems\nregarding the benchmark datasets such as the long-tailed predicate distribution\nand missing annotation problems. In this work, we aim to alleviate the\nlong-tailed problem of SGG by utilizing unannotated triplets. To this end, we\nintroduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels\nfor unannotated triplets based on which the SGG models are trained. While there\nhas been significant progress in self-training for image recognition, designing\na self-training framework for the SGG task is more challenging due to its\ninherent nature such as the semantic ambiguity and the long-tailed distribution\nof predicate classes. Hence, we propose a novel pseudo-labeling technique for\nSGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is\na model-agnostic framework that can be applied to any existing SGG models.\nFurthermore, we devise a graph structure learner (GSL) that is beneficial when\nadopting our proposed self-training framework to the state-of-the-art\nmessage-passing neural network (MPNN)-based SGG models. Our extensive\nexperiments verify the effectiveness of ST-SGG on various SGG models,\nparticularly in enhancing the performance on fine-grained predicate classes.\n","authors":["Kibum Kim","Kanghoon Yoon","Yeonjun In","Jinyoung Moon","Donghyun Kim","Chanyoung Park"],"pdf_url":"https://arxiv.org/pdf/2401.09786v4.pdf","comment":"9 pages; ICLR 2024"},{"id":"http://arxiv.org/abs/2312.11539v3","updated":"2024-08-01T03:19:03Z","published":"2023-12-15T23:34:05Z","title":"KGLens: Towards Efficient and Effective Knowledge Probing of Large\n  Language Models with Knowledge Graphs","summary":"  Large Language Models (LLMs) might hallucinate facts, while curated Knowledge\nGraph (KGs) are typically factually reliable especially with domain-specific\nknowledge. Measuring the alignment between KGs and LLMs can effectively probe\nthe factualness and identify the knowledge blind spots of LLMs. However,\nverifying the LLMs over extensive KGs can be expensive. In this paper, we\npresent KGLens, a Thompson-sampling-inspired framework aimed at effectively and\nefficiently measuring the alignment between KGs and LLMs. KGLens features a\ngraph-guided question generator for converting KGs into natural language, along\nwith a carefully designed importance sampling strategy based on parameterized\nKG structure to expedite KG traversal. Our simulation experiment compares the\nbrute force method with KGLens under six different sampling methods,\ndemonstrating that our approach achieves superior probing efficiency.\nLeveraging KGLens, we conducted in-depth analyses of the factual accuracy of\nten LLMs across three large domain-specific KGs from Wikidata, composing over\n19K edges, 700 relations, and 21K entities. Human evaluation results indicate\nthat KGLens can assess LLMs with a level of accuracy nearly equivalent to that\nof human annotators, achieving 95.7% of the accuracy rate.\n","authors":["Shangshang Zheng","He Bai","Yizhe Zhang","Yi Su","Xiaochuan Niu","Navdeep Jaitly"],"pdf_url":"https://arxiv.org/pdf/2312.11539v3.pdf","comment":"ACL 2024 Workshop Towards Knowledgeable Language Models"},{"id":"http://arxiv.org/abs/2407.01614v2","updated":"2024-08-01T02:56:58Z","published":"2024-06-28T01:46:10Z","title":"Enhancing Stability for Large Models Training in Constrained Bandwidth\n  Networks","summary":"  Training extremely large language models with billions of parameters is a\ncomputationally intensive task that pushes the limits of current data parallel\ntraining systems. While techniques like ZeRO++ have enabled efficient\ndistributed training of such giant models on inexpensive low-bandwidth\nclusters, they can suffer from convergence issues due to potential race\nconditions in the hierarchical partitioning (hpZ) scheme employed to reduce\ncross-machine communication. In this work, we first show how these race\nconditions cause instability when training models with billions of parameters.\nWe then propose a modification to the partitioning algorithm that addresses\nthese convergence challenges while maintaining competitive training efficiency.\nEmpirical evaluation on training the multi-billion parameters Falcon Models and\nLlama-2 models demonstrates the updated algorithm's ability to achieve reliable\nconvergence on these massive models, where stock ZeRO++ hpZ fails to converge.\nThe updated algorithm enables robust training of larger models with 98\\%\nthroughput and model training speed improvement without sacrificing the quality\nof convergence.\n","authors":["Yun Dai","Tejas Dharamsi","Byron Hsu","Tao Song","Hamed Firooz"],"pdf_url":"https://arxiv.org/pdf/2407.01614v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11816v2","updated":"2024-08-01T02:49:00Z","published":"2023-12-19T03:15:50Z","title":"A Dual-way Enhanced Framework from Text Matching Point of View for\n  Multimodal Entity Linking","summary":"  Multimodal Entity Linking (MEL) aims at linking ambiguous mentions with\nmultimodal information to entity in Knowledge Graph (KG) such as Wikipedia,\nwhich plays a key role in many applications. However, existing methods suffer\nfrom shortcomings, including modality impurity such as noise in raw image and\nambiguous textual entity representation, which puts obstacles to MEL. We\nformulate multimodal entity linking as a neural text matching problem where\neach multimodal information (text and image) is treated as a query, and the\nmodel learns the mapping from each query to the relevant entity from candidate\nentities. This paper introduces a dual-way enhanced (DWE) framework for MEL:\n(1) our model refines queries with multimodal data and addresses semantic gaps\nusing cross-modal enhancers between text and image information. Besides, DWE\ninnovatively leverages fine-grained image attributes, including facial\ncharacteristic and scene feature, to enhance and refine visual features. (2)By\nusing Wikipedia descriptions, DWE enriches entity semantics and obtains more\ncomprehensive textual representation, which reduces between textual\nrepresentation and the entities in KG. Extensive experiments on three public\nbenchmarks demonstrate that our method achieves state-of-the-art (SOTA)\nperformance, indicating the superiority of our model. The code is released on\nhttps://github.com/season1blue/DWE\n","authors":["Shezheng Song","Shan Zhao","Chengyu Wang","Tianwei Yan","Shasha Li","Xiaoguang Mao","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2312.11816v2.pdf","comment":"AAAI23 Accept"},{"id":"http://arxiv.org/abs/2407.08516v3","updated":"2024-08-01T02:29:50Z","published":"2024-07-11T14:00:53Z","title":"Converging Paradigms: The Synergy of Symbolic and Connectionist AI in\n  LLM-Empowered Autonomous Agents","summary":"  This article explores the convergence of connectionist and symbolic\nartificial intelligence (AI), from historical debates to contemporary\nadvancements. Traditionally considered distinct paradigms, connectionist AI\nfocuses on neural networks, while symbolic AI emphasizes symbolic\nrepresentation and logic. Recent advancements in large language models (LLMs),\nexemplified by ChatGPT and GPT-4, highlight the potential of connectionist\narchitectures in handling human language as a form of symbols. The study argues\nthat LLM-empowered Autonomous Agents (LAAs) embody this paradigm convergence.\nBy utilizing LLMs for text-based knowledge modeling and representation, LAAs\nintegrate neuro-symbolic AI principles, showcasing enhanced reasoning and\ndecision-making capabilities. Comparing LAAs with Knowledge Graphs within the\nneuro-symbolic AI theme highlights the unique strengths of LAAs in mimicking\nhuman-like reasoning processes, scaling effectively with large datasets, and\nleveraging in-context samples without explicit re-training. The research\nunderscores promising avenues in neuro-vector-symbolic integration,\ninstructional encoding, and implicit reasoning, aimed at further enhancing LAA\ncapabilities. By exploring the progression of neuro-symbolic AI and proposing\nfuture research trajectories, this work advances the understanding and\ndevelopment of AI technologies.\n","authors":["Haoyi Xiong","Zhiyuan Wang","Xuhong Li","Jiang Bian","Zeke Xie","Shahid Mumtaz","Laura E. Barnes"],"pdf_url":"https://arxiv.org/pdf/2407.08516v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14055v2","updated":"2024-08-01T02:19:08Z","published":"2024-07-19T06:31:22Z","title":"Quantum Hamiltonian Embedding of Images for Data Reuploading Classifiers","summary":"  When applying quantum computing to machine learning tasks, one of the first\nconsiderations is the design of the quantum machine learning model itself.\nConventionally, the design of quantum machine learning algorithms relies on the\n``quantisation\" of classical learning algorithms, such as using quantum linear\nalgebra to implement important subroutines of classical algorithms, if not the\nentire algorithm, seeking to achieve quantum advantage through possible\nrun-time accelerations brought by quantum computing. However, recent research\nhas started questioning whether quantum advantage via speedup is the right goal\nfor quantum machine learning [1]. Research also has been undertaken to exploit\nproperties that are unique to quantum systems, such as quantum contextuality,\nto better design quantum machine learning models [2]. In this paper, we take an\nalternative approach by incorporating the heuristics and empirical evidences\nfrom the design of classical deep learning algorithms to the design of quantum\nneural networks. We first construct a model based on the data reuploading\ncircuit [3] with the quantum Hamiltonian data embedding unitary [4]. Through\nnumerical experiments on images datasets, including the famous MNIST and\nFashionMNIST datasets, we demonstrate that our model outperforms the quantum\nconvolutional neural network (QCNN)[5] by a large margin (up to over 40% on\nMNIST test set). Based on the model design process and numerical results, we\nthen laid out six principles for designing quantum machine learning models,\nespecially quantum neural networks.\n","authors":["Peiyong Wang","Casey R. Myers","Lloyd C. L. Hollenberg","Udaya Parampalli"],"pdf_url":"https://arxiv.org/pdf/2407.14055v2.pdf","comment":"11 figures, 31 pages. Code available on\n  https://github.com/peiyong-addwater/HamEmbedding. Author affiliation updated\n  for v2. Acknowledgements and funding information added for v2"},{"id":"http://arxiv.org/abs/2407.20299v2","updated":"2024-08-01T01:33:48Z","published":"2024-07-29T04:02:17Z","title":"Dataset Distillation for Offline Reinforcement Learning","summary":"  Offline reinforcement learning often requires a quality dataset that we can\ntrain a policy on. However, in many situations, it is not possible to get such\na dataset, nor is it easy to train a policy to perform well in the actual\nenvironment given the offline data. We propose using data distillation to train\nand distill a better dataset which can then be used for training a better\npolicy model. We show that our method is able to synthesize a dataset where a\nmodel trained on it achieves similar performance to a model trained on the full\ndataset or a model trained using percentile behavioral cloning. Our project\nsite is available at\n$\\href{https://datasetdistillation4rl.github.io}{\\text{here}}$. We also provide\nour implementation at $\\href{https://github.com/ggflow123/DDRL}{\\text{this\nGitHub repository}}$.\n","authors":["Jonathan Light","Yuanzhe Liu","Ziniu Hu"],"pdf_url":"https://arxiv.org/pdf/2407.20299v2.pdf","comment":"ICML 2024 DMLR Workshop"},{"id":"http://arxiv.org/abs/2407.21315v2","updated":"2024-08-01T01:17:34Z","published":"2024-07-31T03:53:14Z","title":"Beyond Silent Letters: Amplifying LLMs in Emotion Recognition with Vocal\n  Nuances","summary":"  This paper introduces a novel approach to emotion detection in speech using\nLarge Language Models (LLMs). We address the limitation of LLMs in processing\naudio inputs by translating speech characteristics into natural language\ndescriptions. Our method integrates these descriptions into text prompts,\nenabling LLMs to perform multimodal emotion analysis without architectural\nmodifications. We evaluate our approach on two datasets: IEMOCAP and MELD,\ndemonstrating significant improvements in emotion recognition accuracy,\nparticularly for high-quality audio data. Our experiments show that\nincorporating speech descriptions yields a 2 percentage point increase in\nweighted F1 score on IEMOCAP (from 70.111\\% to 72.596\\%). We also compare\nvarious LLM architectures and explore the effectiveness of different feature\nrepresentations. Our findings highlight the potential of this approach in\nenhancing emotion detection capabilities of LLMs and underscore the importance\nof audio quality in speech-based emotion recognition tasks. We'll release the\nsource code on Github.\n","authors":["Zehui Wu","Ziwei Gong","Lin Ai","Pengyuan Shi","Kaan Donbekci","Julia Hirschberg"],"pdf_url":"https://arxiv.org/pdf/2407.21315v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.10467v2","updated":"2024-08-01T01:07:35Z","published":"2024-01-19T03:39:43Z","title":"Learning Backdoors for Mixed Integer Linear Programs with Contrastive\n  Learning","summary":"  Many real-world problems can be efficiently modeled as Mixed Integer Linear\nPrograms (MILPs) and solved with the Branch-and-Bound method. Prior work has\nshown the existence of MILP backdoors, small sets of variables such that\nprioritizing branching on them when possible leads to faster running times.\nHowever, finding high-quality backdoors that improve running times remains an\nopen question. Previous work learns to estimate the relative solver speed of\nrandomly sampled backdoors through ranking and then decide whether to use the\nhighest-ranked backdoor candidate. In this paper, we utilize the Monte-Carlo\ntree search method to collect backdoors for training, rather than relying on\nrandom sampling, and adapt a contrastive learning framework to train a Graph\nAttention Network model to predict backdoors. Our method, evaluated on several\ncommon MILP problem domains, demonstrates performance improvements over both\nGurobi and previous models.\n","authors":["Junyang Cai","Taoan Huang","Bistra Dilkina"],"pdf_url":"https://arxiv.org/pdf/2401.10467v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00765v1","updated":"2024-08-01T17:59:54Z","published":"2024-08-01T17:59:54Z","title":"MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models\n  for Integrated Capabilities","summary":"  MM-Vet, with open-ended vision-language questions targeting at evaluating\nintegrated capabilities, has become one of the most popular benchmarks for\nlarge multimodal model evaluation. MM-Vet assesses six core vision-language\n(VL) capabilities: recognition, knowledge, spatial awareness, language\ngeneration, OCR, and math. However, its question format is restricted to single\nimage-text pairs, lacking the interleaved image and text sequences prevalent in\nreal-world scenarios. To address this limitation, we introduce MM-Vet v2, which\nincludes a new VL capability called \"image-text sequence understanding\",\nevaluating models' ability to process VL sequences. Furthermore, we maintain\nthe high quality of evaluation samples while further expanding the evaluation\nset size. Using MM-Vet v2 to benchmark large multimodal models, we found that\nClaude 3.5 Sonnet is the best model with a score of 71.8, slightly\noutperforming GPT-4o which scored 71.0. Among open-weight models,\nInternVL2-Llama3-76B leads with a score of 68.4.\n","authors":["Weihao Yu","Zhengyuan Yang","Linfeng Ren","Linjie Li","Jianfeng Wang","Kevin Lin","Chung-Ching Lin","Zicheng Liu","Lijuan Wang","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2408.00765v1.pdf","comment":"Extension of MM-Vet: arXiv:2308.02490"},{"id":"http://arxiv.org/abs/2408.00764v1","updated":"2024-08-01T17:59:46Z","published":"2024-08-01T17:59:46Z","title":"AgentGen: Enhancing Planning Abilities for Large Language Model based\n  Agent via Environment and Task Generation","summary":"  Large Language Model (LLM) based agents have garnered significant attention\nand are becoming increasingly popular. Furthermore, planning ability is a\ncrucial component of an LLM-based agent, involving interaction with the\nenvironment and executing actions to complete a planning task, which generally\nentails achieving a desired goal from an initial state. This paper investigates\nenhancing the planning abilities of LLMs through instruction tuning, referred\nto as agent training. Recent studies have demonstrated that utilizing\nexpert-level trajectory for instruction-tuning LLMs effectively enhances their\nplanning capabilities. However, existing work primarily focuses on synthesizing\ntrajectories from manually designed planning tasks and environments. The\nlabor-intensive nature of creating these environments and tasks impedes the\ngeneration of sufficiently varied and extensive trajectories. To address this\nlimitation, this paper explores the automated synthesis of diverse environments\nand a gradual range of planning tasks, from easy to difficult. We introduce a\nframework, AgentGen, that leverages LLMs first to generate environments and\nsubsequently generate planning tasks conditioned on these environments.\nSpecifically, to improve environmental diversity, we propose using an\ninspiration corpus composed of various domain-specific text segments as the\ncontext for synthesizing environments. Moreover, to increase the difficulty\ndiversity of generated planning tasks, we propose a bidirectional evolution\nmethod, Bi-Evol, that evolves planning tasks from easier and harder directions\nto synthesize a task set with a smoother difficulty curve. The evaluation\nresults derived from AgentBoard show that AgentGen greatly improves LLMs'\nplanning ability, e.g., the AgentGen instruction-tuned Llama-3 8B surpasses\nGPT-3.5 in overall performance. Moreover, in certain tasks, it even outperforms\nGPT-4.\n","authors":["Mengkang Hu","Pu Zhao","Can Xu","Qingfeng Sun","Jianguang Lou","Qingwei Lin","Ping Luo","Saravan Rajmohan","Dongmei Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00761v1","updated":"2024-08-01T17:59:12Z","published":"2024-08-01T17:59:12Z","title":"Tamper-Resistant Safeguards for Open-Weight LLMs","summary":"  Rapid advances in the capabilities of large language models (LLMs) have\nraised widespread concerns regarding their potential for malicious use.\nOpen-weight LLMs present unique challenges, as existing safeguards lack\nrobustness to tampering attacks that modify model weights. For example, recent\nworks have demonstrated that refusal and unlearning safeguards can be trivially\nremoved with a few steps of fine-tuning. These vulnerabilities necessitate new\napproaches for enabling the safe release of open-weight LLMs. We develop a\nmethod, called TAR, for building tamper-resistant safeguards into open-weight\nLLMs such that adversaries cannot remove the safeguards even after thousands of\nsteps of fine-tuning. In extensive evaluations and red teaming analyses, we\nfind that our method greatly improves tamper-resistance while preserving benign\ncapabilities. Our results demonstrate that tamper-resistance is a tractable\nproblem, opening up a promising new avenue to improve the safety and security\nof open-weight LLMs.\n","authors":["Rishub Tamirisa","Bhrugu Bharathi","Long Phan","Andy Zhou","Alice Gatti","Tarun Suresh","Maxwell Lin","Justin Wang","Rowan Wang","Ron Arel","Andy Zou","Dawn Song","Bo Li","Dan Hendrycks","Mantas Mazeika"],"pdf_url":"https://arxiv.org/pdf/2408.00761v1.pdf","comment":"Website: https://www.tamper-resistant-safeguards.com"},{"id":"http://arxiv.org/abs/2408.00760v1","updated":"2024-08-01T17:59:09Z","published":"2024-08-01T17:59:09Z","title":"Smoothed Energy Guidance: Guiding Diffusion Models with Reduced Energy\n  Curvature of Attention","summary":"  Conditional diffusion models have shown remarkable success in visual content\ngeneration, producing high-quality samples across various domains, largely due\nto classifier-free guidance (CFG). Recent attempts to extend guidance to\nunconditional models have relied on heuristic techniques, resulting in\nsuboptimal generation quality and unintended effects. In this work, we propose\nSmoothed Energy Guidance (SEG), a novel training- and condition-free approach\nthat leverages the energy-based perspective of the self-attention mechanism to\nenhance image generation. By defining the energy of self-attention, we\nintroduce a method to reduce the curvature of the energy landscape of attention\nand use the output as the unconditional prediction. Practically, we control the\ncurvature of the energy landscape by adjusting the Gaussian kernel parameter\nwhile keeping the guidance scale parameter fixed. Additionally, we present a\nquery blurring method that is equivalent to blurring the entire attention\nweights without incurring quadratic complexity in the number of tokens. In our\nexperiments, SEG achieves a Pareto improvement in both quality and the\nreduction of side effects. The code is available at\n\\url{https://github.com/SusungHong/SEG-SDXL}.\n","authors":["Susung Hong"],"pdf_url":"https://arxiv.org/pdf/2408.00760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00756v1","updated":"2024-08-01T17:57:25Z","published":"2024-08-01T17:57:25Z","title":"Segment anything model 2: an application to 2D and 3D medical images","summary":"  Segment Anything Model (SAM) has gained significant attention because of its\nability to segment a variety of objects in images given a prompt. The recently\ndeveloped SAM 2 has extended this ability to video inputs. This opens an\nopportunity to apply SAM to 3D images, one of the fundamental tasks in the\nmedical imaging field. In this paper, we provide an extensive evaluation of SAM\n2's ability to segment both 2D and 3D medical images. We collect 18 medical\nimaging datasets, including common 3D modalities such as computed tomography\n(CT), magnetic resonance imaging (MRI), and positron emission tomography (PET)\nas well as 2D modalities such as X-ray and ultrasound. We consider two\nevaluation pipelines of SAM 2: (1) multi-frame 3D segmentation, where prompts\nare provided to one or multiple slice(s) selected from the volume, and (2)\nsingle-frame 2D segmentation, where prompts are provided to each slice. The\nformer is only applicable to 3D modalities, while the latter applies to both 2D\nand 3D modalities. We learn that SAM 2 exhibits similar performance as SAM\nunder single-frame 2D segmentation, and has variable performance under\nmulti-frame 3D segmentation depending on the choices of slices to annotate, the\ndirection of the propagation, the predictions utilized during the propagation,\netc.\n","authors":["Haoyu Dong","Hanxue Gu","Yaqian Chen","Jichen Yang","Maciej A. Mazurowski"],"pdf_url":"https://arxiv.org/pdf/2408.00756v1.pdf","comment":"11 pages, 7 figures. A first attempt on evaluating SAM 2 on medical\n  images"},{"id":"http://arxiv.org/abs/2408.00753v1","updated":"2024-08-01T17:56:25Z","published":"2024-08-01T17:56:25Z","title":"A deep learning-enabled smart garment for versatile sleep behaviour\n  monitoring","summary":"  Continuous monitoring and accurate detection of complex sleep patterns\nassociated to different sleep-related conditions is essential, not only for\nenhancing sleep quality but also for preventing the risk of developing chronic\nillnesses associated to unhealthy sleep. Despite significant advances in\nresearch, achieving versatile recognition of various unhealthy and sub-healthy\nsleep patterns with simple wearable devices at home remains a significant\nchallenge. Here, we report a robust and durable ultrasensitive strain sensor\narray printed on a smart garment, in its collar region. This solution allows\ndetecting subtle vibrations associated with multiple sleep patterns at the\nextrinsic laryngeal muscles. Equipped with a deep learning neural network, it\ncan precisely identify six sleep states-nasal breathing, mouth breathing,\nsnoring, bruxism, central sleep apnea (CSA), and obstructive sleep apnea\n(OSA)-with an impressive accuracy of 98.6%, all without requiring specific\npositioning. We further demonstrate its explainability and generalization\ncapabilities in practical applications. Explainable artificial intelligence\n(XAI) visualizations reflect comprehensive signal pattern analysis with low\nbias. Transfer learning tests show that the system can achieve high accuracy\n(overall accuracy of 95%) on new users with very few-shot learning (less than\n15 samples per class). The scalable manufacturing process, robustness, high\naccuracy, and excellent generalization of the smart garment make it a promising\ntool for next-generation continuous sleep monitoring.\n","authors":["Chenyu Tang","Wentian Yi","Muzi Xu","Yuxuan Jin","Zibo Zhang","Xuhang Chen","Caizhi Liao","Peter Smielewski","Luigi G. Occhipinti"],"pdf_url":"https://arxiv.org/pdf/2408.00753v1.pdf","comment":"18 pages, 5 figures, 1 table"},{"id":"http://arxiv.org/abs/2408.00751v1","updated":"2024-08-01T17:54:01Z","published":"2024-08-01T17:54:01Z","title":"A Policy-Gradient Approach to Solving Imperfect-Information Games with\n  Iterate Convergence","summary":"  Policy gradient methods have become a staple of any single-agent\nreinforcement learning toolbox, due to their combination of desirable\nproperties: iterate convergence, efficient use of stochastic trajectory\nfeedback, and theoretically-sound avoidance of importance sampling corrections.\nIn multi-agent imperfect-information settings (extensive-form games), however,\nit is still unknown whether the same desiderata can be guaranteed while\nretaining theoretical guarantees. Instead, sound methods for extensive-form\ngames rely on approximating counterfactual values (as opposed to Q values),\nwhich are incompatible with policy gradient methodologies. In this paper, we\ninvestigate whether policy gradient can be safely used in two-player zero-sum\nimperfect-information extensive-form games (EFGs). We establish positive\nresults, showing for the first time that a policy gradient method leads to\nprovable best-iterate convergence to a regularized Nash equilibrium in\nself-play.\n","authors":["Mingyang Liu","Gabriele Farina","Asuman Ozdaglar"],"pdf_url":"https://arxiv.org/pdf/2408.00751v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00749v1","updated":"2024-08-01T17:52:10Z","published":"2024-08-01T17:52:10Z","title":"Leaf Angle Estimation using Mask R-CNN and LETR Vision Transformer","summary":"  Modern day studies show a high degree of correlation between high yielding\ncrop varieties and plants with upright leaf angles. It is observed that plants\nwith upright leaf angles intercept more light than those without upright leaf\nangles, leading to a higher rate of photosynthesis. Plant scientists and\nbreeders benefit from tools that can directly measure plant parameters in the\nfield i.e. on-site phenotyping. The estimation of leaf angles by manual means\nin a field setting is tedious and cumbersome. We mitigate the tedium using a\ncombination of the Mask R-CNN instance segmentation neural network, and Line\nSegment Transformer (LETR), a vision transformer. The proposed Computer Vision\n(CV) pipeline is applied on two image datasets, Summer 2015-Ames ULA and Summer\n2015- Ames MLA, with a combined total of 1,827 plant images collected in the\nfield using FieldBook, an Android application aimed at on-site phenotyping. The\nleaf angles estimated by the proposed pipeline on the image datasets are\ncompared to two independent manual measurements using ImageJ, a Java-based\nimage processing program developed at the National Institutes of Health and the\nLaboratory for Optical and Computational Instrumentation. The results, when\ncompared for similarity using the Cosine Similarity measure, exhibit 0.98\nsimilarity scores on both independent measurements of Summer 2015-Ames ULA and\nSummer 2015-Ames MLA image datasets, demonstrating the feasibility of the\nproposed pipeline for on-site measurement of leaf angles.\n","authors":["Venkat Margapuri","Prapti Thapaliya","Trevor Rife"],"pdf_url":"https://arxiv.org/pdf/2408.00749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00741v1","updated":"2024-08-01T17:40:45Z","published":"2024-08-01T17:40:45Z","title":"DynamoLLM: Designing LLM Inference Clusters for Performance and Energy\n  Efficiency","summary":"  The rapid evolution and widespread adoption of generative large language\nmodels (LLMs) have made them a pivotal workload in various applications. Today,\nLLM inference clusters receive a large number of queries with strict Service\nLevel Objectives (SLOs). To achieve the desired performance, these models\nexecute on power-hungry GPUs causing the inference clusters to consume large\namount of energy and, consequently, result in excessive carbon emissions.\nFortunately, we find that there is a great opportunity to exploit the\nheterogeneity in inference compute properties and fluctuations in inference\nworkloads, to significantly improve energy-efficiency. However, such a diverse\nand dynamic environment creates a large search-space where different system\nconfigurations (e.g., number of instances, model parallelism, and GPU\nfrequency) translate into different energy-performance trade-offs. To address\nthese challenges, we propose DynamoLLM, the first energy-management framework\nfor LLM inference environments. DynamoLLM automatically and dynamically\nreconfigures the inference cluster to optimize for energy and cost of LLM\nserving under the service's performance SLOs. We show that at a service-level,\nDynamoLLM conserves 53% energy and 38% operational carbon emissions, and\nreduces 61% cost to the customer, while meeting the latency SLOs.\n","authors":["Jovan Stojkovic","Chaojie Zhang","Íñigo Goiri","Josep Torrellas","Esha Choukse"],"pdf_url":"https://arxiv.org/pdf/2408.00741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00727v1","updated":"2024-08-01T17:18:17Z","published":"2024-08-01T17:18:17Z","title":"Improving Retrieval-Augmented Generation in Medicine with Iterative\n  Follow-up Questions","summary":"  The emergent abilities of large language models (LLMs) have demonstrated\ngreat potential in solving medical questions. They can possess considerable\nmedical knowledge, but may still hallucinate and are inflexible in the\nknowledge updates. While Retrieval-Augmented Generation (RAG) has been proposed\nto enhance the medical question-answering capabilities of LLMs with external\nknowledge bases, it may still fail in complex cases where multiple rounds of\ninformation-seeking are required. To address such an issue, we propose\niterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up\nqueries based on previous information-seeking attempts. In each iteration of\ni-MedRAG, the follow-up queries will be answered by a vanilla RAG system and\nthey will be further used to guide the query generation in the next iteration.\nOur experiments show the improved performance of various LLMs brought by\ni-MedRAG compared with vanilla RAG on complex questions from clinical vignettes\nin the United States Medical Licensing Examination (USMLE), as well as various\nknowledge tests in the Massive Multitask Language Understanding (MMLU) dataset.\nNotably, our zero-shot i-MedRAG outperforms all existing prompt engineering and\nfine-tuning methods on GPT-3.5, achieving an accuracy of 69.68\\% on the MedQA\ndataset. In addition, we characterize the scaling properties of i-MedRAG with\ndifferent iterations of follow-up queries and different numbers of queries per\niteration. Our case studies show that i-MedRAG can flexibly ask follow-up\nqueries to form reasoning chains, providing an in-depth analysis of medical\nquestions. To the best of our knowledge, this is the first-of-its-kind study on\nincorporating follow-up queries into medical RAG.\n","authors":["Guangzhi Xiong","Qiao Jin","Xiao Wang","Minjia Zhang","Zhiyong Lu","Aidong Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00724v1","updated":"2024-08-01T17:16:04Z","published":"2024-08-01T17:16:04Z","title":"An Empirical Analysis of Compute-Optimal Inference for Problem-Solving\n  with Language Models","summary":"  The optimal training configurations of large language models (LLMs) with\nrespect to model sizes and compute budgets have been extensively studied. But\nhow to optimally configure LLMs during inference has not been explored in\nsufficient depth. We study compute-optimal inference: designing models and\ninference strategies that optimally trade off additional inference-time compute\nfor improved performance. As a first step towards understanding and designing\ncompute-optimal inference methods, we assessed the effectiveness and\ncomputational efficiency of multiple inference strategies such as Greedy\nSearch, Majority Voting, Best-of-N, Weighted Voting, and their variants on two\ndifferent Tree Search algorithms, involving different model sizes and\ncomputational budgets. We found that a smaller language model with a novel tree\nsearch algorithm typically achieves a Pareto-optimal trade-off. These results\nhighlight the potential benefits of deploying smaller models equipped with more\nsophisticated decoding algorithms in budget-constrained scenarios, e.g., on\nend-devices, to enhance problem-solving accuracy. For instance, we show that\nthe Llemma-7B model can achieve competitive accuracy to a Llemma-34B model on\nMATH500 while using $2\\times$ less FLOPs. Our findings could potentially apply\nto any generation task with a well-defined measure of success.\n","authors":["Yangzhen Wu","Zhiqing Sun","Shanda Li","Sean Welleck","Yiming Yang"],"pdf_url":"https://arxiv.org/pdf/2408.00724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00722v1","updated":"2024-08-01T17:15:13Z","published":"2024-08-01T17:15:13Z","title":"Pathway to Secure and Trustworthy 6G for LLMs: Attacks, Defense, and\n  Opportunities","summary":"  Recently, large language models (LLMs) have been gaining a lot of interest\ndue to their adaptability and extensibility in emerging applications, including\ncommunication networks. It is anticipated that 6G mobile edge computing\nnetworks will be able to support LLMs as a service, as they provide ultra\nreliable low-latency communications and closed loop massive connectivity.\nHowever, LLMs are vulnerable to data and model privacy issues that affect the\ntrustworthiness of LLMs to be deployed for user-based services. In this paper,\nwe explore the security vulnerabilities associated with fine-tuning LLMs in 6G\nnetworks, in particular the membership inference attack. We define the\ncharacteristics of an attack network that can perform a membership inference\nattack if the attacker has access to the fine-tuned model for the downstream\ntask. We show that the membership inference attacks are effective for any\ndownstream task, which can lead to a personal data breach when using LLM as a\nservice. The experimental results show that the attack success rate of maximum\n92% can be achieved on named entity recognition task. Based on the experimental\nanalysis, we discuss possible defense mechanisms and present possible research\ndirections to make the LLMs more trustworthy in the context of 6G networks.\n","authors":["Sunder Ali Khowaja","Parus Khuwaja","Kapal Dev","Hussam Al Hamadi","Engin Zeydan"],"pdf_url":"https://arxiv.org/pdf/2408.00722v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.00714v1","updated":"2024-08-01T17:00:08Z","published":"2024-08-01T17:00:08Z","title":"SAM 2: Segment Anything in Images and Videos","summary":"  We present Segment Anything Model 2 (SAM 2), a foundation model towards\nsolving promptable visual segmentation in images and videos. We build a data\nengine, which improves model and data via user interaction, to collect the\nlargest video segmentation dataset to date. Our model is a simple transformer\narchitecture with streaming memory for real-time video processing. SAM 2\ntrained on our data provides strong performance across a wide range of tasks.\nIn video segmentation, we observe better accuracy, using 3x fewer interactions\nthan prior approaches. In image segmentation, our model is more accurate and 6x\nfaster than the Segment Anything Model (SAM). We believe that our data, model,\nand insights will serve as a significant milestone for video segmentation and\nrelated perception tasks. We are releasing a version of our model, the dataset\nand an interactive demo.\n","authors":["Nikhila Ravi","Valentin Gabeur","Yuan-Ting Hu","Ronghang Hu","Chaitanya Ryali","Tengyu Ma","Haitham Khedr","Roman Rädle","Chloe Rolland","Laura Gustafson","Eric Mintun","Junting Pan","Kalyan Vasudev Alwala","Nicolas Carion","Chao-Yuan Wu","Ross Girshick","Piotr Dollár","Christoph Feichtenhofer"],"pdf_url":"https://arxiv.org/pdf/2408.00714v1.pdf","comment":"Website: https://ai.meta.com/sam2"},{"id":"http://arxiv.org/abs/2408.00711v1","updated":"2024-08-01T16:58:21Z","published":"2024-08-01T16:58:21Z","title":"Investigating Brain Connectivity and Regional Statistics from EEG for\n  early stage Parkinson's Classification","summary":"  We evaluate the effectiveness of combining brain connectivity metrics with\nsignal statistics for early stage Parkinson's Disease (PD) classification using\nelectroencephalogram data (EEG). The data is from 5 arousal states - wakeful\nand four sleep stages (N1, N2, N3 and REM). Our pipeline uses an Ada Boost\nmodel for classification on a challenging early stage PD classification task\nwith with only 30 participants (11 PD , 19 Healthy Control). Evaluating 9 brain\nconnectivity metrics we find the best connectivity metric to be different for\neach arousal state with Phase Lag Index achieving the highest individual\nclassification accuracy of 86\\% on N1 data. Further to this our pipeline using\nregional signal statistics achieves an accuracy of 78\\%, using brain\nconnectivity only achieves an accuracy of 86\\% whereas combining the two\nachieves a best accuracy of 91\\%. This best performance is achieved on N1 data\nusing Phase Lag Index (PLI) combined with statistics derived from the frequency\ncharacteristics of the EEG signal. This model also achieves a recall of 80 \\%\nand precision of 96\\%. Furthermore we find that on data from each arousal\nstate, combining PLI with regional signal statistics improves classification\naccuracy versus using signal statistics or brain connectivity alone. Thus we\nconclude that combining brain connectivity statistics with regional EEG\nstatistics is optimal for classifier performance on early stage Parkinson's.\nAdditionally, we find outperformance of N1 EEG for classification of\nParkinson's and expect this could be due to disrupted N1 sleep in PD. This\nshould be explored in future work.\n","authors":["Amarpal Sahota","Amber Roguski","Matthew W Jones","Zahraa S. Abdallah","Raul Santos-Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2408.00711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00706v1","updated":"2024-08-01T16:52:39Z","published":"2024-08-01T16:52:39Z","title":"Point-supervised Brain Tumor Segmentation with Box-prompted MedSAM","summary":"  Delineating lesions and anatomical structure is important for image-guided\ninterventions. Point-supervised medical image segmentation (PSS) has great\npotential to alleviate costly expert delineation labeling. However, due to the\nlack of precise size and boundary guidance, the effectiveness of PSS often\nfalls short of expectations. Although recent vision foundational models, such\nas the medical segment anything model (MedSAM), have made significant\nadvancements in bounding-box-prompted segmentation, it is not straightforward\nto utilize point annotation, and is prone to semantic ambiguity. In this\npreliminary study, we introduce an iterative framework to facilitate\nsemantic-aware point-supervised MedSAM. Specifically, the semantic box-prompt\ngenerator (SBPG) module has the capacity to convert the point input into\npotential pseudo bounding box suggestions, which are explicitly refined by the\nprototype-based semantic similarity. This is then succeeded by a prompt-guided\nspatial refinement (PGSR) module that harnesses the exceptional\ngeneralizability of MedSAM to infer the segmentation mask, which also updates\nthe box proposal seed in SBPG. Performance can be progressively improved with\nadequate iterations. We conducted an evaluation on BraTS2018 for the\nsegmentation of whole brain tumors and demonstrated its superior performance\ncompared to traditional PSS methods and on par with box-supervised methods.\n","authors":["Xiaofeng Liu","Jonghye Woo","Chao Ma","Jinsong Ouyang","Georges El Fakhri"],"pdf_url":"https://arxiv.org/pdf/2408.00706v1.pdf","comment":"2024 IEEE Nuclear Science Symposium and Medical Imaging Conference"},{"id":"http://arxiv.org/abs/2408.00703v1","updated":"2024-08-01T16:49:50Z","published":"2024-08-01T16:49:50Z","title":"Future of Artificial Intelligence in Agile Software Development","summary":"  The advent of Artificial intelligence has promising advantages that can be\nutilized to transform the landscape of software project development. The\nSoftware process framework consists of activities that constantly require\nroutine human interaction, leading to the possibility of errors and\nuncertainties. AI can assist software development managers, software testers,\nand other team members by leveraging LLMs, GenAI models, and AI agents to\nperform routine tasks, risk analysis and prediction, strategy recommendations,\nand support decision making. AI has the potential to increase efficiency and\nreduce the risks encountered by the project management team while increasing\nthe project success rates. Additionally, it can also break down complex notions\nand development processes for stakeholders to make informed decisions. In this\npaper, we propose an approach in which AI tools and technologies can be\nutilized to bestow maximum assistance for agile software projects, which have\nbecome increasingly favored in the industry in recent years.\n","authors":["Mariyam Mahboob","Mohammed Rayyan Uddin Ahmed","Zoiba Zia","Mariam Shakeel Ali","Ayman Khaleel Ahmed"],"pdf_url":"https://arxiv.org/pdf/2408.00703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00695v1","updated":"2024-08-01T16:39:06Z","published":"2024-08-01T16:39:06Z","title":"Accelerating Full Waveform Inversion By Transfer Learning","summary":"  Full waveform inversion (FWI) is a powerful tool for reconstructing material\nfields based on sparsely measured data obtained by wave propagation. For\nspecific problems, discretizing the material field with a neural network (NN)\nimproves the robustness and reconstruction quality of the corresponding\noptimization problem. We call this method NN-based FWI. Starting from an\ninitial guess, the weights of the NN are iteratively updated to fit the\nsimulated wave signals to the sparsely measured data set. For gradient-based\noptimization, a suitable choice of the initial guess, i.e., a suitable NN\nweight initialization, is crucial for fast and robust convergence.\n  In this paper, we introduce a novel transfer learning approach to further\nimprove NN-based FWI. This approach leverages supervised pretraining to provide\na better NN weight initialization, leading to faster convergence of the\nsubsequent optimization problem. Moreover, the inversions yield physically more\nmeaningful local minima. The network is pretrained to predict the unknown\nmaterial field using the gradient information from the first iteration of\nconventional FWI. In our computational experiments on two-dimensional domains,\nthe training data set consists of reference simulations with arbitrarily\npositioned elliptical voids of different shapes and orientations. We compare\nthe performance of the proposed transfer learning NN-based FWI with three other\nmethods: conventional FWI, NN-based FWI without pretraining and conventional\nFWI with an initial guess predicted from the pretrained NN. Our results show\nthat transfer learning NN-based FWI outperforms the other methods in terms of\nconvergence speed and reconstruction quality.\n","authors":["Divya Shyam Singh","Leon Herrmann","Qing Sun","Tim Bürchner","Felix Dietrich","Stefan Kollmannsberger"],"pdf_url":"https://arxiv.org/pdf/2408.00695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00686v1","updated":"2024-08-01T16:28:14Z","published":"2024-08-01T16:28:14Z","title":"Can Developers Prompt? A Controlled Experiment for Code Documentation\n  Generation","summary":"  Large language models (LLMs) bear great potential for automating tedious\ndevelopment tasks such as creating and maintaining code documentation. However,\nit is unclear to what extent developers can effectively prompt LLMs to create\nconcise and useful documentation. We report on a controlled experiment with 20\nprofessionals and 30 computer science students tasked with code documentation\ngeneration for two Python functions. The experimental group freely entered\nad-hoc prompts in a ChatGPT-like extension of Visual Studio Code, while the\ncontrol group executed a predefined few-shot prompt. Our results reveal that\nprofessionals and students were unaware of or unable to apply prompt\nengineering techniques. Especially students perceived the documentation\nproduced from ad-hoc prompts as significantly less readable, less concise, and\nless helpful than documentation from prepared prompts. Some professionals\nproduced higher quality documentation by just including the keyword Docstring\nin their ad-hoc prompts. While students desired more support in formulating\nprompts, professionals appreciated the flexibility of ad-hoc prompting.\nParticipants in both groups rarely assessed the output as perfect. Instead,\nthey understood the tools as support to iteratively refine the documentation.\nFurther research is needed to understand which prompting skills and preferences\ndevelopers have and which support they need for certain tasks.\n","authors":["Hans-Alexander Kruse","Tim Puhlfürß","Walid Maalej"],"pdf_url":"https://arxiv.org/pdf/2408.00686v1.pdf","comment":"Accepted at the 40th IEEE International Conference on Software\n  Maintenance and Evolution (ICSME)"},{"id":"http://arxiv.org/abs/2408.00682v1","updated":"2024-08-01T16:24:37Z","published":"2024-08-01T16:24:37Z","title":"Learning in Multi-Objective Public Goods Games with Non-Linear Utilities","summary":"  Addressing the question of how to achieve optimal decision-making under risk\nand uncertainty is crucial for enhancing the capabilities of artificial agents\nthat collaborate with or support humans. In this work, we address this question\nin the context of Public Goods Games. We study learning in a novel\nmulti-objective version of the Public Goods Game where agents have different\nrisk preferences, by means of multi-objective reinforcement learning. We\nintroduce a parametric non-linear utility function to model risk preferences at\nthe level of individual agents, over the collective and individual reward\ncomponents of the game. We study the interplay between such preference\nmodelling and environmental uncertainty on the incentive alignment level in the\ngame. We demonstrate how different combinations of individual preferences and\nenvironmental uncertainties sustain the emergence of cooperative patterns in\nnon-cooperative environments (i.e., where competitive strategies are dominant),\nwhile others sustain competitive patterns in cooperative environments (i.e.,\nwhere cooperative strategies are dominant).\n","authors":["Nicole Orzan","Erman Acar","Davide Grossi","Patrick Mannion","Roxana Rădulescu"],"pdf_url":"https://arxiv.org/pdf/2408.00682v1.pdf","comment":"In press at ECAI 2024"},{"id":"http://arxiv.org/abs/2408.00655v1","updated":"2024-08-01T15:45:19Z","published":"2024-08-01T15:45:19Z","title":"SentenceVAE: Faster, Longer and More Accurate Inference with\n  Next-sentence Prediction for Large Language Models","summary":"  Contemporary large language models (LLMs) predominantly utilize a next-token\nprediction method for inference, which significantly impedes their processing\nspeed. In this paper, we introduce a novel inference methodology termed\nnext-sentence prediction, aimed at enhancing the inference efficiency of LLMs.\nWe present SentenceVAE, a tiny model consisting of an encoder and a decoder.\nThe encoder effectively condenses the information within a sentence into a\nsingular token, while the decoder reconstructs this compressed data back into\nits original sentential form. By integrating SentenceVAE into the input and\noutput layers of LLMs, we develop Sentence-level LLMs (SLLMs) that employ a\nsentence-by-sentence inference approach, markedly accelerating inference\nspeeds. SentenceVAE also maintains the integrity of the original semantic\ncontent by segmenting the text into sentences, thereby preserving accuracy\nwhile boosting inference speeds. Compared to traditional LLMs, SLLMs process\nfewer tokens over equivalent context lengths, significantly reducing memory\ndemands for Self-Attention computations and facilitating the handling of longer\ncontexts. Our experimental findings reveal that this method can increase\ninference speeds by 204~365%, reduce perplexity (PPL) to 46~75% of its original\nmetric, and decrease memory overhead by 86~91% for the same context length. The\nadvantages of this approach are further amplified with increases in model\nparameters.\n","authors":["Hongjun An","Yifan Chen","Xiaozhen Qiao","Zhe Sun","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.00655v1.pdf","comment":"First preview version"},{"id":"http://arxiv.org/abs/2408.00640v1","updated":"2024-08-01T15:27:48Z","published":"2024-08-01T15:27:48Z","title":"AMAES: Augmented Masked Autoencoder Pretraining on Public Brain MRI Data\n  for 3D-Native Segmentation","summary":"  This study investigates the impact of self-supervised pretraining of 3D\nsemantic segmentation models on a large-scale, domain-specific dataset. We\nintroduce BRAINS-45K, a dataset of 44,756 brain MRI volumes from public\nsources, the largest public dataset available, and revisit a number of design\nchoices for pretraining modern segmentation architectures by simplifying and\noptimizing state-of-the-art methods, and combining them with a novel\naugmentation strategy. The resulting AMAES framework is based on\nmasked-image-modeling and intensity-based augmentation reversal and balances\nmemory usage, runtime, and finetuning performance. Using the popular U-Net and\nthe recent MedNeXt architecture as backbones, we evaluate the effect of\npretraining on three challenging downstream tasks, covering single-sequence,\nlow-resource settings, and out-of-domain generalization. The results highlight\nthat pretraining on the proposed dataset with AMAES significantly improves\nsegmentation performance in the majority of evaluated cases, and that it is\nbeneficial to pretrain the model with augmentations, despite pretraing on a\nlarge-scale dataset. Code and model checkpoints for reproducing results, as\nwell as the BRAINS-45K dataset are available at\n\\url{https://github.com/asbjrnmunk/amaes}.\n","authors":["Asbjørn Munk","Jakob Ambsdorf","Sebastian Llambias","Mads Nielsen"],"pdf_url":"https://arxiv.org/pdf/2408.00640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00633v1","updated":"2024-08-01T15:17:33Z","published":"2024-08-01T15:17:33Z","title":"DisTrack: a new Tool for Semi-automatic Misinformation Tracking in\n  Online Social Networks","summary":"  Introduction: This article introduces DisTrack, a methodology and a tool\ndeveloped for tracking and analyzing misinformation within Online Social\nNetworks (OSNs). DisTrack is designed to combat the spread of misinformation\nthrough a combination of Natural Language Processing (NLP) Social Network\nAnalysis (SNA) and graph visualization. The primary goal is to detect\nmisinformation, track its propagation, identify its sources, and assess the\ninfluence of various actors within the network.\n  Methods: DisTrack's architecture incorporates a variety of methodologies\nincluding keyword search, semantic similarity assessments, and graph generation\ntechniques. These methods collectively facilitate the monitoring of\nmisinformation, the categorization of content based on alignment with known\nfalse claims, and the visualization of dissemination cascades through detailed\ngraphs. The tool is tailored to capture and analyze the dynamic nature of\nmisinformation spread in digital environments.\n  Results: The effectiveness of DisTrack is demonstrated through three case\nstudies focused on different themes: discredit/hate speech, anti-vaccine\nmisinformation, and false narratives about the Russia-Ukraine conflict. These\nstudies show DisTrack's capabilities in distinguishing posts that propagate\nfalsehoods from those that counteract them, and tracing the evolution of\nmisinformation from its inception.\n  Conclusions: The research confirms that DisTrack is a valuable tool in the\nfield of misinformation analysis. It effectively distinguishes between\ndifferent types of misinformation and traces their development over time. By\nproviding a comprehensive approach to understanding and combating\nmisinformation in digital spaces, DisTrack proves to be an essential asset for\nresearchers and practitioners working to mitigate the impact of false\ninformation in online social environments.\n","authors":["Guillermo Villar-Rodríguez","Álvaro Huertas-García","Alejandro Martín","Javier Huertas-Tato","David Camacho"],"pdf_url":"https://arxiv.org/pdf/2408.00633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00613v1","updated":"2024-08-01T14:53:11Z","published":"2024-08-01T14:53:11Z","title":"Unlocking Fair Use in the Generative AI Supply Chain: A Systematized\n  Literature Review","summary":"  Through a systematization of generative AI (GenAI) stakeholder goals and\nexpectations, this work seeks to uncover what value different stakeholders see\nin their contributions to the GenAI supply line. This valuation enables us to\nunderstand whether fair use advocated by GenAI companies to train model\nprogresses the copyright law objective of promoting science and arts. While\nassessing the validity and efficacy of the fair use argument, we uncover\nresearch gaps and potential avenues for future works for researchers and\npolicymakers to address.\n","authors":["Amruta Mahuli","Asia Biega"],"pdf_url":"https://arxiv.org/pdf/2408.00613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00584v1","updated":"2024-08-01T14:14:15Z","published":"2024-08-01T14:14:15Z","title":"Non Verbis, Sed Rebus: Large Language Models are Weak Solvers of Italian\n  Rebuses","summary":"  Rebuses are puzzles requiring constrained multi-step reasoning to identify a\nhidden phrase from a set of images and letters. In this work, we introduce a\nlarge collection of verbalized rebuses for the Italian language and use it to\nassess the rebus-solving capabilities of state-of-the-art large language\nmodels. While general-purpose systems such as LLaMA-3 and GPT-4o perform poorly\non this task, ad-hoc fine-tuning seems to improve models' performance. However,\nwe find that performance gains from training are largely motivated by\nmemorization. Our results suggest that rebus solving remains a challenging test\nbed to evaluate large language models' linguistic proficiency and sequential\ninstruction-following skills.\n","authors":["Gabriele Sarti","Tommaso Caselli","Malvina Nissim","Arianna Bisazza"],"pdf_url":"https://arxiv.org/pdf/2408.00584v1.pdf","comment":"Code: https://github.com/gsarti/verbalized-rebus. Artifacts:\n  https://huggingface.co/collections/gsarti/verbalized-rebus-clic-it-2024-66ab8f11cb04e68bdf4fb028"},{"id":"http://arxiv.org/abs/2408.00555v1","updated":"2024-08-01T13:38:58Z","published":"2024-08-01T13:38:58Z","title":"Alleviating Hallucination in Large Vision-Language Models with Active\n  Retrieval Augmentation","summary":"  Despite the remarkable ability of large vision-language models (LVLMs) in\nimage comprehension, these models frequently generate plausible yet factually\nincorrect responses, a phenomenon known as hallucination.Recently, in large\nlanguage models (LLMs), augmenting LLMs by retrieving information from external\nknowledge resources has been proven as a promising solution to mitigate\nhallucinations.However, the retrieval augmentation in LVLM significantly lags\nbehind the widespread applications of LVLM. Moreover, when transferred to\naugmenting LVLMs, sometimes the hallucination degree of the model is even\nexacerbated.Motivated by the research gap and counter-intuitive phenomenon, we\nintroduce a novel framework, the Active Retrieval-Augmented large\nvision-language model (ARA), specifically designed to address hallucinations by\nincorporating three critical dimensions: (i) dissecting the retrieval targets\nbased on the inherent hierarchical structures of images. (ii) pinpointing the\nmost effective retrieval methods and filtering out the reliable retrieval\nresults. (iii) timing the retrieval process to coincide with episodes of low\ncertainty, while circumventing unnecessary retrieval during periods of high\ncertainty. To assess the capability of our proposed ARA model in reducing\nhallucination, we employ three widely used LVLM models (LLaVA-1.5, Qwen-VL, and\nmPLUG-Owl2) across four benchmarks. Our empirical observations suggest that by\nutilizing fitting retrieval mechanisms and timing the retrieval judiciously, we\ncan effectively mitigate the hallucination problem. We hope that this study can\nprovide deeper insights into how to adapt the retrieval augmentation to LVLMs\nfor reducing hallucinations with more effective retrieval and minimal retrieval\noccurrences.\n","authors":["Xiaoye Qu","Qiyuan Chen","Wei Wei","Jishuo Sun","Jianfeng Dong"],"pdf_url":"https://arxiv.org/pdf/2408.00555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00550v1","updated":"2024-08-01T13:34:35Z","published":"2024-08-01T13:34:35Z","title":"Mitigating Multilingual Hallucination in Large Vision-Language Models","summary":"  While Large Vision-Language Models (LVLMs) have exhibited remarkable\ncapabilities across a wide range of tasks, they suffer from hallucination\nproblems, where models generate plausible yet incorrect answers given the input\nimage-query pair. This hallucination phenomenon is even more severe when\nquerying the image in non-English languages, while existing methods for\nmitigating hallucinations in LVLMs only consider the English scenarios. In this\npaper, we make the first attempt to mitigate this important multilingual\nhallucination in LVLMs. With thorough experiment analysis, we found that\nmultilingual hallucination in LVLMs is a systemic problem that could arise from\ndeficiencies in multilingual capabilities or inadequate multimodal abilities.\nTo this end, we propose a two-stage Multilingual Hallucination Removal (MHR)\nframework for LVLMs, aiming to improve resistance to hallucination for both\nhigh-resource and low-resource languages. Instead of relying on the intricate\nmanual annotations of multilingual resources, we fully leverage the inherent\ncapabilities of the LVLM and propose a novel cross-lingual alignment method,\nwhich generates multiple responses for each image-query input and then\nidentifies the hallucination-aware pairs for each language. These data pairs\nare finally used for direct preference optimization to prompt the LVLMs to\nfavor non-hallucinating responses. Experimental results show that our MHR\nachieves a substantial reduction in hallucination generation for LVLMs.\nNotably, on our extended multilingual POPE benchmark, our framework delivers an\naverage increase of 19.0% in accuracy across 13 different languages. Our code\nand model weights are available at https://github.com/ssmisya/MHR\n","authors":["Xiaoye Qu","Mingyang Song","Wei Wei","Jianfeng Dong","Yu Cheng"],"pdf_url":"https://arxiv.org/pdf/2408.00550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00549v1","updated":"2024-08-01T13:34:19Z","published":"2024-08-01T13:34:19Z","title":"Learning to Embed Distributions via Maximum Kernel Entropy","summary":"  Empirical data can often be considered as samples from a set of probability\ndistributions. Kernel methods have emerged as a natural approach for learning\nto classify these distributions. Although numerous kernels between\ndistributions have been proposed, applying kernel methods to distribution\nregression tasks remains challenging, primarily because selecting a suitable\nkernel is not straightforward. Surprisingly, the question of learning a\ndata-dependent distribution kernel has received little attention. In this\npaper, we propose a novel objective for the unsupervised learning of\ndata-dependent distribution kernel, based on the principle of entropy\nmaximization in the space of probability measure embeddings. We examine the\ntheoretical properties of the latent embedding space induced by our objective,\ndemonstrating that its geometric structure is well-suited for solving\ndownstream discriminative tasks. Finally, we demonstrate the performance of the\nlearned kernel across different modalities.\n","authors":["Oleksii Kachaiev","Stefano Recanatesi"],"pdf_url":"https://arxiv.org/pdf/2408.00549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00544v1","updated":"2024-08-01T13:28:15Z","published":"2024-08-01T13:28:15Z","title":"Illustrating Classic Brazilian Books using a Text-To-Image Diffusion\n  Model","summary":"  In recent years, Generative Artificial Intelligence (GenAI) has undergone a\nprofound transformation in addressing intricate tasks involving diverse\nmodalities such as textual, auditory, visual, and pictorial generation. Within\nthis spectrum, text-to-image (TTI) models have emerged as a formidable approach\nto generating varied and aesthetically appealing compositions, spanning\napplications from artistic creation to realistic facial synthesis, and\ndemonstrating significant advancements in computer vision, image processing,\nand multimodal tasks. The advent of Latent Diffusion Models (LDMs) signifies a\nparadigm shift in the domain of AI capabilities. This article delves into the\nfeasibility of employing the Stable Diffusion LDM to illustrate literary works.\nFor this exploration, seven classic Brazilian books have been selected as case\nstudies. The objective is to ascertain the practicality of this endeavor and to\nevaluate the potential of Stable Diffusion in producing illustrations that\naugment and enrich the reader's experience. We will outline the beneficial\naspects, such as the capacity to generate distinctive and contextually\npertinent images, as well as the drawbacks, including any shortcomings in\nfaithfully capturing the essence of intricate literary depictions. Through this\nstudy, we aim to provide a comprehensive assessment of the viability and\nefficacy of utilizing AI-generated illustrations in literary contexts,\nelucidating both the prospects and challenges encountered in this pioneering\napplication of technology.\n","authors":["Felipe Mahlow","André Felipe Zanella","William Alberto Cruz Castañeda","Regilene Aparecida Sarzi-Ribeiro"],"pdf_url":"https://arxiv.org/pdf/2408.00544v1.pdf","comment":"7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2408.00540v1","updated":"2024-08-01T13:23:15Z","published":"2024-08-01T13:23:15Z","title":"The Energy Cost of Artificial Intelligence of Things Lifecycle","summary":"  Artificial intelligence (AI)coupled with existing Internet of Things (IoT)\nenables more streamlined and autonomous operations across various economic\nsectors. Consequently, the paradigm of Artificial Intelligence of Things (AIoT)\nhaving AI techniques at its core implies additional energy and carbon costs\nthat may become significant with more complex neural architectures. To better\nunderstand the energy and Carbon Footprint (CF) of some AIoT components, very\nrecent studies employ conventional metrics. However, these metrics are not\ndesigned to capture energy efficiency aspects of inference. In this paper, we\npropose a new metric, the Energy Cost of AIoT Lifecycle (eCAL) to capture the\noverall energy cost of inference over the lifecycle of an AIoT system. We\ndevise a new methodology for determining eCAL of an AIoT system by analyzing\nthe complexity of data manipulation in individual components involved in the\nAIoT lifecycle and derive the overall and per bit energy consumption. With eCAL\nwe show that the better a model is and the more it is used, the more energy\nefficient an inference is. For an example AIoT configuration, eCAL for making\n$100$ inferences is $1.43$ times higher than for $1000$ inferences. We also\nevaluate the CF of the AIoT system by calculating the equivalent CO$_{2}$\nemissions based on the energy consumption and the Carbon Intensity (CI) across\ndifferent countries. Using 2023 renewable data, our analysis reveals that\ndeploying an AIoT system in Germany results in emitting $4.62$ times higher\nCO$_2$ than in Finland, due to latter using more low-CI energy sources.\n","authors":["Shih-Kai Chou","Jernej Hribar","Mihael Mohorčič","Carolina Fortuna"],"pdf_url":"https://arxiv.org/pdf/2408.00540v1.pdf","comment":"12 pages, 13 figures"},{"id":"http://arxiv.org/abs/2408.00539v1","updated":"2024-08-01T13:22:01Z","published":"2024-08-01T13:22:01Z","title":"Intermittent Semi-working Mask: A New Masking Paradigm for LLMs","summary":"  Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.\n","authors":["Mingcong Lu","Jiangcai Zhu","Wang Hao","Zheng Li","Shusheng Zhang","Kailai Shao","Chao Chen","Nan Li","Feng Wang","Xin Lu"],"pdf_url":"https://arxiv.org/pdf/2408.00539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00526v1","updated":"2024-08-01T12:57:35Z","published":"2024-08-01T12:57:35Z","title":"Hilbert curves for efficient exploratory landscape analysis\n  neighbourhood sampling","summary":"  Landscape analysis aims to characterise optimisation problems based on their\nobjective (or fitness) function landscape properties. The problem search space\nis typically sampled, and various landscape features are estimated based on the\nsamples. One particularly salient set of features is information content, which\nrequires the samples to be sequences of neighbouring solutions, such that the\nlocal relationships between consecutive sample points are preserved. Generating\nsuch spatially correlated samples that also provide good search space coverage\nis challenging. It is therefore common to first obtain an unordered sample with\ngood search space coverage, and then apply an ordering algorithm such as the\nnearest neighbour to minimise the distance between consecutive points in the\nsample. However, the nearest neighbour algorithm becomes computationally\nprohibitive in higher dimensions, thus there is a need for more efficient\nalternatives. In this study, Hilbert space-filling curves are proposed as a\nmethod to efficiently obtain high-quality ordered samples. Hilbert curves are a\nspecial case of fractal curves, and guarantee uniform coverage of a bounded\nsearch space while providing a spatially correlated sample. We study the\neffectiveness of Hilbert curves as samplers, and discover that they are capable\nof extracting salient features at a fraction of the computational cost compared\nto Latin hypercube sampling with post-factum ordering. Further, we investigate\nthe use of Hilbert curves as an ordering strategy, and find that they order the\nsample significantly faster than the nearest neighbour ordering, without\nsacrificing the saliency of the extracted features.\n","authors":["Johannes J. Pienaar","Anna S. Bosman","Katherine M. Malan"],"pdf_url":"https://arxiv.org/pdf/2408.00526v1.pdf","comment":"A version of this paper is published as conference proceedings of\n  EvoApps 2024"},{"id":"http://arxiv.org/abs/2408.00523v1","updated":"2024-08-01T12:54:46Z","published":"2024-08-01T12:54:46Z","title":"Jailbreaking Text-to-Image Models with LLM-Based Agents","summary":"  Recent advancements have significantly improved automated task-solving\ncapabilities using autonomous agents powered by large language models (LLMs).\nHowever, most LLM-based agents focus on dialogue, programming, or specialized\ndomains, leaving gaps in addressing generative AI safety tasks. These gaps are\nprimarily due to the challenges posed by LLM hallucinations and the lack of\nclear guidelines. In this paper, we propose Atlas, an advanced LLM-based\nmulti-agent framework that integrates an efficient fuzzing workflow to target\ngenerative AI models, specifically focusing on jailbreak attacks against\ntext-to-image (T2I) models with safety filters. Atlas utilizes a\nvision-language model (VLM) to assess whether a prompt triggers the T2I model's\nsafety filter. It then iteratively collaborates with both LLM and VLM to\ngenerate an alternative prompt that bypasses the filter. Atlas also enhances\nthe reasoning abilities of LLMs in attack scenarios by leveraging multi-agent\ncommunication, in-context learning (ICL) memory mechanisms, and the\nchain-of-thought (COT) approach. Our evaluation demonstrates that Atlas\nsuccessfully jailbreaks several state-of-the-art T2I models in a black-box\nsetting, which are equipped with multi-modal safety filters. In addition, Atlas\noutperforms existing methods in both query efficiency and the quality of the\ngenerated images.\n","authors":["Yingkai Dong","Zheng Li","Xiangtao Meng","Ning Yu","Shanqing Guo"],"pdf_url":"https://arxiv.org/pdf/2408.00523v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00521v1","updated":"2024-08-01T12:52:48Z","published":"2024-08-01T12:52:48Z","title":"A new approach for encoding code and assisting code understanding","summary":"  Some companies(e.g., Microsoft Research and Google DeepMind) have discovered\nsome of the limitations of GPTs autoregressive paradigm next-word prediction,\nmanifested in the model lack of planning, working memory, backtracking, and\nreasoning skills. GPTs rely on a local and greedy process of generating the\nnext word, without a global understanding of the task or the output.We have\nconfirmed the above limitations through specialized empirical studies of code\ncomprehension. Although GPT4 is good at producing fluent and coherent text, it\ncannot handle complex logic and generate new code that haven not been seen, and\nit relies too much on the formatting of the prompt to generate the correct\ncode.We propose a new paradigm for code understanding that goes beyond the\nnext-word prediction paradigm, inspired by the successful application of\ndiffusion techniques to image generation(Dalle2, Sora) and protein structure\ngeneration(AlphaFold3), which have no autoregressive constraints.Instead of\nencoding the code in a form that mimics natural language, we encode the code as\na heterogeneous image paradigm with a memory of global information that mimics\nboth images and protein structures.We then refer to Sora's CLIP upstream\ntext-to-image encoder model to design a text-to-code encoder model that can be\napplied to various downstream code understanding tasks.The model learns the\nglobal understanding of code under the new paradigm heterogeneous image,\nconnects the encoding space of text and code, and encodes the input of text\ninto the vector of code most similar to it.Using self-supervised comparative\nlearning on 456,360 text-code pairs, the model achieved a zero-shot prediction\nof new data. This work is the basis for future work on code generation using\ndiffusion techniques under a new paradigm to avoid autoregressive limitations.\n","authors":["Mengdan Fan","Wei Zhang","Haiyan Zhao","Zhi Jin"],"pdf_url":"https://arxiv.org/pdf/2408.00521v1.pdf","comment":"10 page, 14 figures"},{"id":"http://arxiv.org/abs/2408.00490v1","updated":"2024-08-01T11:51:52Z","published":"2024-08-01T11:51:52Z","title":"Graph Representation Learning via Causal Diffusion for\n  Out-of-Distribution Recommendation","summary":"  Graph Neural Networks (GNNs)-based recommendation algorithms typically assume\nthat training and testing data are drawn from independent and identically\ndistributed (IID) spaces. However, this assumption often fails in the presence\nof out-of-distribution (OOD) data, resulting in significant performance\ndegradation. In this study, we construct a Structural Causal Model (SCM) to\nanalyze interaction data, revealing that environmental confounders (e.g., the\nCOVID-19 pandemic) lead to unstable correlations in GNN-based models, thus\nimpairing their generalization to OOD data. To address this issue, we propose a\nnovel approach, graph representation learning via causal diffusion\n(CausalDiffRec) for OOD recommendation. This method enhances the model's\ngeneralization on OOD data by eliminating environmental confounding factors and\nlearning invariant graph representations. Specifically, we use backdoor\nadjustment and variational inference to infer the real environmental\ndistribution, thereby eliminating the impact of environmental confounders. This\ninferred distribution is then used as prior knowledge to guide the\nrepresentation learning in the reverse phase of the diffusion process to learn\nthe invariant representation. In addition, we provide a theoretical derivation\nthat proves optimizing the objective function of CausalDiffRec can encourage\nthe model to learn environment-invariant graph representations, thereby\nachieving excellent generalization performance in recommendations under\ndistribution shifts. Our extensive experiments validate the effectiveness of\nCausalDiffRec in improving the generalization of OOD data, and the average\nimprovement is up to 10.69% on Food, 18.83% on KuaiRec, 22.41% on Yelp2018, and\n11.65% on Douban datasets.\n","authors":["Chu Zhao","Enneng Yang","Yuliang Liang","Pengxiang Lan","Yuting Liu","Jianzhe Zhao","Guibing Guo","Xingwei Wang"],"pdf_url":"https://arxiv.org/pdf/2408.00490v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2408.00483v1","updated":"2024-08-01T11:39:45Z","published":"2024-08-01T11:39:45Z","title":"A Systematic Review on Long-Tailed Learning","summary":"  Long-tailed data is a special type of multi-class imbalanced data with a very\nlarge amount of minority/tail classes that have a very significant combined\ninfluence. Long-tailed learning aims to build high-performance models on\ndatasets with long-tailed distributions, which can identify all the classes\nwith high accuracy, in particular the minority/tail classes. It is a\ncutting-edge research direction that has attracted a remarkable amount of\nresearch effort in the past few years. In this paper, we present a\ncomprehensive survey of latest advances in long-tailed visual learning. We\nfirst propose a new taxonomy for long-tailed learning, which consists of eight\ndifferent dimensions, including data balancing, neural architecture, feature\nenrichment, logits adjustment, loss function, bells and whistles, network\noptimization, and post hoc processing techniques. Based on our proposed\ntaxonomy, we present a systematic review of long-tailed learning methods,\ndiscussing their commonalities and alignable differences. We also analyze the\ndifferences between imbalance learning and long-tailed learning approaches.\nFinally, we discuss prospects and future directions in this field.\n","authors":["Chongsheng Zhang","George Almpanidis","Gaojuan Fan","Binquan Deng","Yanbo Zhang","Ji Liu","Aouaidjia Kamel","Paolo Soda","João Gama"],"pdf_url":"https://arxiv.org/pdf/2408.00483v1.pdf","comment":"Current Under Revision at IEEE TNNLS. [This is the long/Full-length\n  version of our Long-Tailed Learning Survey paper]"},{"id":"http://arxiv.org/abs/2408.00481v1","updated":"2024-08-01T11:36:18Z","published":"2024-08-01T11:36:18Z","title":"HBot: A Chatbot for Healthcare Applications in Traditional Chinese\n  Medicine Based on Human Body 3D Visualization","summary":"  The unique diagnosis and treatment techniques and remarkable clinical\nefficacy of traditional Chinese medicine (TCM) make it play an important role\nin the field of elderly care and healthcare, especially in the rehabilitation\nof some common chronic diseases of the elderly. Therefore, building a TCM\nchatbot for healthcare application will help users obtain consultation services\nin a direct and natural way. However, concepts such as acupuncture points\n(acupoints) and meridians involved in TCM always appear in the consultation,\nwhich cannot be displayed intuitively. To this end, we develop a\n\\textbf{h}ealthcare chat\\textbf{bot} (HBot) based on a human body model in 3D\nand knowledge graph, which provides conversational services such as knowledge\nQ\\&A, prescription recommendation, moxibustion therapy recommendation, and\nacupoint search. When specific acupoints are involved in the conversations\nbetween user and HBot, the 3D body will jump to the corresponding acupoints and\nhighlight them. Moreover, Hbot can also be used in training scenarios to\naccelerate the teaching process of TCM by intuitively displaying acupuncture\npoints and knowledge cards. The demonstration video is available at\nhttps://www.youtube.com/watch?v=UhQhutSKkTU . Our code and dataset are publicly\navailable at Gitee: https://gitee.com/plabrolin/interactive-3d-acup.git\n","authors":["Bolin Zhang","Zhiwei Yi","Jiahao Wang","Dianbo Sui","Zhiying Tu","Dianhui Chu"],"pdf_url":"https://arxiv.org/pdf/2408.00481v1.pdf","comment":"System Demonstration"},{"id":"http://arxiv.org/abs/2408.00473v1","updated":"2024-08-01T11:23:42Z","published":"2024-08-01T11:23:42Z","title":"Towards Explainable and Interpretable Musical Difficulty Estimation: A\n  Parameter-efficient Approach","summary":"  Estimating music piece difficulty is important for organizing educational\nmusic collections. This process could be partially automatized to facilitate\nthe educator's role. Nevertheless, the decisions performed by prevalent\ndeep-learning models are hardly understandable, which may impair the acceptance\nof such a technology in music education curricula. Our work employs explainable\ndescriptors for difficulty estimation in symbolic music representations.\nFurthermore, through a novel parameter-efficient white-box model, we outperform\nprevious efforts while delivering interpretable results. These comprehensible\noutcomes emulate the functionality of a rubric, a tool widely used in music\neducation. Our approach, evaluated in piano repertoire categorized in 9\nclasses, achieved 41.4% accuracy independently, with a mean squared error (MSE)\nof 1.7, showing precise difficulty estimation. Through our baseline, we\nillustrate how building on top of past research can offer alternatives for\nmusic difficulty assessment which are explainable and interpretable. With this,\nwe aim to promote a more effective communication between the Music Information\nRetrieval (MIR) community and the music education one.\n","authors":["Pedro Ramoneda","Vsevolod Eremenko","Alexandre D'Hooge","Emilia Parada-Cabaleiro","Xavier Serra"],"pdf_url":"https://arxiv.org/pdf/2408.00473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00470v1","updated":"2024-08-01T11:16:26Z","published":"2024-08-01T11:16:26Z","title":"Image Super-Resolution with Taylor Expansion Approximation and Large\n  Field Reception","summary":"  Self-similarity techniques are booming in blind super-resolution (SR) due to\naccurate estimation of the degradation types involved in low-resolution images.\nHowever, high-dimensional matrix multiplication within self-similarity\ncomputation prohibitively consumes massive computational costs. We find that\nthe high-dimensional attention map is derived from the matrix multiplication\nbetween Query and Key, followed by a softmax function. This softmax makes the\nmatrix multiplication between Query and Key inseparable, posing a great\nchallenge in simplifying computational complexity. To address this issue, we\nfirst propose a second-order Taylor expansion approximation (STEA) to separate\nthe matrix multiplication of Query and Key, resulting in the complexity\nreduction from $\\mathcal{O}(N^2)$ to $\\mathcal{O}(N)$. Then, we design a\nmulti-scale large field reception (MLFR) to compensate for the performance\ndegradation caused by STEA. Finally, we apply these two core designs to\nlaboratory and real-world scenarios by constructing LabNet and RealNet,\nrespectively. Extensive experimental results tested on five synthetic datasets\ndemonstrate that our LabNet sets a new benchmark in qualitative and\nquantitative evaluations. Tested on the RealWorld38 dataset, our RealNet\nachieves superior visual quality over existing methods. Ablation studies\nfurther verify the contributions of STEA and MLFR towards both LabNet and\nRealNet frameworks.\n","authors":["Jiancong Feng","Yuan-Gen Wang","Mingjie Li","Fengchuang Xing"],"pdf_url":"https://arxiv.org/pdf/2408.00470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00447v1","updated":"2024-08-01T10:36:00Z","published":"2024-08-01T10:36:00Z","title":"DiscipLink: Unfolding Interdisciplinary Information Seeking Process via\n  Human-AI Co-Exploration","summary":"  Interdisciplinary studies often require researchers to explore literature in\ndiverse branches of knowledge. Yet, navigating through the highly scattered\nknowledge from unfamiliar disciplines poses a significant challenge. In this\npaper, we introduce DiscipLink, a novel interactive system that facilitates\ncollaboration between researchers and large language models (LLMs) in\ninterdisciplinary information seeking (IIS). Based on users' topics of\ninterest, DiscipLink initiates exploratory questions from the perspectives of\npossible relevant fields of study, and users can further tailor these\nquestions. DiscipLink then supports users in searching and screening papers\nunder selected questions by automatically expanding queries with\ndisciplinary-specific terminologies, extracting themes from retrieved papers,\nand highlighting the connections between papers and questions. Our evaluation,\ncomprising a within-subject comparative experiment and an open-ended\nexploratory study, reveals that DiscipLink can effectively support researchers\nin breaking down disciplinary boundaries and integrating scattered knowledge in\ndiverse fields. The findings underscore the potential of LLM-powered tools in\nfostering information-seeking practices and bolstering interdisciplinary\nresearch.\n","authors":["Chengbo Zheng","Yuanhao Zhang","Zeyu Huang","Chuhan Shi","Minrui Xu","Xiaojuan Ma"],"pdf_url":"https://arxiv.org/pdf/2408.00447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00444v1","updated":"2024-08-01T10:31:32Z","published":"2024-08-01T10:31:32Z","title":"Ontological Relations from Word Embeddings","summary":"  It has been reliably shown that the similarity of word embeddings obtained\nfrom popular neural models such as BERT approximates effectively a form of\nsemantic similarity of the meaning of those words. It is therefore natural to\nwonder if those embeddings contain enough information to be able to connect\nthose meanings through ontological relationships such as the one of\nsubsumption. If so, large knowledge models could be built that are capable of\nsemantically relating terms based on the information encapsulated in word\nembeddings produced by pre-trained models, with implications not only for\nontologies (ontology matching, ontology evolution, etc.) but also on the\nability to integrate ontological knowledge in neural models. In this paper, we\ntest how embeddings produced by several pre-trained models can be used to\npredict relations existing between classes and properties of popular\nupper-level and general ontologies. We show that even a simple feed-forward\narchitecture on top of those embeddings can achieve promising accuracies, with\nvarying generalisation abilities depending on the input data. To achieve that,\nwe produce a dataset that can be used to further enhance those models, opening\nnew possibilities for applications integrating knowledge from web ontologies.\n","authors":["Mathieu d'Aquin","Emmanuel Nauer"],"pdf_url":"https://arxiv.org/pdf/2408.00444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00441v1","updated":"2024-08-01T10:25:14Z","published":"2024-08-01T10:25:14Z","title":"Focus, Distinguish, and Prompt: Unleashing CLIP for Efficient and\n  Flexible Scene Text Retrieval","summary":"  Scene text retrieval aims to find all images containing the query text from\nan image gallery. Current efforts tend to adopt an Optical Character\nRecognition (OCR) pipeline, which requires complicated text detection and/or\nrecognition processes, resulting in inefficient and inflexible retrieval.\nDifferent from them, in this work we propose to explore the intrinsic potential\nof Contrastive Language-Image Pre-training (CLIP) for OCR-free scene text\nretrieval. Through empirical analysis, we observe that the main challenges of\nCLIP as a text retriever are: 1) limited text perceptual scale, and 2)\nentangled visual-semantic concepts. To this end, a novel model termed FDP\n(Focus, Distinguish, and Prompt) is developed. FDP first focuses on scene text\nvia shifting the attention to the text area and probing the hidden text\nknowledge, and then divides the query text into content word and function word\nfor processing, in which a semantic-aware prompting scheme and a distracted\nqueries assistance module are utilized. Extensive experiments show that FDP\nsignificantly enhances the inference speed while achieving better or\ncompetitive retrieval accuracy compared to existing methods. Notably, on the\nIIIT-STR benchmark, FDP surpasses the state-of-the-art model by 4.37% with a 4\ntimes faster speed. Furthermore, additional experiments under phrase-level and\nattribute-aware scene text retrieval settings validate FDP's particular\nadvantages in handling diverse forms of query text. The source code will be\npublicly available at https://github.com/Gyann-z/FDP.\n","authors":["Gangyan Zeng","Yuan Zhang","Jin Wei","Dongbao Yang","Peng Zhang","Yiwen Gao","Xugong Qin","Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.00441v1.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2408.00435v1","updated":"2024-08-01T10:14:05Z","published":"2024-08-01T10:14:05Z","title":"A Qualitative Study on Using ChatGPT for Software Security: Perception\n  vs. Practicality","summary":"  Artificial Intelligence (AI) advancements have enabled the development of\nLarge Language Models (LLMs) that can perform a variety of tasks with\nremarkable semantic understanding and accuracy. ChatGPT is one such LLM that\nhas gained significant attention due to its impressive capabilities for\nassisting in various knowledge-intensive tasks. Due to the knowledge-intensive\nnature of engineering secure software, ChatGPT's assistance is expected to be\nexplored for security-related tasks during the development/evolution of\nsoftware. To gain an understanding of the potential of ChatGPT as an emerging\ntechnology for supporting software security, we adopted a two-fold approach.\nInitially, we performed an empirical study to analyse the perceptions of those\nwho had explored the use of ChatGPT for security tasks and shared their views\non Twitter. It was determined that security practitioners view ChatGPT as\nbeneficial for various software security tasks, including vulnerability\ndetection, information retrieval, and penetration testing. Secondly, we\ndesigned an experiment aimed at investigating the practicality of this\ntechnology when deployed as an oracle in real-world settings. In particular, we\nfocused on vulnerability detection and qualitatively examined ChatGPT outputs\nfor given prompts within this prominent software security task. Based on our\nanalysis, responses from ChatGPT in this task are largely filled with generic\nsecurity information and may not be appropriate for industry use. To prevent\ndata leakage, we performed this analysis on a vulnerability dataset compiled\nafter the OpenAI data cut-off date from real-world projects covering 40\ndistinct vulnerability types and 12 programming languages. We assert that the\nfindings from this study would contribute to future research aimed at\ndeveloping and evaluating LLMs dedicated to software security.\n","authors":["M. Mehdi Kholoosi","M. Ali Babar","Roland Croft"],"pdf_url":"https://arxiv.org/pdf/2408.00435v1.pdf","comment":"Accepted for publication at International Conference on Trust,\n  Privacy and Security - 2024"},{"id":"http://arxiv.org/abs/2408.00429v1","updated":"2024-08-01T10:06:02Z","published":"2024-08-01T10:06:02Z","title":"Augmenting Channel Simulator and Semi- Supervised Learning for Efficient\n  Indoor Positioning","summary":"  This work aims to tackle the labor-intensive and resource-consuming task of\nindoor positioning by proposing an efficient approach. The proposed approach\ninvolves the introduction of a semi-supervised learning (SSL) with a biased\nteacher (SSLB) algorithm, which effectively utilizes both labeled and unlabeled\nchannel data. To reduce measurement expenses, unlabeled data is generated using\nan updated channel simulator (UCHS), and then weighted by adaptive confidence\nvalues to simplify the tuning of hyperparameters. Simulation results\ndemonstrate that the proposed strategy achieves superior performance while\nminimizing measurement overhead and training expense compared to existing\nbenchmarks, offering a valuable and practical solution for indoor positioning.\n","authors":["Yupeng Li","Xinyu Ning","Shijian Gao","Yitong Liu","Zhi Sun","Qixing Wang","Jiangzhou Wang"],"pdf_url":"https://arxiv.org/pdf/2408.00429v1.pdf","comment":"ACCEPTED for presentation at 2024 IEEE Global Communications\n  Conference"},{"id":"http://arxiv.org/abs/2408.00427v1","updated":"2024-08-01T09:59:57Z","published":"2024-08-01T09:59:57Z","title":"CARMIL: Context-Aware Regularization on Multiple Instance Learning\n  models for Whole Slide Images","summary":"  Multiple Instance Learning (MIL) models have proven effective for cancer\nprognosis from Whole Slide Images. However, the original MIL formulation\nincorrectly assumes the patches of the same image to be independent, leading to\na loss of spatial context as information flows through the network.\nIncorporating contextual knowledge into predictions is particularly important\ngiven the inclination for cancerous cells to form clusters and the presence of\nspatial indicators for tumors. State-of-the-art methods often use attention\nmechanisms eventually combined with graphs to capture spatial knowledge. In\nthis paper, we take a novel and transversal approach, addressing this issue\nthrough the lens of regularization. We propose Context-Aware Regularization for\nMultiple Instance Learning (CARMIL), a versatile regularization scheme designed\nto seamlessly integrate spatial knowledge into any MIL model. Additionally, we\npresent a new and generic metric to quantify the Context-Awareness of any MIL\nmodel when applied to Whole Slide Images, resolving a previously unexplored gap\nin the field. The efficacy of our framework is evaluated for two survival\nanalysis tasks on glioblastoma (TCGA GBM) and colon cancer data (TCGA COAD).\n","authors":["Thiziri Nait Saada","Valentina Di-Proietto","Benoit Schmauch","Katharina Von Loga","Lucas Fidon"],"pdf_url":"https://arxiv.org/pdf/2408.00427v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00421v1","updated":"2024-08-01T09:46:06Z","published":"2024-08-01T09:46:06Z","title":"Towards Evolutionary-based Automated Machine Learning for Small Molecule\n  Pharmacokinetic Prediction","summary":"  Machine learning (ML) is revolutionising drug discovery by expediting the\nprediction of small molecule properties essential for developing new drugs.\nThese properties -- including absorption, distribution, metabolism and\nexcretion (ADME)-- are crucial in the early stages of drug development since\nthey provide an understanding of the course of the drug in the organism, i.e.,\nthe drug's pharmacokinetics. However, existing methods lack personalisation and\nrely on manually crafted ML algorithms or pipelines, which can introduce\ninefficiencies and biases into the process. To address these challenges, we\npropose a novel evolutionary-based automated ML method (AutoML) specifically\ndesigned for predicting small molecule properties, with a particular focus on\npharmacokinetics. Leveraging the advantages of grammar-based genetic\nprogramming, our AutoML method streamlines the process by automatically\nselecting algorithms and designing predictive pipelines tailored to the\nparticular characteristics of input molecular data. Results demonstrate\nAutoML's effectiveness in selecting diverse ML algorithms, resulting in\ncomparable or even improved predictive performances compared to conventional\napproaches. By offering personalised ML-driven pipelines, our method promises\nto enhance small molecule research in drug discovery, providing researchers\nwith a valuable tool for accelerating the development of novel therapeutic\ndrugs.\n","authors":["Alex G. C. de Sá","David B. Ascher"],"pdf_url":"https://arxiv.org/pdf/2408.00421v1.pdf","comment":"Paper accepted and presented at the 14th Workshop on Evolutionary\n  Computation for the Automated Design of Algorithms (ECADA), which happened\n  during the Genetic and Evolutionary Computation Conference (GECCO)"},{"id":"http://arxiv.org/abs/2408.00420v1","updated":"2024-08-01T09:42:44Z","published":"2024-08-01T09:42:44Z","title":"MPT-PAR:Mix-Parameters Transformer for Panoramic Activity Recognition","summary":"  The objective of the panoramic activity recognition task is to identify\nbehaviors at various granularities within crowded and complex environments,\nencompassing individual actions, social group activities, and global\nactivities. Existing methods generally use either parameter-independent modules\nto capture task-specific features or parameter-sharing modules to obtain common\nfeatures across all tasks. However, there is often a strong interrelatedness\nand complementary effect between tasks of different granularities that previous\nmethods have yet to notice. In this paper, we propose a model called MPT-PAR\nthat considers both the unique characteristics of each task and the synergies\nbetween different tasks simultaneously, thereby maximizing the utilization of\nfeatures across multi-granularity activity recognition. Furthermore, we\nemphasize the significance of temporal and spatial information by introducing a\nspatio-temporal relation-enhanced module and a scene representation learning\nmodule, which integrate the the spatio-temporal context of action and global\nscene into the feature map of each granularity. Our method achieved an overall\nF1 score of 47.5\\% on the JRDB-PAR dataset, significantly outperforming all the\nstate-of-the-art methods.\n","authors":["Wenqing Gan","Yan Sun","Feiran Liu","Xiangfeng Luo"],"pdf_url":"https://arxiv.org/pdf/2408.00420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00415v1","updated":"2024-08-01T09:32:01Z","published":"2024-08-01T09:32:01Z","title":"DriveArena: A Closed-loop Generative Simulation Platform for Autonomous\n  Driving","summary":"  This paper presented DriveArena, the first high-fidelity closed-loop\nsimulation system designed for driving agents navigating in real scenarios.\nDriveArena features a flexible, modular architecture, allowing for the seamless\ninterchange of its core components: Traffic Manager, a traffic simulator\ncapable of generating realistic traffic flow on any worldwide street map, and\nWorld Dreamer, a high-fidelity conditional generative model with infinite\nautoregression. This powerful synergy empowers any driving agent capable of\nprocessing real-world images to navigate in DriveArena's simulated environment.\nThe agent perceives its surroundings through images generated by World Dreamer\nand output trajectories. These trajectories are fed into Traffic Manager,\nachieving realistic interactions with other vehicles and producing a new scene\nlayout. Finally, the latest scene layout is relayed back into World Dreamer,\nperpetuating the simulation cycle. This iterative process fosters closed-loop\nexploration within a highly realistic environment, providing a valuable\nplatform for developing and evaluating driving agents across diverse and\nchallenging scenarios. DriveArena signifies a substantial leap forward in\nleveraging generative image data for the driving simulation platform, opening\ninsights for closed-loop autonomous driving. Code will be available soon on\nGitHub: https://github.com/PJLab-ADG/DriveArena\n","authors":["Xuemeng Yang","Licheng Wen","Yukai Ma","Jianbiao Mei","Xin Li","Tiantian Wei","Wenjie Lei","Daocheng Fu","Pinlong Cai","Min Dou","Botian Shi","Liang He","Yong Liu","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2408.00415v1.pdf","comment":"19 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.00399v1","updated":"2024-08-01T09:11:08Z","published":"2024-08-01T09:11:08Z","title":"Unsupervised Pairwise Causal Discovery on Heterogeneous Data using\n  Mutual Information Measures","summary":"  A fundamental task in science is to determine the underlying causal relations\nbecause it is the knowledge of this functional structure what leads to the\ncorrect interpretation of an effect given the apparent associations in the\nobserved data. In this sense, Causal Discovery is a technique that tackles this\nchallenge by analyzing the statistical properties of the constituent variables.\nIn this work, we target the generalizability of the discovery method by\nfollowing a reductionist approach that only involves two variables, i.e., the\npairwise or bi-variate setting. We question the current (possibly misleading)\nbaseline results on the basis that they were obtained through supervised\nlearning, which is arguably contrary to this genuinely exploratory endeavor. In\nconsequence, we approach this problem in an unsupervised way, using robust\nMutual Information measures, and observing the impact of the different variable\ntypes, which is oftentimes ignored in the design of solutions. Thus, we provide\na novel set of standard unbiased results that can serve as a reference to guide\nfuture discovery tasks in completely unknown environments.\n","authors":["Alexandre Trilla","Nenad Mijatovic"],"pdf_url":"https://arxiv.org/pdf/2408.00399v1.pdf","comment":"26th International Conference of the Catalan Association for\n  Artificial Intelligence"},{"id":"http://arxiv.org/abs/2408.00380v1","updated":"2024-08-01T08:41:13Z","published":"2024-08-01T08:41:13Z","title":"Enhancing Whole Slide Pathology Foundation Models through Stain\n  Normalization","summary":"  Recent advancements in digital pathology have led to the development of\nnumerous foundational models that utilize self-supervised learning on patches\nextracted from gigapixel whole slide images (WSIs). While this approach\nleverages vast amounts of unlabeled data, we have discovered a significant\nissue: features extracted from these self-supervised models tend to cluster by\nindividual WSIs, a phenomenon we term WSI-specific feature collapse. This\nproblem can potentially limit the model's generalization ability and\nperformance on various downstream tasks. To address this issue, we introduce\nStain Normalized Pathology Foundational Model, a novel foundational model\ntrained on patches that have undergone stain normalization. Stain normalization\nhelps reduce color variability arising from different laboratories and\nscanners, enabling the model to learn more consistent features. Stain\nNormalized Pathology Foundational Model is trained using 285,153,903 patches\nextracted from a total of 34,795 WSIs, combining data from The Cancer Genome\nAtlas (TCGA) and the Genotype-Tissue Expression (GTEx) project. Our experiments\ndemonstrate that Stain Normalized Pathology Foundational Model significantly\nmitigates the feature collapse problem, indicating that the model has learned\nmore generalized features rather than overfitting to individual WSI\ncharacteristics. We compared Stain Normalized Pathology Foundational Model with\nstate-of-the-art models across six downstream task datasets, and our results\nshow that \\name{} achieves excellent performance relative to the number of WSIs\nused and the model's parameter count. This suggests that the application of\nstain normalization has substantially improved the model's efficiency and\ngeneralization capabilities.\n","authors":["Juseung Yun","Yi Hu","Jinhyung Kim","Jongseong Jang","Soonyoung Lee"],"pdf_url":"https://arxiv.org/pdf/2408.00380v1.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2408.00376v1","updated":"2024-08-01T08:35:40Z","published":"2024-08-01T08:35:40Z","title":"On the Limitations and Prospects of Machine Unlearning for Generative AI","summary":"  Generative AI (GenAI), which aims to synthesize realistic and diverse data\nsamples from latent variables or other data modalities, has achieved remarkable\nresults in various domains, such as natural language, images, audio, and\ngraphs. However, they also pose challenges and risks to data privacy, security,\nand ethics. Machine unlearning is the process of removing or weakening the\ninfluence of specific data samples or features from a trained model, without\naffecting its performance on other data or tasks. While machine unlearning has\nshown significant efficacy in traditional machine learning tasks, it is still\nunclear if it could help GenAI become safer and aligned with human desire. To\nthis end, this position paper provides an in-depth discussion of the machine\nunlearning approaches for GenAI. Firstly, we formulate the problem of machine\nunlearning tasks on GenAI and introduce the background. Subsequently, we\nsystematically examine the limitations of machine unlearning on GenAI models by\nfocusing on the two representative branches: LLMs and image generative\n(diffusion) models. Finally, we provide our prospects mainly from three\naspects: benchmark, evaluation metrics, and utility-unlearning trade-off, and\nconscientiously advocate for the future development of this field.\n","authors":["Shiji Zhou","Lianzhe Wang","Jiangnan Ye","Yongliang Wu","Heng Chang"],"pdf_url":"https://arxiv.org/pdf/2408.00376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00374v1","updated":"2024-08-01T08:32:03Z","published":"2024-08-01T08:32:03Z","title":"Conformal Trajectory Prediction with Multi-View Data Integration in\n  Cooperative Driving","summary":"  Current research on trajectory prediction primarily relies on data collected\nby onboard sensors of an ego vehicle. With the rapid advancement in connected\ntechnologies, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure\n(V2I) communication, valuable information from alternate views becomes\naccessible via wireless networks. The integration of information from\nalternative views has the potential to overcome the inherent limitations\nassociated with a single viewpoint, such as occlusions and limited field of\nview. In this work, we introduce V2INet, a novel trajectory prediction\nframework designed to model multi-view data by extending existing single-view\nmodels. Unlike previous approaches where the multi-view data is manually fused\nor formulated as a separate training stage, our model supports end-to-end\ntraining, enhancing both flexibility and performance. Moreover, the predicted\nmultimodal trajectories are calibrated by a post-hoc conformal prediction\nmodule to get valid and efficient confidence regions. We evaluated the entire\nframework using the real-world V2I dataset V2X-Seq. Our results demonstrate\nsuperior performance in terms of Final Displacement Error (FDE) and Miss Rate\n(MR) using a single GPU. The code is publicly available at:\n\\url{https://github.com/xichennn/V2I_trajectory_prediction}.\n","authors":["Xi Chen","Rahul Bhadani","Larry Head"],"pdf_url":"https://arxiv.org/pdf/2408.00374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00370v1","updated":"2024-08-01T08:22:47Z","published":"2024-08-01T08:22:47Z","title":"DiM-Gesture: Co-Speech Gesture Generation with Adaptive Layer\n  Normalization Mamba-2 framework","summary":"  Speech-driven gesture generation is an emerging domain within virtual human\ncreation, where current methods predominantly utilize Transformer-based\narchitectures that necessitate extensive memory and are characterized by slow\ninference speeds. In response to these limitations, we propose\n\\textit{DiM-Gestures}, a novel end-to-end generative model crafted to create\nhighly personalized 3D full-body gestures solely from raw speech audio,\nemploying Mamba-based architectures. This model integrates a Mamba-based fuzzy\nfeature extractor with a non-autoregressive Adaptive Layer Normalization\n(AdaLN) Mamba-2 diffusion architecture. The extractor, leveraging a Mamba\nframework and a WavLM pre-trained model, autonomously derives implicit,\ncontinuous fuzzy features, which are then unified into a singular latent\nfeature. This feature is processed by the AdaLN Mamba-2, which implements a\nuniform conditional mechanism across all tokens to robustly model the interplay\nbetween the fuzzy features and the resultant gesture sequence. This innovative\napproach guarantees high fidelity in gesture-speech synchronization while\nmaintaining the naturalness of the gestures. Employing a diffusion model for\ntraining and inference, our framework has undergone extensive subjective and\nobjective evaluations on the ZEGGS and BEAT datasets. These assessments\nsubstantiate our model's enhanced performance relative to contemporary\nstate-of-the-art methods, demonstrating competitive outcomes with the DiTs\narchitecture (Persona-Gestors) while optimizing memory usage and accelerating\ninference speed.\n","authors":["Fan Zhang","Naye Ji","Fuxing Gao","Bozuo Zhao","Jingmei Wu","Yanbing Jiang","Hui Du","Zhenqing Ye","Jiayang Zhu","WeiFan Zhong","Leyao Yan","Xiaomeng Ma"],"pdf_url":"https://arxiv.org/pdf/2408.00370v1.pdf","comment":"10 pages,10 figures. arXiv admin note: text overlap with\n  arXiv:2403.10805"},{"id":"http://arxiv.org/abs/2408.00365v1","updated":"2024-08-01T08:10:32Z","published":"2024-08-01T08:10:32Z","title":"Multimodal Fusion and Coherence Modeling for Video Topic Segmentation","summary":"  The video topic segmentation (VTS) task segments videos into intelligible,\nnon-overlapping topics, facilitating efficient comprehension of video content\nand quick access to specific content. VTS is also critical to various\ndownstream video understanding tasks. Traditional VTS methods using shallow\nfeatures or unsupervised approaches struggle to accurately discern the nuances\nof topical transitions. Recently, supervised approaches have achieved superior\nperformance on video action or scene segmentation over unsupervised approaches.\nIn this work, we improve supervised VTS by thoroughly exploring multimodal\nfusion and multimodal coherence modeling. Specifically, (1) we enhance\nmultimodal fusion by exploring different architectures using cross-attention\nand mixture of experts. (2) To generally strengthen multimodality alignment and\nfusion, we pre-train and fine-tune the model with multimodal contrastive\nlearning. (3) We propose a new pre-training task tailored for the VTS task, and\na novel fine-tuning task for enhancing multimodal coherence modeling for VTS.\nWe evaluate the proposed approaches on educational videos, in the form of\nlectures, due to the vital role of topic segmentation of educational videos in\nboosting learning experiences. Additionally, we introduce a large-scale Chinese\nlecture video dataset to augment the existing English corpus, promoting further\nresearch in VTS. Experiments on both English and Chinese lecture datasets\ndemonstrate that our model achieves superior VTS performance compared to\ncompetitive unsupervised and supervised baselines.\n","authors":["Hai Yu","Chong Deng","Qinglin Zhang","Jiaqing Liu","Qian Chen","Wen Wang"],"pdf_url":"https://arxiv.org/pdf/2408.00365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00355v1","updated":"2024-08-01T07:52:07Z","published":"2024-08-01T07:52:07Z","title":"DNTextSpotter: Arbitrary-Shaped Scene Text Spotting via Improved\n  Denoising Training","summary":"  More and more end-to-end text spotting methods based on Transformer\narchitecture have demonstrated superior performance. These methods utilize a\nbipartite graph matching algorithm to perform one-to-one optimal matching\nbetween predicted objects and actual objects. However, the instability of\nbipartite graph matching can lead to inconsistent optimization targets, thereby\naffecting the training performance of the model. Existing literature applies\ndenoising training to solve the problem of bipartite graph matching instability\nin object detection tasks. Unfortunately, this denoising training method cannot\nbe directly applied to text spotting tasks, as these tasks need to perform\nirregular shape detection tasks and more complex text recognition tasks than\nclassification. To address this issue, we propose a novel denoising training\nmethod (DNTextSpotter) for arbitrary-shaped text spotting. Specifically, we\ndecompose the queries of the denoising part into noised positional queries and\nnoised content queries. We use the four Bezier control points of the Bezier\ncenter curve to generate the noised positional queries. For the noised content\nqueries, considering that the output of the text in a fixed positional order is\nnot conducive to aligning position with content, we employ a masked character\nsliding method to initialize noised content queries, thereby assisting in the\nalignment of text content and position. To improve the model's perception of\nthe background, we further utilize an additional loss function for background\ncharacters classification in the denoising training part.Although DNTextSpotter\nis conceptually simple, it outperforms the state-of-the-art methods on four\nbenchmarks (Total-Text, SCUT-CTW1500, ICDAR15, and Inverse-Text), especially\nyielding an improvement of 11.3% against the best approach in Inverse-Text\ndataset.\n","authors":["Yu Xie","Qian Qiao","Jun Gao","Tianxiang Wu","Shaoyao Huang","Jiaqing Fan","Ziqiang Cao","Zili Wang","Yue Zhang","Jielei Zhang","Huyang Sun"],"pdf_url":"https://arxiv.org/pdf/2408.00355v1.pdf","comment":"Accepted by ACMMM2024"},{"id":"http://arxiv.org/abs/2408.00350v1","updated":"2024-08-01T07:40:00Z","published":"2024-08-01T07:40:00Z","title":"A Simple Background Augmentation Method for Object Detection with\n  Diffusion Model","summary":"  In computer vision, it is well-known that a lack of data diversity will\nimpair model performance. In this study, we address the challenges of enhancing\nthe dataset diversity problem in order to benefit various downstream tasks such\nas object detection and instance segmentation. We propose a simple yet\neffective data augmentation approach by leveraging advancements in generative\nmodels, specifically text-to-image synthesis technologies like Stable\nDiffusion. Our method focuses on generating variations of labeled real images,\nutilizing generative object and background augmentation via inpainting to\naugment existing training data without the need for additional annotations. We\nfind that background augmentation, in particular, significantly improves the\nmodels' robustness and generalization capabilities. We also investigate how to\nadjust the prompt and mask to ensure the generated content comply with the\nexisting annotations. The efficacy of our augmentation techniques is validated\nthrough comprehensive evaluations of the COCO dataset and several other key\nobject detection benchmarks, demonstrating notable enhancements in model\nperformance across diverse scenarios. This approach offers a promising solution\nto the challenges of dataset enhancement, contributing to the development of\nmore accurate and robust computer vision models.\n","authors":["Yuhang Li","Xin Dong","Chen Chen","Weiming Zhuang","Lingjuan Lyu"],"pdf_url":"https://arxiv.org/pdf/2408.00350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00348v1","updated":"2024-08-01T07:37:27Z","published":"2024-08-01T07:37:27Z","title":"Securing the Diagnosis of Medical Imaging: An In-depth Analysis of\n  AI-Resistant Attacks","summary":"  Machine learning (ML) is a rapidly developing area of medicine that uses\nsignificant resources to apply computer science and statistics to medical\nissues. ML's proponents laud its capacity to handle vast, complicated, and\nerratic medical data. It's common knowledge that attackers might cause\nmisclassification by deliberately creating inputs for machine learning\nclassifiers. Research on adversarial examples has been extensively conducted in\nthe field of computer vision applications. Healthcare systems are thought to be\nhighly difficult because of the security and life-or-death considerations they\ninclude, and performance accuracy is very important. Recent arguments have\nsuggested that adversarial attacks could be made against medical image analysis\n(MedIA) technologies because of the accompanying technology infrastructure and\npowerful financial incentives. Since the diagnosis will be the basis for\nimportant decisions, it is essential to assess how strong medical DNN tasks are\nagainst adversarial attacks. Simple adversarial attacks have been taken into\naccount in several earlier studies. However, DNNs are susceptible to more risky\nand realistic attacks. The present paper covers recent proposed adversarial\nattack strategies against DNNs for medical imaging as well as countermeasures.\nIn this study, we review current techniques for adversarial imaging attacks,\ndetections. It also encompasses various facets of these techniques and offers\nsuggestions for the robustness of neural networks to be improved in the future.\n","authors":["Angona Biswas","MD Abdullah Al Nasim","Kishor Datta Gupta","Roy George","Abdur Rashid"],"pdf_url":"https://arxiv.org/pdf/2408.00348v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00347v1","updated":"2024-08-01T07:35:54Z","published":"2024-08-01T07:35:54Z","title":"Advancing Medical Image Segmentation: Morphology-Driven Learning with\n  Diffusion Transformer","summary":"  Understanding the morphological structure of medical images and precisely\nsegmenting the region of interest or abnormality is an important task that can\nassist in diagnosis. However, the unique properties of medical imaging make\nclear segmentation difficult, and the high cost and time-consuming task of\nlabeling leads to a coarse-grained representation of ground truth. Facing with\nthese problems, we propose a novel Diffusion Transformer Segmentation (DTS)\nmodel for robust segmentation in the presence of noise. We propose an\nalternative to the dominant Denoising U-Net encoder through experiments\napplying a transformer architecture, which captures global dependency through\nself-attention. Additionally, we propose k-neighbor label smoothing, reverse\nboundary attention, and self-supervised learning with morphology-driven\nlearning to improve the ability to identify complex structures. Our model,\nwhich analyzes the morphological representation of images, shows better results\nthan the previous models in various medical imaging modalities, including CT,\nMRI, and lesion images.\n","authors":["Sungmin Kang","Jaeha Song","Jihie Kim"],"pdf_url":"https://arxiv.org/pdf/2408.00347v1.pdf","comment":"Accepted in BMVC 2024"},{"id":"http://arxiv.org/abs/2408.00346v1","updated":"2024-08-01T07:31:23Z","published":"2024-08-01T07:31:23Z","title":"Neural Graph Matching for Video Retrieval in Large-Scale Video-driven\n  E-commerce","summary":"  With the rapid development of the short video industry, traditional\ne-commerce has encountered a new paradigm, video-driven e-commerce, which\nleverages attractive videos for product showcases and provides both video and\nitem services for users. Benefitting from the dynamic and visualized\nintroduction of items,video-driven e-commerce has shown huge potential in\nstimulating consumer confidence and promoting sales. In this paper, we focus on\nthe video retrieval task, facing the following challenges: (1) Howto handle the\nheterogeneities among users, items, and videos? (2)How to mine the\ncomplementarity between items and videos for better user understanding? In this\npaper, we first leverage the dual graph to model the co-existing of user-video\nand user-item interactions in video-driven e-commerce and innovatively reduce\nuser preference understanding to a graph matching problem. To solve it, we\nfurther propose a novel bi-level Graph Matching Network(GMN), which mainly\nconsists of node- and preference-level graph matching. Given a user, node-level\ngraph matching aims to match videos and items, while preference-level graph\nmatching aims to match multiple user preferences extracted from both videos and\nitems. Then the proposed GMN can generate and improve user embedding by\naggregating matched nodes or preferences from the dual graph in a bi-level\nmanner. Comprehensive experiments show the superiority of the proposed GMN with\nsignificant improvements over state-of-the-art approaches (e.g., AUC+1.9% and\nCTR+7.15%). We have developed it on a well-known video-driven e-commerce\nplatform, serving hundreds of millions of users every day\n","authors":["Houye Ji","Ye Tang","Zhaoxin Chen","Lixi Deng","Jun Hu","Lei Su"],"pdf_url":"https://arxiv.org/pdf/2408.00346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00342v1","updated":"2024-08-01T07:27:18Z","published":"2024-08-01T07:27:18Z","title":"MuJoCo MPC for Humanoid Control: Evaluation on HumanoidBench","summary":"  We tackle the recently introduced benchmark for whole-body humanoid control\nHumanoidBench using MuJoCo MPC. We find that sparse reward functions of\nHumanoidBench yield undesirable and unrealistic behaviors when optimized;\ntherefore, we propose a set of regularization terms that stabilize the robot\nbehavior across tasks. Current evaluations on a subset of tasks demonstrate\nthat our proposed reward function allows achieving the highest HumanoidBench\nscores while maintaining realistic posture and smooth control signals. Our code\nis publicly available and will become a part of MuJoCo MPC, enabling rapid\nprototyping of robot behaviors.\n","authors":["Moritz Meser","Aditya Bhatt","Boris Belousov","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2408.00342v1.pdf","comment":"3 pages, 3 figures, submitted to IEEE Conference on Robotics and\n  Automation (ICRA@40)"},{"id":"http://arxiv.org/abs/2408.00329v1","updated":"2024-08-01T07:04:18Z","published":"2024-08-01T07:04:18Z","title":"OTAD: An Optimal Transport-Induced Robust Model for Agnostic Adversarial\n  Attack","summary":"  Deep neural networks (DNNs) are vulnerable to small adversarial perturbations\nof the inputs, posing a significant challenge to their reliability and\nrobustness. Empirical methods such as adversarial training can defend against\nparticular attacks but remain vulnerable to more powerful attacks.\nAlternatively, Lipschitz networks provide certified robustness to unseen\nperturbations but lack sufficient expressive power. To harness the advantages\nof both approaches, we design a novel two-step Optimal Transport induced\nAdversarial Defense (OTAD) model that can fit the training data accurately\nwhile preserving the local Lipschitz continuity. First, we train a DNN with a\nregularizer derived from optimal transport theory, yielding a discrete optimal\ntransport map linking data to its features. By leveraging the map's inherent\nregularity, we interpolate the map by solving the convex integration problem\n(CIP) to guarantee the local Lipschitz property. OTAD is extensible to diverse\narchitectures of ResNet and Transformer, making it suitable for complex data.\nFor efficient computation, the CIP can be solved through training neural\nnetworks. OTAD opens a novel avenue for developing reliable and secure deep\nlearning systems through the regularity of optimal transport maps. Empirical\nresults demonstrate that OTAD can outperform other robust models on diverse\ndatasets.\n","authors":["Kuo Gai","Sicong Wang","Shihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00329v1.pdf","comment":"14 pages, 2 figures"},{"id":"http://arxiv.org/abs/2408.00315v1","updated":"2024-08-01T06:26:05Z","published":"2024-08-01T06:26:05Z","title":"ADBM: Adversarial diffusion bridge model for reliable adversarial\n  purification","summary":"  Recently Diffusion-based Purification (DiffPure) has been recognized as an\neffective defense method against adversarial examples. However, we find\nDiffPure which directly employs the original pre-trained diffusion models for\nadversarial purification, to be suboptimal. This is due to an inherent\ntrade-off between noise purification performance and data recovery quality.\nAdditionally, the reliability of existing evaluations for DiffPure is\nquestionable, as they rely on weak adaptive attacks. In this work, we propose a\nnovel Adversarial Diffusion Bridge Model, termed ADBM. ADBM directly constructs\na reverse bridge from the diffused adversarial data back to its original clean\nexamples, enhancing the purification capabilities of the original diffusion\nmodels. Through theoretical analysis and experimental validation across various\nscenarios, ADBM has proven to be a superior and robust defense mechanism,\noffering significant promise for practical applications.\n","authors":["Xiao Li","Wenxuan Sun","Huanran Chen","Qiongxiu Li","Yining Liu","Yingzhe He","Jie Shi","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2408.00315v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2408.00309v1","updated":"2024-08-01T06:06:53Z","published":"2024-08-01T06:06:53Z","title":"Discretizing Continuous Action Space with Unimodal Probability\n  Distributions for On-Policy Reinforcement Learning","summary":"  For on-policy reinforcement learning, discretizing action space for\ncontinuous control can easily express multiple modes and is straightforward to\noptimize. However, without considering the inherent ordering between the\ndiscrete atomic actions, the explosion in the number of discrete actions can\npossess undesired properties and induce a higher variance for the policy\ngradient estimator. In this paper, we introduce a straightforward architecture\nthat addresses this issue by constraining the discrete policy to be unimodal\nusing Poisson probability distributions. This unimodal architecture can better\nleverage the continuity in the underlying continuous action space using\nexplicit unimodal probability distributions. We conduct extensive experiments\nto show that the discrete policy with the unimodal probability distribution\nprovides significantly faster convergence and higher performance for on-policy\nreinforcement learning algorithms in challenging control tasks, especially in\nhighly complex tasks such as Humanoid. We provide theoretical analysis on the\nvariance of the policy gradient estimator, which suggests that our attentively\ndesigned unimodal discrete policy can retain a lower variance and yield a\nstable learning process.\n","authors":["Yuanyang Zhu","Zhi Wang","Yuanheng Zhu","Chunlin Chen","Dongbin Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.00309v1.pdf","comment":"IEEE Transactions on Neural Networks and Learning Systems"},{"id":"http://arxiv.org/abs/2408.00307v1","updated":"2024-08-01T06:06:25Z","published":"2024-08-01T06:06:25Z","title":"ABC Align: Large Language Model Alignment for Safety & Accuracy","summary":"  Alignment of Large Language Models (LLMs) remains an unsolved problem. Human\npreferences are highly distributed and can be captured at multiple levels of\nabstraction, from the individual to diverse populations. Organisational\npreferences, represented by standards and principles, are defined to mitigate\nreputational risk or meet legislative obligations. In this paper, we present\nABC Align, a novel alignment methodology for LLMs that enables integration of\nthe standards and preferences of a large media organisation into the LLM\nitself. We combine a set of data and methods that build on recent breakthroughs\nin synthetic data generation, preference optimisation, and post-training model\nquantisation. Our unified approach mitigates bias and improves accuracy, while\npreserving reasoning capability, as measured against standard benchmarks.\n","authors":["Gareth Seneque","Lap-Hang Ho","Ariel Kuperman","Nafise Erfanian Saeedi","Jeffrey Molendijk"],"pdf_url":"https://arxiv.org/pdf/2408.00307v1.pdf","comment":"23 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.00295v1","updated":"2024-08-01T05:45:21Z","published":"2024-08-01T05:45:21Z","title":"Contrastive Graph Representation Learning with Adversarial Cross-view\n  Reconstruction and Information Bottleneck","summary":"  Graph Neural Networks (GNNs) have received extensive research attention due\nto their powerful information aggregation capabilities. Despite the success of\nGNNs, most of them suffer from the popularity bias issue in a graph caused by a\nsmall number of popular categories. Additionally, real graph datasets always\ncontain incorrect node labels, which hinders GNNs from learning effective node\nrepresentations. Graph contrastive learning (GCL) has been shown to be\neffective in solving the above problems for node classification tasks. Most\nexisting GCL methods are implemented by randomly removing edges and nodes to\ncreate multiple contrasting views, and then maximizing the mutual information\n(MI) between these contrasting views to improve the node feature\nrepresentation. However, maximizing the mutual information between multiple\ncontrasting views may lead the model to learn some redundant information\nirrelevant to the node classification task. To tackle this issue, we propose an\neffective Contrastive Graph Representation Learning with Adversarial Cross-view\nReconstruction and Information Bottleneck (CGRL) for node classification, which\ncan adaptively learn to mask the nodes and edges in the graph to obtain the\noptimal graph structure representation. Furthermore, we innovatively introduce\nthe information bottleneck theory into GCLs to remove redundant information in\nmultiple contrasting views while retaining as much information as possible\nabout node classification. Moreover, we add noise perturbations to the original\nviews and reconstruct the augmented views by constructing adversarial views to\nimprove the robustness of node feature representation. Extensive experiments on\nreal-world public datasets demonstrate that our method significantly\noutperforms existing state-of-the-art algorithms.\n","authors":["Yuntao Shou","Haozhi Lan","Xiangyong Cao"],"pdf_url":"https://arxiv.org/pdf/2408.00295v1.pdf","comment":"13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.00290v1","updated":"2024-08-01T05:24:20Z","published":"2024-08-01T05:24:20Z","title":"Multi-Modal Parameter-Efficient Fine-tuning via Graph Neural Network","summary":"  With the advent of the era of foundation models, pre-training and fine-tuning\nhave become common paradigms. Recently, parameter-efficient fine-tuning has\ngarnered widespread attention due to its better balance between the number of\nlearnable parameters and performance. However, some current parameter-efficient\nfine-tuning methods only model a single modality and lack the utilization of\nstructural knowledge in downstream tasks. To address this issue, this paper\nproposes a multi-modal parameter-efficient fine-tuning method based on graph\nnetworks. Each image is fed into a multi-modal large language model (MLLM) to\ngenerate a text description. The image and its corresponding text description\nare then processed by a frozen image encoder and text encoder to generate image\nfeatures and text features, respectively. A graph is constructed based on the\nsimilarity of the multi-modal feature nodes, and knowledge and relationships\nrelevant to these features are extracted from each node. Additionally, Elastic\nWeight Consolidation (EWC) regularization is incorporated into the loss\nfunction to mitigate the problem of forgetting during task learning. The\nproposed model achieves test accuracies on the OxfordPets, Flowers102, and\nFood101 datasets that improve by 4.45%, 2.92%, and 0.23%, respectively. The\ncode is available at https://github.com/yunche0/GA-Net/tree/master.\n","authors":["Bin Cheng","Jiaxuan Lu"],"pdf_url":"https://arxiv.org/pdf/2408.00290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.05480v4","updated":"2024-08-01T22:57:53Z","published":"2024-05-09T00:37:56Z","title":"FloorSet -- a VLSI Floorplanning Dataset with Design Constraints of\n  Real-World SoCs","summary":"  Floorplanning for systems-on-a-chip (SoCs) and its sub-systems is a crucial\nand non-trivial step of the physical design flow. It represents a difficult\ncombinatorial optimization problem. A typical large scale SoC with 120\npartitions generates a search-space of nearly 10E250. As novel machine learning\n(ML) approaches emerge to tackle such problems, there is a growing need for a\nmodern benchmark that comprises a large training dataset and performance\nmetrics that better reflect real-world constraints and objectives compared to\nexisting benchmarks. To address this need, we present FloorSet -- two\ncomprehensive datasets of synthetic fixed-outline floorplan layouts that\nreflect the distribution of real SoCs. Each dataset has 1M training samples and\n100 test samples where each sample is a synthetic floor-plan. FloorSet-Prime\ncomprises fully-abutted rectilinear partitions and near-optimal wire-length. A\nsimplified dataset that reflects early design phases, FloorSet-Lite comprises\nrectangular partitions, with under 5 percent white-space and near-optimal\nwire-length. Both datasets define hard constraints seen in modern design flows\nsuch as shape constraints, edge-affinity, grouping constraints, and\npre-placement constraints. FloorSet is intended to spur fundamental research on\nlarge-scale constrained optimization problems. Crucially, FloorSet alleviates\nthe core issue of reproducibility in modern ML driven solutions to such\nproblems. FloorSet is available as an open-source repository for the research\ncommunity.\n","authors":["Uday Mallappa","Hesham Mostafa","Mikhail Galkin","Mariano Phielipp","Somdeb Majumdar"],"pdf_url":"https://arxiv.org/pdf/2405.05480v4.pdf","comment":"10 pages, 11 figures"},{"id":"http://arxiv.org/abs/2408.00946v1","updated":"2024-08-01T22:55:40Z","published":"2024-08-01T22:55:40Z","title":"Generalisation of Total Uncertainty in AI: A Theoretical Study","summary":"  AI has been dealing with uncertainty to have highly accurate results. This\nbecomes even worse with reasonably small data sets or a variation in the data\nsets. This has far-reaching effects on decision-making, forecasting and\nlearning mechanisms. This study seeks to unpack the nature of uncertainty that\nexists within AI by drawing ideas from established works, the latest\ndevelopments and practical applications and provide a novel total uncertainty\ndefinition in AI.\n  From inception theories up to current methodologies, this paper provides an\nintegrated view of dealing with better total uncertainty as well as\ncomplexities of uncertainty in AI that help us understand its meaning and value\nacross different domains.\n","authors":["Keivan Shariatmadar"],"pdf_url":"https://arxiv.org/pdf/2408.00946v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2405.19024v3","updated":"2024-08-01T22:45:45Z","published":"2024-05-29T12:07:17Z","title":"Inverse Concave-Utility Reinforcement Learning is Inverse Game Theory","summary":"  We consider inverse reinforcement learning problems with concave utilities.\nConcave Utility Reinforcement Learning (CURL) is a generalisation of the\nstandard RL objective, which employs a concave function of the state occupancy\nmeasure, rather than a linear function. CURL has garnered recent attention for\nits ability to represent instances of many important applications including the\nstandard RL such as imitation learning, pure exploration, constrained MDPs,\noffline RL, human-regularized RL, and others. Inverse reinforcement learning is\na powerful paradigm that focuses on recovering an unknown reward function that\ncan rationalize the observed behaviour of an agent. There has been recent\ntheoretical advances in inverse RL where the problem is formulated as\nidentifying the set of feasible reward functions. However, inverse RL for CURL\nproblems has not been considered previously. In this paper we show that most of\nthe standard IRL results do not apply to CURL in general, since CURL\ninvalidates the classical Bellman equations. This calls for a new theoretical\nframework for the inverse CURL problem. Using a recent equivalence result\nbetween CURL and Mean-field Games, we propose a new definition for the feasible\nrewards for I-CURL by proving that this problem is equivalent to an inverse\ngame theory problem in a subclass of mean-field games. We outline future\ndirections and applications in human--AI collaboration enabled by our results.\n","authors":["Mustafa Mert Çelikok","Frans A. Oliehoek","Jan-Willem van de Meent"],"pdf_url":"https://arxiv.org/pdf/2405.19024v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02652v2","updated":"2024-08-01T22:39:20Z","published":"2024-06-04T16:14:19Z","title":"RepCNN: Micro-sized, Mighty Models for Wakeword Detection","summary":"  Always-on machine learning models require a very low memory and compute\nfootprint. Their restricted parameter count limits the model's capacity to\nlearn, and the effectiveness of the usual training algorithms to find the best\nparameters. Here we show that a small convolutional model can be better trained\nby first refactoring its computation into a larger redundant multi-branched\narchitecture. Then, for inference, we algebraically re-parameterize the trained\nmodel into the single-branched form with fewer parameters for a lower memory\nfootprint and compute cost. Using this technique, we show that our always-on\nwake-word detector model, RepCNN, provides a good trade-off between latency and\naccuracy during inference. RepCNN re-parameterized models are 43% more accurate\nthan a uni-branch convolutional model while having the same runtime. RepCNN\nalso meets the accuracy of complex architectures like BC-ResNet, while having\n2x lesser peak memory usage and 10x faster runtime.\n","authors":["Arnav Kundu","Prateeth Nayak","Priyanka Padmanabhan","Devang Naik"],"pdf_url":"https://arxiv.org/pdf/2406.02652v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17374v2","updated":"2024-08-01T22:11:48Z","published":"2024-07-24T15:53:04Z","title":"Co-designing an AI Impact Assessment Report Template with AI\n  Practitioners and AI Compliance Experts","summary":"  In the evolving landscape of AI regulation, it is crucial for companies to\nconduct impact assessments and document their compliance through comprehensive\nreports. However, current reports lack grounding in regulations and often focus\non specific aspects like privacy in relation to AI systems, without addressing\nthe real-world uses of these systems. Moreover, there is no systematic effort\nto design and evaluate these reports with both AI practitioners and AI\ncompliance experts. To address this gap, we conducted an iterative co-design\nprocess with 14 AI practitioners and 6 AI compliance experts and proposed a\ntemplate for impact assessment reports grounded in the EU AI Act, NIST's AI\nRisk Management Framework, and ISO 42001 AI Management System. We evaluated the\ntemplate by producing an impact assessment report for an AI-based meeting\ncompanion at a major tech company. A user study with 8 AI practitioners from\nthe same company and 5 AI compliance experts from industry and academia\nrevealed that our template effectively provides necessary information for\nimpact assessments and documents the broad impacts of AI systems. Participants\nenvisioned using the template not only at the pre-deployment stage for\ncompliance but also as a tool to guide the design stage of AI uses.\n","authors":["Edyta Bogucka","Marios Constantinides","Sanja Šćepanović","Daniele Quercia"],"pdf_url":"https://arxiv.org/pdf/2407.17374v2.pdf","comment":"16 pages, 6 figures"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2407.21740v2","updated":"2024-08-01T03:16:43Z","published":"2024-07-31T16:52:00Z","title":"Contrastive Factor Analysis","summary":"  Factor analysis, often regarded as a Bayesian variant of matrix\nfactorization, offers superior capabilities in capturing uncertainty, modeling\ncomplex dependencies, and ensuring robustness. As the deep learning era\narrives, factor analysis is receiving less and less attention due to their\nlimited expressive ability. On the contrary, contrastive learning has emerged\nas a potent technique with demonstrated efficacy in unsupervised\nrepresentational learning. While the two methods are different paradigms,\nrecent theoretical analysis has revealed the mathematical equivalence between\ncontrastive learning and matrix factorization, providing a potential\npossibility for factor analysis combined with contrastive learning. Motivated\nby the interconnectedness of contrastive learning, matrix factorization, and\nfactor analysis, this paper introduces a novel Contrastive Factor Analysis\nframework, aiming to leverage factor analysis's advantageous properties within\nthe realm of contrastive learning. To further leverage the interpretability\nproperties of non-negative factor analysis, which can learn disentangled\nrepresentations, contrastive factor analysis is extended to a non-negative\nversion. Finally, extensive experimental validation showcases the efficacy of\nthe proposed contrastive (non-negative) factor analysis methodology across\nmultiple key properties, including expressiveness, robustness,\ninterpretability, and accurate uncertainty estimation.\n","authors":["Zhibin Duan","Tiansheng Wen","Yifei Wang","Chen Zhu","Bo Chen","Mingyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.21740v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.03733v3","updated":"2024-08-01T17:47:24Z","published":"2022-12-07T15:55:00Z","title":"Tiered Reward: Designing Rewards for Specification and Fast Learning of\n  Desired Behavior","summary":"  Reinforcement-learning agents seek to maximize a reward signal through\nenvironmental interactions. As humans, our job in the learning process is to\ndesign reward functions to express desired behavior and enable the agent to\nlearn such behavior swiftly. However, designing good reward functions to induce\nthe desired behavior is generally hard, let alone the question of which rewards\nmake learning fast. In this work, we introduce a family of a reward structures\nwe call Tiered Reward that addresses both of these questions. We consider the\nreward-design problem in tasks formulated as reaching desirable states and\navoiding undesirable states. To start, we propose a strict partial ordering of\nthe policy space to resolve trade-offs in behavior preference. We prefer\npolicies that reach the good states faster and with higher probability while\navoiding the bad states longer. Next, we introduce Tiered Reward, a class of\nenvironment-independent reward functions and show it is guaranteed to induce\npolicies that are Pareto-optimal according to our preference relation. Finally,\nwe demonstrate that Tiered Reward leads to fast learning with multiple tabular\nand deep reinforcement-learning algorithms.\n","authors":["Zhiyuan Zhou","Shreyas Sundara Raman","Henry Sowerby","Michael L. Littman"],"pdf_url":"https://arxiv.org/pdf/2212.03733v3.pdf","comment":"For code, see https://github.com/zhouzypaul/tiered-reward"},{"id":"http://arxiv.org/abs/2407.14435v3","updated":"2024-08-01T17:42:04Z","published":"2024-07-19T16:07:19Z","title":"Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse\n  Autoencoders","summary":"  Sparse autoencoders (SAEs) are a promising unsupervised approach for\nidentifying causally relevant and interpretable linear features in a language\nmodel's (LM) activations. To be useful for downstream tasks, SAEs need to\ndecompose LM activations faithfully; yet to be interpretable the decomposition\nmust be sparse -- two objectives that are in tension. In this paper, we\nintroduce JumpReLU SAEs, which achieve state-of-the-art reconstruction fidelity\nat a given sparsity level on Gemma 2 9B activations, compared to other recent\nadvances such as Gated and TopK SAEs. We also show that this improvement does\nnot come at the cost of interpretability through manual and automated\ninterpretability studies. JumpReLU SAEs are a simple modification of vanilla\n(ReLU) SAEs -- where we replace the ReLU with a discontinuous JumpReLU\nactivation function -- and are similarly efficient to train and run. By\nutilising straight-through-estimators (STEs) in a principled manner, we show\nhow it is possible to train JumpReLU SAEs effectively despite the discontinuous\nJumpReLU function introduced in the SAE's forward pass. Similarly, we use STEs\nto directly train L0 to be sparse, instead of training on proxies such as L1,\navoiding problems like shrinkage.\n","authors":["Senthooran Rajamanoharan","Tom Lieberum","Nicolas Sonnerat","Arthur Conmy","Vikrant Varma","János Kramár","Neel Nanda"],"pdf_url":"https://arxiv.org/pdf/2407.14435v3.pdf","comment":"v2: new appendix H comparing kernel functions & bug-fixes to\n  pseudo-code in Appendix J v3: further bug-fix to pseudo-code in Appendix J"},{"id":"http://arxiv.org/abs/2308.01674v4","updated":"2024-08-01T17:41:27Z","published":"2023-08-03T10:21:53Z","title":"End-to-End Reinforcement Learning of Koopman Models for Economic\n  Nonlinear Model Predictive Control","summary":"  (Economic) nonlinear model predictive control ((e)NMPC) requires dynamic\nmodels that are sufficiently accurate and computationally tractable.\nData-driven surrogate models for mechanistic models can reduce the\ncomputational burden of (e)NMPC; however, such models are typically trained by\nsystem identification for maximum prediction accuracy on simulation samples and\nperform suboptimally in (e)NMPC. We present a method for end-to-end\nreinforcement learning of Koopman surrogate models for optimal performance as\npart of (e)NMPC. We apply our method to two applications derived from an\nestablished nonlinear continuous stirred-tank reactor model. The controller\nperformance is compared to that of (e)NMPCs utilizing models trained using\nsystem identification, and model-free neural network controllers trained using\nreinforcement learning. We show that the end-to-end trained models outperform\nthose trained using system identification in (e)NMPC, and that, in contrast to\nthe neural network controllers, the (e)NMPC controllers can react to changes in\nthe control setting without retraining.\n","authors":["Daniel Mayfrank","Alexander Mitsos","Manuel Dahmen"],"pdf_url":"https://arxiv.org/pdf/2308.01674v4.pdf","comment":"manuscript (20 pages, 7 figures, 6 tables), supplementary materials\n  (3 pages, 2 tables)"},{"id":"http://arxiv.org/abs/2407.16020v3","updated":"2024-08-01T17:40:36Z","published":"2024-07-22T19:55:44Z","title":"Sparks of Quantum Advantage and Rapid Retraining in Machine Learning","summary":"  The advent of quantum computing holds the potential to revolutionize various\nfields by solving complex problems more efficiently than classical computers.\nDespite this promise, practical quantum advantage is hindered by current\nhardware limitations, notably the small number of qubits and high noise levels.\nIn this study, we leverage adiabatic quantum computers to optimize\nKolmogorov-Arnold Networks, a powerful neural network architecture for\nrepresenting complex functions with minimal parameters. By modifying the\nnetwork to use Bezier curves as the basis functions and formulating the\noptimization problem into a Quadratic Unconstrained Binary Optimization\nproblem, we create a fixed-sized solution space, independent of the number of\ntraining samples. Our approach demonstrates sparks of quantum advantage through\nfaster training times compared to classical optimizers such as the Adam,\nStochastic Gradient Descent, Adaptive Gradient, and simulated annealing.\nAdditionally, we introduce a novel rapid retraining capability, enabling the\nnetwork to be retrained with new data without reprocessing old samples, thus\nenhancing learning efficiency in dynamic environments. Experimental results on\ninitial training of classification and regression tasks validate the efficacy\nof our approach, showcasing significant speedups and comparable performance to\nclassical methods. While experiments on retraining demonstrate a sixty times\nspeed up using adiabatic quantum computing based optimization compared to that\nof the gradient descent based optimizers, with theoretical models allowing this\nspeed up to be even larger! Our findings suggest that with further advancements\nin quantum hardware and algorithm optimization, quantum-optimized machine\nlearning models could have broad applications across various domains, with\ninitial focus on rapid retraining.\n","authors":["William Troy"],"pdf_url":"https://arxiv.org/pdf/2407.16020v3.pdf","comment":"Major updates to the paper for timings and explanations of\n  optimization strategies used. Further optimized the code and updated the\n  figures to reflect the faster timings for v3"},{"id":"http://arxiv.org/abs/2310.12428v2","updated":"2024-08-01T17:38:27Z","published":"2023-10-19T02:42:20Z","title":"Enhanced Local Explainability and Trust Scores with Random Forest\n  Proximities","summary":"  We initiate a novel approach to explain the predictions and out of sample\nperformance of random forest (RF) regression and classification models by\nexploiting the fact that any RF can be mathematically formulated as an adaptive\nweighted K nearest-neighbors model. Specifically, we employ a recent result\nthat, for both regression and classification tasks, any RF prediction can be\nrewritten exactly as a weighted sum of the training targets, where the weights\nare RF proximities between the corresponding pairs of data points. We show that\nthis linearity facilitates a local notion of explainability of RF predictions\nthat generates attributions for any model prediction across observations in the\ntraining set, and thereby complements established feature-based methods like\nSHAP, which generate attributions for a model prediction across input features.\nWe show how this proximity-based approach to explainability can be used in\nconjunction with SHAP to explain not just the model predictions, but also\nout-of-sample performance, in the sense that proximities furnish a novel means\nof assessing when a given model prediction is more or less likely to be\ncorrect. We demonstrate this approach in the modeling of US corporate bond\nprices and returns in both regression and classification cases.\n","authors":["Joshua Rosaler","Dhruv Desai","Bhaskarjit Sarmah","Dimitrios Vamvourellis","Deran Onay","Dhagash Mehta","Stefano Pasquali"],"pdf_url":"https://arxiv.org/pdf/2310.12428v2.pdf","comment":"5 pages, 6 figures"},{"id":"http://arxiv.org/abs/2402.15402v2","updated":"2024-08-01T16:31:00Z","published":"2024-02-23T16:05:51Z","title":"Grasp, See and Place: Efficient Unknown Object Rearrangement with Policy\n  Structure Prior","summary":"  We focus on the task of unknown object rearrangement, where a robot is\nsupposed to re-configure the objects into a desired goal configuration\nspecified by an RGB-D image. Recent works explore unknown object rearrangement\nsystems by incorporating learning-based perception modules. However, they are\nsensitive to perception error, and pay less attention to task-level\nperformance. In this paper, we aim to develop an effective system for unknown\nobject rearrangement amidst perception noise. We theoretically reveal the noisy\nperception impacts grasp and place in a decoupled way, and show such a\ndecoupled structure is valuable to improve task optimality. We propose GSP, a\ndual-loop system with the decoupled structure as prior. For the inner loop, we\nlearn a see policy for self-confident in-hand object matching. For the outer\nloop, we learn a grasp policy aware of object matching and grasp capability\nguided by task-level rewards. We leverage the foundation model CLIP for object\nmatching, policy learning and self-termination. A series of experiments\nindicate that GSP can conduct unknown object rearrangement with higher\ncompletion rates and fewer steps.\n","authors":["Kechun Xu","Zhongxiang Zhou","Jun Wu","Haojian Lu","Rong Xiong","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2402.15402v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00050v2","updated":"2024-08-01T16:14:27Z","published":"2024-03-25T15:11:15Z","title":"Grappa -- A Machine Learned Molecular Mechanics Force Field","summary":"  Simulating large molecular systems over long timescales requires force fields\nthat are both accurate and efficient. In recent years, E(3) equivariant neural\nnetworks have lifted the tension between computational efficiency and accuracy\nof force fields, but they are still several orders of magnitude more expensive\nthan established molecular mechanics (MM) force fields. Here, we propose\nGrappa, a machine learning framework to predict MM parameters from the\nmolecular graph, employing a graph attentional neural network and a transformer\nwith symmetry-preserving positional encoding. The resulting Grappa force field\noutperformstabulated and machine-learned MM force fields in terms of accuracy\nat the same computational efficiency and can be used in existing Molecular\nDynamics (MD) engines like GROMACS and OpenMM. It predicts energies and forces\nof small molecules, peptides, RNA and - showcasing its extensibility to\nuncharted regions of chemical space - radicals at state-of-the-art MM accuracy.\nWe demonstrate Grappa's transferability to macromolecules in MD simulations\nfrom a small fast folding protein up to a whole virus particle. Our force field\nsets the stage for biomolecular simulations closer to chemical accuracy, but\nwith the same computational cost as established protein force fields.\n","authors":["Leif Seute","Eric Hartmann","Jan Stühmer","Frauke Gräter"],"pdf_url":"https://arxiv.org/pdf/2404.00050v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20021v3","updated":"2024-08-01T16:13:45Z","published":"2024-07-29T13:57:40Z","title":"MimiQ: Low-Bit Data-Free Quantization of Vision Transformers with\n  Encouraging Inter-Head Attention Similarity","summary":"  Data-free quantization (DFQ) is a technique that creates a lightweight\nnetwork from its full-precision counterpart without the original training data,\noften through a synthetic dataset. Although several DFQ methods have been\nproposed for vision transformer (ViT) architectures, they fail to achieve\nefficacy in low-bit settings. Examining the existing methods, we identify that\ntheir synthetic data produce misaligned attention maps, while those of the real\nsamples are highly aligned. From the observation of aligned attention, we find\nthat aligning attention maps of synthetic data helps to improve the overall\nperformance of quantized ViTs. Motivated by this finding, we devise MimiQ, a\nnovel DFQ method designed for ViTs that focuses on inter-head attention\nsimilarity. First, we generate synthetic data by aligning head-wise attention\nresponses in relation to spatial query patches. Then, we apply head-wise\nstructural attention distillation to align the attention maps of the quantized\nnetwork to those of the full-precision teacher. The experimental results show\nthat the proposed method significantly outperforms baselines, setting a new\nstate-of-the-art performance for data-free ViT quantization.\n","authors":["Kanghyun Choi","Hye Yoon Lee","Dain Kwon","SunJong Park","Kyuyeun Kim","Noseong Park","Jinho Lee"],"pdf_url":"https://arxiv.org/pdf/2407.20021v3.pdf","comment":"Author Preprint"},{"id":"http://arxiv.org/abs/2307.09067v2","updated":"2024-08-01T16:09:50Z","published":"2023-07-18T08:37:58Z","title":"Evaluate Fine-tuning Strategies for Fetal Head Ultrasound Image\n  Segmentation with U-Net","summary":"  Fetal head segmentation is a crucial step in measuring the fetal head\ncircumference (HC) during gestation, an important biometric in obstetrics for\nmonitoring fetal growth. However, manual biometry generation is time-consuming\nand results in inconsistent accuracy. To address this issue, convolutional\nneural network (CNN) models have been utilized to improve the efficiency of\nmedical biometry. But training a CNN network from scratch is a challenging\ntask, we proposed a Transfer Learning (TL) method. Our approach involves\nfine-tuning (FT) a U-Net network with a lightweight MobileNet as the encoder to\nperform segmentation on a set of fetal head ultrasound (US) images with limited\neffort. This method addresses the challenges associated with training a CNN\nnetwork from scratch. It suggests that our proposed FT strategy yields\nsegmentation performance that is comparable when trained with a reduced number\nof parameters by 85.8%. And our proposed FT strategy outperforms other\nstrategies with smaller trainable parameter sizes below 4.4 million. Thus, we\ncontend that it can serve as a dependable FT approach for reducing the size of\nmodels in medical image analysis. Our key findings highlight the importance of\nthe balance between model performance and size in developing Artificial\nIntelligence (AI) applications by TL methods. Code is available at\nhttps://github.com/13204942/FT_Methods_for_Fetal_Head_Segmentation.\n","authors":["Fangyijie Wang","Guénolé Silvestre","Kathleen M. Curran"],"pdf_url":"https://arxiv.org/pdf/2307.09067v2.pdf","comment":"Irish Machine Vision and Image Processing Conference Proceedings 2023"},{"id":"http://arxiv.org/abs/2311.07315v2","updated":"2024-08-01T16:07:02Z","published":"2023-11-13T13:10:52Z","title":"An introduction to reinforcement learning for neuroscience","summary":"  Reinforcement learning has a rich history in neuroscience, from early work on\ndopamine as a reward prediction error signal for temporal difference learning\n(Schultz et al., 1997) to recent work suggesting that dopamine could implement\na form of 'distributional reinforcement learning' popularized in deep learning\n(Dabney et al., 2020). Throughout this literature, there has been a tight link\nbetween theoretical advances in reinforcement learning and neuroscientific\nexperiments and findings. As a result, the theories describing our experimental\ndata have become increasingly complex and difficult to navigate. In this\nreview, we cover the basic theory underlying classical work in reinforcement\nlearning and build up to an introductory overview of methods in modern deep\nreinforcement learning that have found applications in systems neuroscience. We\nstart with an overview of the reinforcement learning problem and classical\ntemporal difference algorithms, followed by a discussion of 'model-free' and\n'model-based' reinforcement learning together with methods such as DYNA and\nsuccessor representations that fall in between these two extremes. Throughout\nthese sections, we highlight the close parallels between such machine learning\nmethods and related work in both experimental and theoretical neuroscience. We\nthen provide an introduction to deep reinforcement learning with examples of\nhow these methods have been used to model different learning phenomena in\nsystems neuroscience, such as meta-reinforcement learning (Wang et al., 2018)\nand distributional reinforcement learning (Dabney et al., 2020). Code that\nimplements the methods discussed in this work and generates the figures is also\nprovided.\n","authors":["Kristopher T. Jensen"],"pdf_url":"https://arxiv.org/pdf/2311.07315v2.pdf","comment":"Code available at:\n  https://colab.research.google.com/drive/1ZC4lR8kTO48yySDZtcOEdMKd3NqY_ly1?usp=sharing"},{"id":"http://arxiv.org/abs/2403.05385v5","updated":"2024-08-01T16:02:06Z","published":"2024-03-08T15:30:58Z","title":"Switching the Loss Reduces the Cost in Batch (Offline) Reinforcement\n  Learning","summary":"  We propose training fitted Q-iteration with log-loss (FQI-log) for batch\nreinforcement learning (RL). We show that the number of samples needed to learn\na near-optimal policy with FQI-log scales with the accumulated cost of the\noptimal policy, which is zero in problems where acting optimally achieves the\ngoal and incurs no cost. In doing so, we provide a general framework for\nproving small-cost bounds, i.e. bounds that scale with the optimal achievable\ncost, in batch RL. Moreover, we empirically verify that FQI-log uses fewer\nsamples than FQI trained with squared loss on problems where the optimal policy\nreliably achieves the goal.\n","authors":["Alex Ayoub","Kaiwen Wang","Vincent Liu","Samuel Robertson","James McInerney","Dawen Liang","Nathan Kallus","Csaba Szepesvári"],"pdf_url":"https://arxiv.org/pdf/2403.05385v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.13312v2","updated":"2024-08-01T15:54:29Z","published":"2023-04-26T06:33:31Z","title":"Technical Note: Defining and Quantifying AND-OR Interactions for\n  Faithful and Concise Explanation of DNNs","summary":"  In this technical note, we aim to explain a deep neural network (DNN) by\nquantifying the encoded interactions between input variables, which reflects\nthe DNN's inference logic. Specifically, we first rethink the definition of\ninteractions, and then formally define faithfulness and conciseness for\ninteraction-based explanation. To this end, we propose two kinds of\ninteractions, i.e., the AND interaction and the OR interaction. For\nfaithfulness, we prove the uniqueness of the AND (OR) interaction in\nquantifying the effect of the AND (OR) relationship between input variables.\nBesides, based on AND-OR interactions, we design techniques to boost the\nconciseness of the explanation, while not hurting the faithfulness. In this\nway, the inference logic of a DNN can be faithfully and concisely explained by\na set of symbolic concepts.\n","authors":["Mingjie Li","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.13312v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2111.06206"},{"id":"http://arxiv.org/abs/2304.13119v3","updated":"2024-08-01T15:52:32Z","published":"2023-04-25T19:48:54Z","title":"Application of Transformers for Nonlinear Channel Compensation in\n  Optical Systems","summary":"  In this paper, we introduce a new nonlinear optical channel equalizer based\non Transformers. By leveraging parallel computation and attending directly to\nthe memory across a sequence of symbols, we show that Transformers can be used\neffectively for nonlinear compensation (NLC) in coherent long-haul transmission\nsystems. For this application, we present an implementation of the encoder part\nof the Transformer and analyze its performance over a wide range of different\nhyper-parameters. It is shown that by proper embeddings and processing blocks\nof symbols at each iteration and also carefully selecting subsets of the\nencoder's output to be processed together, an efficient nonlinear equalization\ncan be achieved for different complexity constraints. To reduce the\ncomputational complexity of the attention mechanism, we further propose the use\nof a physic-informed mask inspired by nonlinear perturbation theory. We also\ncompare the Transformer-NLC with digital back-propagation (DBP) under different\ntransmission scenarios in order to demonstrate the flexibility and\ngeneralizability of the proposed data-driven solution.\n","authors":["Behnam Behinaein Hamgini","Hossein Najafi","Ali Bakhshali","Zhuhong Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.13119v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11911v2","updated":"2024-08-01T15:44:19Z","published":"2024-06-16T16:46:55Z","title":"A Notion of Complexity for Theory of Mind via Discrete World Models","summary":"  Theory of Mind (ToM) can be used to assess the capabilities of Large Language\nModels (LLMs) in complex scenarios where social reasoning is required. While\nthe research community has proposed many ToM benchmarks, their hardness varies\ngreatly, and their complexity is not well defined. This work proposes a\nframework to measure the complexity of ToM tasks. We quantify a problem's\ncomplexity as the number of states necessary to solve it correctly. Our\ncomplexity measure also accounts for spurious states of a ToM problem designed\nto make it apparently harder. We use our method to assess the complexity of\nfive widely adopted ToM benchmarks. On top of this framework, we design a\nprompting technique that augments the information available to a model with a\ndescription of how the environment changes with the agents' interactions. We\nname this technique Discrete World Models (DWM) and show how it elicits\nsuperior performance on ToM tasks.\n","authors":["X. Angelo Huang","Emanuele La Malfa","Samuele Marro","Andrea Asperti","Anthony Cohn","Michael Wooldridge"],"pdf_url":"https://arxiv.org/pdf/2406.11911v2.pdf","comment":"https://flecart.github.io/complexity-tom-dwm"},{"id":"http://arxiv.org/abs/2311.00048v2","updated":"2024-08-01T15:37:52Z","published":"2023-10-31T18:01:41Z","title":"SC-MIL: Sparsely Coded Multiple Instance Learning for Whole Slide Image\n  Classification","summary":"  Multiple Instance Learning (MIL) has been widely used in weakly supervised\nwhole slide image (WSI) classification. Typical MIL methods include a feature\nembedding part, which embeds the instances into features via a pre-trained\nfeature extractor, and an MIL aggregator that combines instance embeddings into\npredictions. Most efforts have typically focused on improving these parts. This\ninvolves refining the feature embeddings through self-supervised pre-training\nas well as modeling the correlations between instances separately.\n  In this paper, we proposed a sparsely coding MIL (SC-MIL) method that\naddresses those two aspects at the same time by leveraging sparse dictionary\nlearning. The sparse dictionary learning captures the similarities of instances\nby expressing them as sparse linear combinations of atoms in an over-complete\ndictionary. In addition, imposing sparsity improves instance feature embeddings\nby suppressing irrelevant instances while retaining the most relevant ones. To\nmake the conventional sparse coding algorithm compatible with deep learning, we\nunrolled it into a sparsely coded module leveraging deep unrolling. The\nproposed SC module can be incorporated into any existing MIL framework in a\nplug-and-play manner with an acceptable computational cost. The experimental\nresults on multiple datasets demonstrated that the proposed SC module could\nsubstantially boost the performance of state-of-the-art MIL methods. The codes\nare available at\n\\href{https://github.com/sotiraslab/SCMIL.git}{https://github.com/sotiraslab/SCMIL.git}.\n","authors":["Peijie Qiu","Pan Xiao","Wenhui Zhu","Yalin Wang","Aristeidis Sotiras"],"pdf_url":"https://arxiv.org/pdf/2311.00048v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08448v2","updated":"2024-08-01T15:16:26Z","published":"2024-03-13T12:03:27Z","title":"Actor-Critic Physics-informed Neural Lyapunov Control","summary":"  Designing control policies for stabilization tasks with provable guarantees\nis a long-standing problem in nonlinear control. A crucial performance metric\nis the size of the resulting region of attraction, which essentially serves as\na robustness \"margin\" of the closed-loop system against uncertainties. In this\npaper, we propose a new method to train a stabilizing neural network controller\nalong with its corresponding Lyapunov certificate, aiming to maximize the\nresulting region of attraction while respecting the actuation constraints.\nCrucial to our approach is the use of Zubov's Partial Differential Equation\n(PDE), which precisely characterizes the true region of attraction of a given\ncontrol policy. Our framework follows an actor-critic pattern where we\nalternate between improving the control policy (actor) and learning a Zubov\nfunction (critic). Finally, we compute the largest certifiable region of\nattraction by invoking an SMT solver after the training procedure. Our\nnumerical experiments on several design problems show consistent and\nsignificant improvements in the size of the resulting region of attraction.\n","authors":["Jiarui Wang","Mahyar Fazlyab"],"pdf_url":"https://arxiv.org/pdf/2403.08448v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14814v3","updated":"2024-08-01T15:15:34Z","published":"2024-03-21T19:59:52Z","title":"The opportunities and risks of large language models in mental health","summary":"  Global rates of mental health concerns are rising, and there is increasing\nrealization that existing models of mental health care will not adequately\nexpand to meet the demand. With the emergence of large language models (LLMs)\nhas come great optimism regarding their promise to create novel, large-scale\nsolutions to support mental health. Despite their nascence, LLMs have already\nbeen applied to mental health related tasks. In this paper, we summarize the\nextant literature on efforts to use LLMs to provide mental health education,\nassessment, and intervention and highlight key opportunities for positive\nimpact in each area. We then highlight risks associated with LLMs' application\nto mental health and encourage the adoption of strategies to mitigate these\nrisks. The urgent need for mental health support must be balanced with\nresponsible development, testing, and deployment of mental health LLMs. It is\nespecially critical to ensure that mental health LLMs are fine-tuned for mental\nhealth, enhance mental health equity, and adhere to ethical standards and that\npeople, including those with lived experience with mental health concerns, are\ninvolved in all stages from development through deployment. Prioritizing these\nefforts will minimize potential harms to mental health and maximize the\nlikelihood that LLMs will positively impact mental health globally.\n","authors":["Hannah R. Lawrence","Renee A. Schneider","Susan B. Rubin","Maja J. Mataric","Daniel J. McDuff","Megan Jones Bell"],"pdf_url":"https://arxiv.org/pdf/2403.14814v3.pdf","comment":"15 pages, 2 tables, 4 figures"},{"id":"http://arxiv.org/abs/2404.10324v2","updated":"2024-08-01T15:10:45Z","published":"2024-04-16T07:08:04Z","title":"Graph neural network-based surrogate modelling for real-time hydraulic\n  prediction of urban drainage networks","summary":"  Physics-based models are computationally time-consuming and infeasible for\nreal-time scenarios of urban drainage networks, and a surrogate model is needed\nto accelerate the online predictive modelling. Fully-connected neural networks\n(NNs) are potential surrogate models, but may suffer from low interpretability\nand efficiency in fitting complex targets. Owing to the state-of-the-art\nmodelling power of graph neural networks (GNNs) and their match with urban\ndrainage networks in the graph structure, this work proposes a GNN-based\nsurrogate of the flow routing model for the hydraulic prediction problem of\ndrainage networks, which regards recent hydraulic states as initial conditions,\nand future runoff and control policy as boundary conditions. To incorporate\nhydraulic constraints and physical relationships into drainage modelling,\nphysics-guided mechanisms are designed on top of the surrogate model to\nrestrict the prediction variables with flow balance and flooding occurrence\nconstraints. According to case results in a stormwater network, the GNN-based\nmodel is more cost-effective with better hydraulic prediction accuracy than the\nNN-based model after equal training epochs, and the designed mechanisms further\nlimit prediction errors with interpretable domain knowledge. As the model\nstructure adheres to the flow routing mechanisms and hydraulic constraints in\nurban drainage networks, it provides an interpretable and effective solution\nfor data-driven surrogate modelling. Simultaneously, the surrogate model\naccelerates the predictive modelling of urban drainage networks for real-time\nuse compared with the physics-based model.\n","authors":["Zhiyu Zhang","Chenkaixiang Lu","Wenchong Tian","Zhenliang Liao","Zhiguo Yuan"],"pdf_url":"https://arxiv.org/pdf/2404.10324v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.04597v4","updated":"2024-08-01T14:38:06Z","published":"2021-11-08T16:09:39Z","title":"Neyman-Pearson Multi-class Classification via Cost-sensitive Learning","summary":"  Most existing classification methods aim to minimize the overall\nmisclassification error rate. However, in applications such as loan default\nprediction, different types of errors can have varying consequences. To address\nthis asymmetry issue, two popular paradigms have been developed: the\nNeyman-Pearson (NP) paradigm and the cost-sensitive (CS) paradigm. Previous\nstudies on the NP paradigm have primarily focused on the binary case, while the\nmulti-class NP problem poses a greater challenge due to its unknown\nfeasibility. In this work, we tackle the multi-class NP problem by establishing\na connection with the CS problem via strong duality and propose two algorithms.\nWe extend the concept of NP oracle inequalities, crucial in binary\nclassifications, to NP oracle properties in the multi-class context. Our\nalgorithms satisfy these NP oracle properties under certain conditions.\nFurthermore, we develop practical algorithms to assess the feasibility and\nstrong duality in multi-class NP problems, which can offer practitioners the\nlandscape of a multi-class NP problem with various target error levels.\nSimulations and real data studies validate the effectiveness of our algorithms.\nTo our knowledge, this is the first study to address the multi-class NP problem\nwith theoretical guarantees. The proposed algorithms have been implemented in\nthe R package \\texttt{npcs}, which is available on CRAN.\n","authors":["Ye Tian","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2111.04597v4.pdf","comment":"114 pages, 18 figures"},{"id":"http://arxiv.org/abs/2407.04822v3","updated":"2024-08-01T14:32:05Z","published":"2024-07-05T19:18:33Z","title":"YourMT3+: Multi-instrument Music Transcription with Enhanced Transformer\n  Architectures and Cross-dataset Stem Augmentation","summary":"  Multi-instrument music transcription aims to convert polyphonic music\nrecordings into musical scores assigned to each instrument. This task is\nchallenging for modeling as it requires simultaneously identifying multiple\ninstruments and transcribing their pitch and precise timing, and the lack of\nfully annotated data adds to the training difficulties. This paper introduces\nYourMT3+, a suite of models for enhanced multi-instrument music transcription\nbased on the recent language token decoding approach of MT3. We enhance its\nencoder by adopting a hierarchical attention transformer in the time-frequency\ndomain and integrating a mixture of experts. To address data limitations, we\nintroduce a new multi-channel decoding method for training with incomplete\nannotations and propose intra- and cross-stem augmentation for dataset mixing.\nOur experiments demonstrate direct vocal transcription capabilities,\neliminating the need for voice separation pre-processors. Benchmarks across ten\npublic datasets show our models' competitiveness with, or superiority to,\nexisting transcription models. Further testing on pop music recordings\nhighlights the limitations of current models. Fully reproducible code and\ndatasets are available with demos at \\url{https://github.com/mimbres/YourMT3}.\n","authors":["Sungkyun Chang","Emmanouil Benetos","Holger Kirchhoff","Simon Dixon"],"pdf_url":"https://arxiv.org/pdf/2407.04822v3.pdf","comment":"Accepted at IEEE International Workshop on Machine Learning for\n  Signal Processing (MLSP) 2024, London"},{"id":"http://arxiv.org/abs/2407.04724v2","updated":"2024-08-01T14:18:32Z","published":"2024-06-26T07:32:04Z","title":"A Likelihood-Based Generative Approach for Spatially Consistent\n  Precipitation Downscaling","summary":"  Deep learning has emerged as a promising tool for precipitation downscaling.\nHowever, current models rely on likelihood-based loss functions to properly\nmodel the precipitation distribution, leading to spatially inconsistent\nprojections when sampling. This work explores a novel approach by fusing the\nstrengths of likelihood-based and adversarial losses used in generative models.\nAs a result, we propose a likelihood-based generative approach for\nprecipitation downscaling, leveraging the benefits of both methods.\n","authors":["Jose González-Abad"],"pdf_url":"https://arxiv.org/pdf/2407.04724v2.pdf","comment":"Accepted at ICML 2024 Machine Learning for Earth System Modeling\n  workshop"},{"id":"http://arxiv.org/abs/2309.07716v2","updated":"2024-08-01T14:16:23Z","published":"2023-09-14T13:48:16Z","title":"Understanding Vector-Valued Neural Networks and Their Relationship with\n  Real and Hypercomplex-Valued Neural Networks","summary":"  Despite the many successful applications of deep learning models for\nmultidimensional signal and image processing, most traditional neural networks\nprocess data represented by (multidimensional) arrays of real numbers. The\nintercorrelation between feature channels is usually expected to be learned\nfrom the training data, requiring numerous parameters and careful training. In\ncontrast, vector-valued neural networks are conceived to process arrays of\nvectors and naturally consider the intercorrelation between feature channels.\nConsequently, they usually have fewer parameters and often undergo more robust\ntraining than traditional neural networks. This paper aims to present a broad\nframework for vector-valued neural networks, referred to as V-nets. In this\ncontext, hypercomplex-valued neural networks are regarded as vector-valued\nmodels with additional algebraic properties. Furthermore, this paper explains\nthe relationship between vector-valued and traditional neural networks.\nPrecisely, a vector-valued neural network can be obtained by placing\nrestrictions on a real-valued model to consider the intercorrelation between\nfeature channels. Finally, we show how V-nets, including hypercomplex-valued\nneural networks, can be implemented in current deep-learning libraries as\nreal-valued networks.\n","authors":["Marcos Eduardo Valle"],"pdf_url":"https://arxiv.org/pdf/2309.07716v2.pdf","comment":"Accepted for publication in IEEE Signal Processing Magazine"},{"id":"http://arxiv.org/abs/2407.11054v2","updated":"2024-08-01T14:10:22Z","published":"2024-07-09T09:25:27Z","title":"Generative AI for Health Technology Assessment: Opportunities,\n  Challenges, and Policy Considerations","summary":"  This review introduces the transformative potential of generative Artificial\nIntelligence (AI) and foundation models, including large language models\n(LLMs), for health technology assessment (HTA). We explore their applications\nin four critical areas, evidence synthesis, evidence generation, clinical\ntrials and economic modeling: (1) Evidence synthesis: Generative AI has the\npotential to assist in automating literature reviews and meta-analyses by\nproposing search terms, screening abstracts, and extracting data with notable\naccuracy; (2) Evidence generation: These models can potentially facilitate\nautomating the process and analyze the increasingly available large collections\nof real-world data (RWD), including unstructured clinical notes and imaging,\nenhancing the speed and quality of real-world evidence (RWE) generation; (3)\nClinical trials: Generative AI can be used to optimize trial design, improve\npatient matching, and manage trial data more efficiently; and (4) Economic\nmodeling: Generative AI can also aid in the development of health economic\nmodels, from conceptualization to validation, thus streamlining the overall HTA\nprocess. Despite their promise, these technologies, while rapidly improving,\nare still nascent and continued careful evaluation in their applications to HTA\nis required. To ensure their responsible use and implementation, both\ndevelopers and users of research incorporating these tools, should familiarize\nthemselves with their current limitations, including the issues related to\nscientific validity, risk of bias, and consider equity and ethical\nimplications. We also surveyed the current policy landscape and provide\nsuggestions for HTA agencies on responsibly integrating generative AI into\ntheir workflows, emphasizing the importance of human oversight and the\nfast-evolving nature of these tools.\n","authors":["Rachael Fleurence","Jiang Bian","Xiaoyan Wang","Hua Xu","Dalia Dawoud","Mitch Higashi","Jagpreet Chhatwal"],"pdf_url":"https://arxiv.org/pdf/2407.11054v2.pdf","comment":"24 pages, 1 figure, 1 table, 2 boxes, 103 references"},{"id":"http://arxiv.org/abs/2407.20678v2","updated":"2024-08-01T14:09:12Z","published":"2024-07-30T09:20:15Z","title":"The Susceptibility of Example-Based Explainability Methods to Class\n  Outliers","summary":"  This study explores the impact of class outliers on the effectiveness of\nexample-based explainability methods for black-box machine learning models. We\nreformulate existing explainability evaluation metrics, such as correctness and\nrelevance, specifically for example-based methods, and introduce a new metric,\ndistinguishability. Using these metrics, we highlight the shortcomings of\ncurrent example-based explainability methods, including those who attempt to\nsuppress class outliers. We conduct experiments on two datasets, a text\nclassification dataset and an image classification dataset, and evaluate the\nperformance of four state-of-the-art explainability methods. Our findings\nunderscore the need for robust techniques to tackle the challenges posed by\nclass outliers.\n","authors":["Ikhtiyor Nematov","Dimitris Sacharidis","Tomer Sagi","Katja Hose"],"pdf_url":"https://arxiv.org/pdf/2407.20678v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2407.16010"},{"id":"http://arxiv.org/abs/2407.03194v3","updated":"2024-08-01T14:08:53Z","published":"2024-07-03T15:26:02Z","title":"Prediction Instability in Machine Learning Ensembles","summary":"  In machine learning ensembles predictions from multiple models are\naggregated. Despite widespread use and strong performance of ensembles in\napplied problems little is known about the mathematical properties of\naggregating models and associated consequences for safe, explainable use of\nsuch models. In this paper we prove a theorem that shows that any ensemble will\nexhibit at least one of the following forms of prediction instability. It will\neither ignore agreement among all underlying models, change its mind when none\nof the underlying models have done so, or be manipulable through inclusion or\nexclusion of options it would never actually predict. As a consequence,\nensemble aggregation procedures will always need to balance the benefits of\ninformation use against the risk of these prediction instabilities. This\nanalysis also sheds light on what specific forms of prediction instability to\nexpect from particular ensemble algorithms; for example popular tree ensembles\nlike random forest, or xgboost will violate basic, intuitive fairness\nproperties. Finally, we show that this can be ameliorated by using consistent\nmodels in asymptotic conditions.\n","authors":["Jeremy Kedziora"],"pdf_url":"https://arxiv.org/pdf/2407.03194v3.pdf","comment":"15 pages, uses a modified version of ICML2024.sty"},{"id":"http://arxiv.org/abs/2312.02111v3","updated":"2024-08-01T14:01:56Z","published":"2023-12-04T18:43:45Z","title":"TriDeNT: Triple Deep Network Training for Privileged Knowledge\n  Distillation in Histopathology","summary":"  Computational pathology models rarely utilise data that will not be available\nfor inference. This means most models cannot learn from highly informative data\nsuch as additional immunohistochemical (IHC) stains and spatial\ntranscriptomics. We present TriDeNT, a novel self-supervised method for\nutilising privileged data that is not available during inference to improve\nperformance. We demonstrate the efficacy of this method for a range of\ndifferent paired data including immunohistochemistry, spatial transcriptomics\nand expert nuclei annotations. In all settings, TriDeNT outperforms other\nstate-of-the-art methods in downstream tasks, with observed improvements of up\nto 101%. Furthermore, we provide qualitative and quantitative measurements of\nthe features learned by these models and how they differ from baselines.\nTriDeNT offers a novel method to distil knowledge from scarce or costly data\nduring training, to create significantly better models for routine inputs.\n","authors":["Lucas Farndale","Robert Insall","Ke Yuan"],"pdf_url":"https://arxiv.org/pdf/2312.02111v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.10821v2","updated":"2024-08-01T13:53:48Z","published":"2023-08-21T16:13:23Z","title":"Optimized Deep Learning Models for Malware Detection under Concept Drift","summary":"  Despite the promising results of machine learning models in malicious files\ndetection, they face the problem of concept drift due to their constant\nevolution. This leads to declining performance over time, as the data\ndistribution of the new files differs from the training one, requiring frequent\nmodel update. In this work, we propose a model-agnostic protocol to improve a\nbaseline neural network against drift. We show the importance of feature\nreduction and training with the most recent validation set possible, and\npropose a loss function named Drift-Resilient Binary Cross-Entropy, an\nimprovement to the classical Binary Cross-Entropy more effective against drift.\nWe train our model on the EMBER dataset, published in2018, and evaluate it on a\ndataset of recent malicious files, collected between 2020 and 2023. Our\nimproved model shows promising results, detecting 15.2% more malware than a\nbaseline model.\n","authors":["William Maillet","Benjamin Marais"],"pdf_url":"https://arxiv.org/pdf/2308.10821v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.12715v2","updated":"2024-08-01T13:53:43Z","published":"2022-09-26T14:11:05Z","title":"Enhancing convolutional neural network generalizability via low-rank\n  weight approximation","summary":"  Noise is ubiquitous during image acquisition. Sufficient denoising is often\nan important first step for image processing. In recent decades, deep neural\nnetworks (DNNs) have been widely used for image denoising. Most DNN-based image\ndenoising methods require a large-scale dataset or focus on supervised\nsettings, in which single/pairs of clean images or a set of noisy images are\nrequired. This poses a significant burden on the image acquisition process.\nMoreover, denoisers trained on datasets of limited scale may incur\nover-fitting. To mitigate these issues, we introduce a new self-supervised\nframework for image denoising based on the Tucker low-rank tensor\napproximation. With the proposed design, we are able to characterize our\ndenoiser with fewer parameters and train it based on a single image, which\nconsiderably improves the model's generalizability and reduces the cost of data\nacquisition. Extensive experiments on both synthetic and real-world noisy\nimages have been conducted. Empirical results show that our proposed method\noutperforms existing non-learning-based methods (e.g., low-pass filter,\nnon-local mean), single-image unsupervised denoisers (e.g., DIP, NN+BM3D)\nevaluated on both in-sample and out-sample datasets. The proposed method even\nachieves comparable performances with some supervised methods (e.g., DnCNN).\n","authors":["Chenyin Gao","Shu Yang","Anru R. Zhang"],"pdf_url":"https://arxiv.org/pdf/2209.12715v2.pdf","comment":"accepted by IET Image Processing"},{"id":"http://arxiv.org/abs/2401.14361v2","updated":"2024-08-01T13:21:24Z","published":"2024-01-25T18:07:50Z","title":"MoE-Infinity: Offloading-Efficient MoE Model Serving","summary":"  This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity\n","authors":["Leyang Xue","Yao Fu","Zhan Lu","Luo Mai","Mahesh Marina"],"pdf_url":"https://arxiv.org/pdf/2401.14361v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11749v3","updated":"2024-08-01T12:37:13Z","published":"2023-11-20T13:21:10Z","title":"A causal intervention framework for synthesizing mobility data and\n  evaluating predictive neural networks","summary":"  Deep neural networks are increasingly utilized in mobility prediction tasks,\nyet their intricate internal workings pose challenges for interpretability,\nespecially in comprehending how various aspects of mobility behavior affect\npredictions. This study introduces a causal intervention framework to assess\nthe impact of mobility-related factors on neural networks designed for next\nlocation prediction -- a task focusing on predicting the immediate next\nlocation of an individual. To achieve this, we employ individual mobility\nmodels to synthesize location visit sequences and control behavior dynamics by\nintervening in their data generation process. We evaluate the interventional\nlocation sequences using mobility metrics and input them into well-trained\nnetworks to analyze performance variations. The results demonstrate the\neffectiveness in producing location sequences with distinct mobility behaviors,\nthereby facilitating the simulation of diverse yet realistic spatial and\ntemporal changes. These changes result in performance fluctuations in next\nlocation prediction networks, revealing impacts of critical mobility behavior\nfactors, including sequential patterns in location transitions, proclivity for\nexploring new locations, and preferences in location choices at population and\nindividual levels. The gained insights hold value for the real-world\napplication of mobility prediction networks, and the framework is expected to\npromote the use of causal inference to enhance the interpretability and\nrobustness of neural networks in mobility applications.\n","authors":["Ye Hong","Yanan Xin","Simon Dirmeier","Fernando Perez-Cruz","Martin Raubal"],"pdf_url":"https://arxiv.org/pdf/2311.11749v3.pdf","comment":"34 pages, 8 figures"},{"id":"http://arxiv.org/abs/2307.13124v3","updated":"2024-08-01T11:51:44Z","published":"2023-07-24T20:45:39Z","title":"Conformal prediction for frequency-severity modeling","summary":"  We present a model-agnostic framework for the construction of prediction\nintervals of insurance claims, with finite sample statistical guarantees,\nextending the technique of split conformal prediction to the domain of\ntwo-stage frequency-severity modeling. The framework effectiveness is showcased\nwith simulated and real datasets using classical parametric models and\ncontemporary machine learning methods. When the underlying severity model is a\nrandom forest, we extend the two-stage split conformal prediction algorithm,\nshowing how the out-of-bag mechanism can be leveraged to eliminate the need for\na calibration set in the conformal procedure.\n","authors":["Helton Graziadei","Paulo C. Marques F.","Eduardo F. L. de Melo","Rodrigo S. Targino"],"pdf_url":"https://arxiv.org/pdf/2307.13124v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.07484v6","updated":"2024-08-01T11:49:04Z","published":"2022-11-14T16:08:44Z","title":"Contextual Bandits with Packing and Covering Constraints: A Modular\n  Lagrangian Approach via Regression","summary":"  We consider contextual bandits with linear constraints (CBwLC), a variant of\ncontextual bandits in which the algorithm consumes multiple resources subject\nto linear constraints on total consumption. This problem generalizes contextual\nbandits with knapsacks (CBwK), allowing for packing and covering constraints,\nas well as positive and negative resource consumption. We provide the first\nalgorithm for CBwLC (or CBwK) that is based on regression oracles. The\nalgorithm is simple, computationally efficient, and statistically optimal under\nmild assumptions. Further, we provide the first vanishing-regret guarantees for\nCBwLC (or CBwK) that extend beyond the stochastic environment. We side-step\nstrong impossibility results from prior work by identifying a weaker (and,\narguably, fairer) benchmark to compare against. Our algorithm builds on\nLagrangeBwK (Immorlica et al., FOCS 2019), a Lagrangian-based technique for\nCBwK, and SquareCB (Foster and Rakhlin, ICML 2020), a regression-based\ntechnique for contextual bandits. Our analysis leverages the inherent\nmodularity of both techniques.\n","authors":["Aleksandrs Slivkins","Xingyu Zhou","Karthik Abinav Sankararaman","Dylan J. Foster"],"pdf_url":"https://arxiv.org/pdf/2211.07484v6.pdf","comment":"A preliminary version of this paper, authored by A. Slivkins, K.A.\n  Sankararaman and D.J. Foster, has been published at COLT 2023. The present\n  version features an important improvement, due to Xingyu Zhou. Specifically,\n  the $\\sqrt{T}$-regret result in Theorem 3.6(a) holds under a much weaker\n  assumption, and is now positioned as the main guarantee"},{"id":"http://arxiv.org/abs/2407.16888v2","updated":"2024-08-01T11:46:26Z","published":"2024-06-08T12:46:12Z","title":"A Nested Model for AI Design and Validation","summary":"  The growing AI field faces trust, transparency, fairness, and discrimination\nchallenges. Despite the need for new regulations, there is a mismatch between\nregulatory science and AI, preventing a consistent framework. A five-layer\nnested model for AI design and validation aims to address these issues and\nstreamline AI application design and validation, improving fairness, trust, and\nAI adoption. This model aligns with regulations, addresses AI practitioner's\ndaily challenges, and offers prescriptive guidance for determining appropriate\nevaluation approaches by identifying unique validity threats. We have three\nrecommendations motivated by this model: authors should distinguish between\nlayers when claiming contributions to clarify the specific areas in which the\ncontribution is made and to avoid confusion, authors should explicitly state\nupstream assumptions to ensure that the context and limitations of their AI\nsystem are clearly understood, AI venues should promote thorough testing and\nvalidation of AI systems and their compliance with regulatory requirements.\n","authors":["Akshat Dubey","Zewen Yang","Georges Hattab"],"pdf_url":"https://arxiv.org/pdf/2407.16888v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12925v2","updated":"2024-08-01T10:09:15Z","published":"2024-06-14T13:54:29Z","title":"GLiNER multi-task: Generalist Lightweight Model for Various Information\n  Extraction Tasks","summary":"  Information extraction tasks require both accurate, efficient, and\ngeneralisable models. Classical supervised deep learning approaches can achieve\nthe required performance, but they need large datasets and are limited in their\nability to adapt to different tasks. On the other hand, large language models\n(LLMs) demonstrate good generalization, meaning that they can adapt to many\ndifferent tasks based on user requests. However, LLMs are computationally\nexpensive and tend to fail to generate structured outputs. In this article, we\nwill introduce a new kind of GLiNER model that can be used for various\ninformation extraction tasks while being a small encoder model. Our model\nachieved SoTA performance on zero-shot NER benchmarks and leading performance\non question-answering, summarization and relation extraction tasks.\nAdditionally, in this article, we will cover experimental results on\nself-learning approaches for named entity recognition using GLiNER models.\n","authors":["Ihor Stepanov","Mykhailo Shtopko"],"pdf_url":"https://arxiv.org/pdf/2406.12925v2.pdf","comment":"11 pages, 1 figure, 6 tables"},{"id":"http://arxiv.org/abs/2402.16105v4","updated":"2024-08-01T09:53:03Z","published":"2024-02-25T15:08:37Z","title":"Informed Meta-Learning","summary":"  In noisy and low-data regimes prevalent in real-world applications, a key\nchallenge of machine learning lies in effectively incorporating inductive\nbiases that promote data efficiency and robustness. Meta-learning and informed\nML stand out as two approaches for incorporating prior knowledge into ML\npipelines. While the former relies on a purely data-driven source of priors,\nthe latter is guided by prior domain knowledge. In this paper, we formalise a\nhybrid paradigm, informed meta-learning, facilitating the incorporation of\npriors from unstructured knowledge representations, such as natural language;\nthus, unlocking complementarity in cross-task knowledge sharing of humans and\nmachines. We establish the foundational components of informed meta-learning\nand present a concrete instantiation of this framework--the Informed Neural\nProcess. Through a series of experiments, we demonstrate the potential benefits\nof informed meta-learning in improving data efficiency, robustness to\nobservational noise and task distribution shifts.\n","authors":["Katarzyna Kobalczyk","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2402.16105v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18615v2","updated":"2024-08-01T09:43:57Z","published":"2023-10-28T06:46:03Z","title":"Temporally Disentangled Representation Learning under Unknown\n  Nonstationarity","summary":"  In unsupervised causal representation learning for sequential data with\ntime-delayed latent causal influences, strong identifiability results for the\ndisentanglement of causally-related latent variables have been established in\nstationary settings by leveraging temporal structure. However, in nonstationary\nsetting, existing work only partially addressed the problem by either utilizing\nobserved auxiliary variables (e.g., class labels and/or domain indexes) as side\ninformation or assuming simplified latent causal dynamics. Both constrain the\nmethod to a limited range of scenarios. In this study, we further explored the\nMarkov Assumption under time-delayed causally related process in nonstationary\nsetting and showed that under mild conditions, the independent latent\ncomponents can be recovered from their nonlinear mixture up to a permutation\nand a component-wise transformation, without the observation of auxiliary\nvariables. We then introduce NCTRL, a principled estimation framework, to\nreconstruct time-delayed latent causal variables and identify their relations\nfrom measured sequential data only. Empirical evaluations demonstrated the\nreliable identification of time-delayed latent causal influences, with our\nmethodology substantially outperforming existing baselines that fail to exploit\nthe nonstationarity adequately and then, consequently, cannot distinguish\ndistribution shifts.\n","authors":["Xiangchen Song","Weiran Yao","Yewen Fan","Xinshuai Dong","Guangyi Chen","Juan Carlos Niebles","Eric Xing","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.18615v2.pdf","comment":"NeurIPS 2023. arXiv admin note: text overlap with arXiv:2210.13647"},{"id":"http://arxiv.org/abs/2405.05638v3","updated":"2024-08-01T09:25:20Z","published":"2024-05-09T09:27:18Z","title":"A Correlation-induced Finite Difference Estimator","summary":"  Finite difference (FD) approximation is a classic approach to stochastic\ngradient estimation when only noisy function realizations are available. In\nthis paper, we first provide a sample-driven method via the bootstrap technique\nto estimate the optimal perturbation, and then propose an efficient FD\nestimator based on correlated samples at the estimated optimal perturbation.\nFurthermore, theoretical analyses of both the perturbation estimator and the FD\nestimator reveal that, {\\it surprisingly}, the correlation enables the proposed\nFD estimator to achieve a reduction in variance and, in some cases, a decrease\nin bias compared to the traditional optimal FD estimator. Numerical results\nconfirm the efficiency of our estimators and align well with the theory\npresented, especially in scenarios with small sample sizes. Finally, we apply\nthe estimator to solve derivative-free optimization (DFO) problems, and\nnumerical studies show that DFO problems with 100 dimensions can be effectively\nsolved.\n","authors":["Guo Liang","Guangwu Liu","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.05638v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.05012v2","updated":"2024-08-01T09:18:17Z","published":"2024-01-10T09:00:03Z","title":"HiMTM: Hierarchical Multi-Scale Masked Time Series Modeling with\n  Self-Distillation for Long-Term Forecasting","summary":"  Time series forecasting is a critical and challenging task in practical\napplication. Recent advancements in pre-trained foundation models for time\nseries forecasting have gained significant interest. However, current methods\noften overlook the multi-scale nature of time series, which is essential for\naccurate forecasting. To address this, we propose HiMTM, a hierarchical\nmulti-scale masked time series modeling with self-distillation for long-term\nforecasting. HiMTM integrates four key components: (1) hierarchical multi-scale\ntransformer (HMT) to capture temporal information at different scales; (2)\ndecoupled encoder-decoder (DED) that directs the encoder towards feature\nextraction while the decoder focuses on pretext tasks; (3) hierarchical\nself-distillation (HSD) for multi-stage feature-level supervision signals\nduring pre-training; and (4) cross-scale attention fine-tuning (CSA-FT) to\ncapture dependencies between different scales for downstream tasks. These\ncomponents collectively enhance multi-scale feature extraction in masked time\nseries modeling, improving forecasting accuracy. Extensive experiments on seven\nmainstream datasets show that HiMTM surpasses state-of-the-art self-supervised\nand end-to-end learning methods by a considerable margin of 3.16-68.54\\%.\nAdditionally, HiMTM outperforms the latest robust self-supervised learning\nmethod, PatchTST, in cross-domain forecasting by a significant margin of 2.3\\%.\nThe effectiveness of HiMTM is further demonstrated through its application in\nnatural gas demand forecasting.\n","authors":["Shubao Zhao","Ming Jin","Zhaoxiang Hou","Chengyi Yang","Zengxiang Li","Qingsong Wen","Yi Wang"],"pdf_url":"https://arxiv.org/pdf/2401.05012v2.pdf","comment":"accepted by CIKM 2024"},{"id":"http://arxiv.org/abs/2209.15224v3","updated":"2024-08-01T08:54:39Z","published":"2022-09-30T04:35:12Z","title":"Robust Unsupervised Multi-task and Transfer Learning on Gaussian Mixture\n  Models","summary":"  Unsupervised learning has been widely used in many real-world applications.\nOne of the simplest and most important unsupervised learning models is the\nGaussian mixture model (GMM). In this work, we study the multi-task learning\nproblem on GMMs, which aims to leverage potentially similar GMM parameter\nstructures among tasks to obtain improved learning performance compared to\nsingle-task learning. We propose a multi-task GMM learning procedure based on\nthe EM algorithm that effectively utilizes unknown similarities between related\ntasks and is robust against a fraction of outlier tasks from arbitrary\ndistributions. The proposed procedure is shown to achieve the minimax optimal\nrate of convergence for both parameter estimation error and the excess\nmis-clustering error, in a wide range of regimes. Moreover, we generalize our\napproach to tackle the problem of transfer learning for GMMs, where similar\ntheoretical results are derived. Additionally, iterative unsupervised\nmulti-task and transfer learning methods may suffer from an initialization\nalignment problem, and two alignment algorithms are proposed to resolve the\nissue. Finally, we demonstrate the effectiveness of our methods through\nsimulations and real data examples. To the best of our knowledge, this is the\nfirst work studying multi-task and transfer learning on GMMs with theoretical\nguarantees.\n","authors":["Ye Tian","Haolei Weng","Lucy Xia","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2209.15224v3.pdf","comment":"162 pages, 15 figures, 2 tables"},{"id":"http://arxiv.org/abs/2404.17591v2","updated":"2024-08-01T08:54:15Z","published":"2024-04-19T13:28:36Z","title":"Large Language Models for Next Point-of-Interest Recommendation","summary":"  The next Point of Interest (POI) recommendation task is to predict users'\nimmediate next POI visit given their historical data. Location-Based Social\nNetwork (LBSN) data, which is often used for the next POI recommendation task,\ncomes with challenges. One frequently disregarded challenge is how to\neffectively use the abundant contextual information present in LBSN data.\nPrevious methods are limited by their numerical nature and fail to address this\nchallenge. In this paper, we propose a framework that uses pretrained Large\nLanguage Models (LLMs) to tackle this challenge. Our framework allows us to\npreserve heterogeneous LBSN data in its original format, hence avoiding the\nloss of contextual information. Furthermore, our framework is capable of\ncomprehending the inherent meaning of contextual information due to the\ninclusion of commonsense knowledge. In experiments, we test our framework on\nthree real-world LBSN datasets. Our results show that the proposed framework\noutperforms the state-of-the-art models in all three datasets. Our analysis\ndemonstrates the effectiveness of the proposed framework in using contextual\ninformation as well as alleviating the commonly encountered cold-start and\nshort trajectory problems.\n","authors":["Peibo Li","Maarten de Rijke","Hao Xue","Shuang Ao","Yang Song","Flora D. Salim"],"pdf_url":"https://arxiv.org/pdf/2404.17591v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06925v2","updated":"2024-08-01T08:51:46Z","published":"2024-01-12T23:14:34Z","title":"Modeling Latent Selection with Structural Causal Models","summary":"  Selection bias is ubiquitous in real-world data, and can lead to misleading\nresults if not dealt with properly. We introduce a conditioning operation on\nStructural Causal Models (SCMs) to model latent selection from a causal\nperspective. We show that the conditioning operation transforms an SCM with the\npresence of an explicit latent selection mechanism into an SCM without such\nselection mechanism, which partially encodes the causal semantics of the\nselected subpopulation according to the original SCM. Furthermore, we show that\nthis conditioning operation preserves the simplicity, acyclicity, and linearity\nof SCMs, and commutes with marginalization. Thanks to these properties,\ncombined with marginalization and intervention, the conditioning operation\noffers a valuable tool for conducting causal reasoning tasks within causal\nmodels where latent details have been abstracted away. We demonstrate by\nexample how classical results of causal inference can be generalized to include\nselection bias and how the conditioning operation helps with modeling of\nreal-world problems.\n","authors":["Leihao Chen","Onno Zoeter","Joris M. Mooij"],"pdf_url":"https://arxiv.org/pdf/2401.06925v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19872v2","updated":"2024-08-01T08:38:35Z","published":"2024-07-29T10:43:15Z","title":"OpenUAS: Embeddings of Cities in Japan with Anchor Data for Cross-city\n  Analysis of Area Usage Patterns","summary":"  We publicly release OpenUAS, a dataset of area embeddings based on urban\nusage patterns, including embeddings for over 1.3 million 50-meter square\nmeshes covering a total area of 3,300 square kilometers. This dataset is\nvaluable for analyzing area functions in fields such as market analysis, urban\nplanning, transportation infrastructure, and infection prediction. It captures\nthe characteristics of each area in the city, such as office districts and\nresidential areas, by employing an area embedding technique that utilizes\nlocation information typically obtained by GPS. Numerous area embedding\ntechniques have been proposed, and while the public release of such embedding\ndatasets is technically feasible, it has not been realized. One of the\nobstacles has been the integration of data from different cities and periods\ninto a unified space without sharing raw location data. We address this issue\nby developing an anchoring method that establishes anchors within a shared\nembedding space. We publicly release this anchor dataset along with area\nembedding datasets from several periods in eight major Japanese cities. This\ndataset allows users to analyze urban usage patterns in Japanese cities and\nembed their urban dataset into the same embedding space using the anchoring\nmethod. Our key contributions include the development of the anchoring method,\nreleasing area embedding datasets for Japanese cities, and providing tools for\neffective data utilization.\n","authors":["Naoki Tamura","Kazuyuki Shoji","Shin Katayama","Kenta Urano","Takuro Yonezawa","Nobuo Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2407.19872v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08604v2","updated":"2024-08-01T07:55:49Z","published":"2024-06-12T19:17:17Z","title":"GRU-Net: Gaussian Attention Aided Dense Skip Connection Based\n  MultiResUNet for Breast Histopathology Image Segmentation","summary":"  Breast cancer is a major global health concern. Pathologists face challenges\nin analyzing complex features from pathological images, which is a\ntime-consuming and labor-intensive task. Therefore, efficient computer-based\ndiagnostic tools are needed for early detection and treatment planning. This\npaper presents a modified version of MultiResU-Net for histopathology image\nsegmentation, which is selected as the backbone for its ability to analyze and\nsegment complex features at multiple scales and ensure effective feature flow\nvia skip connections. The modified version also utilizes the Gaussian\ndistribution-based Attention Module (GdAM) to incorporate\nhistopathology-relevant text information in a Gaussian distribution. The\nsampled features from the Gaussian text feature-guided distribution highlight\nspecific spatial regions based on prior knowledge. Finally, using the\nControlled Dense Residual Block (CDRB) on skip connections of MultiResU-Net,\nthe information is transferred from the encoder layers to the decoder layers in\na controlled manner using a scaling parameter derived from the extracted\nspatial features. We validate our approach on two diverse breast cancer\nhistopathology image datasets: TNBC and MonuSeg, demonstrating superior\nsegmentation performance compared to state-of-the-art methods. The code for our\nproposed model is available on https://github.com/AyushRoy2001/GRU-Net.\n","authors":["Ayush Roy","Payel Pramanik","Sohom Ghosal","Daria Valenkova","Dmitrii Kaplun","Ram Sarkar"],"pdf_url":"https://arxiv.org/pdf/2406.08604v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15220v4","updated":"2024-08-01T07:51:25Z","published":"2024-02-23T09:29:19Z","title":"ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition","summary":"  Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.\n","authors":["Lu Ye","Ze Tao","Yong Huang","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2402.15220v4.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2402.02563v3","updated":"2024-08-01T07:46:54Z","published":"2024-02-04T16:45:01Z","title":"DefInt: A Default-interventionist Framework for Efficient Reasoning with\n  Hybrid Large Language Models","summary":"  Large language models (LLMs) have shown impressive emergent abilities in a\nwide range of tasks, but still face challenges in handling complex reasoning\nproblems. Previous works like chain-of-thought (CoT) and tree-of-thoughts (ToT)\nhave predominately focused on enhancing accuracy, but overlook the rapidly\nincreasing token cost, which could be particularly problematic for open-ended\nreal-world tasks with huge solution spaces. Motivated by the dual process\ntheory of human cognition, we propose a Default-Interventionist framework\n(DefInt) to unleash the synergistic potential of hybrid LLMs. By default,\nDefInt uses smaller-scale language models to generate low-cost reasoning\nthoughts, which resembles the fast intuitions produced by System 1. If the\nintuitions are considered with low confidence, DefInt will invoke the\nreflective reasoning of scaled-up language models as the intervention of System\n2, which can override the default thoughts and rectify the reasoning process.\nExperiments on five representative reasoning tasks show that DefInt\nconsistently achieves state-of-the-art reasoning accuracy and solution\ndiversity. More importantly, it substantially reduces the token cost by 49%-79%\ncompared to the second accurate baselines. Specifically, the open-ended tasks\nhave an average 75% token cost reduction. Code repo with all prompts will be\nreleased upon publication.\n","authors":["Yu Shang","Yu Li","Fengli Xu","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2402.02563v3.pdf","comment":"18 pages, 10 figures, 14 tables"},{"id":"http://arxiv.org/abs/2406.03920v2","updated":"2024-08-01T07:29:42Z","published":"2024-06-06T10:02:49Z","title":"Towards Physically Consistent Deep Learning For Climate Model\n  Parameterizations","summary":"  Climate models play a critical role in understanding and projecting climate\nchange. Due to their complexity, their horizontal resolution of about 40-100 km\nremains too coarse to resolve processes such as clouds and convection, which\nneed to be approximated via parameterizations. These parameterizations are a\nmajor source of systematic errors and large uncertainties in climate\nprojections. Deep learning (DL)-based parameterizations, trained on data from\ncomputationally expensive short, high-resolution simulations, have shown great\npromise for improving climate models in that regard. However, their lack of\ninterpretability and tendency to learn spurious non-physical correlations\nresult in reduced trust in the climate simulation. We propose an efficient\nsupervised learning framework for DL-based parameterizations that leads to\nphysically consistent models with improved interpretability and negligible\ncomputational overhead compared to standard supervised training. First, key\nfeatures determining the target physical processes are uncovered. Subsequently,\nthe neural network is fine-tuned using only those relevant features. We show\nempirically that our method robustly identifies a small subset of the inputs as\nactual physical drivers, therefore, removing spurious non-physical\nrelationships. This results in by design physically consistent and\ninterpretable neural networks while maintaining the predictive performance of\nunconstrained black-box DL-based parameterizations.\n","authors":["Birgit Kühbacher","Fernando Iglesias-Suarez","Niki Kilbertus","Veronika Eyring"],"pdf_url":"https://arxiv.org/pdf/2406.03920v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.09634v4","updated":"2024-08-01T05:09:34Z","published":"2022-10-18T07:03:14Z","title":"DPIS: An Enhanced Mechanism for Differentially Private SGD with\n  Importance Sampling","summary":"  Nowadays, differential privacy (DP) has become a well-accepted standard for\nprivacy protection, and deep neural networks (DNN) have been immensely\nsuccessful in machine learning. The combination of these two techniques, i.e.,\ndeep learning with differential privacy, promises the privacy-preserving\nrelease of high-utility models trained with sensitive data such as medical\nrecords. A classic mechanism for this purpose is DP-SGD, which is a\ndifferentially private version of the stochastic gradient descent (SGD)\noptimizer commonly used for DNN training. Subsequent approaches have improved\nvarious aspects of the model training process, including noise decay schedule,\nmodel architecture, feature engineering, and hyperparameter tuning. However,\nthe core mechanism for enforcing DP in the SGD optimizer remains unchanged ever\nsince the original DP-SGD algorithm, which has increasingly become a\nfundamental barrier limiting the performance of DP-compliant machine learning\nsolutions.\n  Motivated by this, we propose DPIS, a novel mechanism for differentially\nprivate SGD training that can be used as a drop-in replacement of the core\noptimizer of DP-SGD, with consistent and significant accuracy gains over the\nlatter. The main idea is to employ importance sampling (IS) in each SGD\niteration for mini-batch selection, which reduces both sampling variance and\nthe amount of random noise injected to the gradients that is required to\nsatisfy DP. Integrating IS into the complex mathematical machinery of DP-SGD is\nhighly non-trivial. DPIS addresses the challenge through novel mechanism\ndesigns, fine-grained privacy analysis, efficiency enhancements, and an\nadaptive gradient clipping optimization. Extensive experiments on four\nbenchmark datasets, namely MNIST, FMNIST, CIFAR-10 and IMDb, demonstrate the\nsuperior effectiveness of DPIS over existing solutions for deep learning with\ndifferential privacy.\n","authors":["Jianxin Wei","Ergute Bao","Xiaokui Xiao","Yin Yang"],"pdf_url":"https://arxiv.org/pdf/2210.09634v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12384v4","updated":"2024-08-01T03:32:49Z","published":"2024-03-19T02:49:32Z","title":"AlignRec: Aligning and Training in Multimodal Recommendations","summary":"  With the development of multimedia systems, multimodal recommendations are\nplaying an essential role, as they can leverage rich contexts beyond\ninteractions. Existing methods mainly regard multimodal information as an\nauxiliary, using them to help learn ID features; However, there exist semantic\ngaps among multimodal content features and ID-based features, for which\ndirectly using multimodal information as an auxiliary would lead to\nmisalignment in representations of users and items. In this paper, we first\nsystematically investigate the misalignment issue in multimodal\nrecommendations, and propose a solution named AlignRec. In AlignRec, the\nrecommendation objective is decomposed into three alignments, namely alignment\nwithin contents, alignment between content and categorical ID, and alignment\nbetween users and items. Each alignment is characterized by a specific\nobjective function and is integrated into our multimodal recommendation\nframework. To effectively train AlignRec, we propose starting from pre-training\nthe first alignment to obtain unified multimodal features and subsequently\ntraining the following two alignments together with these features as input. As\nit is essential to analyze whether each multimodal feature helps in training\nand accelerate the iteration cycle of recommendation models, we design three\nnew classes of metrics to evaluate intermediate performance. Our extensive\nexperiments on three real-world datasets consistently verify the superiority of\nAlignRec compared to nine baselines. We also find that the multimodal features\ngenerated by AlignRec are better than currently used ones, which are to be\nopen-sourced in our repository https://github.com/sjtulyf123/AlignRec_CIKM24.\n","authors":["Yifan Liu","Kangning Zhang","Xiangyuan Ren","Yanhua Huang","Jiarui Jin","Yingjie Qin","Ruilong Su","Ruiwen Xu","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.12384v4.pdf","comment":"9 page paper, 2 page appendix. Accepted by CIKM24"},{"id":"http://arxiv.org/abs/2406.01572v2","updated":"2024-08-01T03:29:32Z","published":"2024-06-03T17:51:54Z","title":"Unlocking Guidance for Discrete State-Space Diffusion and Flow Models","summary":"  Generative models on discrete state-spaces have a wide range of potential\napplications, particularly in the domain of natural sciences. In continuous\nstate-spaces, controllable and flexible generation of samples with desired\nproperties has been realized using guidance on diffusion and flow models.\nHowever, these guidance approaches are not readily amenable to discrete\nstate-space models. Consequently, we introduce a general and principled method\nfor applying guidance on such models. Our method depends on leveraging\ncontinuous-time Markov processes on discrete state-spaces, which unlocks\ncomputational tractability for sampling from a desired guided distribution. We\ndemonstrate the utility of our approach, Discrete Guidance, on a range of\napplications including guided generation of images, small-molecules, DNA\nsequences and protein sequences.\n","authors":["Hunter Nisonoff","Junhao Xiong","Stephan Allenspach","Jennifer Listgarten"],"pdf_url":"https://arxiv.org/pdf/2406.01572v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11539v3","updated":"2024-08-01T03:19:03Z","published":"2023-12-15T23:34:05Z","title":"KGLens: Towards Efficient and Effective Knowledge Probing of Large\n  Language Models with Knowledge Graphs","summary":"  Large Language Models (LLMs) might hallucinate facts, while curated Knowledge\nGraph (KGs) are typically factually reliable especially with domain-specific\nknowledge. Measuring the alignment between KGs and LLMs can effectively probe\nthe factualness and identify the knowledge blind spots of LLMs. However,\nverifying the LLMs over extensive KGs can be expensive. In this paper, we\npresent KGLens, a Thompson-sampling-inspired framework aimed at effectively and\nefficiently measuring the alignment between KGs and LLMs. KGLens features a\ngraph-guided question generator for converting KGs into natural language, along\nwith a carefully designed importance sampling strategy based on parameterized\nKG structure to expedite KG traversal. Our simulation experiment compares the\nbrute force method with KGLens under six different sampling methods,\ndemonstrating that our approach achieves superior probing efficiency.\nLeveraging KGLens, we conducted in-depth analyses of the factual accuracy of\nten LLMs across three large domain-specific KGs from Wikidata, composing over\n19K edges, 700 relations, and 21K entities. Human evaluation results indicate\nthat KGLens can assess LLMs with a level of accuracy nearly equivalent to that\nof human annotators, achieving 95.7% of the accuracy rate.\n","authors":["Shangshang Zheng","He Bai","Yizhe Zhang","Yi Su","Xiaochuan Niu","Navdeep Jaitly"],"pdf_url":"https://arxiv.org/pdf/2312.11539v3.pdf","comment":"ACL 2024 Workshop Towards Knowledgeable Language Models"},{"id":"http://arxiv.org/abs/2404.07221v2","updated":"2024-08-01T03:02:44Z","published":"2024-03-23T00:49:40Z","title":"Improving Retrieval for RAG based Question Answering Models on Financial\n  Documents","summary":"  The effectiveness of Large Language Models (LLMs) in generating accurate\nresponses relies heavily on the quality of input provided, particularly when\nemploying Retrieval Augmented Generation (RAG) techniques. RAG enhances LLMs by\nsourcing the most relevant text chunk(s) to base queries upon. Despite the\nsignificant advancements in LLMs' response quality in recent years, users may\nstill encounter inaccuracies or irrelevant answers; these issues often stem\nfrom suboptimal text chunk retrieval by RAG rather than the inherent\ncapabilities of LLMs. To augment the efficacy of LLMs, it is crucial to refine\nthe RAG process. This paper explores the existing constraints of RAG pipelines\nand introduces methodologies for enhancing text retrieval. It delves into\nstrategies such as sophisticated chunking techniques, query expansion, the\nincorporation of metadata annotations, the application of re-ranking\nalgorithms, and the fine-tuning of embedding algorithms. Implementing these\napproaches can substantially improve the retrieval quality, thereby elevating\nthe overall performance and reliability of LLMs in processing and responding to\nqueries.\n","authors":["Spurthi Setty","Harsh Thakkar","Alyssa Lee","Eden Chung","Natan Vidra"],"pdf_url":"https://arxiv.org/pdf/2404.07221v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01614v2","updated":"2024-08-01T02:56:58Z","published":"2024-06-28T01:46:10Z","title":"Enhancing Stability for Large Models Training in Constrained Bandwidth\n  Networks","summary":"  Training extremely large language models with billions of parameters is a\ncomputationally intensive task that pushes the limits of current data parallel\ntraining systems. While techniques like ZeRO++ have enabled efficient\ndistributed training of such giant models on inexpensive low-bandwidth\nclusters, they can suffer from convergence issues due to potential race\nconditions in the hierarchical partitioning (hpZ) scheme employed to reduce\ncross-machine communication. In this work, we first show how these race\nconditions cause instability when training models with billions of parameters.\nWe then propose a modification to the partitioning algorithm that addresses\nthese convergence challenges while maintaining competitive training efficiency.\nEmpirical evaluation on training the multi-billion parameters Falcon Models and\nLlama-2 models demonstrates the updated algorithm's ability to achieve reliable\nconvergence on these massive models, where stock ZeRO++ hpZ fails to converge.\nThe updated algorithm enables robust training of larger models with 98\\%\nthroughput and model training speed improvement without sacrificing the quality\nof convergence.\n","authors":["Yun Dai","Tejas Dharamsi","Byron Hsu","Tao Song","Hamed Firooz"],"pdf_url":"https://arxiv.org/pdf/2407.01614v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14055v2","updated":"2024-08-01T02:19:08Z","published":"2024-07-19T06:31:22Z","title":"Quantum Hamiltonian Embedding of Images for Data Reuploading Classifiers","summary":"  When applying quantum computing to machine learning tasks, one of the first\nconsiderations is the design of the quantum machine learning model itself.\nConventionally, the design of quantum machine learning algorithms relies on the\n``quantisation\" of classical learning algorithms, such as using quantum linear\nalgebra to implement important subroutines of classical algorithms, if not the\nentire algorithm, seeking to achieve quantum advantage through possible\nrun-time accelerations brought by quantum computing. However, recent research\nhas started questioning whether quantum advantage via speedup is the right goal\nfor quantum machine learning [1]. Research also has been undertaken to exploit\nproperties that are unique to quantum systems, such as quantum contextuality,\nto better design quantum machine learning models [2]. In this paper, we take an\nalternative approach by incorporating the heuristics and empirical evidences\nfrom the design of classical deep learning algorithms to the design of quantum\nneural networks. We first construct a model based on the data reuploading\ncircuit [3] with the quantum Hamiltonian data embedding unitary [4]. Through\nnumerical experiments on images datasets, including the famous MNIST and\nFashionMNIST datasets, we demonstrate that our model outperforms the quantum\nconvolutional neural network (QCNN)[5] by a large margin (up to over 40% on\nMNIST test set). Based on the model design process and numerical results, we\nthen laid out six principles for designing quantum machine learning models,\nespecially quantum neural networks.\n","authors":["Peiyong Wang","Casey R. Myers","Lloyd C. L. Hollenberg","Udaya Parampalli"],"pdf_url":"https://arxiv.org/pdf/2407.14055v2.pdf","comment":"11 figures, 31 pages. Code available on\n  https://github.com/peiyong-addwater/HamEmbedding. Author affiliation updated\n  for v2. Acknowledgements and funding information added for v2"},{"id":"http://arxiv.org/abs/2407.21266v2","updated":"2024-08-01T01:59:58Z","published":"2024-07-31T01:07:21Z","title":"DDU-Net: A Domain Decomposition-based CNN for High-Resolution Image\n  Segmentation on Multiple GPUs","summary":"  The segmentation of ultra-high resolution images poses challenges such as\nloss of spatial information or computational inefficiency. In this work, a\nnovel approach that combines encoder-decoder architectures with domain\ndecomposition strategies to address these challenges is proposed. Specifically,\na domain decomposition-based U-Net (DDU-Net) architecture is introduced, which\npartitions input images into non-overlapping patches that can be processed\nindependently on separate devices. A communication network is added to\nfacilitate inter-patch information exchange to enhance the understanding of\nspatial context. Experimental validation is performed on a synthetic dataset\nthat is designed to measure the effectiveness of the communication network.\nThen, the performance is tested on the DeepGlobe land cover classification\ndataset as a real-world benchmark data set. The results demonstrate that the\napproach, which includes inter-patch communication for images divided into\n$16\\times16$ non-overlapping subimages, achieves a $2-3\\,\\%$ higher\nintersection over union (IoU) score compared to the same network without\ninter-patch communication. The performance of the network which includes\ncommunication is equivalent to that of a baseline U-Net trained on the full\nimage, showing that our model provides an effective solution for segmenting\nultra-high-resolution images while preserving spatial context. The code is\navailable at https://github.com/corne00/HiRes-Seg-CNN.\n","authors":["Corné Verburg","Alexander Heinlein","Eric C. Cyr"],"pdf_url":"https://arxiv.org/pdf/2407.21266v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.01593v6","updated":"2024-08-01T01:49:47Z","published":"2021-10-04T17:41:53Z","title":"Generalized Kernel Thinning","summary":"  The kernel thinning (KT) algorithm of Dwivedi and Mackey (2021) compresses a\nprobability distribution more effectively than independent sampling by\ntargeting a reproducing kernel Hilbert space (RKHS) and leveraging a less\nsmooth square-root kernel. Here we provide four improvements. First, we show\nthat KT applied directly to the target RKHS yields tighter, dimension-free\nguarantees for any kernel, any distribution, and any fixed function in the\nRKHS. Second, we show that, for analytic kernels like Gaussian, inverse\nmultiquadric, and sinc, target KT admits maximum mean discrepancy (MMD)\nguarantees comparable to or better than those of square-root KT without making\nexplicit use of a square-root kernel. Third, we prove that KT with a fractional\npower kernel yields better-than-Monte-Carlo MMD guarantees for non-smooth\nkernels, like Laplace and Mat\\'ern, that do not have square-roots. Fourth, we\nestablish that KT applied to a sum of the target and power kernels (a procedure\nwe call KT+) simultaneously inherits the improved MMD guarantees of power KT\nand the tighter individual function guarantees of target KT. In our experiments\nwith target KT and KT+, we witness significant improvements in integration\nerror even in $100$ dimensions and when compressing challenging differential\nequation posteriors.\n","authors":["Raaz Dwivedi","Lester Mackey"],"pdf_url":"https://arxiv.org/pdf/2110.01593v6.pdf","comment":"Corrected B-spline and Sinc rates in Table 3"},{"id":"http://arxiv.org/abs/2202.05525v2","updated":"2024-08-01T01:42:10Z","published":"2022-02-11T09:45:11Z","title":"From Unsupervised to Few-shot Graph Anomaly Detection: A Multi-scale\n  Contrastive Learning Approach","summary":"  Anomaly detection from graph data is an important data mining task in many\napplications such as social networks, finance, and e-commerce. Existing efforts\nin graph anomaly detection typically only consider the information in a single\nscale (view), thus inevitably limiting their capability in capturing anomalous\npatterns in complex graph data. To address this limitation, we propose a novel\nframework, graph ANomaly dEtection framework with Multi-scale cONtrastive\nlEarning (ANEMONE in short). By using a graph neural network as a backbone to\nencode the information from multiple graph scales (views), we learn better\nrepresentation for nodes in a graph. In maximizing the agreements between\ninstances at both the patch and context levels concurrently, we estimate the\nanomaly score of each node with a statistical anomaly estimator according to\nthe degree of agreement from multiple perspectives. To further exploit a\nhandful of ground-truth anomalies (few-shot anomalies) that may be collected in\nreal-life applications, we further propose an extended algorithm, ANEMONE-FS,\nto integrate valuable information in our method. We conduct extensive\nexperiments under purely unsupervised settings and few-shot anomaly detection\nsettings, and we demonstrate that the proposed method ANEMONE and its variant\nANEMONE-FS consistently outperform state-of-the-art algorithms on six benchmark\ndatasets.\n","authors":["Yu Zheng","Ming Jin","Yixin Liu","Lianhua Chi","Khoa T. Phan","Yi-Ping Phoebe Chen"],"pdf_url":"https://arxiv.org/pdf/2202.05525v2.pdf","comment":"13 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2407.20299v2","updated":"2024-08-01T01:33:48Z","published":"2024-07-29T04:02:17Z","title":"Dataset Distillation for Offline Reinforcement Learning","summary":"  Offline reinforcement learning often requires a quality dataset that we can\ntrain a policy on. However, in many situations, it is not possible to get such\na dataset, nor is it easy to train a policy to perform well in the actual\nenvironment given the offline data. We propose using data distillation to train\nand distill a better dataset which can then be used for training a better\npolicy model. We show that our method is able to synthesize a dataset where a\nmodel trained on it achieves similar performance to a model trained on the full\ndataset or a model trained using percentile behavioral cloning. Our project\nsite is available at\n$\\href{https://datasetdistillation4rl.github.io}{\\text{here}}$. We also provide\nour implementation at $\\href{https://github.com/ggflow123/DDRL}{\\text{this\nGitHub repository}}$.\n","authors":["Jonathan Light","Yuanzhe Liu","Ziniu Hu"],"pdf_url":"https://arxiv.org/pdf/2407.20299v2.pdf","comment":"ICML 2024 DMLR Workshop"},{"id":"http://arxiv.org/abs/2407.12254v2","updated":"2024-08-01T01:21:57Z","published":"2024-07-17T01:51:27Z","title":"COKE: Causal Discovery with Chronological Order and Expert Knowledge in\n  High Proportion of Missing Manufacturing Data","summary":"  Understanding causal relationships between machines is crucial for fault\ndiagnosis and optimization in manufacturing processes. Real-world datasets\nfrequently exhibit up to 90% missing data and high dimensionality from hundreds\nof sensors. These datasets also include domain-specific expert knowledge and\nchronological order information, reflecting the recording order across\ndifferent machines, which is pivotal for discerning causal relationships within\nthe manufacturing data. However, previous methods for handling missing data in\nscenarios akin to real-world conditions have not been able to effectively\nutilize expert knowledge. Conversely, prior methods that can incorporate expert\nknowledge struggle with datasets that exhibit missing values. Therefore, we\npropose COKE to construct causal graphs in manufacturing datasets by leveraging\nexpert knowledge and chronological order among sensors without imputing missing\ndata. Utilizing the characteristics of the recipe, we maximize the use of\nsamples with missing values, derive embeddings from intersections with an\ninitial graph that incorporates expert knowledge and chronological order, and\ncreate a sensor ordering graph. The graph-generating process has been optimized\nby an actor-critic architecture to obtain a final graph that has a maximum\nreward. Experimental evaluations in diverse settings of sensor quantities and\nmissing proportions demonstrate that our approach compared with the benchmark\nmethods shows an average improvement of 39.9% in the F1-score. Moreover, the\nF1-score improvement can reach 62.6% when considering the configuration similar\nto real-world datasets, and 85.0% in real-world semiconductor datasets. The\nsource code is available at https://github.com/OuTingYun/COKE.\n","authors":["Ting-Yun Ou","Ching Chang","Wen-Chih Peng"],"pdf_url":"https://arxiv.org/pdf/2407.12254v2.pdf","comment":"This paper has been accepted by the ACM International Conference on\n  Information and Knowledge Management (CIKM) 2024"},{"id":"http://arxiv.org/abs/2401.10467v2","updated":"2024-08-01T01:07:35Z","published":"2024-01-19T03:39:43Z","title":"Learning Backdoors for Mixed Integer Linear Programs with Contrastive\n  Learning","summary":"  Many real-world problems can be efficiently modeled as Mixed Integer Linear\nPrograms (MILPs) and solved with the Branch-and-Bound method. Prior work has\nshown the existence of MILP backdoors, small sets of variables such that\nprioritizing branching on them when possible leads to faster running times.\nHowever, finding high-quality backdoors that improve running times remains an\nopen question. Previous work learns to estimate the relative solver speed of\nrandomly sampled backdoors through ranking and then decide whether to use the\nhighest-ranked backdoor candidate. In this paper, we utilize the Monte-Carlo\ntree search method to collect backdoors for training, rather than relying on\nrandom sampling, and adapt a contrastive learning framework to train a Graph\nAttention Network model to predict backdoors. Our method, evaluated on several\ncommon MILP problem domains, demonstrates performance improvements over both\nGurobi and previous models.\n","authors":["Junyang Cai","Taoan Huang","Bistra Dilkina"],"pdf_url":"https://arxiv.org/pdf/2401.10467v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00764v1","updated":"2024-08-01T17:59:46Z","published":"2024-08-01T17:59:46Z","title":"AgentGen: Enhancing Planning Abilities for Large Language Model based\n  Agent via Environment and Task Generation","summary":"  Large Language Model (LLM) based agents have garnered significant attention\nand are becoming increasingly popular. Furthermore, planning ability is a\ncrucial component of an LLM-based agent, involving interaction with the\nenvironment and executing actions to complete a planning task, which generally\nentails achieving a desired goal from an initial state. This paper investigates\nenhancing the planning abilities of LLMs through instruction tuning, referred\nto as agent training. Recent studies have demonstrated that utilizing\nexpert-level trajectory for instruction-tuning LLMs effectively enhances their\nplanning capabilities. However, existing work primarily focuses on synthesizing\ntrajectories from manually designed planning tasks and environments. The\nlabor-intensive nature of creating these environments and tasks impedes the\ngeneration of sufficiently varied and extensive trajectories. To address this\nlimitation, this paper explores the automated synthesis of diverse environments\nand a gradual range of planning tasks, from easy to difficult. We introduce a\nframework, AgentGen, that leverages LLMs first to generate environments and\nsubsequently generate planning tasks conditioned on these environments.\nSpecifically, to improve environmental diversity, we propose using an\ninspiration corpus composed of various domain-specific text segments as the\ncontext for synthesizing environments. Moreover, to increase the difficulty\ndiversity of generated planning tasks, we propose a bidirectional evolution\nmethod, Bi-Evol, that evolves planning tasks from easier and harder directions\nto synthesize a task set with a smoother difficulty curve. The evaluation\nresults derived from AgentBoard show that AgentGen greatly improves LLMs'\nplanning ability, e.g., the AgentGen instruction-tuned Llama-3 8B surpasses\nGPT-3.5 in overall performance. Moreover, in certain tasks, it even outperforms\nGPT-4.\n","authors":["Mengkang Hu","Pu Zhao","Can Xu","Qingfeng Sun","Jianguang Lou","Qingwei Lin","Ping Luo","Saravan Rajmohan","Dongmei Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00761v1","updated":"2024-08-01T17:59:12Z","published":"2024-08-01T17:59:12Z","title":"Tamper-Resistant Safeguards for Open-Weight LLMs","summary":"  Rapid advances in the capabilities of large language models (LLMs) have\nraised widespread concerns regarding their potential for malicious use.\nOpen-weight LLMs present unique challenges, as existing safeguards lack\nrobustness to tampering attacks that modify model weights. For example, recent\nworks have demonstrated that refusal and unlearning safeguards can be trivially\nremoved with a few steps of fine-tuning. These vulnerabilities necessitate new\napproaches for enabling the safe release of open-weight LLMs. We develop a\nmethod, called TAR, for building tamper-resistant safeguards into open-weight\nLLMs such that adversaries cannot remove the safeguards even after thousands of\nsteps of fine-tuning. In extensive evaluations and red teaming analyses, we\nfind that our method greatly improves tamper-resistance while preserving benign\ncapabilities. Our results demonstrate that tamper-resistance is a tractable\nproblem, opening up a promising new avenue to improve the safety and security\nof open-weight LLMs.\n","authors":["Rishub Tamirisa","Bhrugu Bharathi","Long Phan","Andy Zhou","Alice Gatti","Tarun Suresh","Maxwell Lin","Justin Wang","Rowan Wang","Ron Arel","Andy Zou","Dawn Song","Bo Li","Dan Hendrycks","Mantas Mazeika"],"pdf_url":"https://arxiv.org/pdf/2408.00761v1.pdf","comment":"Website: https://www.tamper-resistant-safeguards.com"},{"id":"http://arxiv.org/abs/2408.00760v1","updated":"2024-08-01T17:59:09Z","published":"2024-08-01T17:59:09Z","title":"Smoothed Energy Guidance: Guiding Diffusion Models with Reduced Energy\n  Curvature of Attention","summary":"  Conditional diffusion models have shown remarkable success in visual content\ngeneration, producing high-quality samples across various domains, largely due\nto classifier-free guidance (CFG). Recent attempts to extend guidance to\nunconditional models have relied on heuristic techniques, resulting in\nsuboptimal generation quality and unintended effects. In this work, we propose\nSmoothed Energy Guidance (SEG), a novel training- and condition-free approach\nthat leverages the energy-based perspective of the self-attention mechanism to\nenhance image generation. By defining the energy of self-attention, we\nintroduce a method to reduce the curvature of the energy landscape of attention\nand use the output as the unconditional prediction. Practically, we control the\ncurvature of the energy landscape by adjusting the Gaussian kernel parameter\nwhile keeping the guidance scale parameter fixed. Additionally, we present a\nquery blurring method that is equivalent to blurring the entire attention\nweights without incurring quadratic complexity in the number of tokens. In our\nexperiments, SEG achieves a Pareto improvement in both quality and the\nreduction of side effects. The code is available at\n\\url{https://github.com/SusungHong/SEG-SDXL}.\n","authors":["Susung Hong"],"pdf_url":"https://arxiv.org/pdf/2408.00760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00754v1","updated":"2024-08-01T17:57:12Z","published":"2024-08-01T17:57:12Z","title":"Coarse Correspondence Elicit 3D Spacetime Understanding in Multimodal\n  Language Model","summary":"  Multimodal language models (MLLMs) are increasingly being implemented in\nreal-world environments, necessitating their ability to interpret 3D spaces and\ncomprehend temporal dynamics. Despite their potential, current top models\nwithin our community still fall short in adequately understanding spatial and\ntemporal dimensions. We introduce Coarse Correspondence, a simple,\ntraining-free, effective, and general-purpose visual prompting method to elicit\n3D and temporal understanding in multimodal LLMs. Our method uses a lightweight\ntracking model to find object correspondences between frames in a video or\nbetween sets of image viewpoints. It selects the most frequent object instances\nand visualizes them with markers with unique IDs in the image. With this simple\napproach, we achieve state-of-the-art results on 3D understanding benchmarks\nincluding ScanQA (+20.5\\%) and a subset of OpenEQA (+9.7\\%), and on long-form\nvideo benchmarks such as EgoSchema (+6.0\\%). We also curate a small diagnostic\ndataset to evaluate whether MLLMs can reason about space from a described\nviewpoint other than the camera viewpoint. Again, Coarse Correspondence\nimproves spatial perspective-taking abilities but we highlight that MLLMs\nstruggle with this task. Together, we demonstrate that our simple prompting\nmethod can significantly aid downstream tasks that require 3D or temporal\nreasoning.\n","authors":["Benlin Liu","Yuhao Dong","Yiqin Wang","Yongming Rao","Yansong Tang","Wei-Chiu Ma","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2408.00754v1.pdf","comment":"project page: https://coarse-correspondence.github.io"},{"id":"http://arxiv.org/abs/2408.00751v1","updated":"2024-08-01T17:54:01Z","published":"2024-08-01T17:54:01Z","title":"A Policy-Gradient Approach to Solving Imperfect-Information Games with\n  Iterate Convergence","summary":"  Policy gradient methods have become a staple of any single-agent\nreinforcement learning toolbox, due to their combination of desirable\nproperties: iterate convergence, efficient use of stochastic trajectory\nfeedback, and theoretically-sound avoidance of importance sampling corrections.\nIn multi-agent imperfect-information settings (extensive-form games), however,\nit is still unknown whether the same desiderata can be guaranteed while\nretaining theoretical guarantees. Instead, sound methods for extensive-form\ngames rely on approximating counterfactual values (as opposed to Q values),\nwhich are incompatible with policy gradient methodologies. In this paper, we\ninvestigate whether policy gradient can be safely used in two-player zero-sum\nimperfect-information extensive-form games (EFGs). We establish positive\nresults, showing for the first time that a policy gradient method leads to\nprovable best-iterate convergence to a regularized Nash equilibrium in\nself-play.\n","authors":["Mingyang Liu","Gabriele Farina","Asuman Ozdaglar"],"pdf_url":"https://arxiv.org/pdf/2408.00751v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00749v1","updated":"2024-08-01T17:52:10Z","published":"2024-08-01T17:52:10Z","title":"Leaf Angle Estimation using Mask R-CNN and LETR Vision Transformer","summary":"  Modern day studies show a high degree of correlation between high yielding\ncrop varieties and plants with upright leaf angles. It is observed that plants\nwith upright leaf angles intercept more light than those without upright leaf\nangles, leading to a higher rate of photosynthesis. Plant scientists and\nbreeders benefit from tools that can directly measure plant parameters in the\nfield i.e. on-site phenotyping. The estimation of leaf angles by manual means\nin a field setting is tedious and cumbersome. We mitigate the tedium using a\ncombination of the Mask R-CNN instance segmentation neural network, and Line\nSegment Transformer (LETR), a vision transformer. The proposed Computer Vision\n(CV) pipeline is applied on two image datasets, Summer 2015-Ames ULA and Summer\n2015- Ames MLA, with a combined total of 1,827 plant images collected in the\nfield using FieldBook, an Android application aimed at on-site phenotyping. The\nleaf angles estimated by the proposed pipeline on the image datasets are\ncompared to two independent manual measurements using ImageJ, a Java-based\nimage processing program developed at the National Institutes of Health and the\nLaboratory for Optical and Computational Instrumentation. The results, when\ncompared for similarity using the Cosine Similarity measure, exhibit 0.98\nsimilarity scores on both independent measurements of Summer 2015-Ames ULA and\nSummer 2015-Ames MLA image datasets, demonstrating the feasibility of the\nproposed pipeline for on-site measurement of leaf angles.\n","authors":["Venkat Margapuri","Prapti Thapaliya","Trevor Rife"],"pdf_url":"https://arxiv.org/pdf/2408.00749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00728v1","updated":"2024-08-01T17:20:24Z","published":"2024-08-01T17:20:24Z","title":"CERT-ED: Certifiably Robust Text Classification for Edit Distance","summary":"  With the growing integration of AI in daily life, ensuring the robustness of\nsystems to inference-time attacks is crucial. Among the approaches for\ncertifying robustness to such adversarial examples, randomized smoothing has\nemerged as highly promising due to its nature as a wrapper around arbitrary\nblack-box models. Previous work on randomized smoothing in natural language\nprocessing has primarily focused on specific subsets of edit distance\noperations, such as synonym substitution or word insertion, without exploring\nthe certification of all edit operations. In this paper, we adapt Randomized\nDeletion (Huang et al., 2023) and propose, CERTified Edit Distance defense\n(CERT-ED) for natural language classification. Through comprehensive\nexperiments, we demonstrate that CERT-ED outperforms the existing Hamming\ndistance method RanMASK (Zeng et al., 2023) in 4 out of 5 datasets in terms of\nboth accuracy and the cardinality of the certificate. By covering various\nthreat models, including 5 direct and 5 transfer attacks, our method improves\nempirical robustness in 38 out of 50 settings.\n","authors":["Zhuoqun Huang","Neil G Marchant","Olga Ohrimenko","Benjamin I. P. Rubinstein"],"pdf_url":"https://arxiv.org/pdf/2408.00728v1.pdf","comment":"22 pages, 3 figures, 12 tables. Include 11 pages of appendices"},{"id":"http://arxiv.org/abs/2408.00716v1","updated":"2024-08-01T17:01:29Z","published":"2024-08-01T17:01:29Z","title":"A Natural Language Processing Framework for Hotel Recommendation Based\n  on Users' Text Reviews","summary":"  Recently, the application of Artificial Intelligence algorithms in hotel\nrecommendation systems has become an increasingly popular topic. One such\nmethod that has proven to be effective in this field is Deep Learning,\nespecially Natural Language processing models, which are able to extract\nsemantic knowledge from user's text reviews to create more efficient\nrecommendation systems. This can lead to the development of intelligent models\nthat can classify a user's preferences and emotions based on their feedback in\nthe form of text reviews about their hotel stay experience. In this study, we\npropose a Natural Language Processing framework that utilizes customer text\nreviews to provide personalized recommendations for the most appropriate hotel\nbased on their preferences. The framework is based on Bidirectional Encoder\nRepresentations from Transformers (BERT) and a fine-tuning/validation pipeline\nthat categorizes customer hotel review texts into \"Bad,\" \"Good,\" or \"Excellent\"\nrecommended hotels. Our findings indicate that the hotel recommendation system\nwe propose can significantly enhance the user experience of booking\naccommodations by providing personalized recommendations based on user\npreferences and previous booking history.\n","authors":["Lavrentia Aravani","Emmanuel Pintelas","Christos Pierrakeas","Panagiotis Pintelas"],"pdf_url":"https://arxiv.org/pdf/2408.00716v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00714v1","updated":"2024-08-01T17:00:08Z","published":"2024-08-01T17:00:08Z","title":"SAM 2: Segment Anything in Images and Videos","summary":"  We present Segment Anything Model 2 (SAM 2), a foundation model towards\nsolving promptable visual segmentation in images and videos. We build a data\nengine, which improves model and data via user interaction, to collect the\nlargest video segmentation dataset to date. Our model is a simple transformer\narchitecture with streaming memory for real-time video processing. SAM 2\ntrained on our data provides strong performance across a wide range of tasks.\nIn video segmentation, we observe better accuracy, using 3x fewer interactions\nthan prior approaches. In image segmentation, our model is more accurate and 6x\nfaster than the Segment Anything Model (SAM). We believe that our data, model,\nand insights will serve as a significant milestone for video segmentation and\nrelated perception tasks. We are releasing a version of our model, the dataset\nand an interactive demo.\n","authors":["Nikhila Ravi","Valentin Gabeur","Yuan-Ting Hu","Ronghang Hu","Chaitanya Ryali","Tengyu Ma","Haitham Khedr","Roman Rädle","Chloe Rolland","Laura Gustafson","Eric Mintun","Junting Pan","Kalyan Vasudev Alwala","Nicolas Carion","Chao-Yuan Wu","Ross Girshick","Piotr Dollár","Christoph Feichtenhofer"],"pdf_url":"https://arxiv.org/pdf/2408.00714v1.pdf","comment":"Website: https://ai.meta.com/sam2"},{"id":"http://arxiv.org/abs/2408.00713v1","updated":"2024-08-01T16:58:54Z","published":"2024-08-01T16:58:54Z","title":"Insurance Portfolio Pursuit with Reinforcement Learning","summary":"  When faced with a new customer, many factors contribute to an insurance\nfirm's decision of what offer to make to that customer. In addition to the\nexpected cost of providing the insurance, the firm must consider the other\noffers likely to be made to the customer, and how sensitive the customer is to\ndifferences in price. Moreover, firms often target a specific portfolio of\ncustomers that could depend on, e.g., age, location, and occupation. Given such\na target portfolio, firms may choose to modulate an individual customer's offer\nbased on whether the firm desires the customer within their portfolio. Given a\ntarget portfolio, we term the problem of modulating offers to achieve this\ntarget portfolio the portfolio pursuit problem. We give a formulation of\nportfolio pursuit as a sequential decision making problem, and devise a novel\nreinforcement learning algorithm for its solution. We test our method on a\ncomplex synthetic market environment, and demonstrate that it outperforms a\nbaseline method which mimics current industry approaches to portfolio pursuit.\n","authors":["Edward James Young","Alistair Rogers","Elliott Tong","James Jordon"],"pdf_url":"https://arxiv.org/pdf/2408.00713v1.pdf","comment":"16 pages, 1 figure"},{"id":"http://arxiv.org/abs/2408.00707v1","updated":"2024-08-01T16:54:11Z","published":"2024-08-01T16:54:11Z","title":"Synthetic dual image generation for reduction of labeling efforts in\n  semantic segmentation of micrographs with a customized metric function","summary":"  Training of semantic segmentation models for material analysis requires\nmicrographs and their corresponding masks. It is quite unlikely that perfect\nmasks will be drawn, especially at the edges of objects, and sometimes the\namount of data that can be obtained is small, since only a few samples are\navailable. These aspects make it very problematic to train a robust model. We\ndemonstrate a workflow for the improvement of semantic segmentation models of\nmicrographs through the generation of synthetic microstructural images in\nconjunction with masks. The workflow only requires joining a few micrographs\nwith their respective masks to create the input for a Vector\nQuantised-Variational AutoEncoder model that includes an embedding space, which\nis trained such that a generative model (PixelCNN) learns the distribution of\neach input, transformed into discrete codes, and can be used to sample new\ncodes. The latter will eventually be decoded by VQ-VAE to generate images\nalongside corresponding masks for semantic segmentation. To evaluate the\nsynthetic data, we have trained U-Net models with different amounts of these\nsynthetic data in conjunction with real data. These models were then evaluated\nusing non-synthetic images only. Additionally, we introduce a customized metric\nderived from the mean Intersection over Union (mIoU). The proposed metric\nprevents a few falsely predicted pixels from greatly reducing the value of the\nmIoU. We have achieved a reduction in sample preparation and acquisition times,\nas well as the efforts, needed for image processing and labeling tasks, are\nless when it comes to training semantic segmentation model. The approach could\nbe generalized to various types of image data such that it serves as a\nuser-friendly solution for training models with a small number of real images.\n","authors":["Matias Oscar Volman Stern","Dominic Hohs","Andreas Jansche","Timo Bernthaler","Gerhard Schneider"],"pdf_url":"https://arxiv.org/pdf/2408.00707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00706v1","updated":"2024-08-01T16:52:39Z","published":"2024-08-01T16:52:39Z","title":"Point-supervised Brain Tumor Segmentation with Box-prompted MedSAM","summary":"  Delineating lesions and anatomical structure is important for image-guided\ninterventions. Point-supervised medical image segmentation (PSS) has great\npotential to alleviate costly expert delineation labeling. However, due to the\nlack of precise size and boundary guidance, the effectiveness of PSS often\nfalls short of expectations. Although recent vision foundational models, such\nas the medical segment anything model (MedSAM), have made significant\nadvancements in bounding-box-prompted segmentation, it is not straightforward\nto utilize point annotation, and is prone to semantic ambiguity. In this\npreliminary study, we introduce an iterative framework to facilitate\nsemantic-aware point-supervised MedSAM. Specifically, the semantic box-prompt\ngenerator (SBPG) module has the capacity to convert the point input into\npotential pseudo bounding box suggestions, which are explicitly refined by the\nprototype-based semantic similarity. This is then succeeded by a prompt-guided\nspatial refinement (PGSR) module that harnesses the exceptional\ngeneralizability of MedSAM to infer the segmentation mask, which also updates\nthe box proposal seed in SBPG. Performance can be progressively improved with\nadequate iterations. We conducted an evaluation on BraTS2018 for the\nsegmentation of whole brain tumors and demonstrated its superior performance\ncompared to traditional PSS methods and on par with box-supervised methods.\n","authors":["Xiaofeng Liu","Jonghye Woo","Chao Ma","Jinsong Ouyang","Georges El Fakhri"],"pdf_url":"https://arxiv.org/pdf/2408.00706v1.pdf","comment":"2024 IEEE Nuclear Science Symposium and Medical Imaging Conference"},{"id":"http://arxiv.org/abs/2408.00700v1","updated":"2024-08-01T16:43:55Z","published":"2024-08-01T16:43:55Z","title":"You Can't Ignore Either: Unifying Structure and Feature Denoising for\n  Robust Graph Learning","summary":"  Recent research on the robustness of Graph Neural Networks (GNNs) under\nnoises or attacks has attracted great attention due to its importance in\nreal-world applications. Most previous methods explore a single noise source,\nrecovering corrupt node embedding by reliable structures bias or developing\nstructure learning with reliable node features. However, the noises and attacks\nmay come from both structures and features in graphs, making the graph\ndenoising a dilemma and challenging problem. In this paper, we develop a\nunified graph denoising (UGD) framework to unravel the deadlock between\nstructure and feature denoising. Specifically, a high-order neighborhood\nproximity evaluation method is proposed to recognize noisy edges, considering\nfeatures may be perturbed simultaneously. Moreover, we propose to refine noisy\nfeatures with reconstruction based on a graph auto-encoder. An iterative\nupdating algorithm is further designed to optimize the framework and acquire a\nclean graph, thus enabling robust graph learning for downstream tasks. Our UGD\nframework is self-supervised and can be easily implemented as a plug-and-play\nmodule. We carry out extensive experiments, which proves the effectiveness and\nadvantages of our method. Code is avalaible at\nhttps://github.com/YoungTimmy/UGD.\n","authors":["Tianmeng Yang","Jiahao Meng","Min Zhou","Yaming Yang","Yujing Wang","Xiangtai Li","Yunhai Tong"],"pdf_url":"https://arxiv.org/pdf/2408.00700v1.pdf","comment":"Accepted by CIKM'2024"},{"id":"http://arxiv.org/abs/2408.00699v1","updated":"2024-08-01T16:43:21Z","published":"2024-08-01T16:43:21Z","title":"Granular-Balls based Fuzzy Twin Support Vector Machine for\n  Classification","summary":"  The twin support vector machine (TWSVM) classifier has attracted increasing\nattention because of its low computational complexity. However, its performance\ntends to degrade when samples are affected by noise. The granular-ball fuzzy\nsupport vector machine (GBFSVM) classifier partly alleviates the adverse\neffects of noise, but it relies solely on the distance between the\ngranular-ball's center and the class center to design the granular-ball\nmembership function. In this paper, we first introduce the granular-ball twin\nsupport vector machine (GBTWSVM) classifier, which integrates granular-ball\ncomputing (GBC) with the twin support vector machine (TWSVM) classifier. By\nreplacing traditional point inputs with granular-balls, we demonstrate how to\nderive a pair of non-parallel hyperplanes for the GBTWSVM classifier by solving\na quadratic programming problem. Subsequently, we design the membership and\nnon-membership functions of granular-balls using Pythagorean fuzzy sets to\ndifferentiate the contributions of granular-balls in various regions.\nAdditionally, we develop the granular-ball fuzzy twin support vector machine\n(GBFTSVM) classifier by incorporating GBC with the fuzzy twin support vector\nmachine (FTSVM) classifier. We demonstrate how to derive a pair of non-parallel\nhyperplanes for the GBFTSVM classifier by solving a quadratic programming\nproblem. We also design algorithms for the GBTSVM classifier and the GBFTSVM\nclassifier. Finally, the superior classification performance of the GBTWSVM\nclassifier and the GBFTSVM classifier on 20 benchmark datasets underscores\ntheir scalability, efficiency, and robustness in tackling classification tasks.\n","authors":["Lixi Zhao","Weiping Ding","Duoqian Miao","Guangming Lang"],"pdf_url":"https://arxiv.org/pdf/2408.00699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00695v1","updated":"2024-08-01T16:39:06Z","published":"2024-08-01T16:39:06Z","title":"Accelerating Full Waveform Inversion By Transfer Learning","summary":"  Full waveform inversion (FWI) is a powerful tool for reconstructing material\nfields based on sparsely measured data obtained by wave propagation. For\nspecific problems, discretizing the material field with a neural network (NN)\nimproves the robustness and reconstruction quality of the corresponding\noptimization problem. We call this method NN-based FWI. Starting from an\ninitial guess, the weights of the NN are iteratively updated to fit the\nsimulated wave signals to the sparsely measured data set. For gradient-based\noptimization, a suitable choice of the initial guess, i.e., a suitable NN\nweight initialization, is crucial for fast and robust convergence.\n  In this paper, we introduce a novel transfer learning approach to further\nimprove NN-based FWI. This approach leverages supervised pretraining to provide\na better NN weight initialization, leading to faster convergence of the\nsubsequent optimization problem. Moreover, the inversions yield physically more\nmeaningful local minima. The network is pretrained to predict the unknown\nmaterial field using the gradient information from the first iteration of\nconventional FWI. In our computational experiments on two-dimensional domains,\nthe training data set consists of reference simulations with arbitrarily\npositioned elliptical voids of different shapes and orientations. We compare\nthe performance of the proposed transfer learning NN-based FWI with three other\nmethods: conventional FWI, NN-based FWI without pretraining and conventional\nFWI with an initial guess predicted from the pretrained NN. Our results show\nthat transfer learning NN-based FWI outperforms the other methods in terms of\nconvergence speed and reconstruction quality.\n","authors":["Divya Shyam Singh","Leon Herrmann","Qing Sun","Tim Bürchner","Felix Dietrich","Stefan Kollmannsberger"],"pdf_url":"https://arxiv.org/pdf/2408.00695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00681v1","updated":"2024-08-01T16:22:03Z","published":"2024-08-01T16:22:03Z","title":"Alpha-VI DeepONet: A prior-robust variational Bayesian approach for\n  enhancing DeepONets with uncertainty quantification","summary":"  We introduce a novel deep operator network (DeepONet) framework that\nincorporates generalised variational inference (GVI) using R\\'enyi's\n$\\alpha$-divergence to learn complex operators while quantifying uncertainty.\nBy incorporating Bayesian neural networks as the building blocks for the branch\nand trunk networks, our framework endows DeepONet with uncertainty\nquantification. The use of R\\'enyi's $\\alpha$-divergence, instead of the\nKullback-Leibler divergence (KLD), commonly used in standard variational\ninference, mitigates issues related to prior misspecification that are\nprevalent in Variational Bayesian DeepONets. This approach offers enhanced\nflexibility and robustness. We demonstrate that modifying the variational\nobjective function yields superior results in terms of minimising the mean\nsquared error and improving the negative log-likelihood on the test set. Our\nframework's efficacy is validated across various mechanical systems, where it\noutperforms both deterministic and standard KLD-based VI DeepONets in\npredictive accuracy and uncertainty quantification. The hyperparameter\n$\\alpha$, which controls the degree of robustness, can be tuned to optimise\nperformance for specific problems. We apply this approach to a range of\nmechanics problems, including gravity pendulum, advection-diffusion, and\ndiffusion-reaction systems. Our findings underscore the potential of\n$\\alpha$-VI DeepONet to advance the field of data-driven operator learning and\nits applications in engineering and scientific domains.\n","authors":["Soban Nasir Lone","Subhayan De","Rajdip Nayek"],"pdf_url":"https://arxiv.org/pdf/2408.00681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00676v1","updated":"2024-08-01T16:19:08Z","published":"2024-08-01T16:19:08Z","title":"An effect analysis of the balancing techniques on the counterfactual\n  explanations of student success prediction models","summary":"  In the past decade, we have experienced a massive boom in the usage of\ndigital solutions in higher education. Due to this boom, large amounts of data\nhave enabled advanced data analysis methods to support learners and examine\nlearning processes. One of the dominant research directions in learning\nanalytics is predictive modeling of learners' success using various machine\nlearning methods. To build learners' and teachers' trust in such methods and\nsystems, exploring the methods and methodologies that enable relevant\nstakeholders to deeply understand the underlying machine-learning models is\nnecessary. In this context, counterfactual explanations from explainable\nmachine learning tools are promising. Several counterfactual generation methods\nhold much promise, but the features must be actionable and causal to be\neffective. Thus, obtaining which counterfactual generation method suits the\nstudent success prediction models in terms of desiderata, stability, and\nrobustness is essential. Although a few studies have been published in recent\nyears on the use of counterfactual explanations in educational sciences, they\nhave yet to discuss which counterfactual generation method is more suitable for\nthis problem. This paper analyzed the effectiveness of commonly used\ncounterfactual generation methods, such as WhatIf Counterfactual Explanations,\nMulti-Objective Counterfactual Explanations, and Nearest Instance\nCounterfactual Explanations after balancing. This contribution presents a case\nstudy using the Open University Learning Analytics dataset to demonstrate the\npractical usefulness of counterfactual explanations. The results illustrate the\nmethod's effectiveness and describe concrete steps that could be taken to alter\nthe model's prediction.\n","authors":["Mustafa Cavus","Jakub Kuzilek"],"pdf_url":"https://arxiv.org/pdf/2408.00676v1.pdf","comment":"19 pages, 3 figures"},{"id":"http://arxiv.org/abs/2408.00674v1","updated":"2024-08-01T16:16:29Z","published":"2024-08-01T16:16:29Z","title":"ChordSync: Conformer-Based Alignment of Chord Annotations to Music Audio","summary":"  In the Western music tradition, chords are the main constituent components of\nharmony, a fundamental dimension of music. Despite its relevance for several\nMusic Information Retrieval (MIR) tasks, chord-annotated audio datasets are\nlimited and need more diversity. One way to improve those resources is to\nleverage the large number of chord annotations available online, but this\nrequires aligning them with music audio. However, existing audio-to-score\nalignment techniques, which typically rely on Dynamic Time Warping (DTW), fail\nto address this challenge, as they require weakly aligned data for precise\nsynchronisation. In this paper, we introduce ChordSync, a novel conformer-based\nmodel designed to seamlessly align chord annotations with audio, eliminating\nthe need for weak alignment. We also provide a pre-trained model and a\nuser-friendly library, enabling users to synchronise chord annotations with\naudio tracks effortlessly. In this way, ChordSync creates opportunities for\nharnessing crowd-sourced chord data for MIR, especially in audio chord\nestimation, thereby facilitating the generation of novel datasets.\nAdditionally, our system extends its utility to music education, enhancing\nmusic learning experiences by providing accurately aligned annotations, thus\nenabling learners to engage in synchronised musical practices.\n","authors":["Andrea Poltronieri","Valentina Presutti","Martín Rocamora"],"pdf_url":"https://arxiv.org/pdf/2408.00674v1.pdf","comment":"8 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2408.00665v1","updated":"2024-08-01T16:01:51Z","published":"2024-08-01T16:01:51Z","title":"AutoM3L: An Automated Multimodal Machine Learning Framework with Large\n  Language Models","summary":"  Automated Machine Learning (AutoML) offers a promising approach to streamline\nthe training of machine learning models. However, existing AutoML frameworks\nare often limited to unimodal scenarios and require extensive manual\nconfiguration. Recent advancements in Large Language Models (LLMs) have\nshowcased their exceptional abilities in reasoning, interaction, and code\ngeneration, presenting an opportunity to develop a more automated and\nuser-friendly framework. To this end, we introduce AutoM3L, an innovative\nAutomated Multimodal Machine Learning framework that leverages LLMs as\ncontrollers to automatically construct multimodal training pipelines. AutoM3L\ncomprehends data modalities and selects appropriate models based on user\nrequirements, providing automation and interactivity. By eliminating the need\nfor manual feature engineering and hyperparameter optimization, our framework\nsimplifies user engagement and enables customization through directives,\naddressing the limitations of previous rule-based AutoML approaches. We\nevaluate the performance of AutoM3L on six diverse multimodal datasets spanning\nclassification, regression, and retrieval tasks, as well as a comprehensive set\nof unimodal datasets. The results demonstrate that AutoM3L achieves competitive\nor superior performance compared to traditional rule-based AutoML methods.\nFurthermore, a user study highlights the user-friendliness and usability of our\nframework, compared to the rule-based AutoML methods.\n","authors":["Daqin Luo","Chengjian Feng","Yuxuan Nong","Yiqing Shen"],"pdf_url":"https://arxiv.org/pdf/2408.00665v1.pdf","comment":"Accpeted by ACMMM2024"},{"id":"http://arxiv.org/abs/2408.00662v1","updated":"2024-08-01T15:58:05Z","published":"2024-08-01T15:58:05Z","title":"Aligning Multiple Knowledge Graphs in a Single Pass","summary":"  Entity alignment (EA) is to identify equivalent entities across different\nknowledge graphs (KGs), which can help fuse these KGs into a more comprehensive\none. Previous EA methods mainly focus on aligning a pair of KGs, and to the\nbest of our knowledge, no existing EA method considers aligning multiple (more\nthan two) KGs. To fill this research gap, in this work, we study a novel\nproblem of aligning multiple KGs and propose an effective framework named\nMultiEA to solve the problem. First, we embed the entities of all the candidate\nKGs into a common feature space by a shared KG encoder. Then, we explore three\nalignment strategies to minimize the distances among pre-aligned entities. In\nparticular, we propose an innovative inference enhancement technique to improve\nthe alignment performance by incorporating high-order similarities. Finally, to\nverify the effectiveness of MultiEA, we construct two new real-world benchmark\ndatasets and conduct extensive experiments on them. The results show that our\nMultiEA can effectively and efficiently align multiple KGs in a single pass.\n","authors":["Yaming Yang","Zhe Wang","Ziyu Guan","Wei Zhao","Weigang Lu","Xinyan Huang"],"pdf_url":"https://arxiv.org/pdf/2408.00662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00657v1","updated":"2024-08-01T15:46:22Z","published":"2024-08-01T15:46:22Z","title":"Disentangling Dense Embeddings with Sparse Autoencoders","summary":"  Sparse autoencoders (SAEs) have shown promise in extracting interpretable\nfeatures from complex neural networks. We present one of the first applications\nof SAEs to dense text embeddings from large language models, demonstrating\ntheir effectiveness in disentangling semantic concepts. By training SAEs on\nembeddings of over 420,000 scientific paper abstracts from computer science and\nastronomy, we show that the resulting sparse representations maintain semantic\nfidelity while offering interpretability. We analyse these learned features,\nexploring their behaviour across different model capacities and introducing a\nnovel method for identifying ``feature families'' that represent related\nconcepts at varying levels of abstraction. To demonstrate the practical utility\nof our approach, we show how these interpretable features can be used to\nprecisely steer semantic search, allowing for fine-grained control over query\nsemantics. This work bridges the gap between the semantic richness of dense\nembeddings and the interpretability of sparse representations. We open source\nour embeddings, trained sparse autoencoders, and interpreted features, as well\nas a web app for exploring them.\n","authors":["Charles O'Neill","Christine Ye","Kartheik Iyer","John F. Wu"],"pdf_url":"https://arxiv.org/pdf/2408.00657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00652v1","updated":"2024-08-01T15:41:08Z","published":"2024-08-01T15:41:08Z","title":"Enhancing Multistep Prediction of Multivariate Market Indices Using\n  Weighted Optical Reservoir Computing","summary":"  We propose and experimentally demonstrate an innovative stock index\nprediction method using a weighted optical reservoir computing system. We\nconstruct fundamental market data combined with macroeconomic data and\ntechnical indicators to capture the broader behavior of the stock market. Our\napproach shows significant higher performance than state-of-the-art methods\nsuch as linear regression, decision trees, and neural network architectures\nincluding long short-term memory. It captures well the market's high volatility\nand nonlinear behaviors despite limited data, demonstrating great potential for\nreal-time, parallel, multi-dimensional data processing and predictions.\n","authors":["Fang Wang","Ting Bu","Yuping Huang"],"pdf_url":"https://arxiv.org/pdf/2408.00652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00641v1","updated":"2024-08-01T15:30:43Z","published":"2024-08-01T15:30:43Z","title":"Enhancing Ethereum Fraud Detection via Generative and Contrastive\n  Self-supervision","summary":"  The rampant fraudulent activities on Ethereum hinder the healthy development\nof the blockchain ecosystem, necessitating the reinforcement of regulations.\nHowever, multiple imbalances involving account interaction frequencies and\ninteraction types in the Ethereum transaction environment pose significant\nchallenges to data mining-based fraud detection research. To address this, we\nfirst propose the concept of meta-interactions to refine interaction behaviors\nin Ethereum, and based on this, we present a dual self-supervision enhanced\nEthereum fraud detection framework, named Meta-IFD. This framework initially\nintroduces a generative self-supervision mechanism to augment the interaction\nfeatures of accounts, followed by a contrastive self-supervision mechanism to\ndifferentiate various behavior patterns, and ultimately characterizes the\nbehavioral representations of accounts and mines potential fraud risks through\nmulti-view interaction feature learning. Extensive experiments on real Ethereum\ndatasets demonstrate the effectiveness and superiority of our framework in\ndetecting common Ethereum fraud behaviors such as Ponzi schemes and phishing\nscams. Additionally, the generative module can effectively alleviate the\ninteraction distribution imbalance in Ethereum data, while the contrastive\nmodule significantly enhances the framework's ability to distinguish different\nbehavior patterns. The source code will be released on GitHub soon.\n","authors":["Chenxiang Jin","Jiajun Zhou","Chenxuan Xie","Shanqing Yu","Qi Xuan","Xiaoniu Yang"],"pdf_url":"https://arxiv.org/pdf/2408.00641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00639v1","updated":"2024-08-01T15:26:24Z","published":"2024-08-01T15:26:24Z","title":"Privacy-preserving datasets by capturing feature distributions with\n  Conditional VAEs","summary":"  Large and well-annotated datasets are essential for advancing deep learning\napplications, however often costly or impossible to obtain by a single entity.\nIn many areas, including the medical domain, approaches relying on data sharing\nhave become critical to address those challenges. While effective in increasing\ndataset size and diversity, data sharing raises significant privacy concerns.\nCommonly employed anonymization methods based on the k-anonymity paradigm often\nfail to preserve data diversity, affecting model robustness. This work\nintroduces a novel approach using Conditional Variational Autoencoders (CVAEs)\ntrained on feature vectors extracted from large pre-trained vision foundation\nmodels. Foundation models effectively detect and represent complex patterns\nacross diverse domains, allowing the CVAE to faithfully capture the embedding\nspace of a given data distribution to generate (sample) a diverse,\nprivacy-respecting, and potentially unbounded set of synthetic feature vectors.\nOur method notably outperforms traditional approaches in both medical and\nnatural image domains, exhibiting greater dataset diversity and higher\nrobustness against perturbations while preserving sample privacy. These results\nunderscore the potential of generative models to significantly impact deep\nlearning applications in data-scarce and privacy-sensitive environments. The\nsource code is available at\nhttps://github.com/francescodisalvo05/cvae-anonymization .\n","authors":["Francesco Di Salvo","David Tafler","Sebastian Doerrich","Christian Ledig"],"pdf_url":"https://arxiv.org/pdf/2408.00639v1.pdf","comment":"Accepted at BMVC 2024"},{"id":"http://arxiv.org/abs/2408.00613v1","updated":"2024-08-01T14:53:11Z","published":"2024-08-01T14:53:11Z","title":"Unlocking Fair Use in the Generative AI Supply Chain: A Systematized\n  Literature Review","summary":"  Through a systematization of generative AI (GenAI) stakeholder goals and\nexpectations, this work seeks to uncover what value different stakeholders see\nin their contributions to the GenAI supply line. This valuation enables us to\nunderstand whether fair use advocated by GenAI companies to train model\nprogresses the copyright law objective of promoting science and arts. While\nassessing the validity and efficacy of the fair use argument, we uncover\nresearch gaps and potential avenues for future works for researchers and\npolicymakers to address.\n","authors":["Amruta Mahuli","Asia Biega"],"pdf_url":"https://arxiv.org/pdf/2408.00613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00611v1","updated":"2024-08-01T14:49:43Z","published":"2024-08-01T14:49:43Z","title":"Using CSNNs to Perform Event-based Data Processing & Classification on\n  ASL-DVS","summary":"  Recent advancements in bio-inspired visual sensing and neuromorphic computing\nhave led to the development of various highly efficient bio-inspired solutions\nwith real-world applications. One notable application integrates event-based\ncameras with spiking neural networks (SNNs) to process event-based sequences\nthat are asynchronous and sparse, making them difficult to handle. In this\nproject, we develop a convolutional spiking neural network (CSNN) architecture\nthat leverages convolutional operations and recurrent properties of a spiking\nneuron to learn the spatial and temporal relations in the ASL-DVS gesture\ndataset. The ASL-DVS gesture dataset is a neuromorphic dataset containing hand\ngestures when displaying 24 letters (A to Y, excluding J and Z due to the\nnature of their symbols) from the American Sign Language (ASL). We performed\nclassification on a pre-processed subset of the full ASL-DVS dataset to\nidentify letter signs and achieved 100\\% training accuracy. Specifically, this\nwas achieved by training in the Google Cloud compute platform while using a\nlearning rate of 0.0005, batch size of 25 (total of 20 batches), 200\niterations, and 10 epochs.\n","authors":["Ria Patel","Sujit Tripathy","Zachary Sublett","Seoyoung An","Riya Patel"],"pdf_url":"https://arxiv.org/pdf/2408.00611v1.pdf","comment":"8 pages, 14 figures"},{"id":"http://arxiv.org/abs/2408.00601v1","updated":"2024-08-01T14:35:24Z","published":"2024-08-01T14:35:24Z","title":"AutoPV: Automatically Design Your Photovoltaic Power Forecasting Model","summary":"  Photovoltaic power forecasting (PVPF) is a critical area in time series\nforecasting (TSF), enabling the efficient utilization of solar energy. With\nadvancements in machine learning and deep learning, various models have been\napplied to PVPF tasks. However, constructing an optimal predictive architecture\nfor specific PVPF tasks remains challenging, as it requires cross-domain\nknowledge and significant labor costs. To address this challenge, we introduce\nAutoPV, a novel framework for the automated search and construction of PVPF\nmodels based on neural architecture search (NAS) technology. We develop a brand\nnew NAS search space that incorporates various data processing techniques from\nstate-of-the-art (SOTA) TSF models and typical PVPF deep learning models. The\neffectiveness of AutoPV is evaluated on diverse PVPF tasks using a dataset from\nthe Daqing Photovoltaic Station in China. Experimental results demonstrate that\nAutoPV can complete the predictive architecture construction process in a\nrelatively short time, and the newly constructed architecture is superior to\nSOTA predefined models. This work bridges the gap in applying NAS to TSF\nproblems, assisting non-experts and industries in automatically designing\neffective PVPF models.\n","authors":["Dayin Chen","Xiaodan Shi","Mingkun Jiang","Haoran Zhang","Dongxiao Zhang","Yuntian Chen","Jinyue Yan"],"pdf_url":"https://arxiv.org/pdf/2408.00601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00573v1","updated":"2024-08-01T14:06:34Z","published":"2024-08-01T14:06:34Z","title":"Convergence Analysis of Natural Gradient Descent for Over-parameterized\n  Physics-Informed Neural Networks","summary":"  First-order methods, such as gradient descent (GD) and stochastic gradient\ndescent (SGD) have been proven effective in training neural networks. In the\nsetting of over-parameterization, there is a line of work demonstrating that\nrandomly initialized (stochastic) gradient descent converges to a globally\noptimal solution at a linear convergence rate for the quadratic loss function.\nHowever, the learning rate of GD in training two-layer neural networks has a\npoor dependence on the sample size and the Gram matrix, resulting in a slow\ntraining process. In this paper, we show that for the $L^2$ regression\nproblems, the learning rate can be improved from $\\mathcal{O}(\\lambda_0/n^2)$\nto $\\mathcal{O}(1/\\|\\bm{H}^{\\infty}\\|_2)$, which implies that GD enjoys a\nfaster convergence rate. Moreover, we further generalize the method for GD in\ntraining two-layer Physics-Informed Neural Networks (PINNs), showing a similar\nimprovement for the learning rate. Although the improved learning rate depends\nmildly on the Gram matrix, we still need to set it small enough in practice due\nto the agnostic eigenvalues of the Gram matrix. More importantly, the\nconvergence rate relies on the least eigenvalue of the Gram matrix, leading to\nslow convergence. In this work, we provide the convergence analysis of natural\ngradient descent (NGD) in training two-layer PINNs. We show that the learning\nrate can be $\\mathcal{O}(1)$ and at this time, the convergence rate is\nindependent of the Gram matrix.\n","authors":["Xianliang Xu","Ting Du","Wang Kong","Ye Li","Zhongyi Huang"],"pdf_url":"https://arxiv.org/pdf/2408.00573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00570v1","updated":"2024-08-01T14:03:11Z","published":"2024-08-01T14:03:11Z","title":"Analyzing the Effectiveness of Quantum Annealing with Meta-Learning","summary":"  The field of Quantum Computing has gathered significant popularity in recent\nyears and a large number of papers have studied its effectiveness in tackling\nmany tasks. We focus in particular on Quantum Annealing (QA), a meta-heuristic\nsolver for Quadratic Unconstrained Binary Optimization (QUBO) problems. It is\nknown that the effectiveness of QA is dependent on the task itself, as is the\ncase for classical solvers, but there is not yet a clear understanding of which\nare the characteristics of a problem that makes it difficult to solve with QA.\nIn this work, we propose a new methodology to study the effectiveness of QA\nbased on meta-learning models. To do so, we first build a dataset composed of\nmore than five thousand instances of ten different optimization problems. We\ndefine a set of more than a hundred features to describe their characteristics,\nand solve them with both QA and three classical solvers. We publish this\ndataset online for future research. Then, we train multiple meta-models to\npredict whether QA would solve that instance effectively and use them to probe\nwhich are the features with the strongest impact on the effectiveness of QA.\nOur results indicate that it is possible to accurately predict the\neffectiveness of QA, validating our methodology. Furthermore, we observe that\nthe distribution of the problem coefficients representing the bias and coupling\nterms is very informative to identify the probability of finding good\nsolutions, while the density of these coefficients alone is not enough. The\nmethodology we propose allows to open new research directions to further our\nunderstanding of the effectiveness of QA, by probing specific dimensions or by\ndeveloping new QUBO formulations that are better suited for the particular\nnature of QA. Furthermore, the proposed methodology is flexible and can be\nextended or used to study other quantum or classical solvers.\n","authors":["Riccardo Pellini","Maurizio Ferrari Dacrema"],"pdf_url":"https://arxiv.org/pdf/2408.00570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00549v1","updated":"2024-08-01T13:34:19Z","published":"2024-08-01T13:34:19Z","title":"Learning to Embed Distributions via Maximum Kernel Entropy","summary":"  Empirical data can often be considered as samples from a set of probability\ndistributions. Kernel methods have emerged as a natural approach for learning\nto classify these distributions. Although numerous kernels between\ndistributions have been proposed, applying kernel methods to distribution\nregression tasks remains challenging, primarily because selecting a suitable\nkernel is not straightforward. Surprisingly, the question of learning a\ndata-dependent distribution kernel has received little attention. In this\npaper, we propose a novel objective for the unsupervised learning of\ndata-dependent distribution kernel, based on the principle of entropy\nmaximization in the space of probability measure embeddings. We examine the\ntheoretical properties of the latent embedding space induced by our objective,\ndemonstrating that its geometric structure is well-suited for solving\ndownstream discriminative tasks. Finally, we demonstrate the performance of the\nlearned kernel across different modalities.\n","authors":["Oleksii Kachaiev","Stefano Recanatesi"],"pdf_url":"https://arxiv.org/pdf/2408.00549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00540v1","updated":"2024-08-01T13:23:15Z","published":"2024-08-01T13:23:15Z","title":"The Energy Cost of Artificial Intelligence of Things Lifecycle","summary":"  Artificial intelligence (AI)coupled with existing Internet of Things (IoT)\nenables more streamlined and autonomous operations across various economic\nsectors. Consequently, the paradigm of Artificial Intelligence of Things (AIoT)\nhaving AI techniques at its core implies additional energy and carbon costs\nthat may become significant with more complex neural architectures. To better\nunderstand the energy and Carbon Footprint (CF) of some AIoT components, very\nrecent studies employ conventional metrics. However, these metrics are not\ndesigned to capture energy efficiency aspects of inference. In this paper, we\npropose a new metric, the Energy Cost of AIoT Lifecycle (eCAL) to capture the\noverall energy cost of inference over the lifecycle of an AIoT system. We\ndevise a new methodology for determining eCAL of an AIoT system by analyzing\nthe complexity of data manipulation in individual components involved in the\nAIoT lifecycle and derive the overall and per bit energy consumption. With eCAL\nwe show that the better a model is and the more it is used, the more energy\nefficient an inference is. For an example AIoT configuration, eCAL for making\n$100$ inferences is $1.43$ times higher than for $1000$ inferences. We also\nevaluate the CF of the AIoT system by calculating the equivalent CO$_{2}$\nemissions based on the energy consumption and the Carbon Intensity (CI) across\ndifferent countries. Using 2023 renewable data, our analysis reveals that\ndeploying an AIoT system in Germany results in emitting $4.62$ times higher\nCO$_2$ than in Finland, due to latter using more low-CI energy sources.\n","authors":["Shih-Kai Chou","Jernej Hribar","Mihael Mohorčič","Carolina Fortuna"],"pdf_url":"https://arxiv.org/pdf/2408.00540v1.pdf","comment":"12 pages, 13 figures"},{"id":"http://arxiv.org/abs/2408.00531v1","updated":"2024-08-01T13:08:02Z","published":"2024-08-01T13:08:02Z","title":"ReSi: A Comprehensive Benchmark for Representational Similarity Measures","summary":"  Measuring the similarity of different representations of neural architectures\nis a fundamental task and an open research challenge for the machine learning\ncommunity. This paper presents the first comprehensive benchmark for evaluating\nrepresentational similarity measures based on well-defined groundings of\nsimilarity. The representational similarity (ReSi) benchmark consists of (i)\nsix carefully designed tests for similarity measures, (ii) 23 similarity\nmeasures, (iii) eleven neural network architectures, and (iv) six datasets,\nspanning over the graph, language, and vision domains. The benchmark opens up\nseveral important avenues of research on representational similarity that\nenable novel explorations and applications of neural architectures. We\ndemonstrate the utility of the ReSi benchmark by conducting experiments on\nvarious neural network architectures, real world datasets and similarity\nmeasures. All components of the benchmark are publicly available and thereby\nfacilitate systematic reproduction and production of research results. The\nbenchmark is extensible, future research can build on and further expand it. We\nbelieve that the ReSi benchmark can serve as a sound platform catalyzing future\nresearch that aims to systematically evaluate existing and explore novel ways\nof comparing representations of neural architectures.\n","authors":["Max Klabunde","Tassilo Wald","Tobias Schumacher","Klaus Maier-Hein","Markus Strohmaier","Florian Lemmerich"],"pdf_url":"https://arxiv.org/pdf/2408.00531v1.pdf","comment":"Feedback welcome! Code and data at https://github.com/mklabunde/resi"},{"id":"http://arxiv.org/abs/2408.00527v1","updated":"2024-08-01T12:58:19Z","published":"2024-08-01T12:58:19Z","title":"Contrastive Learning with Dynamic Localized Repulsion for Brain Age\n  Prediction on 3D Stiffness Maps","summary":"  In the field of neuroimaging, accurate brain age prediction is pivotal for\nuncovering the complexities of brain aging and pinpointing early indicators of\nneurodegenerative conditions. Recent advancements in self-supervised learning,\nparticularly in contrastive learning, have demonstrated greater robustness when\ndealing with complex datasets. However, current approaches often fall short in\ngeneralizing across non-uniformly distributed data, prevalent in medical\nimaging scenarios. To bridge this gap, we introduce a novel contrastive loss\nthat adapts dynamically during the training process, focusing on the localized\nneighborhoods of samples. Moreover, we expand beyond traditional structural\nfeatures by incorporating brain stiffness, a mechanical property previously\nunderexplored yet promising due to its sensitivity to age-related changes. This\nwork presents the first application of self-supervised learning to brain\nmechanical properties, using compiled stiffness maps from various clinical\nstudies to predict brain age. Our approach, featuring dynamic localized loss,\nconsistently outperforms existing state-of-the-art methods, demonstrating\nsuperior performance and laying the way for new directions in brain aging\nresearch.\n","authors":["Jakob Träuble","Lucy Hiscox","Curtis Johnson","Carola-Bibiane Schönlieb","Gabriele Kaminski Schierle","Angelica Aviles-Rivero"],"pdf_url":"https://arxiv.org/pdf/2408.00527v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00526v1","updated":"2024-08-01T12:57:35Z","published":"2024-08-01T12:57:35Z","title":"Hilbert curves for efficient exploratory landscape analysis\n  neighbourhood sampling","summary":"  Landscape analysis aims to characterise optimisation problems based on their\nobjective (or fitness) function landscape properties. The problem search space\nis typically sampled, and various landscape features are estimated based on the\nsamples. One particularly salient set of features is information content, which\nrequires the samples to be sequences of neighbouring solutions, such that the\nlocal relationships between consecutive sample points are preserved. Generating\nsuch spatially correlated samples that also provide good search space coverage\nis challenging. It is therefore common to first obtain an unordered sample with\ngood search space coverage, and then apply an ordering algorithm such as the\nnearest neighbour to minimise the distance between consecutive points in the\nsample. However, the nearest neighbour algorithm becomes computationally\nprohibitive in higher dimensions, thus there is a need for more efficient\nalternatives. In this study, Hilbert space-filling curves are proposed as a\nmethod to efficiently obtain high-quality ordered samples. Hilbert curves are a\nspecial case of fractal curves, and guarantee uniform coverage of a bounded\nsearch space while providing a spatially correlated sample. We study the\neffectiveness of Hilbert curves as samplers, and discover that they are capable\nof extracting salient features at a fraction of the computational cost compared\nto Latin hypercube sampling with post-factum ordering. Further, we investigate\nthe use of Hilbert curves as an ordering strategy, and find that they order the\nsample significantly faster than the nearest neighbour ordering, without\nsacrificing the saliency of the extracted features.\n","authors":["Johannes J. Pienaar","Anna S. Bosman","Katherine M. Malan"],"pdf_url":"https://arxiv.org/pdf/2408.00526v1.pdf","comment":"A version of this paper is published as conference proceedings of\n  EvoApps 2024"},{"id":"http://arxiv.org/abs/2408.00525v1","updated":"2024-08-01T12:57:12Z","published":"2024-08-01T12:57:12Z","title":"Identifying the Hierarchical Emotional Areas in the Human Brain Through\n  Information Fusion","summary":"  The brain basis of emotion has consistently received widespread attention,\nattracting a large number of studies to explore this cutting-edge topic.\nHowever, the methods employed in these studies typically only model the\npairwise relationship between two brain regions, while neglecting the\ninteractions and information fusion among multiple brain\nregions$\\unicode{x2014}$one of the key ideas of the psychological\nconstructionist hypothesis. To overcome the limitations of traditional methods,\nthis study provides an in-depth theoretical analysis of how to maximize\ninteractions and information fusion among brain regions. Building on the\nresults of this analysis, we propose to identify the hierarchical emotional\nareas in the human brain through multi-source information fusion and graph\nmachine learning methods. Comprehensive experiments reveal that the identified\nhierarchical emotional areas, from lower to higher levels, primarily facilitate\nthe fundamental process of emotion perception, the construction of basic\npsychological operations, and the coordination and integration of these\noperations. Overall, our findings provide unique insights into the brain\nmechanisms underlying specific emotions based on the psychological\nconstructionist hypothesis.\n","authors":["Zhongyu Huang","Changde Du","Chaozhuo Li","Kaicheng Fu","Huiguang He"],"pdf_url":"https://arxiv.org/pdf/2408.00525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00523v1","updated":"2024-08-01T12:54:46Z","published":"2024-08-01T12:54:46Z","title":"Jailbreaking Text-to-Image Models with LLM-Based Agents","summary":"  Recent advancements have significantly improved automated task-solving\ncapabilities using autonomous agents powered by large language models (LLMs).\nHowever, most LLM-based agents focus on dialogue, programming, or specialized\ndomains, leaving gaps in addressing generative AI safety tasks. These gaps are\nprimarily due to the challenges posed by LLM hallucinations and the lack of\nclear guidelines. In this paper, we propose Atlas, an advanced LLM-based\nmulti-agent framework that integrates an efficient fuzzing workflow to target\ngenerative AI models, specifically focusing on jailbreak attacks against\ntext-to-image (T2I) models with safety filters. Atlas utilizes a\nvision-language model (VLM) to assess whether a prompt triggers the T2I model's\nsafety filter. It then iteratively collaborates with both LLM and VLM to\ngenerate an alternative prompt that bypasses the filter. Atlas also enhances\nthe reasoning abilities of LLMs in attack scenarios by leveraging multi-agent\ncommunication, in-context learning (ICL) memory mechanisms, and the\nchain-of-thought (COT) approach. Our evaluation demonstrates that Atlas\nsuccessfully jailbreaks several state-of-the-art T2I models in a black-box\nsetting, which are equipped with multi-modal safety filters. In addition, Atlas\noutperforms existing methods in both query efficiency and the quality of the\ngenerated images.\n","authors":["Yingkai Dong","Zheng Li","Xiangtao Meng","Ning Yu","Shanqing Guo"],"pdf_url":"https://arxiv.org/pdf/2408.00523v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00516v1","updated":"2024-08-01T12:46:37Z","published":"2024-08-01T12:46:37Z","title":"Low-Power Vibration-Based Predictive Maintenance for Industry 4.0 using\n  Neural Networks: A Survey","summary":"  The advancements in smart sensors for Industry 4.0 offer ample opportunities\nfor low-powered predictive maintenance and condition monitoring. However,\ntraditional approaches in this field rely on processing in the cloud, which\nincurs high costs in energy and storage. This paper investigates the potential\nof neural networks for low-power on-device computation of vibration sensor data\nfor predictive maintenance. We review the literature on Spiking Neural Networks\n(SNNs) and Artificial Neuronal Networks (ANNs) for vibration-based predictive\nmaintenance by analyzing datasets, data preprocessing, network architectures,\nand hardware implementations. Our findings suggest that no satisfactory\nstandard benchmark dataset exists for evaluating neural networks in predictive\nmaintenance tasks. Furthermore frequency domain transformations are commonly\nemployed for preprocessing. SNNs mainly use shallow feed forward architectures,\nwhereas ANNs explore a wider range of models and deeper networks. Finally, we\nhighlight the need for future research on hardware implementations of neural\nnetworks for low-power predictive maintenance applications and the development\nof a standardized benchmark dataset.\n","authors":["Alexandru Vasilache","Sven Nitzsche","Daniel Floegel","Tobias Schuermann","Stefan von Dosky","Thomas Bierweiler","Marvin Mußler","Florian Kälber","Soeren Hohmann","Juergen Becker"],"pdf_url":"https://arxiv.org/pdf/2408.00516v1.pdf","comment":"The final version will be published at the ECML-PKDD 2024 joint\n  post-workshop proceeding in Springer Communications in Computer and\n  Information Science"},{"id":"http://arxiv.org/abs/2408.00513v1","updated":"2024-08-01T12:39:27Z","published":"2024-08-01T12:39:27Z","title":"VecAug: Unveiling Camouflaged Frauds with Cohort Augmentation for\n  Enhanced Detection","summary":"  Fraud detection presents a challenging task characterized by ever-evolving\nfraud patterns and scarce labeled data. Existing methods predominantly rely on\ngraph-based or sequence-based approaches. While graph-based approaches connect\nusers through shared entities to capture structural information, they remain\nvulnerable to fraudsters who can disrupt or manipulate these connections. In\ncontrast, sequence-based approaches analyze users' behavioral patterns,\noffering robustness against tampering but overlooking the interactions between\nsimilar users. Inspired by cohort analysis in retention and healthcare, this\npaper introduces VecAug, a novel cohort-augmented learning framework that\naddresses these challenges by enhancing the representation learning of target\nusers with personalized cohort information. To this end, we first propose a\nvector burn-in technique for automatic cohort identification, which retrieves a\ntask-specific cohort for each target user. Then, to fully exploit the cohort\ninformation, we introduce an attentive cohort aggregation technique for\naugmenting target user representations. To improve the robustness of such\ncohort augmentation, we also propose a novel label-aware cohort neighbor\nseparation mechanism to distance negative cohort neighbors and calibrate the\naggregated cohort information. By integrating this cohort information with\ntarget user representations, VecAug enhances the modeling capacity and\ngeneralization capabilities of the model to be augmented. Our framework is\nflexible and can be seamlessly integrated with existing fraud detection models.\nWe deploy our framework on e-commerce platforms and evaluate it on three fraud\ndetection datasets, and results show that VecAug improves the detection\nperformance of base models by up to 2.48\\% in AUC and 22.5\\% in R@P$_{0.9}$,\noutperforming state-of-the-art methods significantly.\n","authors":["Fei Xiao","Shaofeng Cai","Gang Chen","H. V. Jagadish","Beng Chin Ooi","Meihui Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00513v1.pdf","comment":"Accepted by KDD 2024"},{"id":"http://arxiv.org/abs/2408.00508v1","updated":"2024-08-01T12:28:22Z","published":"2024-08-01T12:28:22Z","title":"Block-Operations: Using Modular Routing to Improve Compositional\n  Generalization","summary":"  We explore the hypothesis that poor compositional generalization in neural\nnetworks is caused by difficulties with learning effective routing. To solve\nthis problem, we propose the concept of block-operations, which is based on\nsplitting all activation tensors in the network into uniformly sized blocks and\nusing an inductive bias to encourage modular routing and modification of these\nblocks. Based on this concept we introduce the Multiplexer, a new architectural\ncomponent that enhances the Feed Forward Neural Network (FNN). We\nexperimentally confirm that Multiplexers exhibit strong compositional\ngeneralization. On both a synthetic and a realistic task our model was able to\nlearn the underlying process behind the task, whereas both FNNs and\nTransformers were only able to learn heuristic approximations. We propose as\nfuture work to use the principles of block-operations to improve other existing\narchitectures.\n","authors":["Florian Dietz","Dietrich Klakow"],"pdf_url":"https://arxiv.org/pdf/2408.00508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00490v1","updated":"2024-08-01T11:51:52Z","published":"2024-08-01T11:51:52Z","title":"Graph Representation Learning via Causal Diffusion for\n  Out-of-Distribution Recommendation","summary":"  Graph Neural Networks (GNNs)-based recommendation algorithms typically assume\nthat training and testing data are drawn from independent and identically\ndistributed (IID) spaces. However, this assumption often fails in the presence\nof out-of-distribution (OOD) data, resulting in significant performance\ndegradation. In this study, we construct a Structural Causal Model (SCM) to\nanalyze interaction data, revealing that environmental confounders (e.g., the\nCOVID-19 pandemic) lead to unstable correlations in GNN-based models, thus\nimpairing their generalization to OOD data. To address this issue, we propose a\nnovel approach, graph representation learning via causal diffusion\n(CausalDiffRec) for OOD recommendation. This method enhances the model's\ngeneralization on OOD data by eliminating environmental confounding factors and\nlearning invariant graph representations. Specifically, we use backdoor\nadjustment and variational inference to infer the real environmental\ndistribution, thereby eliminating the impact of environmental confounders. This\ninferred distribution is then used as prior knowledge to guide the\nrepresentation learning in the reverse phase of the diffusion process to learn\nthe invariant representation. In addition, we provide a theoretical derivation\nthat proves optimizing the objective function of CausalDiffRec can encourage\nthe model to learn environment-invariant graph representations, thereby\nachieving excellent generalization performance in recommendations under\ndistribution shifts. Our extensive experiments validate the effectiveness of\nCausalDiffRec in improving the generalization of OOD data, and the average\nimprovement is up to 10.69% on Food, 18.83% on KuaiRec, 22.41% on Yelp2018, and\n11.65% on Douban datasets.\n","authors":["Chu Zhao","Enneng Yang","Yuliang Liang","Pengxiang Lan","Yuting Liu","Jianzhe Zhao","Guibing Guo","Xingwei Wang"],"pdf_url":"https://arxiv.org/pdf/2408.00490v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2408.00483v1","updated":"2024-08-01T11:39:45Z","published":"2024-08-01T11:39:45Z","title":"A Systematic Review on Long-Tailed Learning","summary":"  Long-tailed data is a special type of multi-class imbalanced data with a very\nlarge amount of minority/tail classes that have a very significant combined\ninfluence. Long-tailed learning aims to build high-performance models on\ndatasets with long-tailed distributions, which can identify all the classes\nwith high accuracy, in particular the minority/tail classes. It is a\ncutting-edge research direction that has attracted a remarkable amount of\nresearch effort in the past few years. In this paper, we present a\ncomprehensive survey of latest advances in long-tailed visual learning. We\nfirst propose a new taxonomy for long-tailed learning, which consists of eight\ndifferent dimensions, including data balancing, neural architecture, feature\nenrichment, logits adjustment, loss function, bells and whistles, network\noptimization, and post hoc processing techniques. Based on our proposed\ntaxonomy, we present a systematic review of long-tailed learning methods,\ndiscussing their commonalities and alignable differences. We also analyze the\ndifferences between imbalance learning and long-tailed learning approaches.\nFinally, we discuss prospects and future directions in this field.\n","authors":["Chongsheng Zhang","George Almpanidis","Gaojuan Fan","Binquan Deng","Yanbo Zhang","Ji Liu","Aouaidjia Kamel","Paolo Soda","João Gama"],"pdf_url":"https://arxiv.org/pdf/2408.00483v1.pdf","comment":"Current Under Revision at IEEE TNNLS. [This is the long/Full-length\n  version of our Long-Tailed Learning Survey paper]"},{"id":"http://arxiv.org/abs/2408.00465v1","updated":"2024-08-01T11:09:01Z","published":"2024-08-01T11:09:01Z","title":"Infrequent Resolving Algorithm for Online Linear Programming","summary":"  Online linear programming (OLP) has gained significant attention from both\nresearchers and practitioners due to its extensive applications, such as online\nauction, network revenue management and advertising. Existing OLP algorithms\nfall into two categories: LP-based algorithms and LP-free algorithms. The\nformer one typically guarantees better performance, even offering a constant\nregret, but requires solving a large number of LPs, which could be\ncomputationally expensive. In contrast, LP-free algorithm only requires\nfirst-order computations but induces a worse performance, lacking a constant\nregret bound. In this work, we bridge the gap between these two extremes by\nproposing an algorithm that achieves a constant regret while solving LPs only\n$O(\\log\\log T)$ times over the time horizon $T$. Moreover, when we are allowed\nto solve LPs only $M$ times, we propose an algorithm that can guarantee an\n$O\\left(T^{(1/2+\\epsilon)^{M-1}}\\right)$ regret. Furthermore, when the arrival\nprobabilities are known at the beginning, our algorithm can guarantee a\nconstant regret by solving LPs $O(\\log\\log T)$ times, and an\n$O\\left(T^{(1/2+\\epsilon)^{M}}\\right)$ regret by solving LPs only $M$ times.\nNumerical experiments are conducted to demonstrate the efficiency of the\nproposed algorithms.\n","authors":["Guokai Li","Zizhuo Wang","Jingwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00465v1.pdf","comment":"35 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.00462v1","updated":"2024-08-01T11:06:05Z","published":"2024-08-01T11:06:05Z","title":"Designing Efficient LLM Accelerators for Edge Devices","summary":"  The increase in open-source availability of Large Language Models (LLMs) has\nenabled users to deploy them on more and more resource-constrained edge devices\nto reduce reliance on network connections and provide more privacy. However,\nthe high computation and memory demands of LLMs make their execution on\nresource-constrained edge devices challenging and inefficient. To address this\nissue, designing new and efficient edge accelerators for LLM inference is\ncrucial. FPGA-based accelerators are ideal for LLM acceleration due to their\nreconfigurability, as they enable model-specific optimizations and higher\nperformance per watt. However, creating and integrating FPGA-based accelerators\nfor LLMs (particularly on edge devices) has proven challenging, mainly due to\nthe limited hardware design flows for LLMs in existing FPGA platforms.\n  To tackle this issue, in this paper we first propose a new design platform,\nnamed SECDA-LLM, that utilizes the SECDA methodology to streamline the process\nof designing, integrating, and deploying efficient FPGA-based LLM accelerators\nfor the llama.cpp inference framework. We then demonstrate, through a case\nstudy, the potential benefits of SECDA-LLM by creating a new MatMul accelerator\nthat supports block floating point quantized operations for LLMs. Our initial\naccelerator design, deployed on the PYNQ-Z1 board, reduces latency 1.7 seconds\nper token or ~2 seconds per word) by 11x over the dual-core Arm NEON-based CPU\nexecution for the TinyLlama model.\n","authors":["Jude Haris","Rappy Saha","Wenhao Hu","José Cano"],"pdf_url":"https://arxiv.org/pdf/2408.00462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00458v1","updated":"2024-08-01T10:55:20Z","published":"2024-08-01T10:55:20Z","title":"Reenact Anything: Semantic Video Motion Transfer Using Motion-Textual\n  Inversion","summary":"  Recent years have seen a tremendous improvement in the quality of video\ngeneration and editing approaches. While several techniques focus on editing\nappearance, few address motion. Current approaches using text, trajectories, or\nbounding boxes are limited to simple motions, so we specify motions with a\nsingle motion reference video instead. We further propose to use a pre-trained\nimage-to-video model rather than a text-to-video model. This approach allows us\nto preserve the exact appearance and position of a target object or scene and\nhelps disentangle appearance from motion. Our method, called motion-textual\ninversion, leverages our observation that image-to-video models extract\nappearance mainly from the (latent) image input, while the text/image embedding\ninjected via cross-attention predominantly controls motion. We thus represent\nmotion using text/image embedding tokens. By operating on an inflated\nmotion-text embedding containing multiple text/image embedding tokens per\nframe, we achieve a high temporal motion granularity. Once optimized on the\nmotion reference video, this embedding can be applied to various target images\nto generate videos with semantically similar motions. Our approach does not\nrequire spatial alignment between the motion reference video and target image,\ngeneralizes across various domains, and can be applied to various tasks such as\nfull-body and face reenactment, as well as controlling the motion of inanimate\nobjects and the camera. We empirically demonstrate the effectiveness of our\nmethod in the semantic video motion transfer task, significantly outperforming\nexisting methods in this context.\n","authors":["Manuel Kansy","Jacek Naruniec","Christopher Schroers","Markus Gross","Romann M. Weber"],"pdf_url":"https://arxiv.org/pdf/2408.00458v1.pdf","comment":"Preprint. All videos in this paper are best viewed as animations with\n  Acrobat Reader by pressing the highlighted frame of each video"},{"id":"http://arxiv.org/abs/2408.00439v1","updated":"2024-08-01T10:19:25Z","published":"2024-08-01T10:19:25Z","title":"Rapid and Power-Aware Learned Optimization for Modular Receive\n  Beamforming","summary":"  Multiple-input multiple-output (MIMO) systems play a key role in wireless\ncommunication technologies. A widely considered approach to realize scalable\nMIMO systems involves architectures comprised of multiple separate modules,\neach with its own beamforming capability. Such models accommodate cell-free\nmassive MIMO and partially connected hybrid MIMO architectures. A core issue\nwith the implementation of modular MIMO arises from the need to rapidly set the\nbeampatterns of the modules, while maintaining their power efficiency. This\nleads to challenging constrained optimization that should be repeatedly solved\non each coherence duration. In this work, we propose a power-oriented\noptimization algorithm for beamforming in uplink modular hybrid MIMO systems,\nwhich learns from data to operate rapidly. We derive our learned optimizer by\ntackling the rate maximization objective using projected gradient ascent steps\nwith momentum. We then leverage data to tune the hyperparameters of the\noptimizer, allowing it to operate reliably in a fixed and small number of\niterations while completely preserving its interpretable operation. We show how\npower efficient beamforming can be encouraged by the learned optimizer, via\nboosting architectures with low-resolution phase shifts and with deactivated\nanalog components. Numerical results show that our learn-to-optimize method\nnotably reduces the number of iterations and computation latency required to\nreliably tune modular MIMO receivers, and that it allows obtaining desirable\nbalances between power efficient designs and throughput.\n","authors":["Ohad Levy","Nir Shlezinger"],"pdf_url":"https://arxiv.org/pdf/2408.00439v1.pdf","comment":"Under review for possible publication in the IEEE"},{"id":"http://arxiv.org/abs/2408.00437v1","updated":"2024-08-01T10:16:57Z","published":"2024-08-01T10:16:57Z","title":"Efficient Patient Fine-Tuned Seizure Detection with a Tensor Kernel\n  Machine","summary":"  Recent developments in wearable devices have made accurate and efficient\nseizure detection more important than ever. A challenge in seizure detection is\nthat patient-specific models typically outperform patient-independent models.\nHowever, in a wearable device one typically starts with a patient-independent\nmodel, until such patient-specific data is available. To avoid having to\nconstruct a new classifier with this data, as required in conventional kernel\nmachines, we propose a transfer learning approach with a tensor kernel machine.\nThis method learns the primal weights in a compressed form using the canonical\npolyadic decomposition, making it possible to efficiently update the weights of\nthe patient-independent model with patient-specific data. The results show that\nthis patient fine-tuned model reaches as high a performance as a\npatient-specific SVM model with a model size that is twice as small as the\npatient-specific model and ten times as small as the patient-independent model.\n","authors":["Seline J. S. de Rooij","Frederiek Wesel","Borbála Hunyadi"],"pdf_url":"https://arxiv.org/pdf/2408.00437v1.pdf","comment":"5 pages, to be published in the EUSIPCO2024 conference proceedings"},{"id":"http://arxiv.org/abs/2408.00426v1","updated":"2024-08-01T09:57:48Z","published":"2024-08-01T09:57:48Z","title":"A Cross-Domain Benchmark for Active Learning","summary":"  Active Learning (AL) deals with identifying the most informative samples for\nlabeling to reduce data annotation costs for supervised learning tasks. AL\nresearch suffers from the fact that lifts from literature generalize poorly and\nthat only a small number of repetitions of experiments are conducted. To\novercome these obstacles, we propose \\emph{CDALBench}, the first active\nlearning benchmark which includes tasks in computer vision, natural language\nprocessing and tabular learning. Furthermore, by providing an efficient, greedy\noracle, \\emph{CDALBench} can be evaluated with 50 runs for each experiment. We\nshow, that both the cross-domain character and a large amount of repetitions\nare crucial for sophisticated evaluation of AL research. Concretely, we show\nthat the superiority of specific methods varies over the different domains,\nmaking it important to evaluate Active Learning with a cross-domain benchmark.\nAdditionally, we show that having a large amount of runs is crucial. With only\nconducting three runs as often done in the literature, the superiority of\nspecific methods can strongly vary with the specific runs. This effect is so\nstrong, that, depending on the seed, even a well-established method's\nperformance can be significantly better and significantly worse than random for\nthe same dataset.\n","authors":["Thorben Werner","Johannes Burchert","Maximilian Stubbemann","Lars Schmidt-Thieme"],"pdf_url":"https://arxiv.org/pdf/2408.00426v1.pdf","comment":"Updated version of paper \"Toward Comparable Active Learning\"\n  (arXiv:2311.18356). \"Toward Comparable Active Learning\" is deprecated, please\n  use this version"},{"id":"http://arxiv.org/abs/2408.00421v1","updated":"2024-08-01T09:46:06Z","published":"2024-08-01T09:46:06Z","title":"Towards Evolutionary-based Automated Machine Learning for Small Molecule\n  Pharmacokinetic Prediction","summary":"  Machine learning (ML) is revolutionising drug discovery by expediting the\nprediction of small molecule properties essential for developing new drugs.\nThese properties -- including absorption, distribution, metabolism and\nexcretion (ADME)-- are crucial in the early stages of drug development since\nthey provide an understanding of the course of the drug in the organism, i.e.,\nthe drug's pharmacokinetics. However, existing methods lack personalisation and\nrely on manually crafted ML algorithms or pipelines, which can introduce\ninefficiencies and biases into the process. To address these challenges, we\npropose a novel evolutionary-based automated ML method (AutoML) specifically\ndesigned for predicting small molecule properties, with a particular focus on\npharmacokinetics. Leveraging the advantages of grammar-based genetic\nprogramming, our AutoML method streamlines the process by automatically\nselecting algorithms and designing predictive pipelines tailored to the\nparticular characteristics of input molecular data. Results demonstrate\nAutoML's effectiveness in selecting diverse ML algorithms, resulting in\ncomparable or even improved predictive performances compared to conventional\napproaches. By offering personalised ML-driven pipelines, our method promises\nto enhance small molecule research in drug discovery, providing researchers\nwith a valuable tool for accelerating the development of novel therapeutic\ndrugs.\n","authors":["Alex G. C. de Sá","David B. Ascher"],"pdf_url":"https://arxiv.org/pdf/2408.00421v1.pdf","comment":"Paper accepted and presented at the 14th Workshop on Evolutionary\n  Computation for the Automated Design of Algorithms (ECADA), which happened\n  during the Genetic and Evolutionary Computation Conference (GECCO)"},{"id":"http://arxiv.org/abs/2408.00399v1","updated":"2024-08-01T09:11:08Z","published":"2024-08-01T09:11:08Z","title":"Unsupervised Pairwise Causal Discovery on Heterogeneous Data using\n  Mutual Information Measures","summary":"  A fundamental task in science is to determine the underlying causal relations\nbecause it is the knowledge of this functional structure what leads to the\ncorrect interpretation of an effect given the apparent associations in the\nobserved data. In this sense, Causal Discovery is a technique that tackles this\nchallenge by analyzing the statistical properties of the constituent variables.\nIn this work, we target the generalizability of the discovery method by\nfollowing a reductionist approach that only involves two variables, i.e., the\npairwise or bi-variate setting. We question the current (possibly misleading)\nbaseline results on the basis that they were obtained through supervised\nlearning, which is arguably contrary to this genuinely exploratory endeavor. In\nconsequence, we approach this problem in an unsupervised way, using robust\nMutual Information measures, and observing the impact of the different variable\ntypes, which is oftentimes ignored in the design of solutions. Thus, we provide\na novel set of standard unbiased results that can serve as a reference to guide\nfuture discovery tasks in completely unknown environments.\n","authors":["Alexandre Trilla","Nenad Mijatovic"],"pdf_url":"https://arxiv.org/pdf/2408.00399v1.pdf","comment":"26th International Conference of the Catalan Association for\n  Artificial Intelligence"},{"id":"http://arxiv.org/abs/2408.00386v1","updated":"2024-08-01T08:50:25Z","published":"2024-08-01T08:50:25Z","title":"What comes after transformers? -- A selective survey connecting ideas in\n  deep learning","summary":"  Transformers have become the de-facto standard model in artificial\nintelligence since 2017 despite numerous shortcomings ranging from energy\ninefficiency to hallucinations. Research has made a lot of progress in\nimproving elements of transformers, and, more generally, deep learning\nmanifesting in many proposals for architectures, layers, optimization\nobjectives, and optimization techniques. For researchers it is difficult to\nkeep track of such developments on a broader level. We provide a comprehensive\noverview of the many important, recent works in these areas to those who\nalready have a basic understanding of deep learning. Our focus differs from\nother works, as we target specifically novel, alternative potentially\ndisruptive approaches to transformers as well as successful ideas of recent\ndeep learning. We hope that such a holistic and unified treatment of\ninfluential, recent works and novel ideas helps researchers to form new\nconnections between diverse areas of deep learning. We identify and discuss\nmultiple patterns that summarize the key strategies for successful innovations\nover the last decade as well as works that can be seen as rising stars.\nEspecially, we discuss attempts on how to improve on transformers covering\n(partially) proven methods such as state space models but also including\nfar-out ideas in deep learning that seem promising despite not achieving\nstate-of-the-art results. We also cover a discussion on recent state-of-the-art\nmodels such as OpenAI's GPT series and Meta's LLama models and, Google's Gemini\nmodel family.\n","authors":["Johannes Schneider"],"pdf_url":"https://arxiv.org/pdf/2408.00386v1.pdf","comment":"This is an extended version of the published paper by Johannes\n  Schneider and Michalis Vlachos titled \"A survey of deep learning: From\n  activations to transformers'' which appeared at the International Conference\n  on Agents and Artificial Intelligence(ICAART) in 2024. It was selected for\n  post-publication and has been submitted to the post-publication proceedings"},{"id":"http://arxiv.org/abs/2408.00380v1","updated":"2024-08-01T08:41:13Z","published":"2024-08-01T08:41:13Z","title":"Enhancing Whole Slide Pathology Foundation Models through Stain\n  Normalization","summary":"  Recent advancements in digital pathology have led to the development of\nnumerous foundational models that utilize self-supervised learning on patches\nextracted from gigapixel whole slide images (WSIs). While this approach\nleverages vast amounts of unlabeled data, we have discovered a significant\nissue: features extracted from these self-supervised models tend to cluster by\nindividual WSIs, a phenomenon we term WSI-specific feature collapse. This\nproblem can potentially limit the model's generalization ability and\nperformance on various downstream tasks. To address this issue, we introduce\nStain Normalized Pathology Foundational Model, a novel foundational model\ntrained on patches that have undergone stain normalization. Stain normalization\nhelps reduce color variability arising from different laboratories and\nscanners, enabling the model to learn more consistent features. Stain\nNormalized Pathology Foundational Model is trained using 285,153,903 patches\nextracted from a total of 34,795 WSIs, combining data from The Cancer Genome\nAtlas (TCGA) and the Genotype-Tissue Expression (GTEx) project. Our experiments\ndemonstrate that Stain Normalized Pathology Foundational Model significantly\nmitigates the feature collapse problem, indicating that the model has learned\nmore generalized features rather than overfitting to individual WSI\ncharacteristics. We compared Stain Normalized Pathology Foundational Model with\nstate-of-the-art models across six downstream task datasets, and our results\nshow that \\name{} achieves excellent performance relative to the number of WSIs\nused and the model's parameter count. This suggests that the application of\nstain normalization has substantially improved the model's efficiency and\ngeneralization capabilities.\n","authors":["Juseung Yun","Yi Hu","Jinhyung Kim","Jongseong Jang","Soonyoung Lee"],"pdf_url":"https://arxiv.org/pdf/2408.00380v1.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2408.00376v1","updated":"2024-08-01T08:35:40Z","published":"2024-08-01T08:35:40Z","title":"On the Limitations and Prospects of Machine Unlearning for Generative AI","summary":"  Generative AI (GenAI), which aims to synthesize realistic and diverse data\nsamples from latent variables or other data modalities, has achieved remarkable\nresults in various domains, such as natural language, images, audio, and\ngraphs. However, they also pose challenges and risks to data privacy, security,\nand ethics. Machine unlearning is the process of removing or weakening the\ninfluence of specific data samples or features from a trained model, without\naffecting its performance on other data or tasks. While machine unlearning has\nshown significant efficacy in traditional machine learning tasks, it is still\nunclear if it could help GenAI become safer and aligned with human desire. To\nthis end, this position paper provides an in-depth discussion of the machine\nunlearning approaches for GenAI. Firstly, we formulate the problem of machine\nunlearning tasks on GenAI and introduce the background. Subsequently, we\nsystematically examine the limitations of machine unlearning on GenAI models by\nfocusing on the two representative branches: LLMs and image generative\n(diffusion) models. Finally, we provide our prospects mainly from three\naspects: benchmark, evaluation metrics, and utility-unlearning trade-off, and\nconscientiously advocate for the future development of this field.\n","authors":["Shiji Zhou","Lianzhe Wang","Jiangnan Ye","Yongliang Wu","Heng Chang"],"pdf_url":"https://arxiv.org/pdf/2408.00376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00374v1","updated":"2024-08-01T08:32:03Z","published":"2024-08-01T08:32:03Z","title":"Conformal Trajectory Prediction with Multi-View Data Integration in\n  Cooperative Driving","summary":"  Current research on trajectory prediction primarily relies on data collected\nby onboard sensors of an ego vehicle. With the rapid advancement in connected\ntechnologies, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure\n(V2I) communication, valuable information from alternate views becomes\naccessible via wireless networks. The integration of information from\nalternative views has the potential to overcome the inherent limitations\nassociated with a single viewpoint, such as occlusions and limited field of\nview. In this work, we introduce V2INet, a novel trajectory prediction\nframework designed to model multi-view data by extending existing single-view\nmodels. Unlike previous approaches where the multi-view data is manually fused\nor formulated as a separate training stage, our model supports end-to-end\ntraining, enhancing both flexibility and performance. Moreover, the predicted\nmultimodal trajectories are calibrated by a post-hoc conformal prediction\nmodule to get valid and efficient confidence regions. We evaluated the entire\nframework using the real-world V2I dataset V2X-Seq. Our results demonstrate\nsuperior performance in terms of Final Displacement Error (FDE) and Miss Rate\n(MR) using a single GPU. The code is publicly available at:\n\\url{https://github.com/xichennn/V2I_trajectory_prediction}.\n","authors":["Xi Chen","Rahul Bhadani","Larry Head"],"pdf_url":"https://arxiv.org/pdf/2408.00374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00359v1","updated":"2024-08-01T07:58:51Z","published":"2024-08-01T07:58:51Z","title":"Memorization Capacity for Additive Fine-Tuning with Small ReLU Networks","summary":"  Fine-tuning large pre-trained models is a common practice in machine learning\napplications, yet its mathematical analysis remains largely unexplored. In this\npaper, we study fine-tuning through the lens of memorization capacity. Our new\nmeasure, the Fine-Tuning Capacity (FTC), is defined as the maximum number of\nsamples a neural network can fine-tune, or equivalently, as the minimum number\nof neurons ($m$) needed to arbitrarily change $N$ labels among $K$ samples\nconsidered in the fine-tuning process. In essence, FTC extends the memorization\ncapacity concept to the fine-tuning scenario. We analyze FTC for the additive\nfine-tuning scenario where the fine-tuned network is defined as the summation\nof the frozen pre-trained network $f$ and a neural network $g$ (with $m$\nneurons) designed for fine-tuning. When $g$ is a ReLU network with either 2 or\n3 layers, we obtain tight upper and lower bounds on FTC; we show that $N$\nsamples can be fine-tuned with $m=\\Theta(N)$ neurons for 2-layer networks, and\nwith $m=\\Theta(\\sqrt{N})$ neurons for 3-layer networks, no matter how large $K$\nis. Our results recover the known memorization capacity results when $N = K$ as\na special case.\n","authors":["Jy-yong Sohn","Dohyun Kwon","Seoyeon An","Kangwook Lee"],"pdf_url":"https://arxiv.org/pdf/2408.00359v1.pdf","comment":"10 pages, 9 figures, UAI 2024"},{"id":"http://arxiv.org/abs/2408.00346v1","updated":"2024-08-01T07:31:23Z","published":"2024-08-01T07:31:23Z","title":"Neural Graph Matching for Video Retrieval in Large-Scale Video-driven\n  E-commerce","summary":"  With the rapid development of the short video industry, traditional\ne-commerce has encountered a new paradigm, video-driven e-commerce, which\nleverages attractive videos for product showcases and provides both video and\nitem services for users. Benefitting from the dynamic and visualized\nintroduction of items,video-driven e-commerce has shown huge potential in\nstimulating consumer confidence and promoting sales. In this paper, we focus on\nthe video retrieval task, facing the following challenges: (1) Howto handle the\nheterogeneities among users, items, and videos? (2)How to mine the\ncomplementarity between items and videos for better user understanding? In this\npaper, we first leverage the dual graph to model the co-existing of user-video\nand user-item interactions in video-driven e-commerce and innovatively reduce\nuser preference understanding to a graph matching problem. To solve it, we\nfurther propose a novel bi-level Graph Matching Network(GMN), which mainly\nconsists of node- and preference-level graph matching. Given a user, node-level\ngraph matching aims to match videos and items, while preference-level graph\nmatching aims to match multiple user preferences extracted from both videos and\nitems. Then the proposed GMN can generate and improve user embedding by\naggregating matched nodes or preferences from the dual graph in a bi-level\nmanner. Comprehensive experiments show the superiority of the proposed GMN with\nsignificant improvements over state-of-the-art approaches (e.g., AUC+1.9% and\nCTR+7.15%). We have developed it on a well-known video-driven e-commerce\nplatform, serving hundreds of millions of users every day\n","authors":["Houye Ji","Ye Tang","Zhaoxin Chen","Lixi Deng","Jun Hu","Lei Su"],"pdf_url":"https://arxiv.org/pdf/2408.00346v1.pdf","comment":null}],"Robotics":[{"id":"http://arxiv.org/abs/2407.21569v2","updated":"2024-08-01T16:43:34Z","published":"2024-07-31T12:52:13Z","title":"Analysis of Functional Insufficiencies and Triggering Conditions to\n  Improve the SOTIF of an MPC-based Trajectory Planner","summary":"  Automated and autonomous driving has made a significant technological leap\nover the past decade. In this process, the complexity of algorithms used for\nvehicle control has grown significantly. Model Predictive Control (MPC) is a\nprominent example, which has gained enormous popularity and is now widely used\nfor vehicle motion planning and control. However, safety concerns constrain its\npractical application, especially since traditional procedures of functional\nsafety (FS), with its universal standard ISO26262, reach their limits.\nConcomitantly, the new aspect of safety-of-the-intended-function (SOTIF) has\nmoved into the center of attention, whose standard, ISO21448, has only been\nreleased in 2022. Thus, experience with SOTIF is low and few case studies are\navailable in industry and research. Hence this paper aims to make two main\ncontributions: (1) an analysis of the SOTIF for a generic MPC-based trajectory\nplanner and (2) an interpretation and concrete application of the generic\nprocedures described in ISO21448 for determining functional insufficiencies\n(FIs) and triggering conditions (TCs). Particular novelties of the paper\ninclude an approach for the out-of-context development of SOTIF-related\nelements (SOTIF-EooC), a compilation of important FIs and TCs for a MPC-based\ntrajectory planner, and an optimized safety concept based on the identified FIs\nand TCs for the MPC-based trajectory planner.\n","authors":["Mirko Conrad","Georg Schildbach"],"pdf_url":"https://arxiv.org/pdf/2407.21569v2.pdf","comment":"Extended Version"},{"id":"http://arxiv.org/abs/2402.15402v2","updated":"2024-08-01T16:31:00Z","published":"2024-02-23T16:05:51Z","title":"Grasp, See and Place: Efficient Unknown Object Rearrangement with Policy\n  Structure Prior","summary":"  We focus on the task of unknown object rearrangement, where a robot is\nsupposed to re-configure the objects into a desired goal configuration\nspecified by an RGB-D image. Recent works explore unknown object rearrangement\nsystems by incorporating learning-based perception modules. However, they are\nsensitive to perception error, and pay less attention to task-level\nperformance. In this paper, we aim to develop an effective system for unknown\nobject rearrangement amidst perception noise. We theoretically reveal the noisy\nperception impacts grasp and place in a decoupled way, and show such a\ndecoupled structure is valuable to improve task optimality. We propose GSP, a\ndual-loop system with the decoupled structure as prior. For the inner loop, we\nlearn a see policy for self-confident in-hand object matching. For the outer\nloop, we learn a grasp policy aware of object matching and grasp capability\nguided by task-level rewards. We leverage the foundation model CLIP for object\nmatching, policy learning and self-termination. A series of experiments\nindicate that GSP can conduct unknown object rearrangement with higher\ncompletion rates and fewer steps.\n","authors":["Kechun Xu","Zhongxiang Zhou","Jun Wu","Haojian Lu","Rong Xiong","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2402.15402v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11114v2","updated":"2024-08-01T15:40:40Z","published":"2024-05-17T23:02:20Z","title":"Advancements in Gravity Compensation and Control for the da Vinci\n  Surgical Robot","summary":"  This research delves into the enhancement of control mechanisms for the da\nVinci Surgical System, focusing on the implementation of gravity compensation\nand refining the modeling of the master and patient side manipulators.\nLeveraging the Robot Operating System (ROS) the study aimed to fortify the\nprecision and stability of the robots movements essential for intricate\nsurgical procedures. Through rigorous parameter identification and the Euler\nLagrange approach the team successfully derived the necessary torque equations\nand established a robust mathematical model. Implementation of the actual robot\nand simulation in Gazebo highlighted the efficacy of the developed control\nstrategies facilitating accurate positioning and minimizing drift.\nAdditionally, the project extended its contributions by constructing a\ncomprehensive model for the patient side manipulator laying the groundwork for\nfuture research endeavors. This work signifies a significant advancement in the\npursuit of enhanced precision and user control in robotic assisted surgeries.\n  NOTE - This work has been submitted to the IEEE for publication. Copyright\nmay be transferred without notice, after which this version may no longer be\naccessible. Copyright on this article is reserved by IEEE\n","authors":["Ankit Shaw"],"pdf_url":"https://arxiv.org/pdf/2405.11114v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2403.08448v2","updated":"2024-08-01T15:16:26Z","published":"2024-03-13T12:03:27Z","title":"Actor-Critic Physics-informed Neural Lyapunov Control","summary":"  Designing control policies for stabilization tasks with provable guarantees\nis a long-standing problem in nonlinear control. A crucial performance metric\nis the size of the resulting region of attraction, which essentially serves as\na robustness \"margin\" of the closed-loop system against uncertainties. In this\npaper, we propose a new method to train a stabilizing neural network controller\nalong with its corresponding Lyapunov certificate, aiming to maximize the\nresulting region of attraction while respecting the actuation constraints.\nCrucial to our approach is the use of Zubov's Partial Differential Equation\n(PDE), which precisely characterizes the true region of attraction of a given\ncontrol policy. Our framework follows an actor-critic pattern where we\nalternate between improving the control policy (actor) and learning a Zubov\nfunction (critic). Finally, we compute the largest certifiable region of\nattraction by invoking an SMT solver after the training procedure. Our\nnumerical experiments on several design problems show consistent and\nsignificant improvements in the size of the resulting region of attraction.\n","authors":["Jiarui Wang","Mahyar Fazlyab"],"pdf_url":"https://arxiv.org/pdf/2403.08448v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04292v2","updated":"2024-08-01T13:15:21Z","published":"2024-07-05T06:54:26Z","title":"Corki: Enabling Real-time Embodied AI Robots via Algorithm-Architecture\n  Co-Design","summary":"  Embodied AI robots have the potential to fundamentally improve the way human\nbeings live and manufacture. Continued progress in the burgeoning field of\nusing large language models to control robots depends critically on an\nefficient computing substrate. In particular, today's computing systems for\nembodied AI robots are designed purely based on the interest of algorithm\ndevelopers, where robot actions are divided into a discrete frame-basis. Such\nan execution pipeline creates high latency and energy consumption. This paper\nproposes Corki, an algorithm-architecture co-design framework for real-time\nembodied AI robot control. Our idea is to decouple LLM inference, robotic\ncontrol and data communication in the embodied AI robots compute pipeline.\nInstead of predicting action for one single frame, Corki predicts the\ntrajectory for the near future to reduce the frequency of LLM inference. The\nalgorithm is coupled with a hardware that accelerates transforming trajectory\ninto actual torque signals used to control robots and an execution pipeline\nthat parallels data communication with computation. Corki largely reduces LLM\ninference frequency by up to 8.0x, resulting in up to 3.6x speed up. The\nsuccess rate improvement can be up to 17.3%. Code is provided for\nre-implementation. https://github.com/hyy0613/Corki\n","authors":["Yiyang Huang","Yuhui Hao","Bo Yu","Feng Yan","Yuxin Yang","Feng Min","Yinhe Han","Lin Ma","Shaoshan Liu","Qiang Liu","Yiming Gan"],"pdf_url":"https://arxiv.org/pdf/2407.04292v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08228v2","updated":"2024-08-01T12:59:42Z","published":"2024-03-13T04:11:41Z","title":"Empowering Robot Path Planning with Large Language Models: osmAG Map\n  Topology & Hierarchy Comprehension with LLMs","summary":"  Large Language Models (LLMs) have demonstrated great potential in robotic\napplications by providing essential general knowledge. Mobile robots rely on\nmap comprehension for tasks like localization and navigation. In this paper, we\nexplore enabling LLMs to comprehend the topology and hierarchy of Area Graph, a\ntext-based hierarchical, topometric semantic map representation utilizing\npolygons to demark areas such as rooms or buildings. Our experiments\ndemonstrate that with the right map representation, LLMs can effectively\ncomprehend Area Graph's topology and hierarchy. After straightforward\nfine-tuning, the LLaMA2 models exceeded ChatGPT-3.5 in mastering these aspects.\nOur dataset, dataset generation code, fine-tuned LoRA adapters can be accessed\nat https://github.com/xiefujing/LLM-osmAG-Comprehension.\n","authors":["Fujing Xie","Sören Schwertfeger"],"pdf_url":"https://arxiv.org/pdf/2403.08228v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11134v3","updated":"2024-08-01T11:46:55Z","published":"2023-09-20T08:30:53Z","title":"GNSS/Multi-Sensor Fusion Using Continuous-Time Factor Graph Optimization\n  for Robust Localization","summary":"  Accurate and robust vehicle localization in highly urbanized areas is\nchallenging. Sensors are often corrupted in those complicated and large-scale\nenvironments. This paper introduces GNSS-FGO, an online and global trajectory\nestimator that fuses GNSS observations alongside multiple sensor measurements\nfor robust vehicle localization. In GNSS-FGO, we fuse asynchronous sensor\nmeasurements into the graph with a continuous-time trajectory representation\nusing Gaussian process regression. This enables querying states at arbitrary\ntimestamps so that sensor observations are fused without requiring strict state\nand measurement synchronization. Thus, the proposed method presents a\ngeneralized factor graph for multi-sensor fusion. To evaluate and study\ndifferent GNSS fusion strategies, we fuse GNSS measurements in loose and tight\ncoupling with a speed sensor, IMU, and lidar-odometry. We employed datasets\nfrom measurement campaigns in Aachen, Duesseldorf, and Cologne in experimental\nstudies and presented comprehensive discussions on sensor observations,\nsmoother types, and hyperparameter tuning. Our results show that the proposed\napproach enables robust trajectory estimation in dense urban areas, where the\nclassic multi-sensor fusion method fails due to sensor degradation. In a test\nsequence containing a 17km route through Aachen, the proposed method results in\na mean 2D positioning error of 0.48m while fusing raw GNSS observations with\nlidar odometry in a tight coupling.\n","authors":["Haoming Zhang","Chih-Chun Chen","Heike Vallery","Timothy D. Barfoot"],"pdf_url":"https://arxiv.org/pdf/2309.11134v3.pdf","comment":"Accepted for publication in the IEEE Transactions on Robotics"},{"id":"http://arxiv.org/abs/2407.21500v2","updated":"2024-08-01T08:23:53Z","published":"2024-07-31T10:14:18Z","title":"DIABLO: A 6-DoF Wheeled Bipedal Robot Composed Entirely of Direct-Drive\n  Joints","summary":"  Wheeled bipedal robots offer the advantages of both wheeled and legged\nrobots, combining the ability to traverse a wide range of terrains and\nenvironments with high efficiency. However, the conventional approach in\nexisting wheeled bipedal robots involves motor-driven joints with high-ratio\ngearboxes. While this approach provides specific benefits, it also presents\nseveral challenges, including increased mechanical complexity, efficiency\nlosses, noise, vibrations, and higher maintenance and lubrication requirements.\nAddressing the aforementioned concerns, we developed a direct-drive wheeled\nbipedal robot called DIABLO, which eliminates the use of gearboxes entirely.\nOur robotic system is simplified as a second-order inverted pendulum, and we\nhave designed an LQR-based balance controller to ensure stability.\nAdditionally, we implemented comprehensive motion controller, including yaw,\nsplit-angle, height, and roll controllers. Through expriments in simulations\nand real-world prototype, we have demonstrated that our platform achieves\nsatisfactory performance.\n","authors":["Dingchuan Liu","Fangfang Yang","Xuanhong Liao","Ximin Lyu"],"pdf_url":"https://arxiv.org/pdf/2407.21500v2.pdf","comment":"This paper has already been accepted by IROS 2024"},{"id":"http://arxiv.org/abs/2210.03437v2","updated":"2024-08-01T06:02:27Z","published":"2022-10-07T10:13:30Z","title":"KRF: Keypoint Refinement with Fusion Network for 6D Pose Estimation","summary":"  Some robust point cloud registration approaches with controllable pose\nrefinement magnitude, such as ICP and its variants, are commonly used to\nimprove 6D pose estimation accuracy. However, the effectiveness of these\nmethods gradually diminishes with the advancement of deep learning techniques\nand the enhancement of initial pose accuracy, primarily due to their lack of\nspecific design for pose refinement. In this paper, we propose Point Cloud\nCompletion and Keypoint Refinement with Fusion Data (PCKRF), a new pose\nrefinement pipeline for 6D pose estimation. The pipeline consists of two steps.\nFirst, it completes the input point clouds via a novel pose-sensitive point\ncompletion network. The network uses both local and global features with pose\ninformation during point completion. Then, it registers the completed object\npoint cloud with the corresponding target point cloud by our proposed Color\nsupported Iterative KeyPoint (CIKP) method. The CIKP method introduces color\ninformation into registration and registers a point cloud around each keypoint\nto increase stability. The PCKRF pipeline can be integrated with existing\npopular 6D pose estimation methods, such as the full flow bidirectional fusion\nnetwork, to further improve their pose estimation accuracy. Experiments\ndemonstrate that our method exhibits superior stability compared to existing\napproaches when optimizing initial poses with relatively high precision.\nNotably, the results indicate that our method effectively complements most\nexisting pose estimation techniques, leading to improved performance in most\ncases. Furthermore, our method achieves promising results even in challenging\nscenarios involving textureless and symmetrical objects. Our source code is\navailable at https://github.com/zhanhz/KRF.\n","authors":["Yiheng Han","Irvin Haozhe Zhan","Long Zeng","Yu-Ping Wang","Ran Yi","Minjing Yu","Matthieu Gaetan Lin","Jenny Sheng","Yong-Jin Liu"],"pdf_url":"https://arxiv.org/pdf/2210.03437v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08028v2","updated":"2024-08-01T01:01:45Z","published":"2024-07-10T20:11:29Z","title":"AutoMate: Specialist and Generalist Assembly Policies over Diverse\n  Geometries","summary":"  Robotic assembly for high-mixture settings requires adaptivity to diverse\nparts and poses, which is an open challenge. Meanwhile, in other areas of\nrobotics, large models and sim-to-real have led to tremendous progress.\nInspired by such work, we present AutoMate, a learning framework and system\nthat consists of 4 parts: 1) a dataset of 100 assemblies compatible with\nsimulation and the real world, along with parallelized simulation environments\nfor policy learning, 2) a novel simulation-based approach for learning\nspecialist (i.e., part-specific) policies and generalist (i.e., unified)\nassembly policies, 3) demonstrations of specialist policies that individually\nsolve 80 assemblies with 80% or higher success rates in simulation, as well as\na generalist policy that jointly solves 20 assemblies with an 80%+ success\nrate, and 4) zero-shot sim-to-real transfer that achieves similar (or better)\nperformance than simulation, including on perception-initialized assembly. The\nkey methodological takeaway is that a union of diverse algorithms from\nmanufacturing engineering, character animation, and time-series analysis\nprovides a generic and robust solution for a diverse range of robotic assembly\nproblems. To our knowledge, AutoMate provides the first simulation-based\nframework for learning specialist and generalist policies over a wide range of\nassemblies, as well as the first system demonstrating zero-shot sim-to-real\ntransfer over such a range. For videos and additional details, please see our\nproject website: https://bingjietang718.github.io/automate/\n","authors":["Bingjie Tang","Iretiayo Akinola","Jie Xu","Bowen Wen","Ankur Handa","Karl Van Wyk","Dieter Fox","Gaurav S. Sukhatme","Fabio Ramos","Yashraj Narang"],"pdf_url":"https://arxiv.org/pdf/2407.08028v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00642v1","updated":"2024-08-01T15:33:16Z","published":"2024-08-01T15:33:16Z","title":"Coverage Path Planning For Minimizing Expected Time to Search For an\n  Object With Continuous Sensing","summary":"  In this paper, we present several results of both theoretical as well as\npractical interests. First, we propose the quota lawn mowing problem, an\nextension of the classic lawn mowing problem in computational geometry, as\nfollows: given a quota of coverage, compute the shortest lawn mowing route to\nachieve said quota. We give constant-factor approximations for the quota lawn\nmowing problem.\n  Second, we investigate the expected detection time minimization problem in\ngeometric coverage path planning with local, continuous sensory information. We\nprovide the first approximation algorithm with provable error bounds with\npseudopolynomial running time. Our ideas also extend to another search\nmechanism, namely visibility-based search, which is related to the watchman\nroute problem. We complement our theoretical analysis with some simple but\neffective heuristics for finding an object in minimum expected time, on which\nwe provide simulation results.\n","authors":["Linh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2408.00642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00638v1","updated":"2024-08-01T15:25:44Z","published":"2024-08-01T15:25:44Z","title":"CrystalTac: 3D-Printed Vision-Based Tactile Sensor Family through Rapid\n  Monolithic Manufacturing Technique","summary":"  Recently, vision-based tactile sensors (VBTSs) have gained popularity in\nrobotics systems. The sensing mechanisms of most VBTSs can be categorised based\non the type of tactile features they capture. Each category requires specific\nstructural designs to convert physical contact into optical information. The\ncomplex architectures of VBTSs pose challenges for traditional manufacturing\ntechniques in terms of design flexibility, cost-effectiveness, and quality\nstability. Previous research has shown that monolithic manufacturing using\nmulti-material 3D printing technology can partially address these challenges.\nThis study introduces the CrystalTac family, a series of VBTSs designed with a\nunique sensing mechanism and fabricated through rapid monolithic manufacturing.\nCase studies on CrystalTac-type sensors demonstrate their effective performance\nin tasks involving tactile perception, along with impressive cost-effectiveness\nand design flexibility. The CrystalTac family aims to highlight the potential\nof monolithic manufacturing in VBTS development and inspire further research in\ntactile sensing and manipulation.\n","authors":["Wen Fan","Haoran Li","Dandan Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00638v1.pdf","comment":"32 pages, 12 figures"},{"id":"http://arxiv.org/abs/2408.00610v1","updated":"2024-08-01T14:48:46Z","published":"2024-08-01T14:48:46Z","title":"In-Hand Singulation and Scooping Manipulation with a 5 DOF Tactile\n  Gripper","summary":"  Manipulation tasks often require a high degree of dexterity, typically\nnecessitating grippers with multiple degrees of freedom (DoF). While a robotic\nhand equipped with multiple fingers can execute precise and intricate\nmanipulation tasks, the inherent redundancy stemming from its extensive DoF\noften adds unnecessary complexity. In this paper, we introduce the design of a\ntactile sensor-equipped gripper with two fingers and five DoF. We present a\nnovel design integrating a GelSight tactile sensor, enhancing sensing\ncapabilities and enabling finer control during specific manipulation tasks. To\nevaluate the gripper's performance, we conduct experiments involving two\nchallenging tasks: 1) retrieving, singularizing, and classification of various\nobjects embedded in granular media, and 2) executing scooping manipulations of\ncredit cards in confined environments to achieve precise insertion. Our results\ndemonstrate the efficiency of the proposed approach, with a high success rate\nfor singulation and classification tasks, particularly for spherical objects at\nhigh as 94.3%, and a 100% success rate for scooping and inserting credit cards.\n","authors":["Yuhao Zhou","Pokuang Zhou","Shaoxiong Wang","Yu She"],"pdf_url":"https://arxiv.org/pdf/2408.00610v1.pdf","comment":"6 pages, 6 figures, accepted to the 2024 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2024). Video is available\n  at: https://youtu.be/6c1AyeaGjbk"},{"id":"http://arxiv.org/abs/2408.00606v1","updated":"2024-08-01T14:43:20Z","published":"2024-08-01T14:43:20Z","title":"U2UData: A Large-scale Cooperative Perception Dataset for Swarm UAVs\n  Autonomous Flight","summary":"  Modern perception systems for autonomous flight are sensitive to occlusion\nand have limited long-range capability, which is a key bottleneck in improving\nlow-altitude economic task performance. Recent research has shown that the\nUAV-to-UAV (U2U) cooperative perception system has great potential to\nrevolutionize the autonomous flight industry. However, the lack of a\nlarge-scale dataset is hindering progress in this area. This paper presents\nU2UData, the first large-scale cooperative perception dataset for swarm UAVs\nautonomous flight. The dataset was collected by three UAVs flying autonomously\nin the U2USim, covering a 9 km$^2$ flight area. It comprises 315K LiDAR frames,\n945K RGB and depth frames, and 2.41M annotated 3D bounding boxes for 3 classes.\nIt also includes brightness, temperature, humidity, smoke, and airflow values\ncovering all flight routes. U2USim is the first real-world mapping swarm UAVs\nsimulation environment. It takes Yunnan Province as the prototype and includes\n4 terrains, 7 weather conditions, and 8 sensor types. U2UData introduces two\nperception tasks: cooperative 3D object detection and cooperative 3D object\ntracking. This paper provides comprehensive benchmarks of recent cooperative\nperception algorithms on these tasks.\n","authors":["Tongtong Feng","Xin Wang","Feilin Han","Leping Zhang","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.00606v1.pdf","comment":"ACM MM"},{"id":"http://arxiv.org/abs/2408.00545v1","updated":"2024-08-01T13:29:26Z","published":"2024-08-01T13:29:26Z","title":"Collecting Larg-Scale Robotic Datasets on a High-Speed Mobile Platform","summary":"  Mobile robotics datasets are essential for research on robotics, for example\nfor research on Simultaneous Localization and Mapping (SLAM). Therefore the\nShanghaiTech Mapping Robot was constructed, that features a multitude\nhigh-performance sensors and a 16-node cluster to collect all this data. That\nrobot is based on a Clearpath Husky mobile base with a maximum speed of 1 meter\nper second. This is fine for indoor datasets, but to collect large-scale\noutdoor datasets a faster platform is needed. This system paper introduces our\nhigh-speed mobile platform for data collection. The mapping robot is secured on\nthe rear-steered flatbed car with maximum field of view. Additionally two\nencoders collect odometry data from two of the car wheels and an external\nsensor plate houses a downlooking RGB and event camera. With this setup a\ndataset of more than 10km in the underground parking garage and the outside of\nour campus was collected and is published with this paper.\n","authors":["Yuxin Lin","Jiaxuan Ma","Sizhe Gu","Jipeng Kong","Bowen Xu","Xiting Zhao","Dengji Zhao","Wenhan Cao","Sören Schwertfeger"],"pdf_url":"https://arxiv.org/pdf/2408.00545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00538v1","updated":"2024-08-01T13:21:34Z","published":"2024-08-01T13:21:34Z","title":"High-Quality, ROS Compatible Video Encoding and Decoding for\n  High-Definition Datasets","summary":"  Robotic datasets are important for scientific benchmarking and developing\nalgorithms, for example for Simultaneous Localization and Mapping (SLAM).\nModern robotic datasets feature video data of high resolution and high\nframerates. Storing and sharing those datasets becomes thus very costly,\nespecially if more than one camera is used for the datasets. It is thus\nessential to store this video data in a compressed format. This paper\ninvestigates the use of modern video encoders for robotic datasets. We provide\na software that can replay mp4 videos within ROS 1 and ROS 2 frameworks,\nsupporting the synchronized playback in simulated time. Furthermore, the paper\nevaluates different encoders and their settings to find optimal configurations\nin terms of resulting size, quality and encoding time. Through this work we\nshow that it is possible to store and share even highest quality video datasets\nwithin reasonable storage constraints.\n","authors":["Jian Li","Bowen Xu","Sören Schwertfeger"],"pdf_url":"https://arxiv.org/pdf/2408.00538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00494v1","updated":"2024-08-01T11:58:35Z","published":"2024-08-01T11:58:35Z","title":"Chance-Constrained Information-Theoretic Stochastic Model Predictive\n  Control with Safety Shielding","summary":"  This paper introduces a novel nonlinear stochastic model predictive control\npath integral (MPPI) method, which considers chance constraints on system\nstates. The proposed belief-space stochastic MPPI (BSS-MPPI) applies\nMonte-Carlo sampling to evaluate state distributions resulting from underlying\nsystematic disturbances, and utilizes a Control Barrier Function (CBF) inspired\nheuristic in belief space to fulfill the specified chance constraints. Compared\nto several previous stochastic predictive control methods, our approach applies\nto general nonlinear dynamics without requiring the computationally expensive\nsystem linearization step. Moreover, the BSS-MPPI controller can solve\noptimization problems without limiting the form of the objective function and\nchance constraints. By multi-threading the sampling process using a GPU, we can\nachieve fast real-time planning for time- and safety-critical tasks such as\nautonomous racing. Our results on a realistic race-car simulation study show\nsignificant reductions in constraint violation compared to some of the prior\nMPPI approaches, while being comparable in computation times.\n","authors":["Ji yin","Panagiotis Tsiotras","Karl Berntorp"],"pdf_url":"https://arxiv.org/pdf/2408.00494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00486v1","updated":"2024-08-01T11:45:26Z","published":"2024-08-01T11:45:26Z","title":"SF-TIM: A Simple Framework for Enhancing Quadrupedal Robot Jumping\n  Agility by Combining Terrain Imagination and Measurement","summary":"  Dynamic jumping on high platforms and over gaps differentiates legged robots\nfrom wheeled counterparts. Compared to walking on rough terrains, dynamic\nlocomotion on abrupt surfaces requires fusing proprioceptive and exteroceptive\nperception for explosive movements. In this paper, we propose SF-TIM (Simple\nFramework combining Terrain Imagination and Measurement), a single-policy\nmethod that enhances quadrupedal robot jumping agility, while preserving their\nfundamental blind walking capabilities. In addition, we introduce a\nterrain-guided reward design specifically to assist quadrupedal robots in high\njumping, improving their performance in this task. To narrow the\nsimulation-to-reality gap in quadrupedal robot learning, we introduce a stable\nand high-speed elevation map generation framework, enabling zero-shot\nsimulation-to-reality transfer of locomotion ability. Our algorithm has been\ndeployed and validated on both the small-/large-size quadrupedal robots,\ndemonstrating its effectiveness in real-world applications: the robot has\nsuccessfully traversed various high platforms and gaps, showing the robustness\nof our proposed approach. A demo video has been made available at\nhttps://flysoaryun.github.io/SF-TIM.\n","authors":["Ze Wang","Yang Li","Long Xu","Hao Shi","Zunwang Ma","Zhen Chu","Chao Li","Fei Gao","Kailun Yang","Kaiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2408.00486v1.pdf","comment":"A demo video has been made available at\n  https://flysoaryun.github.io/SF-TIM"},{"id":"http://arxiv.org/abs/2408.00415v1","updated":"2024-08-01T09:32:01Z","published":"2024-08-01T09:32:01Z","title":"DriveArena: A Closed-loop Generative Simulation Platform for Autonomous\n  Driving","summary":"  This paper presented DriveArena, the first high-fidelity closed-loop\nsimulation system designed for driving agents navigating in real scenarios.\nDriveArena features a flexible, modular architecture, allowing for the seamless\ninterchange of its core components: Traffic Manager, a traffic simulator\ncapable of generating realistic traffic flow on any worldwide street map, and\nWorld Dreamer, a high-fidelity conditional generative model with infinite\nautoregression. This powerful synergy empowers any driving agent capable of\nprocessing real-world images to navigate in DriveArena's simulated environment.\nThe agent perceives its surroundings through images generated by World Dreamer\nand output trajectories. These trajectories are fed into Traffic Manager,\nachieving realistic interactions with other vehicles and producing a new scene\nlayout. Finally, the latest scene layout is relayed back into World Dreamer,\nperpetuating the simulation cycle. This iterative process fosters closed-loop\nexploration within a highly realistic environment, providing a valuable\nplatform for developing and evaluating driving agents across diverse and\nchallenging scenarios. DriveArena signifies a substantial leap forward in\nleveraging generative image data for the driving simulation platform, opening\ninsights for closed-loop autonomous driving. Code will be available soon on\nGitHub: https://github.com/PJLab-ADG/DriveArena\n","authors":["Xuemeng Yang","Licheng Wen","Yukai Ma","Jianbiao Mei","Xin Li","Tiantian Wei","Wenjie Lei","Daocheng Fu","Pinlong Cai","Min Dou","Botian Shi","Liang He","Yong Liu","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2408.00415v1.pdf","comment":"19 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.00370v1","updated":"2024-08-01T08:22:47Z","published":"2024-08-01T08:22:47Z","title":"DiM-Gesture: Co-Speech Gesture Generation with Adaptive Layer\n  Normalization Mamba-2 framework","summary":"  Speech-driven gesture generation is an emerging domain within virtual human\ncreation, where current methods predominantly utilize Transformer-based\narchitectures that necessitate extensive memory and are characterized by slow\ninference speeds. In response to these limitations, we propose\n\\textit{DiM-Gestures}, a novel end-to-end generative model crafted to create\nhighly personalized 3D full-body gestures solely from raw speech audio,\nemploying Mamba-based architectures. This model integrates a Mamba-based fuzzy\nfeature extractor with a non-autoregressive Adaptive Layer Normalization\n(AdaLN) Mamba-2 diffusion architecture. The extractor, leveraging a Mamba\nframework and a WavLM pre-trained model, autonomously derives implicit,\ncontinuous fuzzy features, which are then unified into a singular latent\nfeature. This feature is processed by the AdaLN Mamba-2, which implements a\nuniform conditional mechanism across all tokens to robustly model the interplay\nbetween the fuzzy features and the resultant gesture sequence. This innovative\napproach guarantees high fidelity in gesture-speech synchronization while\nmaintaining the naturalness of the gestures. Employing a diffusion model for\ntraining and inference, our framework has undergone extensive subjective and\nobjective evaluations on the ZEGGS and BEAT datasets. These assessments\nsubstantiate our model's enhanced performance relative to contemporary\nstate-of-the-art methods, demonstrating competitive outcomes with the DiTs\narchitecture (Persona-Gestors) while optimizing memory usage and accelerating\ninference speed.\n","authors":["Fan Zhang","Naye Ji","Fuxing Gao","Bozuo Zhao","Jingmei Wu","Yanbing Jiang","Hui Du","Zhenqing Ye","Jiayang Zhu","WeiFan Zhong","Leyao Yan","Xiaomeng Ma"],"pdf_url":"https://arxiv.org/pdf/2408.00370v1.pdf","comment":"10 pages,10 figures. arXiv admin note: text overlap with\n  arXiv:2403.10805"},{"id":"http://arxiv.org/abs/2408.00343v1","updated":"2024-08-01T07:27:54Z","published":"2024-08-01T07:27:54Z","title":"IN-Sight: Interactive Navigation through Sight","summary":"  Current visual navigation systems often treat the environment as static,\nlacking the ability to adaptively interact with obstacles. This limitation\nleads to navigation failure when encountering unavoidable obstructions. In\nresponse, we introduce IN-Sight, a novel approach to self-supervised path\nplanning, enabling more effective navigation strategies through interaction\nwith obstacles. Utilizing RGB-D observations, IN-Sight calculates\ntraversability scores and incorporates them into a semantic map, facilitating\nlong-range path planning in complex, maze-like environments. To precisely\nnavigate around obstacles, IN-Sight employs a local planner, trained\nimperatively on a differentiable costmap using representation learning\ntechniques. The entire framework undergoes end-to-end training within the\nstate-of-the-art photorealistic Intel SPEAR Simulator. We validate the\neffectiveness of IN-Sight through extensive benchmarking in a variety of\nsimulated scenarios and ablation studies. Moreover, we demonstrate the system's\nreal-world applicability with zero-shot sim-to-real transfer, deploying our\nplanner on the legged robot platform ANYmal, showcasing its practical potential\nfor interactive navigation in real environments.\n","authors":["Philipp Schoch","Fan Yang","Yuntao Ma","Stefan Leutenegger","Marco Hutter","Quentin Leboute"],"pdf_url":"https://arxiv.org/pdf/2408.00343v1.pdf","comment":"The 2024 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS 2024)"},{"id":"http://arxiv.org/abs/2408.00342v1","updated":"2024-08-01T07:27:18Z","published":"2024-08-01T07:27:18Z","title":"MuJoCo MPC for Humanoid Control: Evaluation on HumanoidBench","summary":"  We tackle the recently introduced benchmark for whole-body humanoid control\nHumanoidBench using MuJoCo MPC. We find that sparse reward functions of\nHumanoidBench yield undesirable and unrealistic behaviors when optimized;\ntherefore, we propose a set of regularization terms that stabilize the robot\nbehavior across tasks. Current evaluations on a subset of tasks demonstrate\nthat our proposed reward function allows achieving the highest HumanoidBench\nscores while maintaining realistic posture and smooth control signals. Our code\nis publicly available and will become a part of MuJoCo MPC, enabling rapid\nprototyping of robot behaviors.\n","authors":["Moritz Meser","Aditya Bhatt","Boris Belousov","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2408.00342v1.pdf","comment":"3 pages, 3 figures, submitted to IEEE Conference on Robotics and\n  Automation (ICRA@40)"},{"id":"http://arxiv.org/abs/2408.00332v1","updated":"2024-08-01T07:10:45Z","published":"2024-08-01T07:10:45Z","title":"Vision-based Wearable Steering Assistance for People with Impaired\n  Vision in Jogging","summary":"  Outdoor sports pose a challenge for people with impaired vision. The demand\nfor higher-speed mobility inspired us to develop a vision-based wearable\nsteering assistance. To ensure broad applicability, we focused on a\nrepresentative sports environment, the athletics track. Our efforts centered on\nimproving the speed and accuracy of perception, enhancing planning adaptability\nfor the real world, and providing swift and safe assistance for people with\nimpaired vision. In perception, we engineered a lightweight multitask network\ncapable of simultaneously detecting track lines and obstacles. Additionally,\ndue to the limitations of existing datasets for supporting multi-task detection\nin athletics tracks, we diligently collected and annotated a new dataset (MAT)\ncontaining 1000 images. In planning, we integrated the methods of sampling and\nspline curves, addressing the planning challenges of curves. Meanwhile, we\nutilized the positions of the track lines and obstacles as constraints to guide\npeople with impaired vision safely along the current track. Our system is\ndeployed on an embedded device, Jetson Orin NX. Through outdoor experiments, it\ndemonstrated adaptability in different sports scenarios, assisting users in\nachieving free movement of 400-meter at an average speed of 1.34 m/s, meeting\nthe level of normal people in jogging. Our MAT dataset is publicly available\nfrom https://github.com/snoopy-l/MAT\n","authors":["Xiaotong Liu","Binglu Wang","Zhijun Li"],"pdf_url":"https://arxiv.org/pdf/2408.00332v1.pdf","comment":"Accepted to ICRA 2024"},{"id":"http://arxiv.org/abs/2408.00275v1","updated":"2024-08-01T04:29:34Z","published":"2024-08-01T04:29:34Z","title":"A Reinforcement Learning Based Motion Planner for Quadrotor Autonomous\n  Flight in Dense Environment","summary":"  Quadrotor motion planning is critical for autonomous flight in complex\nenvironments, such as rescue operations. Traditional methods often employ\ntrajectory generation optimization and passive time allocation strategies,\nwhich can limit the exploitation of the quadrotor's dynamic capabilities and\nintroduce delays and inaccuracies. To address these challenges, we propose a\nnovel motion planning framework that integrates visibility path searching and\nreinforcement learning (RL) motion generation. Our method constructs\ncollision-free paths using heuristic search and visibility graphs, which are\nthen refined by an RL policy to generate low-level motion commands. We validate\nour approach in simulated indoor environments, demonstrating better performance\nthan traditional methods in terms of time span.\n","authors":["Zhaohong Liu","Wenxuan Gao","Yinshuai Sun","Peng Dong"],"pdf_url":"https://arxiv.org/pdf/2408.00275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00215v1","updated":"2024-08-01T00:53:36Z","published":"2024-08-01T00:53:36Z","title":"Clutter-Aware Spill-Free Liquid Transport via Learned Dynamics","summary":"  In this work, we present a novel algorithm to perform spill-free handling of\nopen-top liquid-filled containers that operates in cluttered environments. By\nallowing liquid-filled containers to be tilted at higher angles and enabling\nmotion along all axes of end-effector orientation, our work extends the\nreachable space and enhances maneuverability around obstacles, broadening the\nrange of feasible scenarios. Our key contributions include: i) generating\nspill-free paths through the use of RRT* with an informed sampler that\nleverages container properties to avoid spill-inducing states (such as an\nupside-down container), ii) parameterizing the resulting path to generate\nspill-free trajectories through the implementation of a time parameterization\nalgorithm, coupled with a transformer-based machine-learning model capable of\nclassifying trajectories as spill-free or not. We validate our approach in\nreal-world, obstacle-rich task settings using containers of various shapes and\nfill levels and demonstrate an extended solution space that is at least 3x\nlarger than an existing approach.\n","authors":["Ava Abderezaei","Anuj Pasricha","Alex Klausenstock","Alessandro Roncone"],"pdf_url":"https://arxiv.org/pdf/2408.00215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00907v1","updated":"2024-08-01T20:56:28Z","published":"2024-08-01T20:56:28Z","title":"The Harmonic Exponential Filter for Nonparametric Estimation on Motion\n  Groups","summary":"  Bayesian estimation is a vital tool in robotics as it allows systems to\nupdate the belief of the robot state using incomplete information from noisy\nsensors. To render the state estimation problem tractable, many systems assume\nthat the motion and measurement noise, as well as the state distribution, are\nall unimodal and Gaussian. However, there are numerous scenarios and systems\nthat do not comply with these assumptions. Existing non-parametric filters that\nare used to model multimodal distributions have drawbacks that limit their\nability to represent a diverse set of distributions. In this paper, we\nintroduce a novel approach to nonparametric Bayesian filtering to cope with\nmultimodal distributions using harmonic exponential distributions. This\napproach leverages two key insights of harmonic exponential distributions: a)\nthe product of two distributions can be expressed as the element-wise addition\nof their log-likelihood Fourier coefficients, and b) the convolution of two\ndistributions can be efficiently computed as the tensor product of their\nFourier coefficients. These observations enable the development of an efficient\nand exact solution to the Bayes filter up to the band limit of a Fourier\ntransform. We demonstrate our filter's superior performance compared with\nestablished nonparametric filtering methods across a range of simulated and\nreal-world localization tasks.\n","authors":["Miguel Saavedra-Ruiz","Steven A. Parkison","Ria Arora","James Richard Forbes","Liam Paull"],"pdf_url":"https://arxiv.org/pdf/2408.00907v1.pdf","comment":"Preprint under review. Code available at\n  https://github.com/montrealrobotics/harmonic-filter. Webpage and additional\n  videos at https://montrealrobotics.ca/hef/"},{"id":"http://arxiv.org/abs/2312.09436v2","updated":"2024-08-01T20:19:29Z","published":"2023-11-27T21:18:06Z","title":"Temporal Transfer Learning for Traffic Optimization with Coarse-grained\n  Advisory Autonomy","summary":"  The recent development of connected and automated vehicle (CAV) technologies\nhas spurred investigations to optimize dense urban traffic to maximize vehicle\nspeed and throughput. This paper explores advisory autonomy, in which real-time\ndriving advisories are issued to the human drivers, thus achieving near-term\nperformance of automated vehicles. Due to the complexity of traffic systems,\nrecent studies of coordinating CAVs have resorted to leveraging deep\nreinforcement learning (RL). Coarse-grained advisory is formalized as\nzero-order holds, and we consider a range of hold duration from 0.1 to 40\nseconds. However, despite the similarity of the higher frequency tasks on CAVs,\na direct application of deep RL fails to be generalized to advisory autonomy\ntasks. To overcome this, we utilize zero-shot transfer, training policies on a\nset of source tasks--specific traffic scenarios with designated hold\ndurations--and then evaluating the efficacy of these policies on different\ntarget tasks. We introduce Temporal Transfer Learning (TTL) algorithms to\nselect source tasks for zero-shot transfer, systematically leveraging the\ntemporal structure to solve the full range of tasks. TTL selects the most\nsuitable source tasks to maximize the performance of the range of tasks. We\nvalidate our algorithms on diverse mixed-traffic scenarios, demonstrating that\nTTL more reliably solves the tasks than baselines. This paper underscores the\npotential of coarse-grained advisory autonomy with TTL in traffic flow\noptimization.\n","authors":["Jung-Hoon Cho","Sirui Li","Jeongyun Kim","Cathy Wu"],"pdf_url":"https://arxiv.org/pdf/2312.09436v2.pdf","comment":"18 pages, 12 figures"},{"id":"http://arxiv.org/abs/2407.00741v5","updated":"2024-08-01T20:15:37Z","published":"2024-06-30T16:05:31Z","title":"Diffusion Models for Offline Multi-agent Reinforcement Learning with\n  Safety Constraints","summary":"  In recent advancements in Multi-agent Reinforcement Learning (MARL), its\napplication has extended to various safety-critical scenarios. However, most\nmethods focus on online learning, which presents substantial risks when\ndeployed in real-world settings. Addressing this challenge, we introduce an\ninnovative framework integrating diffusion models within the MARL paradigm.\nThis approach notably enhances the safety of actions taken by multiple agents\nthrough risk mitigation while modeling coordinated action. Our framework is\ngrounded in the Centralized Training with Decentralized Execution (CTDE)\narchitecture, augmented by a Diffusion Model for prediction trajectory\ngeneration. Additionally, we incorporate a specialized algorithm to further\nensure operational safety. We evaluate our model against baselines on the DSRL\nbenchmark. Experiment results demonstrate that our model not only adheres to\nstringent safety constraints but also achieves superior performance compared to\nexisting methodologies. This underscores the potential of our approach in\nadvancing the safety and efficacy of MARL in real-world applications.\n","authors":["Jianuo Huang"],"pdf_url":"https://arxiv.org/pdf/2407.00741v5.pdf","comment":"arXiv admin note: text overlap with arXiv:2101.05436 by other authors"},{"id":"http://arxiv.org/abs/2408.00853v1","updated":"2024-08-01T18:07:13Z","published":"2024-08-01T18:07:13Z","title":"Real-time Dexterous Telemanipulation with an End-Effect-Oriented\n  Learning-based Approach","summary":"  Dexterous telemanipulation is crucial in advancing human-robot systems,\nespecially in tasks requiring precise and safe manipulation. However, it faces\nsignificant challenges due to the physical differences between human and\nrobotic hands, the dynamic interaction with objects, and the indirect control\nand perception of the remote environment. Current approaches predominantly\nfocus on mapping the human hand onto robotic counterparts to replicate motions,\nwhich exhibits a critical oversight: it often neglects the physical interaction\nwith objects and relegates the interaction burden to the human to adapt and\nmake laborious adjustments in response to the indirect and counter-intuitive\nobservation of the remote environment. This work develops an\nEnd-Effects-Oriented Learning-based Dexterous Telemanipulation (EFOLD)\nframework to address telemanipulation tasks. EFOLD models telemanipulation as a\nMarkov Game, introducing multiple end-effect features to interpret the human\noperator's commands during interaction with objects. These features are used by\na Deep Reinforcement Learning policy to control the robot and reproduce such\nend effects. EFOLD was evaluated with real human subjects and two end-effect\nextraction methods for controlling a virtual Shadow Robot Hand in\ntelemanipulation tasks. EFOLD achieved real-time control capability with low\ncommand following latency (delay<0.11s) and highly accurate tracking (MSE<0.084\nrad).\n","authors":["Haoyang Wang","He Bai","Xiaoli Zhang","Yunsik Jung","Michel Bowman","Lingfeng Tao"],"pdf_url":"https://arxiv.org/pdf/2408.00853v1.pdf","comment":"Accepted by IROS 2024"},{"id":"http://arxiv.org/abs/2408.00846v1","updated":"2024-08-01T18:01:23Z","published":"2024-08-01T18:01:23Z","title":"Occupation-aware planning method for robotic monitoring missions in\n  dynamic environments","summary":"  This paper presents a method for robotic monitoring missions in the presence\nof moving obstacles. Although the scenario map is known, the robot lacks\ninformation about the movement of dynamic obstacles during the monitoring\nmission. Numerous local planners have been developed in recent years for\nnavigating highly dynamic environments. However, the absence of a global\nplanner for these environments can result in unavoidable collisions or the\ninability to successfully complete missions in densely populated areas, such as\na scenario monitoring in our case. This work addresses the development and\nevaluation of a global planner, $MADA$ (Monitoring Avoiding Dynamic Areas),\naimed at enhancing the deployment of robots in such challenging conditions. The\nrobot plans and executes the mission using the proposed two-step approach. The\nfirst step involves selecting the observation goal based on the environment's\ndistribution and estimated monitoring costs. In the second step, the robot\nidentifies areas with moving obstacles and obtains paths avoiding densely\noccupied dynamic regions based on their occupation. Quantitative and\nqualitative results based on simulations and on real-world experimentation,\nconfirm that the proposed method allows the robot to effectively monitor most\nof the environment while avoiding densely occupied dynamic areas.\n","authors":["Yaroslav Marchukov","Luis Montano"],"pdf_url":"https://arxiv.org/pdf/2408.00846v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01471v1","updated":"2024-08-01T19:39:55Z","published":"2024-08-01T19:39:55Z","title":"Enhancing Online Road Network Perception and Reasoning with Standard\n  Definition Maps","summary":"  Autonomous driving for urban and highway driving applications often requires\nHigh Definition (HD) maps to generate a navigation plan. Nevertheless, various\nchallenges arise when generating and maintaining HD maps at scale. While recent\nonline mapping methods have started to emerge, their performance especially for\nlonger ranges is limited by heavy occlusion in dynamic environments. With these\nconsiderations in mind, our work focuses on leveraging lightweight and scalable\npriors-Standard Definition (SD) maps-in the development of online vectorized HD\nmap representations. We first examine the integration of prototypical\nrasterized SD map representations into various online mapping architectures.\nFurthermore, to identify lightweight strategies, we extend the OpenLane-V2\ndataset with OpenStreetMaps and evaluate the benefits of graphical SD map\nrepresentations. A key finding from designing SD map integration components is\nthat SD map encoders are model agnostic and can be quickly adapted to new\narchitectures that utilize bird's eye view (BEV) encoders. Our results show\nthat making use of SD maps as priors for the online mapping task can\nsignificantly speed up convergence and boost the performance of the online\ncenterline perception task by 30% (mAP). Furthermore, we show that the\nintroduction of the SD maps leads to a reduction of the number of parameters in\nthe perception and reasoning task by leveraging SD map graphs while improving\nthe overall performance. Project Page:\nhttps://henryzhangzhy.github.io/sdhdmap/.\n","authors":["Hengyuan Zhang","David Paz","Yuliang Guo","Arun Das","Xinyu Huang","Karsten Haug","Henrik I. Christensen","Liu Ren"],"pdf_url":"https://arxiv.org/pdf/2408.01471v1.pdf","comment":"Accepted by the 2024 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2024)"}]},"2024-08-02T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2407.04292v3","updated":"2024-08-02T01:59:22Z","published":"2024-07-05T06:54:26Z","title":"Corki: Enabling Real-time Embodied AI Robots via Algorithm-Architecture\n  Co-Design","summary":"  Embodied AI robots have the potential to fundamentally improve the way human\nbeings live and manufacture. Continued progress in the burgeoning field of\nusing large language models to control robots depends critically on an\nefficient computing substrate. In particular, today's computing systems for\nembodied AI robots are designed purely based on the interest of algorithm\ndevelopers, where robot actions are divided into a discrete frame-basis. Such\nan execution pipeline creates high latency and energy consumption. This paper\nproposes Corki, an algorithm-architecture co-design framework for real-time\nembodied AI robot control. Our idea is to decouple LLM inference, robotic\ncontrol and data communication in the embodied AI robots compute pipeline.\nInstead of predicting action for one single frame, Corki predicts the\ntrajectory for the near future to reduce the frequency of LLM inference. The\nalgorithm is coupled with a hardware that accelerates transforming trajectory\ninto actual torque signals used to control robots and an execution pipeline\nthat parallels data communication with computation. Corki largely reduces LLM\ninference frequency by up to 8.0x, resulting in up to 3.6x speed up. The\nsuccess rate improvement can be up to 17.3%. Code is provided for\nre-implementation. https://github.com/hyy0613/Corki\n","authors":["Yiyang Huang","Yuhui Hao","Bo Yu","Feng Yan","Yuxin Yang","Feng Min","Yinhe Han","Lin Ma","Shaoshan Liu","Qiang Liu","Yiming Gan"],"pdf_url":"https://arxiv.org/pdf/2407.04292v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19879v2","updated":"2024-08-02T17:26:18Z","published":"2024-03-28T23:18:33Z","title":"MAC: Graph Sparsification by Maximizing Algebraic Connectivity","summary":"  Simultaneous localization and mapping (SLAM) is a critical capability in\nautonomous navigation, but memory and computational limits make long-term\napplication of common SLAM techniques impractical; a robot must be able to\ndetermine what information should be retained and what can safely be forgotten.\nIn graph-based SLAM, the number of edges (measurements) in a pose graph\ndetermines both the memory requirements of storing a robot's observations and\nthe computational expense of algorithms deployed for performing state\nestimation using those observations, both of which can grow unbounded during\nlong-term navigation. Motivated by these challenges, we propose a new general\npurpose approach to sparsify graphs in a manner that maximizes algebraic\nconnectivity, a key spectral property of graphs which has been shown to control\nthe estimation error of pose graph SLAM solutions. Our algorithm, MAC (for\nmaximizing algebraic connectivity), is simple and computationally inexpensive,\nand admits formal post hoc performance guarantees on the quality of the\nsolution that it provides. In application to the problem of pose-graph SLAM, we\nshow on several benchmark datasets that our approach quickly produces\nhigh-quality sparsification results which retain the connectivity of the graph\nand, in turn, the quality of corresponding SLAM solutions.\n","authors":["Kevin Doherty","Alan Papalia","Yewei Huang","David Rosen","Brendan Englot","John Leonard"],"pdf_url":"https://arxiv.org/pdf/2403.19879v2.pdf","comment":"17 pages, 5 figures. Submitted to IEEE Transactions on Robotics.\n  arXiv admin note: substantial text overlap with arXiv:2203.13897"},{"id":"http://arxiv.org/abs/2407.19631v2","updated":"2024-08-02T17:10:43Z","published":"2024-07-29T01:22:04Z","title":"\"A Good Bot Always Knows Its Limitations\": Assessing Autonomous System\n  Decision-making Competencies through Factorized Machine Self-confidence","summary":"  How can intelligent machines assess their competencies in completing tasks?\nThis question has come into focus for autonomous systems that algorithmically\nreason and make decisions under uncertainty. It is argued here that machine\nself-confidence - a form of meta-reasoning based on self-assessments of an\nagent's knowledge about the state of the world and itself, as well as its\nability to reason about and execute tasks - leads to many eminently computable\nand useful competency indicators for such agents. This paper presents a\nculmination of work on this concept in the form of a computational framework\ncalled Factorized Machine Self-confidence (FaMSeC), which provides a holistic\nengineering-focused description of factors driving an algorithmic\ndecision-making process, including: outcome assessment, solver quality, model\nquality, alignment quality, and past experience. In FaMSeC, self confidence\nindicators are derived from hierarchical `problem-solving statistics' embedded\nwithin broad classes of probabilistic decision-making algorithms such as Markov\ndecision processes. The problem-solving statistics are obtained by evaluating\nand grading probabilistic exceedance margins with respect to given competency\nstandards, which are specified for each of the various decision-making\ncompetency factors by the informee (e.g. a non-expert user or an expert system\ndesigner). This approach allows `algorithmic goodness of fit' evaluations to be\neasily incorporated into the design of many kinds of autonomous agents in the\nform of human-interpretable competency self-assessment reports. Detailed\ndescriptions and application examples for a Markov decision process agent show\nhow two of the FaMSeC factors (outcome assessment and solver quality) can be\ncomputed and reported for a range of possible tasking contexts through novel\nuse of meta-utility functions, behavior simulations, and surrogate prediction\nmodels.\n","authors":["Brett Israelsen","Nisar R. Ahmed","Matthew Aitken","Eric W. Frew","Dale A. Lawrence","Brian M. Argrow"],"pdf_url":"https://arxiv.org/pdf/2407.19631v2.pdf","comment":"59 pages, 22 figures, draft to be submitted for journal review"},{"id":"http://arxiv.org/abs/2403.20328v2","updated":"2024-08-02T16:51:52Z","published":"2024-03-29T17:59:05Z","title":"Learning Visual Quadrupedal Loco-Manipulation from Demonstrations","summary":"  Quadruped robots are progressively being integrated into human environments.\nDespite the growing locomotion capabilities of quadrupedal robots, their\ninteraction with objects in realistic scenes is still limited. While additional\nrobotic arms on quadrupedal robots enable manipulating objects, they are\nsometimes redundant given that a quadruped robot is essentially a mobile unit\nequipped with four limbs, each possessing 3 degrees of freedom (DoFs). Hence,\nwe aim to empower a quadruped robot to execute real-world manipulation tasks\nusing only its legs. We decompose the loco-manipulation process into a\nlow-level reinforcement learning (RL)-based controller and a high-level\nBehavior Cloning (BC)-based planner. By parameterizing the manipulation\ntrajectory, we synchronize the efforts of the upper and lower layers, thereby\nleveraging the advantages of both RL and BC. Our approach is validated through\nsimulations and real-world experiments, demonstrating the robot's ability to\nperform tasks that demand mobility and high precision, such as lifting a basket\nfrom the ground while moving, closing a dishwasher, pressing a button, and\npushing a door. Project website: https://zhengmaohe.github.io/leg-manip\n","authors":["Zhengmao He","Kun Lei","Yanjie Ze","Koushil Sreenath","Zhongyu Li","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2403.20328v2.pdf","comment":"Published at IROS 2024. Project website:\n  https://zhengmaohe.github.io/leg-manip"},{"id":"http://arxiv.org/abs/2408.01370v1","updated":"2024-08-02T16:24:55Z","published":"2024-08-02T16:24:55Z","title":"EVIT: Event-based Visual-Inertial Tracking in Semi-Dense Maps Using\n  Windowed Nonlinear Optimization","summary":"  Event cameras are an interesting visual exteroceptive sensor that reacts to\nbrightness changes rather than integrating absolute image intensities. Owing to\nthis design, the sensor exhibits strong performance in situations of\nchallenging dynamics and illumination conditions. While event-based\nsimultaneous tracking and mapping remains a challenging problem, a number of\nrecent works have pointed out the sensor's suitability for prior map-based\ntracking. By making use of cross-modal registration paradigms, the camera's\nego-motion can be tracked across a large spectrum of illumination and dynamics\nconditions on top of accurate maps that have been created a priori by more\ntraditional sensors. The present paper follows up on a recently introduced\nevent-based geometric semi-dense tracking paradigm, and proposes the addition\nof inertial signals in order to robustify the estimation. More specifically,\nthe added signals provide strong cues for pose initialization as well as\nregularization during windowed, multi-frame tracking. As a result, the proposed\nframework achieves increased performance under challenging illumination\nconditions as well as a reduction of the rate at which intermediate event\nrepresentations need to be registered in order to maintain stable tracking\nacross highly dynamic sequences. Our evaluation focuses on a diverse set of\nreal world sequences and comprises a comparison of our proposed method against\na purely event-based alternative running at different rates.\n","authors":["Runze Yuan","Tao Liu","Zijia Dai","Yi-Fan Zuo","Laurent Kneip"],"pdf_url":"https://arxiv.org/pdf/2408.01370v1.pdf","comment":"8 pages, 5 figures, 3 tables, International Conference on Intelligent\n  Robots and Systems 2024"},{"id":"http://arxiv.org/abs/2408.01366v1","updated":"2024-08-02T16:20:56Z","published":"2024-08-02T16:20:56Z","title":"Play to the Score: Stage-Guided Dynamic Multi-Sensory Fusion for Robotic\n  Manipulation","summary":"  Humans possess a remarkable talent for flexibly alternating to different\nsenses when interacting with the environment. Picture a chef skillfully gauging\nthe timing of ingredient additions and controlling the heat according to the\ncolors, sounds, and aromas, seamlessly navigating through every stage of the\ncomplex cooking process. This ability is founded upon a thorough comprehension\nof task stages, as achieving the sub-goal within each stage can necessitate the\nutilization of different senses. In order to endow robots with similar ability,\nwe incorporate the task stages divided by sub-goals into the imitation learning\nprocess to accordingly guide dynamic multi-sensory fusion. We propose MS-Bot, a\nstage-guided dynamic multi-sensory fusion method with coarse-to-fine stage\nunderstanding, which dynamically adjusts the priority of modalities based on\nthe fine-grained state within the predicted current stage. We train a robot\nsystem equipped with visual, auditory, and tactile sensors to accomplish\nchallenging robotic manipulation tasks: pouring and peg insertion with keyway.\nExperimental results indicate that our approach enables more effective and\nexplainable dynamic fusion, aligning more closely with the human fusion process\nthan existing methods.\n","authors":["Ruoxuan Feng","Di Hu","Wenke Ma","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.01366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01334v1","updated":"2024-08-02T15:32:42Z","published":"2024-08-02T15:32:42Z","title":"A Backbone for Long-Horizon Robot Task Understanding","summary":"  End-to-end robot learning, particularly for long-horizon tasks, often results\nin unpredictable outcomes and poor generalization. To address these challenges,\nwe propose a novel Therblig-based Backbone Framework (TBBF) to enhance robot\ntask understanding and transferability. This framework uses therbligs (basic\naction elements) as the backbone to decompose high-level robot tasks into\nelemental robot configurations, which are then integrated with current\nfoundation models to improve task understanding. The approach consists of two\nstages: offline training and online testing. During the offline training stage,\nwe developed the Meta-RGate SynerFusion (MGSF) network for accurate therblig\nsegmentation across various tasks. In the online testing stage, after a\none-shot demonstration of a new task is collected, our MGSF network extracts\nhigh-level knowledge, which is then encoded into the image using Action\nRegistration (ActionREG). Additionally, the Large Language Model\n(LLM)-Alignment Policy for Visual Correction (LAP-VC) is employed to ensure\nprecise action execution, facilitating trajectory transfer in novel robot\nscenarios. Experimental results validate these methods, achieving 94.37% recall\nin therblig segmentation and success rates of 94.4% and 80% in real-world\nonline robot testing for simple and complex scenarios, respectively.\nSupplementary material is available at:\nhttps://sites.google.com/view/therbligsbasedbackbone/home\n","authors":["Xiaoshuai Chen","Wei Chen","Dongmyoung Lee","Yukun Ge","Nicolas Rojas","Petar Kormushev"],"pdf_url":"https://arxiv.org/pdf/2408.01334v1.pdf","comment":"8 pages, 8 figures. This work is intended to be submitted to IEEE\n  Robotics and Automation Letters (RA-L) for possible publication"},{"id":"http://arxiv.org/abs/2408.01333v1","updated":"2024-08-02T15:30:51Z","published":"2024-08-02T15:30:51Z","title":"Incorporating Control Inputs in the Estimation of Continuous Mobile\n  Robot Trajectories and Continuum Robot Shapes","summary":"  Continuous-time batch state estimation using Gaussian processes is an\nefficient approach to estimate the trajectories of robots over time. In the\npast, relatively simple physics-motivated priors have been considered for such\napproaches, using assumptions such as constant velocity or acceleration. This\npaper presents an approach to incorporating exogenous control inputs, such as\nvelocity or acceleration commands, into the continuous Gaussian process\nstate-estimation framework. It is shown that this approach generalizes across\ndifferent domains in robotics, making it applicable to both the estimation of\ncontinuous-time trajectories for mobile robots and continuum-robot shapes.\nResults show that incorporating control inputs leads to more informed priors,\npotentially requiring less measurements and estimation nodes to obtain accurate\nestimates. This makes the approach particularly useful in situations in which\nlimited sensing is available.\n","authors":["Sven Lilge","Timothy D. Barfoot"],"pdf_url":"https://arxiv.org/pdf/2408.01333v1.pdf","comment":"8 pages, 5 figures, submitted to IEEE Robotics and Automation Letters"},{"id":"http://arxiv.org/abs/2311.15962v4","updated":"2024-08-02T14:18:25Z","published":"2023-11-27T16:07:17Z","title":"Uncertainty Quantification of Set-Membership Estimation in Control and\n  Perception: Revisiting the Minimum Enclosing Ellipsoid","summary":"  Set-membership estimation (SME) outputs a set estimator that guarantees to\ncover the groundtruth. Such sets are, however, defined by (many) abstract (and\npotentially nonconvex) constraints and therefore difficult to manipulate. We\npresent tractable algorithms to compute simple and tight overapproximations of\nSME in the form of minimum enclosing ellipsoids (MEE). We first introduce the\nhierarchy of enclosing ellipsoids proposed by Nie and Demmel (2005), based on\nsums-of-squares relaxations, that asymptotically converge to the MEE of a basic\nsemialgebraic set. This framework, however, struggles in modern control and\nperception problems due to computational challenges. We contribute three\ncomputational enhancements to make this framework practical, namely constraints\npruning, generalized relaxed Chebyshev center, and handling non-Euclidean\ngeometry. We showcase numerical examples on system identification and object\npose estimation.\n","authors":["Yukai Tang","Jean-Bernard Lasserre","Heng Yang"],"pdf_url":"https://arxiv.org/pdf/2311.15962v4.pdf","comment":"Accepted to 6th Learning for Dynamics and Control (L4DC) as oral\n  presentation"},{"id":"http://arxiv.org/abs/2408.01258v1","updated":"2024-08-02T13:28:04Z","published":"2024-08-02T13:28:04Z","title":"Jacta: A Versatile Planner for Learning Dexterous and Whole-body\n  Manipulation","summary":"  Robotic manipulation is challenging due to discontinuous dynamics, as well as\nhigh-dimensional state and action spaces. Data-driven approaches that succeed\nin manipulation tasks require large amounts of data and expert demonstrations,\ntypically from humans. Existing manipulation planners are restricted to\nspecific systems and often depend on specialized algorithms for using\ndemonstration. Therefore, we introduce a flexible motion planner tailored to\ndexterous and whole-body manipulation tasks. Our planner creates readily usable\ndemonstrations for reinforcement learning algorithms, eliminating the need for\nadditional training pipeline complexities. With this approach, we can\nefficiently learn policies for complex manipulation tasks, where traditional\nreinforcement learning alone only makes little progress. Furthermore, we\ndemonstrate that learned policies are transferable to real robotic systems for\nsolving complex dexterous manipulation tasks.\n","authors":["Jan Brüdigam","Ali-Adeeb Abbas","Maks Sorokin","Kuan Fang","Brandon Hung","Maya Guru","Stefan Sosnowski","Jiuguang Wang","Sandra Hirche","Simon Le Cleac'h"],"pdf_url":"https://arxiv.org/pdf/2408.01258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01251v1","updated":"2024-08-02T13:12:27Z","published":"2024-08-02T13:12:27Z","title":"NeRFoot: Robot-Footprint Estimation for Image-Based Visual Servoing","summary":"  This paper investigates the utility of Neural Radiance Fields (NeRF) models\nin extending the regions of operation of a mobile robot, controlled by\nImage-Based Visual Servoing (IBVS) via static CCTV cameras. Using NeRF as a\n3D-representation prior, the robot's footprint may be extrapolated\ngeometrically and used to train a CNN-based network to extract it online from\nthe robot's appearance alone. The resulting footprint results in a tighter\nbound than a robot-wide bounding box, allowing the robot's controller to\nprescribe more optimal trajectories and expand its safe operational floor area.\n","authors":["Daoxin Zhong","Luke Robinson","Daniele De Martini"],"pdf_url":"https://arxiv.org/pdf/2408.01251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21310v2","updated":"2024-08-02T13:03:00Z","published":"2024-07-31T03:26:14Z","title":"MSMA: Multi-agent Trajectory Prediction in Connected and Autonomous\n  Vehicle Environment with Multi-source Data Integration","summary":"  The prediction of surrounding vehicle trajectories is crucial for\ncollision-free path planning. In this study, we focus on a scenario where a\nconnected and autonomous vehicle (CAV) serves as the central agent, utilizing\nboth sensors and communication technologies to perceive its surrounding\ntraffics consisting of autonomous vehicles (AVs), connected vehicles (CVs), and\nhuman-driven vehicles (HDVs). Our trajectory prediction task is aimed at all\nthe detected surrounding vehicles. To effectively integrate the multi-source\ndata from both sensor and communication technologies, we propose a deep\nlearning framework called MSMA utilizing a cross-attention module for\nmulti-source data fusion. Vector map data is utilized to provide contextual\ninformation. The trajectory dataset is collected in CARLA simulator with\nsynthesized data errors introduced. Numerical experiments demonstrate that in a\nmixed traffic flow scenario, the integration of data from different sources\nenhances our understanding of the environment. This notably improves trajectory\nprediction accuracy, particularly in situations with a high CV market\npenetration rate. The code is available at: https://github.com/xichennn/MSMA.\n","authors":["Xi Chen","Rahul Bhadani","Zhanbo Sun","Larry Head"],"pdf_url":"https://arxiv.org/pdf/2407.21310v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01230v1","updated":"2024-08-02T12:40:01Z","published":"2024-08-02T12:40:01Z","title":"HeteroMorpheus: Universal Control Based on Morphological Heterogeneity\n  Modeling","summary":"  In the field of robotic control, designing individual controllers for each\nrobot leads to high computational costs. Universal control policies, applicable\nacross diverse robot morphologies, promise to mitigate this challenge.\nPredominantly, models based on Graph Neural Networks (GNN) and Transformers are\nemployed, owing to their effectiveness in capturing relational dynamics across\na robot's limbs. However, these models typically employ homogeneous graph\nstructures that overlook the functional diversity of different limbs. To bridge\nthis gap, we introduce HeteroMorpheus, a novel method based on heterogeneous\ngraph Transformer. This method uniquely addresses limb heterogeneity, fostering\nbetter representation of robot dynamics of various morphologies. Through\nextensive experiments we demonstrate the superiority of HeteroMorpheus against\nstate-of-the-art methods in the capability of policy generalization, including\nzero-shot generalization and sample-efficient transfer to unfamiliar robot\nmorphologies.\n","authors":["YiFan Hao","Yang Yang","Junru Song","Wei Peng","Weien Zhou","Tingsong Jiang","Wen Yao"],"pdf_url":"https://arxiv.org/pdf/2408.01230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01225v1","updated":"2024-08-02T12:29:02Z","published":"2024-08-02T12:29:02Z","title":"Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation\n  with Volumetric Visual Data Fusion","summary":"  We introduce Reality Fusion, a novel robot teleoperation system that\nlocalizes, streams, projects, and merges a typical onboard depth sensor with a\nphotorealistic, high resolution, high framerate, and wide field of view (FoV)\nrendering of the complex remote environment represented as 3D Gaussian splats\n(3DGS). Our framework enables robust egocentric and exocentric robot\nteleoperation in immersive VR, with the 3DGS effectively extending spatial\ninformation of a depth sensor with limited FoV and balancing the trade-off\nbetween data streaming costs and data visual quality. We evaluated our\nframework through a user study with 24 participants, which revealed that\nReality Fusion leads to significantly better user performance, situation\nawareness, and user preferences. To support further research and development,\nwe provide an open-source implementation with an easy-to-replicate custom-made\ntelepresence robot, a high-performance virtual reality 3DGS renderer, and an\nimmersive robot control package. (Source code:\nhttps://github.com/uhhhci/RealityFusion)\n","authors":["Ke Li","Reinhard Bacher","Susanne Schmidt","Wim Leemans","Frank Steinicke"],"pdf_url":"https://arxiv.org/pdf/2408.01225v1.pdf","comment":"Accepted, to appear at IROS 2024"},{"id":"http://arxiv.org/abs/2408.01210v1","updated":"2024-08-02T11:46:32Z","published":"2024-08-02T11:46:32Z","title":"From Problem to Solution: Bio-inspired 3D Printing for Bonding Soft and\n  Rigid Materials via Underextrusions","summary":"  Vertebrate animals benefit from a combination of rigidity for structural\nsupport and softness for adaptation. Similarly, integrating rigidity and\nsoftness can enhance the versatility of soft robotics. However, the challenges\nassociated with creating durable bonding interfaces between soft and rigid\nmaterials have limited the development of hybrid robots. Existing solutions\nrequire specialized machinery, such as polyjet 3D printers, which are not\ncommonly available. In response to these challenges, we have developed a 3D\nprinting technique that can be used with almost all commercially available FDM\nprinters. This technique leverages the common issue of underextrusion to create\na strong bond between soft and rigid materials. Underextrusion generates a\nporous structure, similar to fibrous connective tissues, that provides a robust\ninterface with the rigid part through layer fusion, while the porosity enables\ninterlocking with the soft material. Our experiments demonstrated that this\nmethod outperforms conventional adhesives commonly used in soft robotics,\nachieving nearly 200\\% of the bonding strength in both lap shear and peeling\ntests. Additionally, we investigated how different porosity levels affect\nbonding strength. We tested the technique under pressure scenarios critical to\nsoft and hybrid robots and achieved three times more pressure than the current\nadhesion solution. Finally, we fabricated various hybrid robots using this\ntechnique to demonstrate the wide range of capabilities this approach and\nhybridity can bring to soft robotics. has context menu\n","authors":["Arman Goshtasbi","Luca Grignaffini","Ali Sadeghi"],"pdf_url":"https://arxiv.org/pdf/2408.01210v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15857v2","updated":"2024-08-02T11:36:14Z","published":"2024-03-23T14:47:26Z","title":"Automated System-level Testing of Unmanned Aerial Systems","summary":"  Unmanned aerial systems (UAS) rely on various avionics systems that are\nsafety-critical and mission-critical. A major requirement of international\nsafety standards is to perform rigorous system-level testing of avionics\nsoftware systems. The current industrial practice is to manually create test\nscenarios, manually/automatically execute these scenarios using simulators, and\nmanually evaluate outcomes. The test scenarios typically consist of setting\ncertain flight or environment conditions and testing the system under test in\nthese settings. The state-of-the-art approaches for this purpose also require\nmanual test scenario development and evaluation. In this paper, we propose a\nnovel approach to automate the system-level testing of the UAS. The proposed\napproach (AITester) utilizes model-based testing and artificial intelligence\n(AI) techniques to automatically generate, execute, and evaluate various test\nscenarios. The test scenarios are generated on the fly, i.e., during test\nexecution based on the environmental context at runtime. The approach is\nsupported by a toolset. We empirically evaluate the proposed approach on two\ncore components of UAS, an autopilot system of an unmanned aerial vehicle (UAV)\nand cockpit display systems (CDS) of the ground control station (GCS). The\nresults show that the AITester effectively generates test scenarios causing\ndeviations from the expected behavior of the UAV autopilot and reveals\npotential flaws in the GCS-CDS.\n","authors":["Hassan Sartaj","Asmar Muqeet","Muhammad Zohaib Iqbal","Muhammad Uzair Khan"],"pdf_url":"https://arxiv.org/pdf/2403.15857v2.pdf","comment":"Published in Automated Software Engineering"},{"id":"http://arxiv.org/abs/2408.01147v1","updated":"2024-08-02T09:55:56Z","published":"2024-08-02T09:55:56Z","title":"Actra: Optimized Transformer Architecture for Vision-Language-Action\n  Models in Robot Learning","summary":"  Vision-language-action models have gained significant attention for their\nability to model trajectories in robot learning. However, most existing models\nrely on Transformer models with vanilla causal attention, which we find\nsuboptimal for processing segmented multi-modal sequences. Additionally, the\nautoregressive generation approach falls short in generating multi-dimensional\nactions. In this paper, we introduce Actra, an optimized Transformer\narchitecture featuring trajectory attention and learnable action queries,\ndesigned for effective encoding and decoding of segmented\nvision-language-action trajectories in robot imitation learning. Furthermore,\nwe devise a multi-modal contrastive learning objective to explicitly align\ndifferent modalities, complementing the primary behavior cloning objective.\nThrough extensive experiments conducted across various environments, Actra\nexhibits substantial performance improvement when compared to state-of-the-art\nmodels in terms of generalizability, dexterity, and precision.\n","authors":["Yueen Ma","Dafeng Chi","Shiguang Wu","Yuecheng Liu","Yuzheng Zhuang","Jianye Hao","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2408.01147v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01130v1","updated":"2024-08-02T09:19:24Z","published":"2024-08-02T09:19:24Z","title":"Closed-loop underwater soft robotic foil shape control using flexible\n  e-skin","summary":"  The use of soft robotics for real-world underwater applications is limited,\neven more than in terrestrial applications, by the ability to accurately\nmeasure and control the deformation of the soft materials in real time without\nthe need for feedback from an external sensor. Real-time underwater shape\nestimation would allow for accurate closed-loop control of soft propulsors,\nenabling high-performance swimming and manoeuvring. We propose and demonstrate\na method for closed-loop underwater soft robotic foil control based on a\nflexible capacitive e-skin and machine learning which does not necessitate\nfeedback from an external sensor. The underwater e-skin is applied to a highly\nflexible foil undergoing deformations from 2% to 9% of its camber by means of\nsoft hydraulic actuators. Accurate set point regulation of the camber is\nsuccessfully tracked during sinusoidal and triangle actuation routines with an\namplitude of 5% peak-to-peak and 10-second period with a normalised RMS error\nof 0.11, and 2% peak-to-peak amplitude with a period of 5 seconds with a\nnormalised RMS error of 0.03. The tail tip deflection can be measured across a\n30 mm (0.15 chords) range. These results pave the way for using e-skin\ntechnology for underwater soft robotic closed-loop control applications.\n","authors":["Leo Micklem","Huazhi Dong","Francesco Giorgio-Serchi","Yunjie Yang","Gabriel D. Weymouth","Blair Thornton"],"pdf_url":"https://arxiv.org/pdf/2408.01130v1.pdf","comment":"6 pages, 10 figures"},{"id":"http://arxiv.org/abs/2408.01126v1","updated":"2024-08-02T09:07:31Z","published":"2024-08-02T09:07:31Z","title":"IG-SLAM: Instant Gaussian SLAM","summary":"  3D Gaussian Splatting has recently shown promising results as an alternative\nscene representation in SLAM systems to neural implicit representations.\nHowever, current methods either lack dense depth maps to supervise the mapping\nprocess or detailed training designs that consider the scale of the\nenvironment. To address these drawbacks, we present IG-SLAM, a dense RGB-only\nSLAM system that employs robust Dense-SLAM methods for tracking and combines\nthem with Gaussian Splatting. A 3D map of the environment is constructed using\naccurate pose and dense depth provided by tracking. Additionally, we utilize\ndepth uncertainty in map optimization to improve 3D reconstruction. Our decay\nstrategy in map optimization enhances convergence and allows the system to run\nat 10 fps in a single process. We demonstrate competitive performance with\nstate-of-the-art RGB-only SLAM systems while achieving faster operation speeds.\nWe present our experiments on the Replica, TUM-RGBD, ScanNet, and EuRoC\ndatasets. The system achieves photo-realistic 3D reconstruction in large-scale\nsequences, particularly in the EuRoC dataset.\n","authors":["Furkan Aykut Sarikamis","Abdullah Aydin Alatan"],"pdf_url":"https://arxiv.org/pdf/2408.01126v1.pdf","comment":"8 pages, 3 page ref, 5 figures, 3DV submission"},{"id":"http://arxiv.org/abs/2403.17247v3","updated":"2024-08-02T09:03:09Z","published":"2024-03-25T22:49:56Z","title":"DASA: Delay-Adaptive Multi-Agent Stochastic Approximation","summary":"  We consider a setting in which $N$ agents aim to speedup a common Stochastic\nApproximation (SA) problem by acting in parallel and communicating with a\ncentral server. We assume that the up-link transmissions to the server are\nsubject to asynchronous and potentially unbounded time-varying delays. To\nmitigate the effect of delays and stragglers while reaping the benefits of\ndistributed computation, we propose \\texttt{DASA}, a Delay-Adaptive algorithm\nfor multi-agent Stochastic Approximation. We provide a finite-time analysis of\n\\texttt{DASA} assuming that the agents' stochastic observation processes are\nindependent Markov chains. Significantly advancing existing results,\n\\texttt{DASA} is the first algorithm whose convergence rate depends only on the\nmixing time $\\tau_{mix}$ and on the average delay $\\tau_{avg}$ while jointly\nachieving an $N$-fold convergence speedup under Markovian sampling. Our work is\nrelevant for various SA applications, including multi-agent and distributed\ntemporal difference (TD) learning, Q-learning and stochastic optimization with\ncorrelated data.\n","authors":["Nicolò Dal Fabbro","Arman Adibi","H. Vincent Poor","Sanjeev R. Kulkarni","Aritra Mitra","George J. Pappas"],"pdf_url":"https://arxiv.org/pdf/2403.17247v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01093v1","updated":"2024-08-02T08:12:04Z","published":"2024-08-02T08:12:04Z","title":"CommonUppRoad: A Framework of Formal Modelling, Verifying, Learning, and\n  Visualisation of Autonomous Vehicles","summary":"  Combining machine learning and formal methods (FMs) provides a possible\nsolution to overcome the safety issue of autonomous driving (AD) vehicles.\nHowever, there are gaps to be bridged before this combination becomes\npractically applicable and useful. In an attempt to facilitate researchers in\nboth FMs and AD areas, this paper proposes a framework that combines two\nwell-known tools, namely CommonRoad and UPPAAL. On the one hand, CommonRoad can\nbe enhanced by the rigorous semantics of models in UPPAAL, which enables a\nsystematic and comprehensive understanding of the AD system's behaviour and\nthus strengthens the safety of the system. On the other hand, controllers\nsynthesised by UPPAAL can be visualised by CommonRoad in real-world road\nnetworks, which facilitates AD vehicle designers greatly adopting formal models\nin system design. In this framework, we provide automatic model conversions\nbetween CommonRoad and UPPAAL. Therefore, users only need to program in Python\nand the framework takes care of the formal models, learning, and verification\nin the backend. We perform experiments to demonstrate the applicability of our\nframework in various AD scenarios, discuss the advantages of solving motion\nplanning in our framework, and show the scalability limit and possible\nsolutions.\n","authors":["Rong Gu","Kaige Tan","Andreas Holck Høeg-Petersen","Lei Feng","Kim Guldstrand Larsen"],"pdf_url":"https://arxiv.org/pdf/2408.01093v1.pdf","comment":"20 pages, 5 figures, ISoLA 2024"},{"id":"http://arxiv.org/abs/2408.01056v1","updated":"2024-08-02T07:03:45Z","published":"2024-08-02T07:03:45Z","title":"The NING Humanoid: The Concurrent Design and Development of a Dynamic\n  and Agile Platform","summary":"  The recent surge of interest in agile humanoid robots achieving dynamic tasks\nlike jumping and flipping necessitates the concurrent design of a robot\nplatform that combines exceptional hardware performance with effective control\nalgorithms. This paper introduces the NING Humanoid, an agile and robust\nplatform aimed at achieving human-like athletic capabilities. The NING humanoid\nfeatures high-torque actuators, a resilient mechanical co-design based on the\nCentroidal dynamics, and a whole-body model predictive control (WB-MPC)\nframework. It stands at 1.1 meters tall and weighs 20 kg with 18 degrees of\nfreedom (DOFs). It demonstrates impressive abilities such as walking, push\nrecovery, and stair climbing at a high control bandwidth. Our presentation will\nencompass a hardware co-design, the control framework, as well as simulation\nand real-time experiments.\n","authors":["Yan Ning","Song Liu","Taiwen Yang","Liang Zheng","Ling Shi"],"pdf_url":"https://arxiv.org/pdf/2408.01056v1.pdf","comment":"This is a workshop paper for ICRA 2024 in Japan. The workshop is\n  Advancements in Trajectory Optimization and Model Predictive Control for\n  Legged System on May 17th 2024, with the URL as:\n  https://atompc-workshop.github.io/"},{"id":"http://arxiv.org/abs/2408.01035v1","updated":"2024-08-02T06:18:39Z","published":"2024-08-02T06:18:39Z","title":"Structure from Motion-based Motion Estimation and 3D Reconstruction of\n  Unknown Shaped Space Debris","summary":"  With the boost in the number of spacecraft launches in the current decades,\nthe space debris problem is daily becoming significantly crucial. For\nsustainable space utilization, the continuous removal of space debris is the\nmost severe problem for humanity. To maximize the reliability of the debris\ncapture mission in orbit, accurate motion estimation of the target is\nessential. Space debris has lost its attitude and orbit control capabilities,\nand its shape is unknown due to the break. This paper proposes the Structure\nfrom Motion-based algorithm to perform unknown shaped space debris motion\nestimation with limited resources, where only 2D images are required as input.\nThe method then outputs the reconstructed shape of the unknown object and the\nrelative pose trajectory between the target and the camera simultaneously,\nwhich are exploited to estimate the target's motion. The method is\nquantitatively validated with the realistic image dataset generated by the\nmicrogravity experiment in a 2D air-floating testbed and 3D kinematic\nsimulation.\n","authors":["Kentaro Uno","Takehiro Matsuoka","Akiyoshi Uchida","Kazuya Yoshida"],"pdf_url":"https://arxiv.org/pdf/2408.01035v1.pdf","comment":"6 pages, 10 figures. Manuscript accepted at the 2024 IEEE 20th\n  International Conference on Automation Science and Engineerin (CASE 2024)"},{"id":"http://arxiv.org/abs/2406.03298v2","updated":"2024-08-02T05:26:14Z","published":"2024-06-05T14:08:13Z","title":"L-PR: Exploiting LiDAR Fiducial Marker for Unordered Low Overlap\n  Multiview Point Cloud Registration","summary":"  Point cloud registration is a prerequisite for many applications in computer\nvision and robotics. Most existing methods focus on pairwise registration of\ntwo point clouds with high overlap. Although there have been some methods for\nlow overlap cases, they struggle in degraded scenarios. This paper introduces a\nnovel framework dubbed L-PR, designed to register unordered low overlap\nmultiview point clouds leveraging LiDAR fiducial markers. We refer to them as\nLiDAR fiducial markers, but they are the same as the popular AprilTag and ArUco\nmarkers, thin sheets of paper that do not affect the 3D geometry of the\nenvironment. We first propose an improved adaptive threshold marker detection\nmethod to provide robust detection results when the viewpoints among point\nclouds change dramatically. Then, we formulate the unordered multiview point\ncloud registration problem as a maximum a-posteriori (MAP) problem and develop\na framework consisting of two levels of graphs to address it. The first-level\ngraph, constructed as a weighted graph, is designed to efficiently and\noptimally infer initial values of scan poses from the unordered set. The\nsecond-level graph is constructed as a factor graph. By globally optimizing the\nvariables on the graph, including scan poses, marker poses, and marker corner\npositions, we tackle the MAP problem. We conduct both qualitative and\nquantitative experiments to demonstrate that the proposed method surpasses\nprevious state-of-the-art (SOTA) methods and to showcase that L-PR can serve as\na low-cost and efficient tool for 3D asset collection and training data\ncollection. In particular, we collect a new dataset named Livox-3DMatch using\nL-PR and incorporate it into the training of the SOTA learning-based method,\nSGHR, which brings evident improvements for SGHR on various benchmarks.\n","authors":["Yibo Liu","Jinjun Shan","Amaldev Haridevan","Shuo Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.03298v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2403.00228v3","updated":"2024-08-02T05:17:10Z","published":"2024-03-01T02:19:40Z","title":"DISORF: A Distributed Online 3D Reconstruction Framework for Mobile\n  Robots","summary":"  We present a framework, DISORF, to enable online 3D reconstruction and\nvisualization of scenes captured by resource-constrained mobile robots and edge\ndevices. To address the limited computing capabilities of edge devices and\npotentially limited network availability, we design a framework that\nefficiently distributes computation between the edge device and the remote\nserver. We leverage on-device SLAM systems to generate posed keyframes and\ntransmit them to remote servers that can perform high-quality 3D reconstruction\nand visualization at runtime by leveraging recent advances in neural 3D\nmethods. We identify a key challenge with online training where naive image\nsampling strategies can lead to significant degradation in rendering quality.\nWe propose a novel shifted exponential frame sampling method that addresses\nthis challenge for online training. We demonstrate the effectiveness of our\nframework in enabling high-quality real-time reconstruction and visualization\nof unknown scenes as they are captured and streamed from cameras in mobile\nrobots and edge devices.\n","authors":["Chunlin Li","Hanrui Fan","Xiaorui Huang","Ruofan Liang","Sankeerth Durvasula","Nandita Vijaykumar"],"pdf_url":"https://arxiv.org/pdf/2403.00228v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02957v2","updated":"2024-08-02T04:59:20Z","published":"2024-07-03T09:49:46Z","title":"Past, Present, and Future: A Survey of The Evolution of Affective\n  Robotics For Well-being","summary":"  Recent research in affective robots has recognized their potential in\nsupporting human well-being. Due to rapidly developing affective and artificial\nintelligence technologies, this field of research has undergone explosive\nexpansion and advancement in recent years. In order to develop a deeper\nunderstanding of recent advancements, we present a systematic review of the\npast 10 years of research in affective robotics for wellbeing. In this review,\nwe identify the domains of well-being that have been studied, the methods used\nto investigate affective robots for well-being, and how these have evolved over\ntime. We also examine the evolution of the multifaceted research topic from\nthree lenses: technical, design, and ethical. Finally, we discuss future\nopportunities for research based on the gaps we have identified in our review\n-- proposing pathways to take affective robotics from the past and present to\nthe future. The results of our review are of interest to human-robot\ninteraction and affective computing researchers, as well as clinicians and\nwell-being professionals who may wish to examine and incorporate affective\nrobotics in their practices.\n","authors":["Micol Spitale","Minja Axelsson","Sooyeon Jeong","Paige Tuttosı","Caitlin A. Stamatis","Guy Laban","Angelica Lim","Hatice Gunes"],"pdf_url":"https://arxiv.org/pdf/2407.02957v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13186v2","updated":"2024-08-02T00:56:39Z","published":"2023-11-22T06:26:24Z","title":"Applications of Spiking Neural Networks in Visual Place Recognition","summary":"  In robotics, Spiking Neural Networks (SNNs) are increasingly recognized for\ntheir largely-unrealized potential energy efficiency and low latency\nparticularly when implemented on neuromorphic hardware. Our paper highlights\nthree advancements for SNNs in Visual Place Recognition (VPR). Firstly, we\npropose Modular SNNs, where each SNN represents a set of non-overlapping\ngeographically distinct places, enabling scalable networks for large\nenvironments. Secondly, we present Ensembles of Modular SNNs, where multiple\nnetworks represent the same place, significantly enhancing accuracy compared to\nsingle-network models. Each of our Modular SNN modules is compact, comprising\nonly 1500 neurons and 474k synapses, making them ideally suited for ensembling\ndue to their small size. Lastly, we investigate the role of sequence matching\nin SNN-based VPR, a technique where consecutive images are used to refine place\nrecognition. We analyze the responsiveness of SNNs to ensembling and sequence\nmatching compared to other VPR techniques. Our contributions highlight the\nviability of SNNs for VPR, offering scalable and robust solutions, and paving\nthe way for their application in various energy-sensitive robotic tasks.\n","authors":["Somayeh Hussaini","Michael Milford","Tobias Fischer"],"pdf_url":"https://arxiv.org/pdf/2311.13186v2.pdf","comment":"20 pages, 10 figures, under review"},{"id":"http://arxiv.org/abs/2408.01604v1","updated":"2024-08-02T23:46:57Z","published":"2024-08-02T23:46:57Z","title":"Efficient Data-driven Joint-level Calibration of Cable-driven Surgical\n  Robots","summary":"  Knowing accurate joint positions is crucial for safe and precise control of\nlaparoscopic surgical robots, especially for the automation of surgical\nsub-tasks. These robots have often been designed with cable-driven arms and\ntools because cables allow for larger motors to be placed at the base of the\nrobot, further from the operating area where space is at a premium. However, by\nconnecting the joint to its motor with a cable, any stretch in the cable can\nlead to errors in kinematic estimation from encoders at the motor, which can\nresult in difficulties for accurate control of the surgical tool. In this work,\nwe propose an efficient data-driven calibration of positioning joints of such\nrobots, in this case the RAVEN-II surgical robotics research platform. While\nthe calibration takes only 8-21 minutes, the accuracy of the calibrated joints\nremains high during a 6-hour heavily loaded operation, suggesting desirable\nfeasibility in real practice. The calibration models take original robot states\nas input and are trained using zig-zag trajectories within a desired sparsity,\nrequiring no additional sensors after training. Compared to fixed offset\ncompensation, the Deep Neural Network calibration model can further reduce 76\npercent of error and achieve accuracy of 0.104 deg, 0.120 deg, and 0.118 mm in\njoints 1, 2, and 3, respectively. In contrast to end-to-end models, experiments\nsuggest that the DNN model achieves better accuracy and faster convergence when\noutputting the error to correct original inaccurate joint positions.\nFurthermore, a linear regression model is shown to have 160 times faster\ninference speed than DNN models for application within the 1000 Hz servo\ncontrol loop, with slightly compromised accuracy.\n","authors":["Haonan Peng","Andrew Lewis","Yun-Hsuan Su","Shan Lin","Dun-Tin Chiang","Wenfan Jiang","Helen Lai","Blake Hannaford"],"pdf_url":"https://arxiv.org/pdf/2408.01604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01589v1","updated":"2024-08-02T21:59:13Z","published":"2024-08-02T21:59:13Z","title":"Soil Sample Search in Partially Observable Environments","summary":"  To work in unknown outdoor environments, autonomous sampling machines need\nthe ability to target samples despite limited visibility and robotic arm reach\ndistance. We design a heuristic guided search method to speed up the search\nprocess and more efficiently localize the approximate center of soil regions.\nThrough simulation experiments, we assess the effectiveness of the proposed\nalgorithm and discover superior performance in terms of speed, distance\ntraveled, and success rate compared to naive baselines.\n","authors":["Han Yang","Andrew Dudash"],"pdf_url":"https://arxiv.org/pdf/2408.01589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20391v2","updated":"2024-08-02T21:31:14Z","published":"2024-07-29T19:34:23Z","title":"Alignment Scores: Robust Metrics for Multiview Pose Accuracy Evaluation","summary":"  We propose three novel metrics for evaluating the accuracy of a set of\nestimated camera poses given the ground truth: Translation Alignment Score\n(TAS), Rotation Alignment Score (RAS), and Pose Alignment Score (PAS). The TAS\nevaluates the translation accuracy independently of the rotations, and the RAS\nevaluates the rotation accuracy independently of the translations. The PAS is\nthe average of the two scores, evaluating the combined accuracy of both\ntranslations and rotations. The TAS is computed in four steps: (1) Find the\nupper quartile of the closest-pair-distances, $d$. (2) Align the estimated\ntrajectory to the ground truth using a robust registration method. (3) Collect\nall distance errors and obtain the cumulative frequencies for multiple\nthresholds ranging from $0.01d$ to $d$ with a resolution $0.01d$. (4) Add up\nthese cumulative frequencies and normalize them such that the theoretical\nmaximum is 1. The TAS has practical advantages over the existing metrics in\nthat (1) it is robust to outliers and collinear motion, and (2) there is no\nneed to adjust parameters on different datasets. The RAS is computed in a\nsimilar manner to the TAS and is also shown to be more robust against outliers\nthan the existing rotation metrics. We verify our claims through extensive\nsimulations and provide in-depth discussion of the strengths and weaknesses of\nthe proposed metrics.\n","authors":["Seong Hun Lee","Javier Civera"],"pdf_url":"https://arxiv.org/pdf/2407.20391v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16598v5","updated":"2024-08-02T21:28:07Z","published":"2024-02-26T14:28:39Z","title":"PCR-99: A Practical Method for Point Cloud Registration with 99 Percent\n  Outliers","summary":"  We propose a robust method for point cloud registration that can handle both\nunknown scales and extreme outlier ratios. Our method, dubbed PCR-99, uses a\ndeterministic 3-point sampling approach with two novel mechanisms that\nsignificantly boost the speed: (1) an improved ordering of the samples based on\npairwise scale consistency, prioritizing the point correspondences that are\nmore likely to be inliers, and (2) an efficient outlier rejection scheme based\non triplet scale consistency, prescreening bad samples and reducing the number\nof hypotheses to be tested. Our evaluation shows that, up to 98% outlier ratio,\nthe proposed method achieves comparable performance to the state of the art. At\n99% outlier ratio, however, it outperforms the state of the art for both\nknown-scale and unknown-scale problems. Especially for the latter, we observe a\nclear superiority in terms of robustness and speed.\n","authors":["Seong Hun Lee","Javier Civera","Patrick Vandewalle"],"pdf_url":"https://arxiv.org/pdf/2402.16598v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01576v1","updated":"2024-08-02T21:15:02Z","published":"2024-08-02T21:15:02Z","title":"Autonomous Integration of Bench-Top Wet Lab Equipment","summary":"  Laboratory automation is an expensive and complicated endeavor with limited\ninflexible options for small-scale labs. We develop a prototype system for\ntending to a bench-top centrifuge using computer vision methods for color\ndetection and circular Hough Transforms to detect and localize centrifuge\nbuckets. Initial results show that the prototype is capable of automating the\nusage of regular bench-top lab equipment.\n","authors":["Zachary Logan","Kam Undieh","Mohammad Goli"],"pdf_url":"https://arxiv.org/pdf/2408.01576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01569v1","updated":"2024-08-02T20:47:36Z","published":"2024-08-02T20:47:36Z","title":"TURTLMap: Real-time Localization and Dense Mapping of Low-texture\n  Underwater Environments with a Low-cost Unmanned Underwater Vehicle","summary":"  Significant work has been done on advancing localization and mapping in\nunderwater environments. Still, state-of-the-art methods are challenged by\nlow-texture environments, which is common for underwater settings. This makes\nit difficult to use existing methods in diverse, real-world scenes. In this\npaper, we present TURTLMap, a novel solution that focuses on textureless\nunderwater environments through a real-time localization and mapping method. We\nshow that this method is low-cost, and capable of tracking the robot\naccurately, while constructing a dense map of a low-textured environment in\nreal-time. We evaluate the proposed method using real-world data collected in\nan indoor water tank with a motion capture system and ground truth reference\nmap. Qualitative and quantitative results validate the proposed system achieves\naccurate and robust localization and precise dense mapping, even when subject\nto wave conditions. The project page for TURTLMap is\nhttps://umfieldrobotics.github.io/TURTLMap.\n","authors":["Jingyu Song","Onur Bagoren","Razan Andigani","Advaith Venkatramanan Sethuraman","Katherine Skinner"],"pdf_url":"https://arxiv.org/pdf/2408.01569v1.pdf","comment":"Accepted to IROS 2024"},{"id":"http://arxiv.org/abs/2408.01554v1","updated":"2024-08-02T20:01:23Z","published":"2024-08-02T20:01:23Z","title":"Robot-Enabled Machine Learning-Based Diagnosis of Gastric Cancer Polyps\n  Using Partial Surface Tactile Imaging","summary":"  In this paper, to collectively address the existing limitations on endoscopic\ndiagnosis of Advanced Gastric Cancer (AGC) Tumors, for the first time, we\npropose (i) utilization and evaluation of our recently developed Vision-based\nTactile Sensor (VTS), and (ii) a complementary Machine Learning (ML) algorithm\nfor classifying tumors using their textural features. Leveraging a seven DoF\nrobotic manipulator and unique custom-designed and additively-manufactured\nrealistic AGC tumor phantoms, we demonstrated the advantages of automated data\ncollection using the VTS addressing the problem of data scarcity and biases\nencountered in traditional ML-based approaches. Our synthetic-data-trained ML\nmodel was successfully evaluated and compared with traditional ML models\nutilizing various statistical metrics even under mixed morphological\ncharacteristics and partial sensor contact.\n","authors":["Siddhartha Kapuria","Jeff Bonyun","Yash Kulkarni","Naruhiko Ikoma","Sandeep Chinchali","Farshid Alambeigi"],"pdf_url":"https://arxiv.org/pdf/2408.01554v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01543v1","updated":"2024-08-02T19:07:09Z","published":"2024-08-02T19:07:09Z","title":"A Decomposition of Interaction Force for Multi-Agent Co-Manipulation","summary":"  Multi-agent human-robot co-manipulation is a poorly understood process with\nmany inputs that potentially affect agent behavior. This paper explores one\nsuch input known as interaction force. Interaction force is potentially a\nprimary component in communication that occurs during co-manipulation. There\nare, however, many different perspectives and definitions of interaction force\nin the literature. Therefore, a decomposition of interaction force is proposed\nthat provides a consistent way of ascertaining the state of an agent relative\nto the group for multi-agent co-manipulation. This proposed method extends a\ncurrent definition from one to four degrees of freedom, does not rely on a\npredefined object path, and is independent of the number of agents acting on\nthe system and their locations and input wrenches (forces and torques). In\naddition, all of the necessary measures can be obtained by a self-contained\nrobotic system, allowing for a more flexible and adaptive approach for future\nco-manipulation robot controllers.\n","authors":["Kody B. Shaw","Dallin L. Cordon","Marc D. Killpack","John L. Salmon"],"pdf_url":"https://arxiv.org/pdf/2408.01543v1.pdf","comment":"11 pages, 19 figures, 4 tables, 3 equations, prepared for submission\n  to transactions on haptics"},{"id":"http://arxiv.org/abs/2408.01537v1","updated":"2024-08-02T18:49:14Z","published":"2024-08-02T18:49:14Z","title":"SceneMotion: From Agent-Centric Embeddings to Scene-Wide Forecasts","summary":"  Self-driving vehicles rely on multimodal motion forecasts to effectively\ninteract with their environment and plan safe maneuvers. We introduce\nSceneMotion, an attention-based model for forecasting scene-wide motion modes\nof multiple traffic agents. Our model transforms local agent-centric embeddings\ninto scene-wide forecasts using a novel latent context module. This module\nlearns a scene-wide latent space from multiple agent-centric embeddings,\nenabling joint forecasting and interaction modeling. The competitive\nperformance in the Waymo Open Interaction Prediction Challenge demonstrates the\neffectiveness of our approach. Moreover, we cluster future waypoints in time\nand space to quantify the interaction between agents. We merge all modes and\nanalyze each mode independently to determine which clusters are resolved\nthrough interaction or result in conflict. Our implementation is available at:\nhttps://github.com/kit-mrt/future-motion\n","authors":["Royden Wagner","Ömer Sahin Tas","Marlon Steiner","Fabian Konstantinidis","Hendrik Königshof","Marvin Klemp","Carlos Fernandez","Christoph Stiller"],"pdf_url":"https://arxiv.org/pdf/2408.01537v1.pdf","comment":"7 pages, 3 figures, ITSC 2024"},{"id":"http://arxiv.org/abs/2408.01510v1","updated":"2024-08-02T18:07:53Z","published":"2024-08-02T18:07:53Z","title":"Adaptive Planning with Generative Models under Uncertainty","summary":"  Planning with generative models has emerged as an effective decision-making\nparadigm across a wide range of domains, including reinforcement learning and\nautonomous navigation. While continuous replanning at each timestep might seem\nintuitive because it allows decisions to be made based on the most recent\nenvironmental observations, it results in substantial computational challenges,\nprimarily due to the complexity of the generative model's underlying deep\nlearning architecture. Our work addresses this challenge by introducing a\nsimple adaptive planning policy that leverages the generative model's ability\nto predict long-horizon state trajectories, enabling the execution of multiple\nactions consecutively without the need for immediate replanning. We propose to\nuse the predictive uncertainty derived from a Deep Ensemble of inverse dynamics\nmodels to dynamically adjust the intervals between planning sessions. In our\nexperiments conducted on locomotion tasks within the OpenAI Gym framework, we\ndemonstrate that our adaptive planning policy allows for a reduction in\nreplanning frequency to only about 10% of the steps without compromising the\nperformance. Our results underscore the potential of generative modeling as an\nefficient and effective tool for decision-making.\n","authors":["Pascal Jutras-Dubé","Ruqi Zhang","Aniket Bera"],"pdf_url":"https://arxiv.org/pdf/2408.01510v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2408.00465v2","updated":"2024-08-02T03:56:14Z","published":"2024-08-01T11:09:01Z","title":"Infrequent Resolving Algorithm for Online Linear Programming","summary":"  Online linear programming (OLP) has gained significant attention from both\nresearchers and practitioners due to its extensive applications, such as online\nauction, network revenue management and advertising. Existing OLP algorithms\nfall into two categories: LP-based algorithms and LP-free algorithms. The\nformer one typically guarantees better performance, even offering a constant\nregret, but requires solving a large number of LPs, which could be\ncomputationally expensive. In contrast, LP-free algorithm only requires\nfirst-order computations but induces a worse performance, lacking a constant\nregret bound. In this work, we bridge the gap between these two extremes by\nproposing an algorithm that achieves a constant regret while solving LPs only\n$O(\\log\\log T)$ times over the time horizon $T$. Moreover, when we are allowed\nto solve LPs only $M$ times, we propose an algorithm that can guarantee an\n$O\\left(T^{(1/2+\\epsilon)^{M-1}}\\right)$ regret. Furthermore, when the arrival\nprobabilities are known at the beginning, our algorithm can guarantee a\nconstant regret by solving LPs $O(\\log\\log T)$ times, and an\n$O\\left(T^{(1/2+\\epsilon)^{M}}\\right)$ regret by solving LPs only $M$ times.\nNumerical experiments are conducted to demonstrate the efficiency of the\nproposed algorithms.\n","authors":["Guokai Li","Zizhuo Wang","Jingwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00465v2.pdf","comment":"35 pages, 7 figures"},{"id":"http://arxiv.org/abs/2402.16827v3","updated":"2024-08-02T17:59:31Z","published":"2024-02-26T18:54:35Z","title":"A Survey on Data Selection for Language Models","summary":"  A major factor in the recent success of large language models is the use of\nenormous and ever-growing text datasets for unsupervised pre-training. However,\nnaively training a model on all available data may not be optimal (or\nfeasible), as the quality of available text data can vary. Filtering out data\ncan also decrease the carbon footprint and financial costs of training models\nby reducing the amount of training required. Data selection methods aim to\ndetermine which candidate data points to include in the training dataset and\nhow to appropriately sample from the selected data points. The promise of\nimproved data selection methods has caused the volume of research in the area\nto rapidly expand. However, because deep learning is mostly driven by empirical\nevidence and experimentation on large-scale data is expensive, few\norganizations have the resources for extensive data selection research.\nConsequently, knowledge of effective data selection practices has become\nconcentrated within a few organizations, many of which do not openly share\ntheir findings and methodologies. To narrow this gap in knowledge, we present a\ncomprehensive review of existing literature on data selection methods and\nrelated research areas, providing a taxonomy of existing approaches. By\ndescribing the current landscape of research, this work aims to accelerate\nprogress in data selection by establishing an entry point for new and\nestablished researchers. Additionally, throughout this review we draw attention\nto noticeable holes in the literature and conclude the paper by proposing\npromising avenues for future research.\n","authors":["Alon Albalak","Yanai Elazar","Sang Michael Xie","Shayne Longpre","Nathan Lambert","Xinyi Wang","Niklas Muennighoff","Bairu Hou","Liangming Pan","Haewon Jeong","Colin Raffel","Shiyu Chang","Tatsunori Hashimoto","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2402.16827v3.pdf","comment":"Paper list available at\n  https://github.com/alon-albalak/data-selection-survey"},{"id":"http://arxiv.org/abs/2408.01420v1","updated":"2024-08-02T17:55:50Z","published":"2024-08-02T17:55:50Z","title":"Mission Impossible: A Statistical Perspective on Jailbreaking LLMs","summary":"  Large language models (LLMs) are trained on a deluge of text data with\nlimited quality control. As a result, LLMs can exhibit unintended or even\nharmful behaviours, such as leaking information, fake news or hate speech.\nCountermeasures, commonly referred to as preference alignment, include\nfine-tuning the pretrained LLMs with carefully crafted text examples of desired\nbehaviour. Even then, empirical evidence shows preference aligned LLMs can be\nenticed to harmful behaviour. This so called jailbreaking of LLMs is typically\nachieved by adversarially modifying the input prompt to the LLM. Our paper\nprovides theoretical insights into the phenomenon of preference alignment and\njailbreaking from a statistical perspective. Under our framework, we first show\nthat pretrained LLMs will mimic harmful behaviour if present in the training\ncorpus. Under that same framework, we then introduce a statistical notion of\nalignment, and lower-bound the jailbreaking probability, showing that it is\nunpreventable under reasonable assumptions. Based on our insights, we propose\nan alteration to the currently prevalent alignment strategy RLHF. Specifically,\nwe introduce a simple modification to the RLHF objective, we call E-RLHF, that\naims to increase the likelihood of safe responses. E-RLHF brings no additional\ntraining cost, and is compatible with other methods. Empirically, we\ndemonstrate that E-RLHF outperforms RLHF on all alignment problems put forward\nby the AdvBench and HarmBench project without sacrificing model performance as\nmeasured by the MT-Bench project.\n","authors":["Jingtong Su","Julia Kempe","Karen Ullrich"],"pdf_url":"https://arxiv.org/pdf/2408.01420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01417v1","updated":"2024-08-02T17:51:57Z","published":"2024-08-02T17:51:57Z","title":"Talk Less, Interact Better: Evaluating In-context Conversational\n  Adaptation in Multimodal LLMs","summary":"  Humans spontaneously use increasingly efficient language as interactions\nprogress, by adapting and forming ad-hoc conventions. This phenomenon has been\nstudied extensively using reference games, showing properties of human language\nthat go beyond relaying intents. It remains unexplored whether multimodal large\nlanguage models (MLLMs) similarly increase communication efficiency during\ninteractions, and what mechanisms they may adopt for this purpose. We introduce\nICCA, an automated framework to evaluate such conversational adaptation as an\nin-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and\nobserve that while they may understand the increasingly efficient language of\ntheir interlocutor, they do not spontaneously make their own language more\nefficient over time. This latter ability can only be elicited in some models\n(e.g., GPT-4) with heavy-handed prompting. This shows that this property of\nlinguistic interaction does not arise from current training regimes, even\nthough it is a common hallmark of human language. ICCA is available at\nhttps://github.com/lil-lab/ICCA.\n","authors":["Yilun Hua","Yoav Artzi"],"pdf_url":"https://arxiv.org/pdf/2408.01417v1.pdf","comment":"Accepted to COLM 2024"},{"id":"http://arxiv.org/abs/2408.01416v1","updated":"2024-08-02T17:51:42Z","published":"2024-08-02T17:51:42Z","title":"The Quest for the Right Mediator: A History, Survey, and Theoretical\n  Grounding of Causal Interpretability","summary":"  Interpretability provides a toolset for understanding how and why neural\nnetworks behave in certain ways. However, there is little unity in the field:\nmost studies employ ad-hoc evaluations and do not share theoretical\nfoundations, making it difficult to measure progress and compare the pros and\ncons of different techniques. Furthermore, while mechanistic understanding is\nfrequently discussed, the basic causal units underlying these mechanisms are\noften not explicitly defined. In this paper, we propose a perspective on\ninterpretability research grounded in causal mediation analysis. Specifically,\nwe describe the history and current state of interpretability taxonomized\naccording to the types of causal units (mediators) employed, as well as methods\nused to search over mediators. We discuss the pros and cons of each mediator,\nproviding insights as to when particular kinds of mediators and search methods\nare most appropriate depending on the goals of a given study. We argue that\nthis framing yields a more cohesive narrative of the field, as well as\nactionable insights for future work. Specifically, we recommend a focus on\ndiscovering new mediators with better trade-offs between human-interpretability\nand compute-efficiency, and which can uncover more sophisticated abstractions\nfrom neural networks than the primarily linear mediators employed in current\nwork. We also argue for more standardized evaluations that enable principled\ncomparisons across mediator types, such that we can better understand when\nparticular causal units are better suited to particular use cases.\n","authors":["Aaron Mueller","Jannik Brinkmann","Millicent Li","Samuel Marks","Koyena Pal","Nikhil Prakash","Can Rager","Aruna Sankaranarayanan","Arnab Sen Sharma","Jiuding Sun","Eric Todd","David Bau","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2408.01416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01415v1","updated":"2024-08-02T17:43:34Z","published":"2024-08-02T17:43:34Z","title":"Conditional LoRA Parameter Generation","summary":"  Generative models have achieved remarkable success in image, video, and text\ndomains. Inspired by this, researchers have explored utilizing generative\nmodels to generate neural network parameters. However, these efforts have been\nlimited by the parameter size and the practicality of generating\nhigh-performance parameters. In this paper, we propose COND P-DIFF, a novel\napproach that demonstrates the feasibility of controllable high-performance\nparameter generation, particularly for LoRA (Low-Rank Adaptation) weights,\nduring the fine-tuning process. Specifically, we employ an autoencoder to\nextract efficient latent representations for parameters. We then train a\nconditional latent diffusion model to synthesize high-performing model\nparameters from random noise based on specific task conditions. Experimental\nresults in both computer vision and natural language processing domains\nconsistently demonstrate that COND P-DIFF can generate high-performance\nparameters conditioned on the given task. Moreover, we observe that the\nparameter distribution generated by COND P-DIFF exhibits differences compared\nto the distribution obtained through normal optimization methods, indicating a\ncertain level of generalization capability. Our work paves the way for further\nexploration of condition-driven parameter generation, offering a promising\ndirection for task-specific adaptation of neural networks.\n","authors":["Xiaolong Jin","Kai Wang","Dongwen Tang","Wangbo Zhao","Yukun Zhou","Junshu Tang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2408.01415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01408v1","updated":"2024-08-02T17:33:52Z","published":"2024-08-02T17:33:52Z","title":"Derivation of Back-propagation for Graph Convolutional Networks using\n  Matrix Calculus and its Application to Explainable Artificial Intelligence","summary":"  This paper provides a comprehensive and detailed derivation of the\nbackpropagation algorithm for graph convolutional neural networks using matrix\ncalculus. The derivation is extended to include arbitrary element-wise\nactivation functions and an arbitrary number of layers. The study addresses two\nfundamental problems, namely node classification and link prediction. To\nvalidate our method, we compare it with reverse-mode automatic differentiation.\nThe experimental results demonstrate that the median sum of squared errors of\nthe updated weight matrices, when comparing our method to the approach using\nreverse-mode automatic differentiation, falls within the range of $10^{-18}$ to\n$10^{-14}$. These outcomes are obtained from conducting experiments on a\nfive-layer graph convolutional network, applied to a node classification\nproblem on Zachary's karate club social network and a link prediction problem\non a drug-drug interaction network. Finally, we show how the derived\nclosed-form solution can facilitate the development of explainable AI and\nsensitivity analysis.\n","authors":["Yen-Che Hsiao","Rongting Yue","Abhishek Dutta"],"pdf_url":"https://arxiv.org/pdf/2408.01408v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.15501v4","updated":"2024-08-02T17:31:24Z","published":"2021-10-29T02:38:54Z","title":"Doubly Robust Interval Estimation for Optimal Policy Evaluation in\n  Online Learning","summary":"  Evaluating the performance of an ongoing policy plays a vital role in many\nareas such as medicine and economics, to provide crucial instructions on the\nearly-stop of the online experiment and timely feedback from the environment.\nPolicy evaluation in online learning thus attracts increasing attention by\ninferring the mean outcome of the optimal policy (i.e., the value) in\nreal-time. Yet, such a problem is particularly challenging due to the dependent\ndata generated in the online environment, the unknown optimal policy, and the\ncomplex exploration and exploitation trade-off in the adaptive experiment. In\nthis paper, we aim to overcome these difficulties in policy evaluation for\nonline learning. We explicitly derive the probability of exploration that\nquantifies the probability of exploring non-optimal actions under commonly used\nbandit algorithms. We use this probability to conduct valid inference on the\nonline conditional mean estimator under each action and develop the doubly\nrobust interval estimation (DREAM) method to infer the value under the\nestimated optimal policy in online learning. The proposed value estimator\nprovides double protection for consistency and is asymptotically normal with a\nWald-type confidence interval provided. Extensive simulation studies and real\ndata applications are conducted to demonstrate the empirical validity of the\nproposed DREAM method.\n","authors":["Ye Shen","Hengrui Cai","Rui Song"],"pdf_url":"https://arxiv.org/pdf/2110.15501v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01402v1","updated":"2024-08-02T17:25:34Z","published":"2024-08-02T17:25:34Z","title":"Pre-trained Language Models Improve the Few-shot Prompt Ability of\n  Decision Transformer","summary":"  Decision Transformer (DT) has emerged as a promising class of algorithms in\noffline reinforcement learning (RL) tasks, leveraging pre-collected datasets\nand Transformer's capability to model long sequences. Recent works have\ndemonstrated that using parts of trajectories from training tasks as prompts in\nDT enhances its performance on unseen tasks, giving rise to Prompt-DT methods.\nHowever, collecting data from specific environments can be both costly and\nunsafe in many scenarios, leading to suboptimal performance and limited\nfew-shot prompt abilities due to the data-hungry nature of Transformer-based\nmodels. Additionally, the limited datasets used in pre-training make it\nchallenging for Prompt-DT type of methods to distinguish between various RL\ntasks through prompts alone. To address these challenges, we introduce the\nLanguage model-initialized Prompt Decision Transformer (LPDT), which leverages\npre-trained language models for meta-RL tasks and fine-tunes the model using\nLow-rank Adaptation (LoRA). We further incorporate prompt regularization to\neffectively differentiate between tasks based on prompt feature\nrepresentations. Our approach integrates pre-trained language model and RL\ntasks seamlessly. Extensive empirical studies demonstrate that initializing\nwith a pre-trained language model significantly enhances the performance of\nPrompt-DT on unseen tasks compared to baseline methods.\n","authors":["Yu Yang","Pan Xu"],"pdf_url":"https://arxiv.org/pdf/2408.01402v1.pdf","comment":"2 figures, 8 tables. Accepted by the Training Agents with Foundation\n  Models Workshop at RLC 2024"},{"id":"http://arxiv.org/abs/2407.19631v2","updated":"2024-08-02T17:10:43Z","published":"2024-07-29T01:22:04Z","title":"\"A Good Bot Always Knows Its Limitations\": Assessing Autonomous System\n  Decision-making Competencies through Factorized Machine Self-confidence","summary":"  How can intelligent machines assess their competencies in completing tasks?\nThis question has come into focus for autonomous systems that algorithmically\nreason and make decisions under uncertainty. It is argued here that machine\nself-confidence - a form of meta-reasoning based on self-assessments of an\nagent's knowledge about the state of the world and itself, as well as its\nability to reason about and execute tasks - leads to many eminently computable\nand useful competency indicators for such agents. This paper presents a\nculmination of work on this concept in the form of a computational framework\ncalled Factorized Machine Self-confidence (FaMSeC), which provides a holistic\nengineering-focused description of factors driving an algorithmic\ndecision-making process, including: outcome assessment, solver quality, model\nquality, alignment quality, and past experience. In FaMSeC, self confidence\nindicators are derived from hierarchical `problem-solving statistics' embedded\nwithin broad classes of probabilistic decision-making algorithms such as Markov\ndecision processes. The problem-solving statistics are obtained by evaluating\nand grading probabilistic exceedance margins with respect to given competency\nstandards, which are specified for each of the various decision-making\ncompetency factors by the informee (e.g. a non-expert user or an expert system\ndesigner). This approach allows `algorithmic goodness of fit' evaluations to be\neasily incorporated into the design of many kinds of autonomous agents in the\nform of human-interpretable competency self-assessment reports. Detailed\ndescriptions and application examples for a Markov decision process agent show\nhow two of the FaMSeC factors (outcome assessment and solver quality) can be\ncomputed and reported for a range of possible tasking contexts through novel\nuse of meta-utility functions, behavior simulations, and surrogate prediction\nmodels.\n","authors":["Brett Israelsen","Nisar R. Ahmed","Matthew Aitken","Eric W. Frew","Dale A. Lawrence","Brian M. Argrow"],"pdf_url":"https://arxiv.org/pdf/2407.19631v2.pdf","comment":"59 pages, 22 figures, draft to be submitted for journal review"},{"id":"http://arxiv.org/abs/2408.01391v1","updated":"2024-08-02T17:01:36Z","published":"2024-08-02T17:01:36Z","title":"FT K-Means: A High-Performance K-Means on GPU with Fault Tolerance","summary":"  K-Means is a widely used algorithm in clustering, however, its efficiency is\nprimarily constrained by the computational cost of distance computing. Existing\nimplementations suffer from suboptimal utilization of computational units and\nlack resilience against soft errors. To address these challenges, we introduce\nFT K-Means, a high-performance GPU-accelerated implementation of K-Means with\nonline fault tolerance. We first present a stepwise optimization strategy that\nachieves competitive performance compared to NVIDIA's cuML library. We further\nimprove FT K-Means with a template-based code generation framework that\nsupports different data types and adapts to different input shapes. A novel\nwarp-level tensor-core error correction scheme is proposed to address the\nfailure of existing fault tolerance methods due to memory asynchronization\nduring copy operations. Our experimental evaluations on NVIDIA T4 GPU and A100\nGPU demonstrate that FT K-Means without fault tolerance outperforms cuML's\nK-Means implementation, showing a performance increase of 10\\%-300\\% in\nscenarios involving irregular data shapes. Moreover, the fault tolerance\nfeature of FT K-Means introduces only an overhead of 11\\%, maintaining robust\nperformance even with tens of errors injected per second.\n","authors":["Shixun Wu","Yitong Ding","Yujia Zhai","Jinyang Liu","Jiajun Huang","Zizhe Jian","Huangliang Dai","Sheng Di","Bryan M. Wong","Zizhong Chen","Franck Cappello"],"pdf_url":"https://arxiv.org/pdf/2408.01391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01387v1","updated":"2024-08-02T16:55:08Z","published":"2024-08-02T16:55:08Z","title":"NeuralBeta: Estimating Beta Using Deep Learning","summary":"  Traditional approaches to estimating beta in finance often involve rigid\nassumptions and fail to adequately capture beta dynamics, limiting their\neffectiveness in use cases like hedging. To address these limitations, we have\ndeveloped a novel method using neural networks called NeuralBeta, which is\ncapable of handling both univariate and multivariate scenarios and tracking the\ndynamic behavior of beta. To address the issue of interpretability, we\nintroduce a new output layer inspired by regularized weighted linear\nregression, which provides transparency into the model's decision-making\nprocess. We conducted extensive experiments on both synthetic and market data,\ndemonstrating NeuralBeta's superior performance compared to benchmark methods\nacross various scenarios, especially instances where beta is highly\ntime-varying, e.g., during regime shifts in the market. This model not only\nrepresents an advancement in the field of beta estimation, but also shows\npotential for applications in other financial contexts that assume linear\nrelationships.\n","authors":["Yuxin Liu","Jimin Lin","Achintya Gopal"],"pdf_url":"https://arxiv.org/pdf/2408.01387v1.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.20328v2","updated":"2024-08-02T16:51:52Z","published":"2024-03-29T17:59:05Z","title":"Learning Visual Quadrupedal Loco-Manipulation from Demonstrations","summary":"  Quadruped robots are progressively being integrated into human environments.\nDespite the growing locomotion capabilities of quadrupedal robots, their\ninteraction with objects in realistic scenes is still limited. While additional\nrobotic arms on quadrupedal robots enable manipulating objects, they are\nsometimes redundant given that a quadruped robot is essentially a mobile unit\nequipped with four limbs, each possessing 3 degrees of freedom (DoFs). Hence,\nwe aim to empower a quadruped robot to execute real-world manipulation tasks\nusing only its legs. We decompose the loco-manipulation process into a\nlow-level reinforcement learning (RL)-based controller and a high-level\nBehavior Cloning (BC)-based planner. By parameterizing the manipulation\ntrajectory, we synchronize the efforts of the upper and lower layers, thereby\nleveraging the advantages of both RL and BC. Our approach is validated through\nsimulations and real-world experiments, demonstrating the robot's ability to\nperform tasks that demand mobility and high precision, such as lifting a basket\nfrom the ground while moving, closing a dishwasher, pressing a button, and\npushing a door. Project website: https://zhengmaohe.github.io/leg-manip\n","authors":["Zhengmao He","Kun Lei","Yanjie Ze","Koushil Sreenath","Zhongyu Li","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2403.20328v2.pdf","comment":"Published at IROS 2024. Project website:\n  https://zhengmaohe.github.io/leg-manip"},{"id":"http://arxiv.org/abs/2408.01382v1","updated":"2024-08-02T16:40:58Z","published":"2024-08-02T16:40:58Z","title":"Explaining a probabilistic prediction on the simplex with Shapley\n  compositions","summary":"  Originating in game theory, Shapley values are widely used for explaining a\nmachine learning model's prediction by quantifying the contribution of each\nfeature's value to the prediction. This requires a scalar prediction as in\nbinary classification, whereas a multiclass probabilistic prediction is a\ndiscrete probability distribution, living on a multidimensional simplex. In\nsuch a multiclass setting the Shapley values are typically computed separately\non each class in a one-vs-rest manner, ignoring the compositional nature of the\noutput distribution. In this paper, we introduce Shapley compositions as a\nwell-founded way to properly explain a multiclass probabilistic prediction,\nusing the Aitchison geometry from compositional data analysis. We prove that\nthe Shapley composition is the unique quantity satisfying linearity, symmetry\nand efficiency on the Aitchison simplex, extending the corresponding axiomatic\nproperties of the standard Shapley value. We demonstrate this proper multiclass\ntreatment in a range of scenarios.\n","authors":["Paul-Gauthier Noé","Miquel Perelló-Nieto","Jean-François Bonastre","Peter Flach"],"pdf_url":"https://arxiv.org/pdf/2408.01382v1.pdf","comment":"To be published in ECAI2024's proceedings"},{"id":"http://arxiv.org/abs/2408.01379v1","updated":"2024-08-02T16:37:33Z","published":"2024-08-02T16:37:33Z","title":"Resampling and averaging coordinates on data","summary":"  We introduce algorithms for robustly computing intrinsic coordinates on point\nclouds. Our approach relies on generating many candidate coordinates by\nsubsampling the data and varying hyperparameters of the embedding algorithm\n(e.g., manifold learning). We then identify a subset of representative\nembeddings by clustering the collection of candidate coordinates and using\nshape descriptors from topological data analysis. The final output is the\nembedding obtained as an average of the representative embeddings using\ngeneralized Procrustes analysis. We validate our algorithm on both synthetic\ndata and experimental measurements from genomics, demonstrating robustness to\nnoise and outliers.\n","authors":["Andrew J. Blumberg","Mathieu Carriere","Jun Hou Fung","Michael A. Mandell"],"pdf_url":"https://arxiv.org/pdf/2408.01379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01375v1","updated":"2024-08-02T16:32:30Z","published":"2024-08-02T16:32:30Z","title":"Adaptive Recruitment Resource Allocation to Improve Cohort\n  Representativeness in Participatory Biomedical Datasets","summary":"  Large participatory biomedical studies, studies that recruit individuals to\njoin a dataset, are gaining popularity and investment, especially for analysis\nby modern AI methods. Because they purposively recruit participants, these\nstudies are uniquely able to address a lack of historical representation, an\nissue that has affected many biomedical datasets. In this work, we define\nrepresentativeness as the similarity to a target population distribution of a\nset of attributes and our goal is to mirror the U.S. population across\ndistributions of age, gender, race, and ethnicity. Many participatory studies\nrecruit at several institutions, so we introduce a computational approach to\nadaptively allocate recruitment resources among sites to improve\nrepresentativeness. In simulated recruitment of 10,000-participant cohorts from\nmedical centers in the STAR Clinical Research Network, we show that our\napproach yields a more representative cohort than existing baselines. Thus, we\nhighlight the value of computational modeling in guiding recruitment efforts.\n","authors":["Victor Borza","Andrew Estornell","Ellen Wright Clayton","Chien-Ju Ho","Russell Rothman","Yevgeniy Vorobeychik","Bradley Malin"],"pdf_url":"https://arxiv.org/pdf/2408.01375v1.pdf","comment":"Accepted for publication at the American Medical Informatics\n  Association Annual Symposium 2024, 10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.11652v3","updated":"2024-08-02T16:30:55Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v3.pdf","comment":"I found critical errors in the manuscript affecting its validity. I\n  need to correct these before resubmitting. Major changes to methodology and\n  results are underway, significantly altering the content. I will resubmit the\n  revised version"},{"id":"http://arxiv.org/abs/2408.01374v1","updated":"2024-08-02T16:29:54Z","published":"2024-08-02T16:29:54Z","title":"Hybrid Coordinate Descent for Efficient Neural Network Learning Using\n  Line Search and Gradient Descent","summary":"  This paper presents a novel coordinate descent algorithm leveraging a\ncombination of one-directional line search and gradient information for\nparameter updates for a squared error loss function. Each parameter undergoes\nupdates determined by either the line search or gradient method, contingent\nupon whether the modulus of the gradient of the loss with respect to that\nparameter surpasses a predefined threshold. Notably, a larger threshold value\nenhances algorithmic efficiency. Despite the potentially slower nature of the\nline search method relative to gradient descent, its parallelizability\nfacilitates computational time reduction. Experimental validation conducted on\na 2-layer Rectified Linear Unit network with synthetic data elucidates the\nimpact of hyperparameters on convergence rates and computational efficiency.\n","authors":["Yen-Che Hsiao","Abhishek Dutta"],"pdf_url":"https://arxiv.org/pdf/2408.01374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01365v1","updated":"2024-08-02T16:17:59Z","published":"2024-08-02T16:17:59Z","title":"Data Debugging is NP-hard for Classifiers Trained with SGD","summary":"  Data debugging is to find a subset of the training data such that the model\nobtained by retraining on the subset has a better accuracy. A bunch of\nheuristic approaches are proposed, however, none of them are guaranteed to\nsolve this problem effectively. This leaves an open issue whether there exists\nan efficient algorithm to find the subset such that the model obtained by\nretraining on it has a better accuracy. To answer this open question and\nprovide theoretical basis for further study on developing better algorithms for\ndata debugging, we investigate the computational complexity of the problem\nnamed Debuggable. Given a machine learning model $\\mathcal{M}$ obtained by\ntraining on dataset $D$ and a test instance\n$(\\mathbf{x}_\\text{test},y_\\text{test})$ where\n$\\mathcal{M}(\\mathbf{x}_\\text{test})\\neq y_\\text{test}$, Debuggable is to\ndetermine whether there exists a subset $D^\\prime$ of $D$ such that the model\n$\\mathcal{M}^\\prime$ obtained by retraining on $D^\\prime$ satisfies\n$\\mathcal{M}^\\prime(\\mathbf{x}_\\text{test})=y_\\text{test}$. To cover a wide\nrange of commonly used models, we take SGD-trained linear classifier as the\nmodel and derive the following main results. (1) If the loss function and the\ndimension of the model are not fixed, Debuggable is NP-complete regardless of\nthe training order in which all the training samples are processed during SGD.\n(2) For hinge-like loss functions, a comprehensive analysis on the\ncomputational complexity of Debuggable is provided; (3) If the loss function is\na linear function, Debuggable can be solved in linear time, that is, data\ndebugging can be solved easily in this case. These results not only highlight\nthe limitations of current approaches but also offer new insights into data\ndebugging.\n","authors":["Zizheng Guo","Pengyu Chen","Yanzhang Fu","Dongjing Miao"],"pdf_url":"https://arxiv.org/pdf/2408.01365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01362v1","updated":"2024-08-02T16:13:51Z","published":"2024-08-02T16:13:51Z","title":"Autoencoders in Function Space","summary":"  Autoencoders have found widespread application, in both their original\ndeterministic form and in their variational formulation (VAEs). In scientific\napplications it is often of interest to consider data that are comprised of\nfunctions; the same perspective is useful in image processing. In practice,\ndiscretisation (of differential equations arising in the sciences) or\npixellation (of images) renders problems finite dimensional, but conceiving\nfirst of algorithms that operate on functions, and only then discretising or\npixellating, leads to better algorithms that smoothly operate between different\nlevels of discretisation or pixellation. In this paper function-space versions\nof the autoencoder (FAE) and variational autoencoder (FVAE) are introduced,\nanalysed, and deployed. Well-definedness of the objective function governing\nVAEs is a subtle issue, even in finite dimension, and more so on function\nspace. The FVAE objective is well defined whenever the data distribution is\ncompatible with the chosen generative model; this happens, for example, when\nthe data arise from a stochastic differential equation. The FAE objective is\nvalid much more broadly, and can be straightforwardly applied to data governed\nby differential equations. Pairing these objectives with neural operator\narchitectures, which can thus be evaluated on any mesh, enables new\napplications of autoencoders to inpainting, superresolution, and generative\nmodelling of scientific data.\n","authors":["Justin Bunker","Mark Girolami","Hefin Lambley","Andrew M. Stuart","T. J. Sullivan"],"pdf_url":"https://arxiv.org/pdf/2408.01362v1.pdf","comment":"56 pages, 25 figures"},{"id":"http://arxiv.org/abs/2406.04551v2","updated":"2024-08-02T16:09:49Z","published":"2024-06-06T23:35:51Z","title":"Improving Geo-diversity of Generated Images with Contextualized Vendi\n  Score Guidance","summary":"  With the growing popularity of text-to-image generative models, there has\nbeen increasing focus on understanding their risks and biases. Recent work has\nfound that state-of-the-art models struggle to depict everyday objects with the\ntrue diversity of the real world and have notable gaps between geographic\nregions. In this work, we aim to increase the diversity of generated images of\ncommon objects such that per-region variations are representative of the real\nworld. We introduce an inference time intervention, contextualized Vendi Score\nGuidance (c-VSG), that guides the backwards steps of latent diffusion models to\nincrease the diversity of a sample as compared to a \"memory bank\" of previously\ngenerated images while constraining the amount of variation within that of an\nexemplar set of real-world contextualizing images. We evaluate c-VSG with two\ngeographically representative datasets and find that it substantially increases\nthe diversity of generated images, both for the worst performing regions and on\naverage, while simultaneously maintaining or improving image quality and\nconsistency. Additionally, qualitative analyses reveal that diversity of\ngenerated images is significantly improved, including along the lines of\nreductive region portrayals present in the original model. We hope that this\nwork is a step towards text-to-image generative models that reflect the true\ngeographic diversity of the world.\n","authors":["Reyhane Askari Hemmat","Melissa Hall","Alicia Sun","Candace Ross","Michal Drozdzal","Adriana Romero-Soriano"],"pdf_url":"https://arxiv.org/pdf/2406.04551v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.05737v3","updated":"2024-08-02T16:09:14Z","published":"2023-02-11T16:26:57Z","title":"A Reparameterized Discrete Diffusion Model for Text Generation","summary":"  This work studies discrete diffusion probabilistic models with applications\nto natural language generation. We derive an alternative yet equivalent\nformulation of the sampling from discrete diffusion processes and leverage this\ninsight to develop a family of reparameterized discrete diffusion models. The\nderived generic framework is highly flexible, offers a fresh perspective of the\ngeneration process in discrete diffusion models, and features more effective\ntraining and decoding techniques. We conduct extensive experiments to evaluate\nthe text generation capability of our model, demonstrating significant\nimprovements over existing diffusion models.\n","authors":["Lin Zheng","Jianbo Yuan","Lei Yu","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2302.05737v3.pdf","comment":"COLM 2024; Code available at\n  https://github.com/hkunlp/reparam-discrete-diffusion"},{"id":"http://arxiv.org/abs/2408.01349v1","updated":"2024-08-02T15:54:49Z","published":"2024-08-02T15:54:49Z","title":"PC$^2$: Pseudo-Classification Based Pseudo-Captioning for Noisy\n  Correspondence Learning in Cross-Modal Retrieval","summary":"  In the realm of cross-modal retrieval, seamlessly integrating diverse\nmodalities within multimedia remains a formidable challenge, especially given\nthe complexities introduced by noisy correspondence learning (NCL). Such noise\noften stems from mismatched data pairs, which is a significant obstacle\ndistinct from traditional noisy labels. This paper introduces\nPseudo-Classification based Pseudo-Captioning (PC$^2$) framework to address\nthis challenge. PC$^2$ offers a threefold strategy: firstly, it establishes an\nauxiliary \"pseudo-classification\" task that interprets captions as categorical\nlabels, steering the model to learn image-text semantic similarity through a\nnon-contrastive mechanism. Secondly, unlike prevailing margin-based techniques,\ncapitalizing on PC$^2$'s pseudo-classification capability, we generate\npseudo-captions to provide more informative and tangible supervision for each\nmismatched pair. Thirdly, the oscillation of pseudo-classification is borrowed\nto assistant the correction of correspondence. In addition to technical\ncontributions, we develop a realistic NCL dataset called Noise of Web (NoW),\nwhich could be a new powerful NCL benchmark where noise exists naturally.\nEmpirical evaluations of PC$^2$ showcase marked improvements over existing\nstate-of-the-art robust cross-modal retrieval techniques on both simulated and\nrealistic datasets with various NCL settings. The contributed dataset and\nsource code are released at https://github.com/alipay/PC2-NoiseofWeb.\n","authors":["Yue Duan","Zhangxuan Gu","Zhenzhe Ying","Lei Qi","Changhua Meng","Yinghuan Shi"],"pdf_url":"https://arxiv.org/pdf/2408.01349v1.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2403.13940v2","updated":"2024-08-02T15:54:21Z","published":"2024-03-20T19:25:11Z","title":"A multi-criteria approach for selecting an explanation from the set of\n  counterfactuals produced by an ensemble of explainers","summary":"  Counterfactuals are widely used to explain ML model predictions by providing\nalternative scenarios for obtaining the more desired predictions. They can be\ngenerated by a variety of methods that optimize different, sometimes\nconflicting, quality measures and produce quite different solutions. However,\nchoosing the most appropriate explanation method and one of the generated\ncounterfactuals is not an easy task. Instead of forcing the user to test many\ndifferent explanation methods and analysing conflicting solutions, in this\npaper, we propose to use a multi-stage ensemble approach that will select\nsingle counterfactual based on the multiple-criteria analysis. It offers a\ncompromise solution that scores well on several popular quality measures. This\napproach exploits the dominance relation and the ideal point decision aid\nmethod, which selects one counterfactual from the Pareto front. The conducted\nexperiments demonstrated that the proposed approach generates fully actionable\ncounterfactuals with attractive compromise values of the considered quality\nmeasures.\n","authors":["Ignacy Stępka","Mateusz Lango","Jerzy Stefanowski"],"pdf_url":"https://arxiv.org/pdf/2403.13940v2.pdf","comment":"17 pages, 2 figures"},{"id":"http://arxiv.org/abs/2408.01343v1","updated":"2024-08-02T15:41:16Z","published":"2024-08-02T15:41:16Z","title":"StitchFusion: Weaving Any Visual Modalities to Enhance Multimodal\n  Semantic Segmentation","summary":"  Multimodal semantic segmentation shows significant potential for enhancing\nsegmentation accuracy in complex scenes. However, current methods often\nincorporate specialized feature fusion modules tailored to specific modalities,\nthereby restricting input flexibility and increasing the number of training\nparameters. To address these challenges, we propose StitchFusion, a\nstraightforward yet effective modal fusion framework that integrates\nlarge-scale pre-trained models directly as encoders and feature fusers. This\napproach facilitates comprehensive multi-modal and multi-scale feature fusion,\naccommodating any visual modal inputs. Specifically, Our framework achieves\nmodal integration during encoding by sharing multi-modal visual information. To\nenhance information exchange across modalities, we introduce a\nmulti-directional adapter module (MultiAdapter) to enable cross-modal\ninformation transfer during encoding. By leveraging MultiAdapter to propagate\nmulti-scale information across pre-trained encoders during the encoding\nprocess, StitchFusion achieves multi-modal visual information integration\nduring encoding. Extensive comparative experiments demonstrate that our model\nachieves state-of-the-art performance on four multi-modal segmentation datasets\nwith minimal additional parameters. Furthermore, the experimental integration\nof MultiAdapter with existing Feature Fusion Modules (FFMs) highlights their\ncomplementary nature. Our code is available at StitchFusion_repo.\n","authors":["Bingyu Li","Da Zhang","Zhiyuan Zhao","Junyu Gao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.01343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11891v3","updated":"2024-08-02T15:38:38Z","published":"2023-10-18T11:20:59Z","title":"A Hyperparameter Study for Quantum Kernel Methods","summary":"  Quantum kernel methods are a promising method in quantum machine learning\nthanks to the guarantees connected to them. Their accessibility for analytic\nconsiderations also opens up the possibility of prescreening datasets based on\ntheir potential for a quantum advantage. To do so, earlier works developed the\ngeometric difference, which can be understood as a closeness measure between\ntwo kernel-based machine learning approaches, most importantly between a\nquantum kernel and a classical kernel. This metric links the quantum and\nclassical model complexities, and it was developed to bound generalization\nerror. Therefore, it raises the question of how this metric behaves in an\nempirical setting. In this work, we investigate the effects of hyperparameter\nchoice on the model performance and the generalization gap between classical\nand quantum kernels. The importance of hyperparameters is well known also for\nclassical machine learning. Of special interest are hyperparameters associated\nwith the quantum Hamiltonian evolution feature map, as well as the number of\nqubits to trace out before computing a projected quantum kernel. We conduct a\nthorough investigation of the hyperparameters across 11 datasets and we\nidentify certain aspects that can be exploited. Analyzing the effects of\ncertain hyperparameter settings on the empirical performance, as measured by\ncross validation accuracy, and generalization ability, as measured by geometric\ndifference described above, brings us one step closer to understanding the\npotential of quantum kernel methods on classical datasets.\n","authors":["Sebastian Egginger","Alona Sakhnenko","Jeanette Miriam Lorenz"],"pdf_url":"https://arxiv.org/pdf/2310.11891v3.pdf","comment":"Expanded implications of the paper"},{"id":"http://arxiv.org/abs/2408.01337v1","updated":"2024-08-02T15:34:05Z","published":"2024-08-02T15:34:05Z","title":"MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language\n  Models","summary":"  Multimodal models that jointly process audio and language hold great promise\nin audio understanding and are increasingly being adopted in the music domain.\nBy allowing users to query via text and obtain information about a given audio\ninput, these models have the potential to enable a variety of music\nunderstanding tasks via language-based interfaces. However, their evaluation\nposes considerable challenges, and it remains unclear how to effectively assess\ntheir ability to correctly interpret music-related inputs with current methods.\nMotivated by this, we introduce MuChoMusic, a benchmark for evaluating music\nunderstanding in multimodal language models focused on audio. MuChoMusic\ncomprises 1,187 multiple-choice questions, all validated by human annotators,\non 644 music tracks sourced from two publicly available music datasets, and\ncovering a wide variety of genres. Questions in the benchmark are crafted to\nassess knowledge and reasoning abilities across several dimensions that cover\nfundamental musical concepts and their relation to cultural and functional\ncontexts. Through the holistic analysis afforded by the benchmark, we evaluate\nfive open-source models and identify several pitfalls, including an\nover-reliance on the language modality, pointing to a need for better\nmultimodal integration. Data and code are open-sourced.\n","authors":["Benno Weck","Ilaria Manco","Emmanouil Benetos","Elio Quinton","George Fazekas","Dmitry Bogdanov"],"pdf_url":"https://arxiv.org/pdf/2408.01337v1.pdf","comment":"Accepted at ISMIR 2024. Data: https://doi.org/10.5281/zenodo.12709974\n  Code: https://github.com/mulab-mir/muchomusic Supplementary material:\n  https://mulab-mir.github.io/muchomusic"},{"id":"http://arxiv.org/abs/2408.01336v1","updated":"2024-08-02T15:33:04Z","published":"2024-08-02T15:33:04Z","title":"Sparse Linear Regression when Noises and Covariates are Heavy-Tailed and\n  Contaminated by Outliers","summary":"  We investigate a problem estimating coefficients of linear regression under\nsparsity assumption when covariates and noises are sampled from heavy tailed\ndistributions. Additionally, we consider the situation where not only\ncovariates and noises are sampled from heavy tailed distributions but also\ncontaminated by outliers. Our estimators can be computed efficiently, and\nexhibit sharp error bounds.\n","authors":["Takeyuki Sasai","Hironori Fujisawa"],"pdf_url":"https://arxiv.org/pdf/2408.01336v1.pdf","comment":"This research builds on and improves the results of arxiv:2206.07594.\n  There will be no further update for the earlier manuscript"},{"id":"http://arxiv.org/abs/2404.02476v3","updated":"2024-08-02T15:30:14Z","published":"2024-04-03T05:32:10Z","title":"Deep Reinforcement Learning for Traveling Purchaser Problems","summary":"  The traveling purchaser problem (TPP) is an important combinatorial\noptimization problem with broad applications. Due to the coupling between\nrouting and purchasing, existing works on TPPs commonly address route\nconstruction and purchase planning simultaneously, which, however, leads to\nexact methods with high computational cost and heuristics with sophisticated\ndesign but limited performance. In sharp contrast, we propose a novel approach\nbased on deep reinforcement learning (DRL), which addresses route construction\nand purchase planning separately, while evaluating and optimizing the solution\nfrom a global perspective. The key components of our approach include a\nbipartite graph representation for TPPs to capture the market-product\nrelations, and a policy network that extracts information from the bipartite\ngraph and uses it to sequentially construct the route. One significant benefit\nof our framework is that we can efficiently construct the route using the\npolicy network, and once the route is determined, the associated purchasing\nplan can be easily derived through linear programming, while, leveraging DRL,\nwe can train the policy network to optimize the global solution objective.\nFurthermore, by introducing a meta-learning strategy, the policy network can be\ntrained stably on large-sized TPP instances, and generalize well across\ninstances of varying sizes and distributions, even to much larger instances\nthat are never seen during training. Experiments on various synthetic TPP\ninstances and the TPPLIB benchmark demonstrate that our DRL-based approach can\nsignificantly outperform well-established TPP heuristics, reducing the\noptimality gap by 40%-90%, and also showing an advantage in runtime, especially\non large-sized instances.\n","authors":["Haofeng Yuan","Rongping Zhu","Wanlu Yang","Shiji Song","Keyou You","Yuli Zhang","C. L. Philip Chen"],"pdf_url":"https://arxiv.org/pdf/2404.02476v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01332v1","updated":"2024-08-02T15:29:59Z","published":"2024-08-02T15:29:59Z","title":"HMDN: Hierarchical Multi-Distribution Network for Click-Through Rate\n  Prediction","summary":"  As the recommendation service needs to address increasingly diverse\ndistributions, such as multi-population, multi-scenario, multitarget, and\nmulti-interest, more and more recent works have focused on multi-distribution\nmodeling and achieved great progress. However, most of them only consider\nmodeling in a single multi-distribution manner, ignoring that mixed\nmulti-distributions often coexist and form hierarchical relationships. To\naddress these challenges, we propose a flexible modeling paradigm, named\nHierarchical Multi-Distribution Network (HMDN), which efficiently models these\nhierarchical relationships and can seamlessly integrate with existing\nmulti-distribution methods, such as Mixture of-Experts (MoE) and Dynamic-Weight\n(DW) models. Specifically, we first design a hierarchical multi-distribution\nrepresentation refinement module, employing a multi-level residual quantization\nto obtain fine-grained hierarchical representation. Then, the refined\nhierarchical representation is integrated into the existing single\nmulti-distribution models, seamlessly expanding them into mixed\nmulti-distribution models. Experimental results on both public and industrial\ndatasets validate the effectiveness and flexibility of HMDN.\n","authors":["Xingyu Lou","Yu Yang","Kuiyao Dong","Heyuan Huang","Wenyi Yu","Ping Wang","Xiu Li","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2408.01332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01331v1","updated":"2024-08-02T15:29:39Z","published":"2024-08-02T15:29:39Z","title":"UnifiedNN: Efficient Neural Network Training on the Cloud","summary":"  Nowadays, cloud-based services are widely favored over the traditional\napproach of locally training a Neural Network (NN) model. Oftentimes, a cloud\nservice processes multiple requests from users--thus training multiple NN\nmodels concurrently. However, training NN models concurrently is a challenging\nprocess, which typically requires significant amounts of available computing\nresources and takes a long time to complete. In this paper, we present\nUnifiedNN to effectively train multiple NN models concurrently on the cloud.\nUnifiedNN effectively \"combines\" multiple NN models and features several memory\nand time conservation mechanisms to train multiple NN models simultaneously\nwithout impacting the accuracy of the training process. Specifically, UnifiedNN\nmerges multiple NN models and creates a large singular unified model in order\nto efficiently train all models at once. We have implemented a prototype of\nUnifiedNN in PyTorch and we have compared its performance with relevant\nstate-of-the-art frameworks. Our experimental results demonstrate that\nUnifiedNN can reduce memory consumption by up to 53% and training time by up to\n81% when compared with vanilla PyTorch without impacting the model training and\ntesting accuracy. Finally, our results indicate that UnifiedNN can reduce\nmemory consumption by up to 52% and training time by up to 41% when compared to\nstate-of-the-art frameworks when training multiple models concurrently.\n","authors":["Sifat Ut Taki","Spyridon Mastorakis","Arthi Padmanabhan"],"pdf_url":"https://arxiv.org/pdf/2408.01331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.00736v4","updated":"2024-08-02T15:26:37Z","published":"2023-01-02T16:11:05Z","title":"Mixed moving average field guided learning for spatio-temporal data","summary":"  Influenced mixed moving average fields are a versatile modeling class for\nspatio-temporal data. However, their predictive distribution is not generally\nknown. Under this modeling assumption, we define a novel spatio-temporal\nembedding and a theory-guided machine learning approach that employs a\ngeneralized Bayesian algorithm to make ensemble forecasts. We use Lipschitz\npredictors and determine fixed-time and any-time PAC Bayesian bounds in the\nbatch learning setting. Performing causal forecast is a highlight of our\nmethodology as its potential application to data with spatial and temporal\nshort and long-range dependence. We then test the performance of our learning\nmethodology by using linear predictors and data sets simulated from a\nspatio-temporal Ornstein-Uhlenbeck process.\n","authors":["Imma Valentina Curato","Orkun Furat","Lorenzo Proietti","Bennet Stroeh"],"pdf_url":"https://arxiv.org/pdf/2301.00736v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02659v2","updated":"2024-08-02T15:13:26Z","published":"2024-07-02T20:49:21Z","title":"LLMs Plagiarize: Ensuring Responsible Sourcing of Large Language Model\n  Training Data Through Knowledge Graph Comparison","summary":"  In light of recent legal allegations brought by publishers, newspapers, and\nother creators of copyrighted corpora against large language model developers\nwho use their copyrighted materials for training or fine-tuning purposes, we\npropose a novel system, a variant of a plagiarism detection system, that\nassesses whether a knowledge source has been used in the training or\nfine-tuning of a large language model. Unlike current methods, we utilize an\napproach that uses Resource Description Framework (RDF) triples to create\nknowledge graphs from both a source document and an LLM continuation of that\ndocument. These graphs are then analyzed with respect to content using cosine\nsimilarity and with respect to structure using a normalized version of graph\nedit distance that shows the degree of isomorphism. Unlike traditional\nplagiarism systems that focus on content matching and keyword identification\nbetween a source and a target corpus, our approach enables a broader and more\naccurate evaluation of similarity between a source document and LLM\ncontinuation by focusing on relationships between ideas and their organization\nwith regards to others. Additionally, our approach does not require access to\nLLM metrics like perplexity that may be unavailable in closed large language\nmodel \"black-box\" systems, as well as the training corpus. We thus assess\nwhether an LLM has \"plagiarized\" a corpus in its continuation through\nsimilarity measures. A prototype of our system will be found on a hyperlinked\nGitHub repository.\n","authors":["Devam Mondal","Carlo Lipizzi"],"pdf_url":"https://arxiv.org/pdf/2407.02659v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01318v1","updated":"2024-08-02T15:12:52Z","published":"2024-08-02T15:12:52Z","title":"Point Prediction for Streaming Data","summary":"  We present two new approaches for point prediction with streaming data. One\nis based on the Count-Min sketch (CMS) and the other is based on Gaussian\nprocess priors with a random bias. These methods are intended for the most\ngeneral predictive problems where no true model can be usefully formulated for\nthe data stream. In statistical contexts, this is often called the\n$\\mathcal{M}$-open problem class. Under the assumption that the data consists\nof i.i.d samples from a fixed distribution function $F$, we show that the\nCMS-based estimates of the distribution function are consistent.\n  We compare our new methods with two established predictors in terms of\ncumulative $L^1$ error. One is based on the Shtarkov solution (often called the\nnormalized maximum likelihood) in the normal experts setting and the other is\nbased on Dirichlet process priors. These comparisons are for two cases. The\nfirst is one-pass meaning that the updating of the predictors is done using the\nfact that the CMS is a sketch. For predictors that are not one-pass, we use\nstreaming $K$-means to give a representative subset of fixed size that can be\nupdated as data accumulate.\n  Preliminary computational work suggests that the one-pass median version of\nthe CMS method is rarely outperformed by the other methods for sufficiently\ncomplex data. We also find that predictors based on Gaussian process priors\nwith random biases perform well. The Shtarkov predictors we use here did not\nperform as well probably because we were only using the simplest example. The\nother predictors seemed to perform well mainly when the data did not look like\nthey came from an M-open data generator.\n","authors":["Aleena Chanda","N. V. Vinodchandran","Bertrand Clarke"],"pdf_url":"https://arxiv.org/pdf/2408.01318v1.pdf","comment":"42 pages, two figures"},{"id":"http://arxiv.org/abs/2408.01307v1","updated":"2024-08-02T15:00:04Z","published":"2024-08-02T15:00:04Z","title":"Decentralized Smoothing ADMM for Quantile Regression with Non-Convex\n  Sparse Penalties","summary":"  In the rapidly evolving internet-of-things (IoT) ecosystem, effective data\nanalysis techniques are crucial for handling distributed data generated by\nsensors. Addressing the limitations of existing methods, such as the\nsub-gradient approach, which fails to distinguish between active and non-active\ncoefficients effectively, this paper introduces the decentralized smoothing\nalternating direction method of multipliers (DSAD) for penalized quantile\nregression. Our method leverages non-convex sparse penalties like the minimax\nconcave penalty (MCP) and smoothly clipped absolute deviation (SCAD), improving\nthe identification and retention of significant predictors. DSAD incorporates a\ntotal variation norm within a smoothing ADMM framework, achieving consensus\namong distributed nodes and ensuring uniform model performance across disparate\ndata sources. This approach overcomes traditional convergence challenges\nassociated with non-convex penalties in decentralized settings. We present\ntheoretical proofs and extensive simulation results to validate the\neffectiveness of the DSAD, demonstrating its superiority in achieving reliable\nconvergence and enhancing estimation accuracy compared with prior methods.\n","authors":["Reza Mirzaeifard","Diyako Ghaderyan","Stefan Werner"],"pdf_url":"https://arxiv.org/pdf/2408.01307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13586v2","updated":"2024-08-02T14:59:48Z","published":"2024-05-22T12:30:25Z","title":"Bond Graphs for multi-physics informed Neural Networks for multi-variate\n  time series","summary":"  In the trend of hybrid Artificial Intelligence techniques, Physical-Informed\nMachine Learning has seen a growing interest. It operates mainly by imposing\ndata, learning, or architecture bias with simulation data, Partial Differential\nEquations, or equivariance and invariance properties. While it has shown great\nsuccess on tasks involving one physical domain, such as fluid dynamics,\nexisting methods are not adapted to tasks with complex multi-physical and\nmulti-domain phenomena. In addition, it is mainly formulated as an end-to-end\nlearning scheme. To address these challenges, we propose to leverage Bond\nGraphs, a multi-physics modeling approach, together with Message Passing Graph\nNeural Networks. We propose a Neural Bond graph Encoder (NBgE) producing\nmulti-physics-informed representations that can be fed into any task-specific\nmodel. It provides a unified way to integrate both data and architecture biases\nin deep learning. Our experiments on two challenging multi-domain physical\nsystems - a Direct Current Motor and the Respiratory System - demonstrate the\neffectiveness of our approach on a multivariate time-series forecasting task.\n","authors":["Alexis-Raja Brachet","Pierre-Yves Richard","Céline Hudelot"],"pdf_url":"https://arxiv.org/pdf/2405.13586v2.pdf","comment":"9 pages, 3 figures, paper under review"},{"id":"http://arxiv.org/abs/2407.21043v2","updated":"2024-08-02T14:58:54Z","published":"2024-07-22T04:07:12Z","title":"CP-Prompt: Composition-Based Cross-modal Prompting for\n  Domain-Incremental Continual Learning","summary":"  The key challenge of cross-modal domain-incremental learning (DIL) is to\nenable the learning model to continuously learn from novel data with different\nfeature distributions under the same task without forgetting old ones. However,\nexisting top-performing methods still cause high forgetting rates, by lacking\nintra-domain knowledge extraction and inter-domain common prompting strategy.\nIn this paper, we propose a simple yet effective framework, CP-Prompt, by\ntraining limited parameters to instruct a pre-trained model to learn new\ndomains and avoid forgetting existing feature distributions. CP-Prompt captures\nintra-domain knowledge by compositionally inserting personalized prompts on\nmulti-head self-attention layers and then learns the inter-domain knowledge\nwith a common prompting strategy. CP-Prompt shows superiority compared with\nstate-of-the-art baselines among three widely evaluated DIL tasks. The source\ncode is available at https://github.com/dannis97500/CP_Prompt.\n","authors":["Yu Feng","Zhen Tian","Yifan Zhu","Zongfu Han","Haoran Luo","Guangwei Zhang","Meina Song"],"pdf_url":"https://arxiv.org/pdf/2407.21043v2.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2401.13979v2","updated":"2024-08-02T14:50:05Z","published":"2024-01-25T06:45:32Z","title":"Routoo: Learning to Route to Large Language Models Effectively","summary":"  Developing foundational large language models (LLMs) is becoming increasingly\ncostly and inefficient. Also, closed-source and larger open-source models\ngenerally offer better response quality but come with higher inference costs\nthan smaller models. In this paper, we introduce Routoo, an architecture\ndesigned to optimize the selection of LLMs for specific prompts based on\nperformance, cost, and efficiency. Routoo consists of two key components: a\nperformance predictor and a cost-aware decoding. The performance predictor is a\nlightweight LLM that estimates the performance of various underlying LLMs\nwithout needing to execute and evaluate them. The cost-aware decoding then\nselects the most suitable model based on these predictions and other\nconstraints like cost and latency. We evaluated Routoo using the MMLU benchmark\nacross 57 domains employing open-source models. Our results show that Routoo\nmatches the performance of the Mixtral 8x7b model while reducing inference\ncosts by one-third. Additionally, by allowing increased costs, Routoo surpasses\nMixtral's accuracy by over 5% at equivalent costs, achieving an accuracy of\n75.9%. When integrating GPT4 into our model pool, Routoo nearly matches GPT4's\nperformance at half the cost and exceeds it with a 25% cost reduction. These\noutcomes highlight Routoo's potential to create new SOTA in a cost-effective\nmanner by leveraging the collective knowledge of multiple LLMs.\n","authors":["Alireza Mohammadshahi","Arshad Rafiq Shaikh","Majid Yazdani"],"pdf_url":"https://arxiv.org/pdf/2401.13979v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01301v1","updated":"2024-08-02T14:43:45Z","published":"2024-08-02T14:43:45Z","title":"A Decision-driven Methodology for Designing Uncertainty-aware AI\n  Self-Assessment","summary":"  Artificial intelligence (AI) has revolutionized decision-making processes and\nsystems throughout society and, in particular, has emerged as a significant\ntechnology in high-impact scenarios of national interest. Yet, despite AI's\nimpressive predictive capabilities in controlled settings, it still suffers\nfrom a range of practical setbacks preventing its widespread use in various\ncritical scenarios. In particular, it is generally unclear if a given AI\nsystem's predictions can be trusted by decision-makers in downstream\napplications. To address the need for more transparent, robust, and trustworthy\nAI systems, a suite of tools has been developed to quantify the uncertainty of\nAI predictions and, more generally, enable AI to \"self-assess\" the reliability\nof its predictions. In this manuscript, we categorize methods for AI\nself-assessment along several key dimensions and provide guidelines for\nselecting and designing the appropriate method for a practitioner's needs. In\nparticular, we focus on uncertainty estimation techniques that consider the\nimpact of self-assessment on the choices made by downstream decision-makers and\non the resulting costs and benefits of decision outcomes. To demonstrate the\nutility of our methodology for self-assessment design, we illustrate its use\nfor two realistic national-interest scenarios. This manuscript is a practical\nguide for machine learning engineers and AI system users to select the ideal\nself-assessment techniques for each problem.\n","authors":["Gregory Canal","Vladimir Leung","Philip Sage","Eric Heim","I-Jeng Wang"],"pdf_url":"https://arxiv.org/pdf/2408.01301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01300v1","updated":"2024-08-02T14:41:36Z","published":"2024-08-02T14:41:36Z","title":"Assessing Robustness of Machine Learning Models using Covariate\n  Perturbations","summary":"  As machine learning models become increasingly prevalent in critical\ndecision-making models and systems in fields like finance, healthcare, etc.,\nensuring their robustness against adversarial attacks and changes in the input\ndata is paramount, especially in cases where models potentially overfit. This\npaper proposes a comprehensive framework for assessing the robustness of\nmachine learning models through covariate perturbation techniques. We explore\nvarious perturbation strategies to assess robustness and examine their impact\non model predictions, including separate strategies for numeric and non-numeric\nvariables, summaries of perturbations to assess and compare model robustness\nacross different scenarios, and local robustness diagnosis to identify any\nregions in the data where a model is particularly unstable. Through empirical\nstudies on real world dataset, we demonstrate the effectiveness of our approach\nin comparing robustness across models, identifying the instabilities in the\nmodel, and enhancing model robustness.\n","authors":["Arun Prakash R","Anwesha Bhattacharyya","Joel Vaughan","Vijayan N. Nair"],"pdf_url":"https://arxiv.org/pdf/2408.01300v1.pdf","comment":"31 pages, 11 figures, 14 tables"},{"id":"http://arxiv.org/abs/2408.00713v2","updated":"2024-08-02T14:40:19Z","published":"2024-08-01T16:58:54Z","title":"Reinforcement Learning applied to Insurance Portfolio Pursuit","summary":"  When faced with a new customer, many factors contribute to an insurance\nfirm's decision of what offer to make to that customer. In addition to the\nexpected cost of providing the insurance, the firm must consider the other\noffers likely to be made to the customer, and how sensitive the customer is to\ndifferences in price. Moreover, firms often target a specific portfolio of\ncustomers that could depend on, e.g., age, location, and occupation. Given such\na target portfolio, firms may choose to modulate an individual customer's offer\nbased on whether the firm desires the customer within their portfolio. We term\nthe problem of modulating offers to achieve a desired target portfolio the\nportfolio pursuit problem. Having formulated the portfolio pursuit problem as a\nsequential decision making problem, we devise a novel reinforcement learning\nalgorithm for its solution. We test our method on a complex synthetic market\nenvironment, and demonstrate that it outperforms a baseline method which mimics\ncurrent industry approaches to portfolio pursuit.\n","authors":["Edward James Young","Alistair Rogers","Elliott Tong","James Jordon"],"pdf_url":"https://arxiv.org/pdf/2408.00713v2.pdf","comment":"16 pages, 1 figure"},{"id":"http://arxiv.org/abs/2408.01297v1","updated":"2024-08-02T14:37:28Z","published":"2024-08-02T14:37:28Z","title":"Optimal Mixed Integer Linear Optimization Trained Multivariate\n  Classification Trees","summary":"  Multivariate decision trees are powerful machine learning tools for\nclassification and regression that attract many researchers and industry\nprofessionals. An optimal binary tree has two types of vertices, (i) branching\nvertices which have exactly two children and where datapoints are assessed on a\nset of discrete features and (ii) leaf vertices at which datapoints are given a\nprediction, and can be obtained by solving a biobjective optimization problem\nthat seeks to (i) maximize the number of correctly classified datapoints and\n(ii) minimize the number of branching vertices. Branching vertices are linear\ncombinations of training features and therefore can be thought of as\nhyperplanes. In this paper, we propose two cut-based mixed integer linear\noptimization (MILO) formulations for designing optimal binary classification\ntrees (leaf vertices assign discrete classes). Our models leverage on-the-fly\nidentification of minimal infeasible subsystems (MISs) from which we derive\ncutting planes that hold the form of packing constraints. We show theoretical\nimprovements on the strongest flow-based MILO formulation currently in the\nliterature and conduct experiments on publicly available datasets to show our\nmodels' ability to scale, strength against traditional branch and bound\napproaches, and robustness in out-of-sample test performance. Our code and data\nare available on GitHub.\n","authors":["Brandon Alston","Illya V. Hicks"],"pdf_url":"https://arxiv.org/pdf/2408.01297v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2206.04857"},{"id":"http://arxiv.org/abs/2405.03389v2","updated":"2024-08-02T14:33:32Z","published":"2024-05-06T11:51:09Z","title":"Don't Waste Your Time: Early Stopping Cross-Validation","summary":"  State-of-the-art automated machine learning systems for tabular data often\nemploy cross-validation; ensuring that measured performances generalize to\nunseen data, or that subsequent ensembling does not overfit. However, using\nk-fold cross-validation instead of holdout validation drastically increases the\ncomputational cost of validating a single configuration. While ensuring better\ngeneralization and, by extension, better performance, the additional cost is\noften prohibitive for effective model selection within a time budget. We aim to\nmake model selection with cross-validation more effective. Therefore, we study\nearly stopping the process of cross-validation during model selection. We\ninvestigate the impact of early stopping on random search for two algorithms,\nMLP and random forest, across 36 classification datasets. We further analyze\nthe impact of the number of folds by considering 3-, 5-, and 10-folds. In\naddition, we investigate the impact of early stopping with Bayesian\noptimization instead of random search and also repeated cross-validation. Our\nexploratory study shows that even a simple-to-understand and easy-to-implement\nmethod consistently allows model selection to converge faster; in ~94% of all\ndatasets, on average by ~214%. Moreover, stopping cross-validation enables\nmodel selection to explore the search space more exhaustively by considering\n+167% configurations on average within one hour, while also obtaining better\noverall performance.\n","authors":["Edward Bergman","Lennart Purucker","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2405.03389v2.pdf","comment":"Accepted at Third International Conference on Automated Machine\n  Learning (AutoML 2024); for code, see\n  https://github.com/automl/DontWasteYourTime-early-stopping"},{"id":"http://arxiv.org/abs/2408.01294v1","updated":"2024-08-02T14:31:37Z","published":"2024-08-02T14:31:37Z","title":"Feature Clock: High-Dimensional Effects in Two-Dimensional Plots","summary":"  Humans struggle to perceive and interpret high-dimensional data. Therefore,\nhigh-dimensional data are often projected into two dimensions for\nvisualization. Many applications benefit from complex nonlinear dimensionality\nreduction techniques, but the effects of individual high-dimensional features\nare hard to explain in the two-dimensional space. Most visualization solutions\nuse multiple two-dimensional plots, each showing the effect of one\nhigh-dimensional feature in two dimensions; this approach creates a need for a\nvisual inspection of k plots for a k-dimensional input space. Our solution,\nFeature Clock, provides a novel approach that eliminates the need to inspect\nthese k plots to grasp the influence of original features on the data structure\ndepicted in two dimensions. Feature Clock enhances the explainability and\ncompactness of visualizations of embedded data and is available in an\nopen-source Python library.\n","authors":["Olga Ovcharenko","Rita Sevastjanova","Valentina Boeva"],"pdf_url":"https://arxiv.org/pdf/2408.01294v1.pdf","comment":"To be published in IEEE VIS 2024"},{"id":"http://arxiv.org/abs/2407.14962v3","updated":"2024-08-02T14:26:55Z","published":"2024-07-20T18:48:35Z","title":"Recent Advances in Generative AI and Large Language Models: Current\n  Status, Challenges, and Perspectives","summary":"  The emergence of Generative Artificial Intelligence (AI) and Large Language\nModels (LLMs) has marked a new era of Natural Language Processing (NLP),\nintroducing unprecedented capabilities that are revolutionizing various\ndomains. This paper explores the current state of these cutting-edge\ntechnologies, demonstrating their remarkable advancements and wide-ranging\napplications. Our paper contributes to providing a holistic perspective on the\ntechnical foundations, practical applications, and emerging challenges within\nthe evolving landscape of Generative AI and LLMs. We believe that understanding\nthe generative capabilities of AI systems and the specific context of LLMs is\ncrucial for researchers, practitioners, and policymakers to collaboratively\nshape the responsible and ethical integration of these technologies into\nvarious domains. Furthermore, we identify and address main research gaps,\nproviding valuable insights to guide future research endeavors within the AI\nresearch community.\n","authors":["Desta Haileselassie Hagos","Rick Battle","Danda B. Rawat"],"pdf_url":"https://arxiv.org/pdf/2407.14962v3.pdf","comment":"This version is accepted for publication in the journal of IEEE\n  Transactions on Artificial Intelligence (TAI)"},{"id":"http://arxiv.org/abs/2308.11635v2","updated":"2024-08-02T14:25:40Z","published":"2023-08-13T23:54:40Z","title":"Semi-Supervised Dual-Stream Self-Attentive Adversarial Graph Contrastive\n  Learning for Cross-Subject EEG-based Emotion Recognition","summary":"  Electroencephalography (EEG) is an objective tool for emotion recognition\nwith promising applications. However, the scarcity of labeled data remains a\nmajor challenge in this field, limiting the widespread use of EEG-based emotion\nrecognition. In this paper, a semi-supervised Dual-stream Self-Attentive\nAdversarial Graph Contrastive learning framework (termed as DS-AGC) is proposed\nto tackle the challenge of limited labeled data in cross-subject EEG-based\nemotion recognition. The DS-AGC framework includes two parallel streams for\nextracting non-structural and structural EEG features. The non-structural\nstream incorporates a semi-supervised multi-domain adaptation method to\nalleviate distribution discrepancy among labeled source domain, unlabeled\nsource domain, and unknown target domain. The structural stream develops a\ngraph contrastive learning method to extract effective graph-based feature\nrepresentation from multiple EEG channels in a semi-supervised manner. Further,\na self-attentive fusion module is developed for feature fusion, sample\nselection, and emotion recognition, which highlights EEG features more relevant\nto emotions and data samples in the labeled source domain that are closer to\nthe target domain. Extensive experiments conducted on two benchmark databases\n(SEED and SEED-IV) using a semi-supervised cross-subject leave-one-subject-out\ncross-validation evaluation scheme show that the proposed model outperforms\nexisting methods under different incomplete label conditions (with an average\nimprovement of 5.83% on SEED and 6.99% on SEED-IV), demonstrating its\neffectiveness in addressing the label scarcity problem in cross-subject\nEEG-based emotion recognition.\n","authors":["Weishan Ye","Zhiguo Zhang","Fei Teng","Min Zhang","Jianhong Wang","Dong Ni","Fali Li","Peng Xu","Zhen Liang"],"pdf_url":"https://arxiv.org/pdf/2308.11635v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2304.06496"},{"id":"http://arxiv.org/abs/2408.01283v1","updated":"2024-08-02T14:09:39Z","published":"2024-08-02T14:09:39Z","title":"A Tiny Supervised ODL Core with Auto Data Pruning for Human Activity\n  Recognition","summary":"  In this paper, we introduce a low-cost and low-power tiny supervised\non-device learning (ODL) core that can address the distributional shift of\ninput data for human activity recognition. Although ODL for resource-limited\nedge devices has been studied recently, how exactly to provide the training\nlabels to these devices at runtime remains an open-issue. To address this\nproblem, we propose to combine an automatic data pruning with supervised ODL to\nreduce the number queries needed to acquire predicted labels from a nearby\nteacher device and thus save power consumption during model retraining. The\ndata pruning threshold is automatically tuned, eliminating a manual threshold\ntuning. As a tinyML solution at a few mW for the human activity recognition, we\ndesign a supervised ODL core that supports our automatic data pruning using a\n45nm CMOS process technology. We show that the required memory size for the\ncore is smaller than the same-shaped multilayer perceptron (MLP) and the power\nconsumption is only 3.39mW. Experiments using a human activity recognition\ndataset show that the proposed automatic data pruning reduces the communication\nvolume by 55.7% and power consumption accordingly with only 0.9% accuracy loss.\n","authors":["Hiroki Matsutani","Radu Marculescu"],"pdf_url":"https://arxiv.org/pdf/2408.01283v1.pdf","comment":"IEEE BSN 2024 (accepted)"},{"id":"http://arxiv.org/abs/2310.04561v2","updated":"2024-08-02T14:08:59Z","published":"2023-10-06T19:55:40Z","title":"DragD3D: Realistic Mesh Editing with Rigidity Control Driven by 2D\n  Diffusion Priors","summary":"  Direct mesh editing and deformation are key components in the geometric\nmodeling and animation pipeline. Mesh editing methods are typically framed as\noptimization problems combining user-specified vertex constraints with a\nregularizer that determines the position of the rest of the vertices. The\nchoice of the regularizer is key to the realism and authenticity of the final\nresult. Physics and geometry-based regularizers are not aware of the global\ncontext and semantics of the object, and the more recent deep learning priors\nare limited to a specific class of 3D object deformations. Our main\ncontribution is a vertex-based mesh editing method called DragD3D based on (1)\na novel optimization formulation that decouples the rotation and stretch\ncomponents of the deformation and combines a 3D geometric regularizer with (2)\nthe recently introduced DDS loss which scores the faithfulness of the rendered\n2D image to one from a diffusion model. Thus, our deformation method achieves\nglobally realistic shape deformation which is not restricted to any class of\nobjects. Our new formulation optimizes directly the transformation of the\nneural Jacobian field explicitly separating the rotational and stretching\ncomponents. The objective function of the optimization combines the approximate\ngradients of DDS and the gradients from the geometric loss to satisfy the\nvertex constraints. Additional user control over desired global shape\ndeformation is made possible by allowing explicit per-triangle deformation\ncontrol as well as explicit separation of rotational and stretching components\nof the deformation. We show that our deformations can be controlled to yield\nrealistic shape deformations that are aware of the global context of the\nobjects, and provide better results than just using geometric regularizers.\n","authors":["Tianhao Xie","Eugene Belilovsky","Sudhir Mudur","Tiberiu Popa"],"pdf_url":"https://arxiv.org/pdf/2310.04561v2.pdf","comment":"11 pages, 8 figures, project page:\n  https://tianhaoxie.github.io/project/DragD3D/"},{"id":"http://arxiv.org/abs/2404.09916v2","updated":"2024-08-02T13:59:12Z","published":"2024-04-15T16:43:13Z","title":"Comprehensive Library of Variational LSE Solvers","summary":"  Linear systems of equations can be found in various mathematical domains, as\nwell as in the field of machine learning. By employing noisy intermediate-scale\nquantum devices, variational solvers promise to accelerate finding solutions\nfor large systems. Although there is a wealth of theoretical research on these\nalgorithms, only fragmentary implementations exist. To fill this gap, we have\ndeveloped the variational-lse-solver framework, which realizes existing\napproaches in literature, and introduces several enhancements. The\nuser-friendly interface is designed for researchers that work at the\nabstraction level of identifying and developing end-to-end applications.\n","authors":["Nico Meyer","Martin Röhn","Jakob Murauer","Axel Plinge","Christopher Mutschler","Daniel D. Scherer"],"pdf_url":"https://arxiv.org/pdf/2404.09916v2.pdf","comment":"Accepted to the 2nd International Workshop on Quantum Machine\n  Learning: From Research to Practice (QML@QCE 2024), Montr\\'eal, Qu\\'ebec,\n  Canada. 4 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2408.01273v1","updated":"2024-08-02T13:55:26Z","published":"2024-08-02T13:55:26Z","title":"Certified Robust Invariant Polytope Training in Neural Controlled ODEs","summary":"  We consider a nonlinear control system modeled as an ordinary differential\nequation subject to disturbance, with a state feedback controller parameterized\nas a feedforward neural network. We propose a framework for training\ncontrollers with certified robust forward invariant polytopes, where any\ntrajectory initialized inside the polytope remains within the polytope,\nregardless of the disturbance. First, we parameterize a family of lifted\ncontrol systems in a higher dimensional space, where the original neural\ncontrolled system evolves on an invariant subspace of each lifted system. We\nuse interval analysis and neural network verifiers to further construct a\nfamily of lifted embedding systems, carefully capturing the knowledge of this\ninvariant subspace. If the vector field of any lifted embedding system\nsatisfies a sign constraint at a single point, then a certain convex polytope\nof the original system is robustly forward invariant. Treating the neural\nnetwork controller and the lifted system parameters as variables, we propose an\nalgorithm to train controllers with certified forward invariant polytopes in\nthe closed-loop control system. Through two examples, we demonstrate how the\nsimplicity of the sign constraint allows our approach to scale with system\ndimension to over $50$ states, and outperform state-of-the-art Lyapunov-based\nsampling approaches in runtime.\n","authors":["Akash Harapanahalli","Samuel Coogan"],"pdf_url":"https://arxiv.org/pdf/2408.01273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02604v2","updated":"2024-08-02T13:45:53Z","published":"2024-07-02T18:43:10Z","title":"D-Rax: Domain-specific Radiologic assistant leveraging multi-modal data\n  and eXpert model predictions","summary":"  Large vision language models (VLMs) have progressed incredibly from research\nto applicability for general-purpose use cases. LLaVA-Med, a pioneering large\nlanguage and vision assistant for biomedicine, can perform multi-modal\nbiomedical image and data analysis to provide a natural language interface for\nradiologists. While it is highly generalizable and works with multi-modal data,\nit is currently limited by well-known challenges that exist in the large\nlanguage model space. Hallucinations and imprecision in responses can lead to\nmisdiagnosis which currently hinder the clinical adaptability of VLMs. To\ncreate precise, user-friendly models in healthcare, we propose D-Rax -- a\ndomain-specific, conversational, radiologic assistance tool that can be used to\ngain insights about a particular radiologic image. In this study, we enhance\nthe conversational analysis of chest X-ray (CXR) images to support radiological\nreporting, offering comprehensive insights from medical imaging and aiding in\nthe formulation of accurate diagnosis. D-Rax is achieved by fine-tuning the\nLLaVA-Med architecture on our curated enhanced instruction-following data,\ncomprising of images, instructions, as well as disease diagnosis and\ndemographic predictions derived from MIMIC-CXR imaging data, CXR-related visual\nquestion answer (VQA) pairs, and predictive outcomes from multiple expert AI\nmodels. We observe statistically significant improvement in responses when\nevaluated for both open and close-ended conversations. Leveraging the power of\nstate-of-the-art diagnostic models combined with VLMs, D-Rax empowers\nclinicians to interact with medical images using natural language, which could\npotentially streamline their decision-making process, enhance diagnostic\naccuracy, and conserve their time.\n","authors":["Hareem Nisar","Syed Muhammad Anwar","Zhifan Jiang","Abhijeet Parida","Ramon Sanchez-Jacob","Vishwesh Nath","Holger R. Roth","Marius George Linguraru"],"pdf_url":"https://arxiv.org/pdf/2407.02604v2.pdf","comment":"accepted to the MICCAI 2024 Second International Workshop on\n  Foundation Models for General Medical AI"},{"id":"http://arxiv.org/abs/2408.01257v1","updated":"2024-08-02T13:27:56Z","published":"2024-08-02T13:27:56Z","title":"Detection and Characterization of Coordinated Online Behavior: A Survey","summary":"  Coordination is a fundamental aspect of life. The advent of social media has\nmade it integral also to online human interactions, such as those that\ncharacterize thriving online communities and social movements. At the same\ntime, coordination is also core to effective disinformation, manipulation, and\nhate campaigns. This survey collects, categorizes, and critically discusses the\nbody of work produced as a result of the growing interest on coordinated online\nbehavior. We reconcile industry and academic definitions, propose a\ncomprehensive framework to study coordinated online behavior, and review and\ncritically discuss the existing detection and characterization methods. Our\nanalysis identifies open challenges and promising directions of research,\nserving as a guide for scholars, practitioners, and policymakers in\nunderstanding and addressing the complexities inherent to online coordination.\n","authors":["Lorenzo Mannocci","Michele Mazza","Anna Monreale","Maurizio Tesconi","Stefano Cresci"],"pdf_url":"https://arxiv.org/pdf/2408.01257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15327v2","updated":"2024-08-02T13:25:16Z","published":"2024-06-21T17:40:46Z","title":"Fine-grained Attention in Hierarchical Transformers for Tabular\n  Time-series","summary":"  Tabular data is ubiquitous in many real-life systems. In particular,\ntime-dependent tabular data, where rows are chronologically related, is\ntypically used for recording historical events, e.g., financial transactions,\nhealthcare records, or stock history. Recently, hierarchical variants of the\nattention mechanism of transformer architectures have been used to model\ntabular time-series data. At first, rows (or columns) are encoded separately by\ncomputing attention between their fields. Subsequently, encoded rows (or\ncolumns) are attended to one another to model the entire tabular time-series.\nWhile efficient, this approach constrains the attention granularity and limits\nits ability to learn patterns at the field-level across separate rows, or\ncolumns. We take a first step to address this gap by proposing Fieldy, a\nfine-grained hierarchical model that contextualizes fields at both the row and\ncolumn levels. We compare our proposal against state of the art models on\nregression and classification tasks using public tabular time-series datasets.\nOur results show that combining row-wise and column-wise attention improves\nperformance without increasing model size. Code and data are available at\nhttps://github.com/raphaaal/fieldy.\n","authors":["Raphael Azorin","Zied Ben Houidi","Massimo Gallo","Alessandro Finamore","Pietro Michiardi"],"pdf_url":"https://arxiv.org/pdf/2406.15327v2.pdf","comment":"9 pages; Camera Ready version"},{"id":"http://arxiv.org/abs/2305.18493v3","updated":"2024-08-02T13:16:48Z","published":"2023-05-29T13:47:51Z","title":"Insights from the Design Space Exploration of Flow-Guided Nanoscale\n  Localization","summary":"  Nanodevices with Terahertz (THz)-based wireless communication capabilities\nare providing a primer for flow-guided localization within the human\nbloodstreams. Such localization is allowing for assigning the locations of\nsensed events with the events themselves, providing benefits along the lines of\nearly and precise diagnostics, and reduced costs and invasiveness. Flow-guided\nlocalization is still in a rudimentary phase, with only a handful of works\ntargeting the problem. Nonetheless, the performance assessments of the proposed\nsolutions are already carried out in a non-standardized way, usually along a\nsingle performance metric, and ignoring various aspects that are relevant at\nsuch a scale (e.g., nanodevices' limited energy) and for such a challenging\nenvironment (e.g., extreme attenuation of in-body THz propagation). As such,\nthese assessments feature low levels of realism and cannot be compared in an\nobjective way. Toward addressing this issue, we account for the environmental\nand scale-related peculiarities of the scenario and assess the performance of\ntwo state-of-the-art flow-guided localization approaches along a set of\nheterogeneous performance metrics such as the accuracy and reliability of\nlocalization.\n","authors":["Filip Lemic","Gerard Calvo Bartra","Arnau Brosa López","Jorge Torres Gómez","Jakob Struye","Falko Dressler","Sergi Abadal","Xavier Costa Perez"],"pdf_url":"https://arxiv.org/pdf/2305.18493v3.pdf","comment":"6 pages, 4 figures, 2 tables, 14 references, accepted at ACM\n  NanoCom'24"},{"id":"http://arxiv.org/abs/2408.01248v1","updated":"2024-08-02T13:10:33Z","published":"2024-08-02T13:10:33Z","title":"Deep progressive reinforcement learning-based flexible resource\n  scheduling framework for IRS and UAV-assisted MEC system","summary":"  The intelligent reflection surface (IRS) and unmanned aerial vehicle\n(UAV)-assisted mobile edge computing (MEC) system is widely used in temporary\nand emergency scenarios. Our goal is to minimize the energy consumption of the\nMEC system by jointly optimizing UAV locations, IRS phase shift, task\noffloading, and resource allocation with a variable number of UAVs. To this\nend, we propose a Flexible REsource Scheduling (FRES) framework by employing a\nnovel deep progressive reinforcement learning which includes the following\ninnovations: Firstly, a novel multi-task agent is presented to deal with the\nmixed integer nonlinear programming (MINLP) problem. The multi-task agent has\ntwo output heads designed for different tasks, in which a classified head is\nemployed to make offloading decisions with integer variables while a fitting\nhead is applied to solve resource allocation with continuous variables.\nSecondly, a progressive scheduler is introduced to adapt the agent to the\nvarying number of UAVs by progressively adjusting a part of neurons in the\nagent. This structure can naturally accumulate experiences and be immune to\ncatastrophic forgetting. Finally, a light taboo search (LTS) is introduced to\nenhance the global search of the FRES. The numerical results demonstrate the\nsuperiority of the FRES framework which can make real-time and optimal resource\nscheduling even in dynamic MEC systems.\n","authors":["Li Dong","Feibo Jiang","Minjie Wang","Yubo Peng","Xiaolong Li"],"pdf_url":"https://arxiv.org/pdf/2408.01248v1.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2408.01244v1","updated":"2024-08-02T13:05:33Z","published":"2024-08-02T13:05:33Z","title":"Automated Classification of Dry Bean Varieties Using XGBoost and SVM\n  Models","summary":"  This paper presents a comparative study on the automated classification of\nseven different varieties of dry beans using machine learning models.\nLeveraging a dataset of 12,909 dry bean samples, reduced from an initial 13,611\nthrough outlier removal and feature extraction, we applied Principal Component\nAnalysis (PCA) for dimensionality reduction and trained two multiclass\nclassifiers: XGBoost and Support Vector Machine (SVM). The models were\nevaluated using nested cross-validation to ensure robust performance assessment\nand hyperparameter tuning. The XGBoost and SVM models achieved overall correct\nclassification rates of 94.00% and 94.39%, respectively. The results underscore\nthe efficacy of these machine learning approaches in agricultural applications,\nparticularly in enhancing the uniformity and efficiency of seed classification.\nThis study contributes to the growing body of work on precision agriculture,\ndemonstrating that automated systems can significantly support seed quality\ncontrol and crop yield optimization. Future work will explore incorporating\nmore diverse datasets and advanced algorithms to further improve classification\naccuracy.\n","authors":["Ramtin Ardeshirifar"],"pdf_url":"https://arxiv.org/pdf/2408.01244v1.pdf","comment":"8 pages, 4 figurs"},{"id":"http://arxiv.org/abs/2407.21310v2","updated":"2024-08-02T13:03:00Z","published":"2024-07-31T03:26:14Z","title":"MSMA: Multi-agent Trajectory Prediction in Connected and Autonomous\n  Vehicle Environment with Multi-source Data Integration","summary":"  The prediction of surrounding vehicle trajectories is crucial for\ncollision-free path planning. In this study, we focus on a scenario where a\nconnected and autonomous vehicle (CAV) serves as the central agent, utilizing\nboth sensors and communication technologies to perceive its surrounding\ntraffics consisting of autonomous vehicles (AVs), connected vehicles (CVs), and\nhuman-driven vehicles (HDVs). Our trajectory prediction task is aimed at all\nthe detected surrounding vehicles. To effectively integrate the multi-source\ndata from both sensor and communication technologies, we propose a deep\nlearning framework called MSMA utilizing a cross-attention module for\nmulti-source data fusion. Vector map data is utilized to provide contextual\ninformation. The trajectory dataset is collected in CARLA simulator with\nsynthesized data errors introduced. Numerical experiments demonstrate that in a\nmixed traffic flow scenario, the integration of data from different sources\nenhances our understanding of the environment. This notably improves trajectory\nprediction accuracy, particularly in situations with a high CV market\npenetration rate. The code is available at: https://github.com/xichennn/MSMA.\n","authors":["Xi Chen","Rahul Bhadani","Zhanbo Sun","Larry Head"],"pdf_url":"https://arxiv.org/pdf/2407.21310v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09602v2","updated":"2024-08-02T13:00:54Z","published":"2024-07-12T18:00:02Z","title":"Real-time gravitational-wave inference for binary neutron stars using\n  machine learning","summary":"  Mergers of binary neutron stars (BNSs) emit signals in both the\ngravitational-wave (GW) and electromagnetic (EM) spectra. Famously, the 2017\nmulti-messenger observation of GW170817 led to scientific discoveries across\ncosmology, nuclear physics, and gravity. Central to these results were the sky\nlocalization and distance obtained from GW data, which, in the case of\nGW170817, helped to identify the associated EM transient, AT 2017gfo, 11 hours\nafter the GW signal. Fast analysis of GW data is critical for directing\ntime-sensitive EM observations; however, due to challenges arising from the\nlength and complexity of signals, it is often necessary to make approximations\nthat sacrifice accuracy. Here, we present a machine learning framework that\nperforms complete BNS inference in just one second without making any such\napproximations. Our approach enhances multi-messenger observations by providing\n(i) accurate localization even before the merger; (ii) improved localization\nprecision by $\\sim30\\%$ compared to approximate low-latency methods; and (iii)\ndetailed information on luminosity distance, inclination, and masses, which can\nbe used to prioritize expensive telescope time. Additionally, the flexibility\nand reduced cost of our method open new opportunities for equation-of-state\nstudies. Finally, we demonstrate that our method scales to extremely long\nsignals, up to an hour in length, thus serving as a blueprint for data analysis\nfor next-generation ground- and space-based detectors.\n","authors":["Maximilian Dax","Stephen R. Green","Jonathan Gair","Nihar Gupte","Michael Pürrer","Vivien Raymond","Jonas Wildberger","Jakob H. Macke","Alessandra Buonanno","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2407.09602v2.pdf","comment":"8+8 pages, 3+7 figures"},{"id":"http://arxiv.org/abs/2408.00374v2","updated":"2024-08-02T13:00:46Z","published":"2024-08-01T08:32:03Z","title":"Conformal Trajectory Prediction with Multi-View Data Integration in\n  Cooperative Driving","summary":"  Current research on trajectory prediction primarily relies on data collected\nby onboard sensors of an ego vehicle. With the rapid advancement in connected\ntechnologies, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure\n(V2I) communication, valuable information from alternate views becomes\naccessible via wireless networks. The integration of information from\nalternative views has the potential to overcome the inherent limitations\nassociated with a single viewpoint, such as occlusions and limited field of\nview. In this work, we introduce V2INet, a novel trajectory prediction\nframework designed to model multi-view data by extending existing single-view\nmodels. Unlike previous approaches where the multi-view data is manually fused\nor formulated as a separate training stage, our model supports end-to-end\ntraining, enhancing both flexibility and performance. Moreover, the predicted\nmultimodal trajectories are calibrated by a post-hoc conformal prediction\nmodule to get valid and efficient confidence regions. We evaluated the entire\nframework using the real-world V2I dataset V2X-Seq. Our results demonstrate\nsuperior performance in terms of Final Displacement Error (FDE) and Miss Rate\n(MR) using a single GPU. The code is publicly available at:\n\\url{https://github.com/xichennn/V2I_trajectory_prediction}.\n","authors":["Xi Chen","Rahul Bhadani","Larry Head"],"pdf_url":"https://arxiv.org/pdf/2408.00374v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01239v1","updated":"2024-08-02T12:58:08Z","published":"2024-08-02T12:58:08Z","title":"Tailoring Graph Neural Network-based Flow-guided Localization to\n  Individual Bloodstreams and Activities","summary":"  Flow-guided localization using in-body nanodevices in the bloodstream is\nexpected to be beneficial for early disease detection, continuous monitoring of\nbiological conditions, and targeted treatment. The nanodevices face size and\npower constraints that produce erroneous raw data for localization purposes.\nOn-body anchors receive this data, and use it to derive the locations of\ndiagnostic events of interest. Different Machine Learning (ML) approaches have\nbeen recently proposed for this task, yet they are currently restricted to a\nreference bloodstream of a resting patient. As such, they are unable to deal\nwith the physical diversity of patients' bloodstreams and cannot provide\ncontinuous monitoring due to changes in individual patient's activities. Toward\naddressing these issues for the current State-of-the-Art (SotA) flow-guided\nlocalization approach based on Graph Neural Networks (GNNs), we propose a\npipeline for GNN adaptation based on individual physiological indicators\nincluding height, weight, and heart rate. Our results indicate that the\nproposed adaptions are beneficial in reconciling the individual differences\nbetween bloodstreams and activities.\n","authors":["Pablo Galván","Filip Lemic","Gerard Calvo Bartra","Sergi Abadal","Xavier Costa Pérez"],"pdf_url":"https://arxiv.org/pdf/2408.01239v1.pdf","comment":"7 pages, 9 figures, 2 tables, 16 references, accepted at ACM\n  NanoCom'25"},{"id":"http://arxiv.org/abs/2408.01230v1","updated":"2024-08-02T12:40:01Z","published":"2024-08-02T12:40:01Z","title":"HeteroMorpheus: Universal Control Based on Morphological Heterogeneity\n  Modeling","summary":"  In the field of robotic control, designing individual controllers for each\nrobot leads to high computational costs. Universal control policies, applicable\nacross diverse robot morphologies, promise to mitigate this challenge.\nPredominantly, models based on Graph Neural Networks (GNN) and Transformers are\nemployed, owing to their effectiveness in capturing relational dynamics across\na robot's limbs. However, these models typically employ homogeneous graph\nstructures that overlook the functional diversity of different limbs. To bridge\nthis gap, we introduce HeteroMorpheus, a novel method based on heterogeneous\ngraph Transformer. This method uniquely addresses limb heterogeneity, fostering\nbetter representation of robot dynamics of various morphologies. Through\nextensive experiments we demonstrate the superiority of HeteroMorpheus against\nstate-of-the-art methods in the capability of policy generalization, including\nzero-shot generalization and sample-efficient transfer to unfamiliar robot\nmorphologies.\n","authors":["YiFan Hao","Yang Yang","Junru Song","Wei Peng","Weien Zhou","Tingsong Jiang","Wen Yao"],"pdf_url":"https://arxiv.org/pdf/2408.01230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01215v1","updated":"2024-08-02T12:04:19Z","published":"2024-08-02T12:04:19Z","title":"ZNorm: Z-Score Gradient Normalization for Accelerating Neural Network\n  Training","summary":"  The rapid advancements in deep learning necessitate efficient training\nmethods for deep neural networks (DNNs). As models grow in complexity,\nvanishing and exploding gradients impede convergence and performance. We\npropose Z-Score Normalization for Gradient Descent (ZNorm), an innovative\ntechnique that adjusts only the gradients to enhance training efficiency and\nimprove model performance. ZNorm normalizes the overall gradients, providing\nconsistent gradient scaling across layers, thereby reducing the risks of\nvanishing and exploding gradients. Our extensive experiments on CIFAR-10 and\nmedical datasets demonstrate that ZNorm not only accelerates convergence but\nalso enhances performance metrics. ZNorm consistently outperforms existing\nmethods, achieving superior results using the same computational settings. In\nmedical imaging applications, ZNorm improves tumor prediction and segmentation\nperformances, underscoring its practical utility. These findings highlight\nZNorm's potential as a robust and versatile tool for improving the efficiency\nand effectiveness of deep neural network training across a wide range of\narchitectures and applications.\n","authors":["Juyoung Yun","Hoyoung Kim","Suin Cho","Hangil Kang"],"pdf_url":"https://arxiv.org/pdf/2408.01215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00950v2","updated":"2024-08-02T11:54:57Z","published":"2023-12-13T12:57:55Z","title":"Unsupervised Graph-based Learning Method for Sub-band Allocation in 6G\n  Subnetworks","summary":"  In this paper, we present an unsupervised approach for frequency sub-band\nallocation in wireless networks using graph-based learning. We consider a dense\ndeployment of subnetworks in the factory environment with a limited number of\nsub-bands which must be optimally allocated to coordinate inter-subnetwork\ninterference. We model the subnetwork deployment as a conflict graph and\npropose an unsupervised learning approach inspired by the graph colouring\nheuristic and the Potts model to optimize the sub-band allocation using graph\nneural networks. The numerical evaluation shows that the proposed method\nachieves close performance to the centralized greedy colouring sub-band\nallocation heuristic with lower computational time complexity. In addition, it\nincurs reduced signalling overhead compared to iterative optimization\nheuristics that require all the mutual interfering channel information. We\nfurther demonstrate that the method is robust to different network settings.\n","authors":["Daniel Abode","Ramoni Adeogun","Lou Salaün","Renato Abreu","Thomas Jacobsen","Gilberto Berardinelli"],"pdf_url":"https://arxiv.org/pdf/2401.00950v2.pdf","comment":"Accepted in VTC Fall 2024"},{"id":"http://arxiv.org/abs/2408.01200v1","updated":"2024-08-02T11:29:21Z","published":"2024-08-02T11:29:21Z","title":"Certifiably Robust Encoding Schemes","summary":"  Quantum machine learning uses principles from quantum mechanics to process\ndata, offering potential advances in speed and performance. However, previous\nwork has shown that these models are susceptible to attacks that manipulate\ninput data or exploit noise in quantum circuits. Following this, various\nstudies have explored the robustness of these models. These works focus on the\nrobustness certification of manipulations of the quantum states. We extend this\nline of research by investigating the robustness against perturbations in the\nclassical data for a general class of data encoding schemes. We show that for\nsuch schemes, the addition of suitable noise channels is equivalent to\nevaluating the mean value of the noiseless classifier at the smoothed data,\nakin to Randomized Smoothing from classical machine learning. Using our general\nframework, we show that suitable additions of phase-damping noise channels\nimprove empirical and provable robustness for the considered class of encoding\nschemes.\n","authors":["Aman Saxena","Tom Wollschläger","Nicola Franco","Jeanette Miriam Lorenz","Stephan Günnemann"],"pdf_url":"https://arxiv.org/pdf/2408.01200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01187v1","updated":"2024-08-02T11:14:41Z","published":"2024-08-02T11:14:41Z","title":"Optimizing Variational Quantum Circuits Using Metaheuristic Strategies\n  in Reinforcement Learning","summary":"  Quantum Reinforcement Learning (QRL) offers potential advantages over\nclassical Reinforcement Learning, such as compact state space representation\nand faster convergence in certain scenarios. However, practical benefits\nrequire further validation. QRL faces challenges like flat solution landscapes,\nwhere traditional gradient-based methods are inefficient, necessitating the use\nof gradient-free algorithms. This work explores the integration of\nmetaheuristic algorithms -- Particle Swarm Optimization, Ant Colony\nOptimization, Tabu Search, Genetic Algorithm, Simulated Annealing, and Harmony\nSearch -- into QRL. These algorithms provide flexibility and efficiency in\nparameter optimization. Evaluations in $5\\times5$ MiniGrid Reinforcement\nLearning environments show that, all algorithms yield near-optimal results,\nwith Simulated Annealing and Particle Swarm Optimization performing best. In\nthe Cart Pole environment, Simulated Annealing, Genetic Algorithms, and\nParticle Swarm Optimization achieve optimal results, while the others perform\nslightly better than random action selection. These findings demonstrate the\npotential of Particle Swarm Optimization and Simulated Annealing for efficient\nQRL learning, emphasizing the need for careful algorithm selection and\nadaptation.\n","authors":["Michael Kölle","Daniel Seidl","Maximilian Zorn","Philipp Altmann","Jonas Stein","Thomas Gabor"],"pdf_url":"https://arxiv.org/pdf/2408.01187v1.pdf","comment":"Accepted at QCE24 - QCRL24 Workshop"},{"id":"http://arxiv.org/abs/2408.01180v1","updated":"2024-08-02T11:02:38Z","published":"2024-08-02T11:02:38Z","title":"Nested Music Transformer: Sequentially Decoding Compound Tokens in\n  Symbolic Music and Audio Generation","summary":"  Representing symbolic music with compound tokens, where each token consists\nof several different sub-tokens representing a distinct musical feature or\nattribute, offers the advantage of reducing sequence length. While previous\nresearch has validated the efficacy of compound tokens in music sequence\nmodeling, predicting all sub-tokens simultaneously can lead to suboptimal\nresults as it may not fully capture the interdependencies between them. We\nintroduce the Nested Music Transformer (NMT), an architecture tailored for\ndecoding compound tokens autoregressively, similar to processing flattened\ntokens, but with low memory usage. The NMT consists of two transformers: the\nmain decoder that models a sequence of compound tokens and the sub-decoder for\nmodeling sub-tokens of each compound token. The experiment results showed that\napplying the NMT to compound tokens can enhance the performance in terms of\nbetter perplexity in processing various symbolic music datasets and discrete\naudio tokens from the MAESTRO dataset.\n","authors":["Jiwoo Ryu","Hao-Wen Dong","Jongmin Jung","Dasaem Jeong"],"pdf_url":"https://arxiv.org/pdf/2408.01180v1.pdf","comment":"Accepted at 25th International Society for Music Information\n  Retrieval Conference (ISMIR 2024)"},{"id":"http://arxiv.org/abs/2408.01173v1","updated":"2024-08-02T10:47:10Z","published":"2024-08-02T10:47:10Z","title":"Sustainable Diffusion-based Incentive Mechanism for Generative AI-driven\n  Digital Twins in Industrial Cyber-Physical Systems","summary":"  Industrial Cyber-Physical Systems (ICPSs) are an integral component of modern\nmanufacturing and industries. By digitizing data throughout the product life\ncycle, Digital Twins (DTs) in ICPSs enable a shift from current industrial\ninfrastructures to intelligent and adaptive infrastructures. Thanks to data\nprocess capability, Generative Artificial Intelligence (GAI) can drive the\nconstruction and update of DTs to improve predictive accuracy and prepare for\ndiverse smart manufacturing. However, mechanisms that leverage sensing\nIndustrial Internet of Things (IIoT) devices to share data for the construction\nof DTs are susceptible to adverse selection problems. In this paper, we first\ndevelop a GAI-driven DT architecture for ICPSs. To address the adverse\nselection problem caused by information asymmetry, we propose a contract theory\nmodel and develop the sustainable diffusion-based soft actor-critic algorithm\nto identify the optimal feasible contract. Specifically, we leverage the\ndynamic structured pruning technique to reduce parameter numbers of actor\nnetworks, allowing sustainability and efficient implementation of the proposed\nalgorithm. Finally, numerical results demonstrate the effectiveness of the\nproposed scheme.\n","authors":["Jinbo Wen","Jiawen Kang","Dusit Niyato","Yang Zhang","Shiwen Mao"],"pdf_url":"https://arxiv.org/pdf/2408.01173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01163v1","updated":"2024-08-02T10:25:19Z","published":"2024-08-02T10:25:19Z","title":"Domain Adaptation-Enhanced Searchlight: Enabling brain decoding from\n  visual perception to mental imagery","summary":"  In cognitive neuroscience and brain-computer interface research, accurately\npredicting imagined stimuli is crucial. This study investigates the\neffectiveness of Domain Adaptation (DA) in enhancing imagery prediction using\nprimarily visual data from fMRI scans of 18 subjects. Initially, we train a\nbaseline model on visual stimuli to predict imagined stimuli, utilizing data\nfrom 14 brain regions. We then develop several models to improve imagery\nprediction, comparing different DA methods. Our results demonstrate that DA\nsignificantly enhances imagery prediction, especially with the Regular Transfer\napproach. We then conduct a DA-enhanced searchlight analysis using Regular\nTransfer, followed by permutation-based statistical tests to identify brain\nregions where imagery decoding is consistently above chance across subjects.\nOur DA-enhanced searchlight predicts imagery contents in a highly distributed\nset of brain regions, including the visual cortex and the frontoparietal\ncortex, thereby outperforming standard cross-domain classification methods. The\ncomplete code and data for this paper have been made openly available for the\nuse of the scientific community.\n","authors":["Alexander Olza","David Soto","Roberto Santana"],"pdf_url":"https://arxiv.org/pdf/2408.01163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01156v1","updated":"2024-08-02T10:16:28Z","published":"2024-08-02T10:16:28Z","title":"TCR-GPT: Integrating Autoregressive Model and Reinforcement Learning for\n  T-Cell Receptor Repertoires Generation","summary":"  T-cell receptors (TCRs) play a crucial role in the immune system by\nrecognizing and binding to specific antigens presented by infected or cancerous\ncells. Understanding the sequence patterns of TCRs is essential for developing\ntargeted immune therapies and designing effective vaccines. Language models,\nsuch as auto-regressive transformers, offer a powerful solution to this problem\nby learning the probability distributions of TCR repertoires, enabling the\ngeneration of new TCR sequences that inherit the underlying patterns of the\nrepertoire. We introduce TCR-GPT, a probabilistic model built on a decoder-only\ntransformer architecture, designed to uncover and replicate sequence patterns\nin TCR repertoires. TCR-GPT demonstrates an accuracy of 0.953 in inferring\nsequence probability distributions measured by Pearson correlation coefficient.\nFurthermore, by leveraging Reinforcement Learning(RL), we adapted the\ndistribution of TCR sequences to generate TCRs capable of recognizing specific\npeptides, offering significant potential for advancing targeted immune\ntherapies and vaccine development. With the efficacy of RL, fine-tuned\npretrained TCR-GPT models demonstrated the ability to produce TCR repertoires\nlikely to bind specific peptides, illustrating RL's efficiency in enhancing the\nmodel's adaptability to the probability distributions of biologically relevant\nTCR sequences.\n","authors":["Yicheng Lin","Dandan Zhang","Yun Liu"],"pdf_url":"https://arxiv.org/pdf/2408.01156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01144v1","updated":"2024-08-02T09:44:18Z","published":"2024-08-02T09:44:18Z","title":"Enhanced Prediction of Ventilator-Associated Pneumonia in Patients with\n  Traumatic Brain Injury Using Advanced Machine Learning Techniques","summary":"  Background: Ventilator-associated pneumonia (VAP) in traumatic brain injury\n(TBI) patients poses a significant mortality risk and imposes a considerable\nfinancial burden on patients and healthcare systems. Timely detection and\nprognostication of VAP in TBI patients are crucial to improve patient outcomes\nand alleviate the strain on healthcare resources.\n  Methods: We implemented six machine learning models using the MIMIC-III\ndatabase. Our methodology included preprocessing steps, such as feature\nselection with CatBoost and expert opinion, addressing class imbalance with the\nSynthetic Minority Oversampling Technique (SMOTE), and rigorous model tuning\nthrough 5-fold cross-validation to optimize hyperparameters. Key models\nevaluated included SVM, Logistic Regression, Random Forest, XGBoost, ANN, and\nAdaBoost. Additionally, we conducted SHAP analysis to determine feature\nimportance and performed an ablation study to assess feature impacts on model\nperformance.\n  Results: XGBoost outperformed the baseline models and the best existing\nliterature. We used metrics, including AUC, Accuracy, Specificity, Sensitivity,\nF1 Score, PPV, and NPV. XGBoost demonstrated the highest performance with an\nAUC of 0.940 and an Accuracy of 0.875, which are 23.4% and 23.5% higher than\nthe best results in the existing literature, with an AUC of 0.706 and an\nAccuracy of 0.640, respectively. This enhanced performance underscores the\nmodels' effectiveness in clinical settings.\n  Conclusions: This study enhances the predictive modeling of VAP in TBI\npatients, improving early detection and intervention potential. Refined feature\nselection and advanced ensemble techniques significantly boosted model accuracy\nand reliability, offering promising directions for future clinical applications\nand medical diagnostics research.\n","authors":["Negin Ashrafi","Armin Abdollahi","Maryam Pishgar"],"pdf_url":"https://arxiv.org/pdf/2408.01144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01141v1","updated":"2024-08-02T09:37:55Z","published":"2024-08-02T09:37:55Z","title":"Machine learning topological energy braiding of non-Bloch bands","summary":"  Machine learning has been used to identify phase transitions in a variety of\nphysical systems. However, there is still a lack of relevant research on\nnon-Bloch energy braiding in non-Hermitian systems. In this work, we study\nnon-Bloch energy braiding in one-dimensional non-Hermitian systems using\nunsupervised and supervised methods. In unsupervised learning, we use diffusion\nmaps to successfully identify non-Bloch energy braiding without any prior\nknowledge and combine it with k-means to cluster different topological elements\ninto clusters, such as Unlink and Hopf link. In supervised learning, we train a\nConvolutional Neural Network (CNN) based on Bloch energy data to predict not\nonly Bloch energy braiding but also non-Bloch energy braiding with an accuracy\napproaching 100%. By analysing the CNN, we can ascertain that the network has\nsuccessfully acquired the ability to recognise the braiding topology of the\nenergy bands. The present study demonstrates the considerable potential of\nmachine learning in the identification of non-Hermitian topological phases and\nenergy braiding.\n","authors":["Shuwei Shi","Shibing Chu","Yuee Xie","Yuanping Chen"],"pdf_url":"https://arxiv.org/pdf/2408.01141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01129v1","updated":"2024-08-02T09:18:41Z","published":"2024-08-02T09:18:41Z","title":"A Survey of Mamba","summary":"  Deep learning, as a vital technique, has sparked a notable revolution in\nartificial intelligence. As the most representative architecture, Transformers\nhave empowered numerous advanced models, especially the large language models\nthat comprise billions of parameters, becoming a cornerstone in deep learning.\nDespite the impressive achievements, Transformers still face inherent\nlimitations, particularly the time-consuming inference resulting from the\nquadratic computation complexity of attention calculation. Recently, a novel\narchitecture named Mamba, drawing inspiration from classical state space\nmodels, has emerged as a promising alternative for building foundation models,\ndelivering comparable modeling abilities to Transformers while preserving\nnear-linear scalability concerning sequence length. This has sparked an\nincreasing number of studies actively exploring Mamba's potential to achieve\nimpressive performance across diverse domains. Given such rapid evolution,\nthere is a critical need for a systematic review that consolidates existing\nMamba-empowered models, offering a comprehensive understanding of this emerging\nmodel architecture. In this survey, we therefore conduct an in-depth\ninvestigation of recent Mamba-associated studies, covering from three main\naspects: the advancements of Mamba-based models, the techniques of adapting\nMamba to diverse data, and the applications where Mamba can excel.\nSpecifically, we first recall the foundational knowledge of various\nrepresentative deep learning models and the details of Mamba as preliminaries.\nThen, to showcase the significance of Mamba, we comprehensively review the\nrelated studies focusing on Mamba models' architecture design, data\nadaptability, and applications. Finally, we present an discussion of current\nlimitations and explore various promising research directions to provide deeper\ninsights for future investigations.\n","authors":["Haohao Qu","Liangbo Ning","Rui An","Wenqi Fan","Tyler Derr","Xin Xu","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2408.01129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17900v3","updated":"2024-08-02T08:55:52Z","published":"2024-07-25T09:42:24Z","title":"The Power of Combining Data and Knowledge: GPT-4o is an Effective\n  Interpreter of Machine Learning Models in Predicting Lymph Node Metastasis of\n  Lung Cancer","summary":"  Lymph node metastasis (LNM) is a crucial factor in determining the initial\ntreatment for patients with lung cancer, yet accurate preoperative diagnosis of\nLNM remains challenging. Recently, large language models (LLMs) have garnered\nsignificant attention due to their remarkable text generation capabilities.\nLeveraging the extensive medical knowledge learned from vast corpora, LLMs can\nestimate probabilities for clinical problems, though their performance has\nhistorically been inferior to data-driven machine learning models. In this\npaper, we propose a novel ensemble method that combines the medical knowledge\nacquired by LLMs with the latent patterns identified by machine learning models\nto enhance LNM prediction performance. Initially, we developed machine learning\nmodels using patient data. We then designed a prompt template to integrate the\npatient data with the predicted probability from the machine learning model.\nSubsequently, we instructed GPT-4o, the most advanced LLM developed by OpenAI,\nto estimate the likelihood of LNM based on patient data and then adjust the\nestimate using the machine learning output. Finally, we collected three outputs\nfrom the GPT-4o using the same prompt and ensembled these results as the final\nprediction. Using the proposed method, our models achieved an AUC value of\n0.765 and an AP value of 0.415 for LNM prediction, significantly improving\npredictive performance compared to baseline machine learning models. The\nexperimental results indicate that GPT-4o can effectively leverage its medical\nknowledge and the probabilities predicted by machine learning models to achieve\nmore accurate LNM predictions. These findings demonstrate that LLMs can perform\nwell in clinical risk prediction tasks, offering a new paradigm for integrating\nmedical knowledge and patient data in clinical predictions.\n","authors":["Danqing Hu","Bing Liu","Xiaofeng Zhu","Nan Wu"],"pdf_url":"https://arxiv.org/pdf/2407.17900v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15132v2","updated":"2024-08-02T08:49:14Z","published":"2024-02-23T06:33:51Z","title":"Improving Sentence Embeddings with Automatic Generation of Training Data\n  Using Few-shot Examples","summary":"  Decoder-based large language models (LLMs) have shown high performance on\nmany tasks in natural language processing. This is also true for sentence\nembedding learning, where a decoder-based model, PromptEOL, has achieved the\nbest performance on semantic textual similarity (STS) tasks. However, PromptEOL\nrequires a manually annotated natural language inference (NLI) dataset for\nfine-tuning. We aim to improve sentence embeddings without using large manually\nannotated datasets by automatically generating an NLI dataset with an LLM and\nusing it for fine-tuning of PromptEOL. To achieve this, we explore methods of\ndata generation suitable for sentence embedding learning in this study.\nSpecifically, we will focus on automatic dataset generation through few-shot\nlearning and explore the appropriate methods to leverage few-shot examples.\nExperimental results on the STS tasks demonstrate that our approach outperforms\nexisting models in settings without large manually annotated datasets.\n","authors":["Soma Sato","Hayato Tsukagoshi","Ryohei Sasano","Koichi Takeda"],"pdf_url":"https://arxiv.org/pdf/2402.15132v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18819v2","updated":"2024-08-02T08:22:57Z","published":"2024-02-29T03:06:10Z","title":"Dual Operating Modes of In-Context Learning","summary":"  In-context learning (ICL) exhibits dual operating modes: task learning, i.e.,\nacquiring a new skill from in-context samples, and task retrieval, i.e.,\nlocating and activating a relevant pretrained skill. Recent theoretical work\ninvestigates various mathematical models to analyze ICL, but existing models\nexplain only one operating mode at a time. We introduce a probabilistic model,\nwith which one can explain the dual operating modes of ICL simultaneously.\nFocusing on in-context learning of linear functions, we extend existing models\nfor pretraining data by introducing multiple task groups and task-dependent\ninput distributions. We then analyze the behavior of the optimally pretrained\nmodel under the squared loss, i.e., the MMSE estimator of the label given\nin-context examples. Regarding pretraining task distribution as prior and\nin-context examples as the observation, we derive the closed-form expression of\nthe task posterior distribution. With the closed-form expression, we obtain a\nquantitative understanding of the two operating modes of ICL. Furthermore, we\nshed light on an unexplained phenomenon observed in practice: under certain\nsettings, the ICL risk initially increases and then decreases with more\nin-context examples. Our model offers a plausible explanation for this \"early\nascent\" phenomenon: a limited number of in-context samples may lead to the\nretrieval of an incorrect skill, thereby increasing the risk, which will\neventually diminish as task learning takes effect with more in-context samples.\nWe also theoretically analyze ICL with biased labels, e.g., zero-shot ICL,\nwhere in-context examples are assigned random labels. Lastly, we validate our\nfindings and predictions via experiments involving Transformers and large\nlanguage models.\n","authors":["Ziqian Lin","Kangwook Lee"],"pdf_url":"https://arxiv.org/pdf/2402.18819v2.pdf","comment":"54 pages, 23 figures"},{"id":"http://arxiv.org/abs/2408.01094v1","updated":"2024-08-02T08:13:18Z","published":"2024-08-02T08:13:18Z","title":"An Encoding--Searching Separation Perspective on Bi-Encoder Neural\n  Search","summary":"  This paper reviews, analyzes, and proposes a new perspective on the\nbi-encoder architecture for neural search. While the bi-encoder architecture is\nwidely used due to its simplicity and scalability at test time, it has some\nnotable issues such as low performance on seen datasets and weak zero-shot\nperformance on new datasets. In this paper, we analyze these issues and\nsummarize two main critiques: the encoding information bottleneck problem and\nlimitations of the basic assumption of embedding search. We then construct a\nthought experiment to logically analyze the encoding and searching operations\nand challenge the basic assumption of embedding search. Building on these\nobservations, we propose a new perspective on the bi-encoder architecture\ncalled the \\textit{encoding--searching separation} perspective, which\nconceptually and practically separates the encoding and searching operations.\nThis new perspective is applied to explain the root cause of the identified\nissues and discuss ways to mitigate the problems. Finally, we discuss the\nimplications of the ideas underlying the new perspective, the design surface\nthat it exposes and the potential research directions arising from it.\n","authors":["Hung-Nghiep Tran","Akiko Aizawa","Atsuhiro Takasu"],"pdf_url":"https://arxiv.org/pdf/2408.01094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11660v2","updated":"2024-08-02T07:42:37Z","published":"2024-01-22T02:33:38Z","title":"Differentiable Tree Search Network","summary":"  In decision-making problems with limited training data, policy functions\napproximated using deep neural networks often exhibit suboptimal performance.\nAn alternative approach involves learning a world model from the limited data\nand determining actions through online search. However, the performance is\nadversely affected by compounding errors arising from inaccuracies in the\nlearned world model. While methods like TreeQN have attempted to address these\ninaccuracies by incorporating algorithmic inductive biases into the neural\nnetwork architectures, the biases they introduce are often weak and\ninsufficient for complex decision-making tasks. In this work, we introduce\nDifferentiable Tree Search Network (D-TSN), a novel neural network architecture\nthat significantly strengthens the inductive bias by embedding the algorithmic\nstructure of a best-first online search algorithm. D-TSN employs a learned\nworld model to conduct a fully differentiable online search. The world model is\njointly optimized with the search algorithm, enabling the learning of a robust\nworld model and mitigating the effect of prediction inaccuracies. Further, we\nnote that a naive incorporation of best-first search could lead to a\ndiscontinuous loss function in the parameter space. We address this issue by\nadopting a stochastic tree expansion policy, formulating search tree expansion\nas another decision-making task, and introducing an effective variance\nreduction technique for the gradient computation. We evaluate D-TSN in an\noffline-RL setting with a limited training data scenario on Procgen games and\ngrid navigation task, and demonstrate that D-TSN outperforms popular model-free\nand model-based baselines.\n","authors":["Dixant Mittal","Wee Sun Lee"],"pdf_url":"https://arxiv.org/pdf/2401.11660v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01062v1","updated":"2024-08-02T07:29:49Z","published":"2024-08-02T07:29:49Z","title":"Universality of kernel random matrices and kernel regression in the\n  quadratic regime","summary":"  Kernel ridge regression (KRR) is a popular class of machine learning models\nthat has become an important tool for understanding deep learning. Much of the\nfocus has been on studying the proportional asymptotic regime, $n \\asymp d$,\nwhere $n$ is the number of training samples and $d$ is the dimension of the\ndataset. In this regime, under certain conditions on the data distribution, the\nkernel random matrix involved in KRR exhibits behavior akin to that of a linear\nkernel. In this work, we extend the study of kernel regression to the quadratic\nasymptotic regime, where $n \\asymp d^2$. In this regime, we demonstrate that a\nbroad class of inner-product kernels exhibit behavior similar to a quadratic\nkernel. Specifically, we establish an operator norm approximation bound for the\ndifference between the original kernel random matrix and a quadratic kernel\nrandom matrix with additional correction terms compared to the Taylor expansion\nof the kernel functions. The approximation works for general data distributions\nunder a Gaussian-moment-matching assumption with a covariance structure. This\nnew approximation is utilized to obtain a limiting spectral distribution of the\noriginal kernel matrix and characterize the precise asymptotic training and\ngeneralization errors for KRR in the quadratic regime when $n/d^2$ converges to\na non-zero constant. The generalization errors are obtained for both\ndeterministic and random teacher models. Our proof techniques combine moment\nmethods, Wick's formula, orthogonal polynomials, and resolvent analysis of\nrandom matrices with correlated entries.\n","authors":["Parthe Pandit","Zhichao Wang","Yizhe Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.01062v1.pdf","comment":"75 pages"},{"id":"http://arxiv.org/abs/2408.01050v1","updated":"2024-08-02T06:56:59Z","published":"2024-08-02T06:56:59Z","title":"The Impact of Hyperparameters on Large Language Model Inference\n  Performance: An Evaluation of vLLM and HuggingFace Pipelines","summary":"  The recent surge of open-source large language models (LLMs) enables\ndevelopers to create AI-based solutions while maintaining control over aspects\nsuch as privacy and compliance, thereby providing governance and ownership of\nthe model deployment process. To utilize these LLMs, inference engines are\nneeded. These engines load the model's weights onto available resources, such\nas GPUs, and process queries to generate responses. The speed of inference, or\nperformance, of the LLM, is critical for real-time applications, as it computes\nmillions or billions of floating point operations per inference. Recently,\nadvanced inference engines such as vLLM have emerged, incorporating novel\nmechanisms such as efficient memory management to achieve state-of-the-art\nperformance. In this paper, we analyze the performance, particularly the\nthroughput (tokens generated per unit of time), of 20 LLMs using two inference\nlibraries: vLLM and HuggingFace's pipelines. We investigate how various\nhyperparameters, which developers must configure, influence inference\nperformance. Our results reveal that throughput landscapes are irregular, with\ndistinct peaks, highlighting the importance of hyperparameter optimization to\nachieve maximum performance. We also show that applying hyperparameter\noptimization when upgrading or downgrading the GPU model used for inference can\nimprove throughput from HuggingFace pipelines by an average of 9.16% and 13.7%,\nrespectively.\n","authors":["Matias Martinez"],"pdf_url":"https://arxiv.org/pdf/2408.01050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01040v1","updated":"2024-08-02T06:24:39Z","published":"2024-08-02T06:24:39Z","title":"Privacy-Preserving Split Learning with Vision Transformers using\n  Patch-Wise Random and Noisy CutMix","summary":"  In computer vision, the vision transformer (ViT) has increasingly superseded\nthe convolutional neural network (CNN) for improved accuracy and robustness.\nHowever, ViT's large model sizes and high sample complexity make it difficult\nto train on resource-constrained edge devices. Split learning (SL) emerges as a\nviable solution, leveraging server-side resources to train ViTs while utilizing\nprivate data from distributed devices. However, SL requires additional\ninformation exchange for weight updates between the device and the server,\nwhich can be exposed to various attacks on private training data. To mitigate\nthe risk of data breaches in classification tasks, inspired from the CutMix\nregularization, we propose a novel privacy-preserving SL framework that injects\nGaussian noise into smashed data and mixes randomly chosen patches of smashed\ndata across clients, coined DP-CutMixSL. Our analysis demonstrates that\nDP-CutMixSL is a differentially private (DP) mechanism that strengthens privacy\nprotection against membership inference attacks during forward propagation.\nThrough simulations, we show that DP-CutMixSL improves privacy protection\nagainst membership inference attacks, reconstruction attacks, and label\ninference attacks, while also improving accuracy compared to DP-SL and\nDP-MixSL.\n","authors":["Seungeun Oh","Sihun Baek","Jihong Park","Hyelin Nam","Praneeth Vepakomma","Ramesh Raskar","Mehdi Bennis","Seong-Lyun Kim"],"pdf_url":"https://arxiv.org/pdf/2408.01040v1.pdf","comment":"23 pages, 11 figures, 8 tables, to be published in Transactions on\n  Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2408.00023v2","updated":"2024-08-02T06:05:19Z","published":"2024-07-31T05:31:28Z","title":"On the Perturbed States for Transformed Input-robust Reinforcement\n  Learning","summary":"  Reinforcement Learning (RL) agents demonstrating proficiency in a training\nenvironment exhibit vulnerability to adversarial perturbations in input\nobservations during deployment. This underscores the importance of building a\nrobust agent before its real-world deployment. To alleviate the challenging\npoint, prior works focus on developing robust training-based procedures,\nencompassing efforts to fortify the deep neural network component's robustness\nor subject the agent to adversarial training against potent attacks. In this\nwork, we propose a novel method referred to as Transformed Input-robust RL\n(TIRL), which explores another avenue to mitigate the impact of adversaries by\nemploying input transformation-based defenses. Specifically, we introduce two\nprinciples for applying transformation-based defenses in learning robust RL\nagents: (1) autoencoder-styled denoising to reconstruct the original state and\n(2) bounded transformations (bit-depth reduction and vector quantization (VQ))\nto achieve close transformed inputs. The transformations are applied to the\nstate before feeding it into the policy network. Extensive experiments on\nmultiple MuJoCo environments demonstrate that input transformation-based\ndefenses, i.e., VQ, defend against several adversaries in the state\nobservations. The official code is available at\nhttps://github.com/tunglm2203/tirl\n","authors":["Tung M. Luu","Haeyong Kang","Tri Ton","Thanh Nguyen","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2408.00023v2.pdf","comment":"12 pages (Code: https://github.com/tunglm2203/tirl)"},{"id":"http://arxiv.org/abs/2407.16944v3","updated":"2024-08-02T06:05:10Z","published":"2024-07-24T02:23:18Z","title":"An Adaptive Gradient Regularization Method","summary":"  Optimizer plays an important role in neural network training with high\nefficiency and performance. Weight update based on its gradient is the central\npart of the optimizer. It has been shown that normalization and standardization\noperation on weight and gradient can accelerate the training process and\nimprove performance such as Weight Standardization (WS), weight normalization\n(WN) and gradient normalization (GN); there is also gradient centralization\n(GC). In this work, we introduce a new optimization technique based on the\ngradient magnitude in a gradient vector named adaptive gradient regularization\n(AGR), which normalizes the gradient vector in all dimensions as a coefficient\nvector and subtracts the product of the gradient and its coefficient vector by\nthe vanilla gradient. It can be viewed as an adaptive gradient clipping method.\nWe show that the AGR can improve the loss function Lipschitzness with a more\nstable training process and better generalization performance. AGR is very\nsimple to be embedded into vanilla optimizers such as Adan and AdamW with only\nthree lines of code. Our experiments are conducted in image generation, image\nclassification and language representation, which shows that our AGR improves\nthe training result.\n","authors":["Huixiu Jiang","Ling Yang","Yu Bao","Rutong Si"],"pdf_url":"https://arxiv.org/pdf/2407.16944v3.pdf","comment":"11 pages, 11 figures"},{"id":"http://arxiv.org/abs/2408.01023v1","updated":"2024-08-02T05:48:15Z","published":"2024-08-02T05:48:15Z","title":"Distilling interpretable causal trees from causal forests","summary":"  Machine learning methods for estimating treatment effect heterogeneity\npromise greater flexibility than existing methods that test a few pre-specified\nhypotheses. However, one problem these methods can have is that it can be\nchallenging to extract insights from complicated machine learning models. A\nhigh-dimensional distribution of conditional average treatment effects may give\naccurate, individual-level estimates, but it can be hard to understand the\nunderlying patterns; hard to know what the implications of the analysis are.\nThis paper proposes the Distilled Causal Tree, a method for distilling a\nsingle, interpretable causal tree from a causal forest. This compares well to\nexisting methods of extracting a single tree, particularly in noisy data or\nhigh-dimensional data where there are many correlated features. Here it even\noutperforms the base causal forest in most simulations. Its estimates are\ndoubly robust and asymptotically normal just as those of the causal forest are.\n","authors":["Patrick Rehill"],"pdf_url":"https://arxiv.org/pdf/2408.01023v1.pdf","comment":"17 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.01022v1","updated":"2024-08-02T05:46:17Z","published":"2024-08-02T05:46:17Z","title":"A Family of Distributions of Random Subsets for Controlling Positive and\n  Negative Dependence","summary":"  Positive and negative dependence are fundamental concepts that characterize\nthe attractive and repulsive behavior of random subsets. Although some\nprobabilistic models are known to exhibit positive or negative dependence, it\nis challenging to seamlessly bridge them with a practicable probabilistic\nmodel. In this study, we introduce a new family of distributions, named the\ndiscrete kernel point process (DKPP), which includes determinantal point\nprocesses and parts of Boltzmann machines. We also develop some computational\nmethods for probabilistic operations and inference with DKPPs, such as\ncalculating marginal and conditional probabilities and learning the parameters.\nOur numerical experiments demonstrate the controllability of positive and\nnegative dependence and the effectiveness of the computational methods for\nDKPPs.\n","authors":["Takahiro Kawashima","Hideitsu Hino"],"pdf_url":"https://arxiv.org/pdf/2408.01022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01018v1","updated":"2024-08-02T05:36:14Z","published":"2024-08-02T05:36:14Z","title":"GNN-MolKAN: Harnessing the Power of KAN to Advance Molecular\n  Representation Learning with GNNs","summary":"  Effective molecular representation learning is crucial for molecular property\nprediction and drug design. However, existing approaches struggle with\nlimitations in insufficient annotations and suboptimal architecture design. For\ninstance, Graph Neural Networks (GNNs) suffer from over-squashing, causing the\nloss of important structural details in molecules, thus impairing molecular\nrepresentations. In this work, we propose a new class of GNNs, GNN-MolKAN and\nits augmented variant, GNN-MolKAN+, that integrate the Kolmogorov-Arnold\nNetworks (KAN) architecture from AI + Science into GNNs to address these\nchallenges. Additionally, we introduce Adaptive FastKAN (AdFastKAN), an\nadvanced KAN that offers increased stability and speed, further enhancing the\nperformance of standard GNNs. Notably, our approach holds three key benefits:\n1) Superior Performance: GNN-MolKAN and GNN-MolKAN+ demonstrate superior\nprediction ability, robust generalization to unseen scaffolds, and versatile\ntransferability across different GNN architectures. 2) Efficiency: These models\nrequire less computational time and fewer parameters while matching or\nsurpassing the state-of-the-art (SOTA) self-supervised methods. 3) Few-shot\nLearning Ability: GNN-MolKAN demonstrates great potential in few-shot learning\nscenarios, achieving an average improvement of 6.97% across few-shot\nbenchmarks. Overall, we validate our architecture on 6 classification datasets,\n6 regression datasets, and 4 few-shot learning datasets, consistently achieving\nhighly competitive results across all of them.\n","authors":["Ruifeng Li"],"pdf_url":"https://arxiv.org/pdf/2408.01018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01016v1","updated":"2024-08-02T05:23:19Z","published":"2024-08-02T05:23:19Z","title":"IBB Traffic Graph Data: Benchmarking and Road Traffic Prediction Model","summary":"  Road traffic congestion prediction is a crucial component of intelligent\ntransportation systems, since it enables proactive traffic management, enhances\nsuburban experience, reduces environmental impact, and improves overall safety\nand efficiency. Although there are several public datasets, especially for\nmetropolitan areas, these datasets may not be applicable to practical scenarios\ndue to insufficiency in the scale of data (i.e. number of sensors and road\nlinks) and several external factors like different characteristics of the\ntarget area such as urban, highways and the data collection location. To\naddress this, this paper introduces a novel IBB Traffic graph dataset as an\nalternative benchmark dataset to mitigate these limitations and enrich the\nliterature with new geographical characteristics. IBB Traffic graph dataset\ncovers the sensor data collected at 2451 distinct locations. Moreover, we\npropose a novel Road Traffic Prediction Model that strengthens temporal links\nthrough feature engineering, node embedding with GLEE to represent\ninter-related relationships within the traffic network, and traffic prediction\nwith ExtraTrees. The results indicate that the proposed model consistently\noutperforms the baseline models, demonstrating an average accuracy improvement\nof 4%.\n","authors":["Eren Olug","Kiymet Kaya","Resul Tugay","Sule Gunduz Oguducu"],"pdf_url":"https://arxiv.org/pdf/2408.01016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.13894v2","updated":"2024-08-02T05:05:48Z","published":"2021-04-28T17:26:29Z","title":"Weighed l1 on the simplex: Compressive sensing meets locality","summary":"  Sparse manifold learning algorithms combine techniques in manifold learning\nand sparse optimization to learn features that could be utilized for downstream\ntasks. The standard setting of compressive sensing can not be immediately\napplied to this setup. Due to the intrinsic geometric structure of data,\ndictionary atoms might be redundant and do not satisfy the restricted isometry\nproperty or coherence condition. In addition, manifold learning emphasizes\nlearning local geometry which is not reflected in a standard $\\ell_1$\nminimization problem. We propose weighted $\\ell_0$ and weighted $\\ell_1$\nmetrics that encourage representation via neighborhood atoms suited for\ndictionary based manifold learning. Assuming that the data is generated from\nDelaunay triangulation, we show the equivalence of weighted $\\ell_0$ and\nweighted $\\ell_1$. We discuss an optimization program that learns the\ndictionaries and sparse coefficients and demonstrate the utility of our\nregularization on synthetic and real datasets.\n","authors":["Abiy Tasissa","Pranay Tankala","Demba Ba"],"pdf_url":"https://arxiv.org/pdf/2104.13894v2.pdf","comment":"7 pages, 2 figures. The proof of theorem 1 in v1 does not hold true\n  in general without additional assumptions. This version fixes this problem.\n  For more details, we refer the interested reader to arXiv:2012.02134 which is\n  the journal version of the workshop paper v1"},{"id":"http://arxiv.org/abs/2408.01008v1","updated":"2024-08-02T04:45:58Z","published":"2024-08-02T04:45:58Z","title":"Tensor Train Low-rank Approximation (TT-LoRA): Democratizing AI with\n  Accelerated LLMs","summary":"  In recent years, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities across a wide range of natural language processing (NLP) tasks,\nsuch as question-answering, sentiment analysis, text summarization, and machine\ntranslation. However, the ever-growing complexity of LLMs demands immense\ncomputational resources, hindering the broader research and application of\nthese models. To address this, various parameter-efficient fine-tuning\nstrategies, such as Low-Rank Approximation (LoRA) and Adapters, have been\ndeveloped. Despite their potential, these methods often face limitations in\ncompressibility. Specifically, LoRA struggles to scale effectively with the\nincreasing number of trainable parameters in modern large scale LLMs.\nAdditionally, Low-Rank Economic Tensor-Train Adaptation (LoRETTA), which\nutilizes tensor train decomposition, has not yet achieved the level of\ncompression necessary for fine-tuning very large scale models with limited\nresources. This paper introduces Tensor Train Low-Rank Approximation (TT-LoRA),\na novel parameter-efficient fine-tuning (PEFT) approach that extends LoRETTA\nwith optimized tensor train (TT) decomposition integration. By eliminating\nAdapters and traditional LoRA-based structures, TT-LoRA achieves greater model\ncompression without compromising downstream task performance, along with\nreduced inference latency and computational overhead. We conduct an exhaustive\nparameter search to establish benchmarks that highlight the trade-off between\nmodel compression and performance. Our results demonstrate significant\ncompression of LLMs while maintaining comparable performance to larger models,\nfacilitating their deployment on resource-constraint platforms.\n","authors":["Afia Anjum","Maksim E. Eren","Ismael Boureima","Boian Alexandrov","Manish Bhattarai"],"pdf_url":"https://arxiv.org/pdf/2408.01008v1.pdf","comment":"LA-UR-24-28177"},{"id":"http://arxiv.org/abs/2408.01005v1","updated":"2024-08-02T04:40:15Z","published":"2024-08-02T04:40:15Z","title":"Enhancing Financial Market Predictions: Causality-Driven Feature\n  Selection","summary":"  This paper introduces the FinSen dataset that revolutionizes financial market\nanalysis by integrating economic and financial news articles from 197 countries\nwith stock market data. The dataset's extensive coverage spans 15 years from\n2007 to 2023 with temporal information, offering a rich, global perspective\nwith 160,000 records on financial market news. Our study leverages causally\nvalidated sentiment scores and LSTM models to enhance market forecast accuracy\nand reliability. Utilizing the FinSen dataset, we introduce an innovative Focal\nCalibration Loss, reducing Expected Calibration Error (ECE) to 3.34 percent\nwith the DAN 3 model. This not only improves prediction accuracy but also\naligns probabilistic forecasts closely with real outcomes, crucial for the\nfinancial sector where predicted probability is paramount. Our approach\ndemonstrates the effectiveness of combining sentiment analysis with precise\ncalibration techniques for trustworthy financial forecasting where the cost of\nmisinterpretation can be high. Finsen Data can be found at [this github\nURL](https://github.com/EagleAdelaide/FinSen_Dataset.git).\n","authors":["Wenhao Liang","Zhengyang Li","Weitong Chen"],"pdf_url":"https://arxiv.org/pdf/2408.01005v1.pdf","comment":"Accepted by The 20th International Conference Advanced Data Mining\n  and Applications 2024 (ADMA 2024)"},{"id":"http://arxiv.org/abs/2408.01000v1","updated":"2024-08-02T04:19:25Z","published":"2024-08-02T04:19:25Z","title":"Adaptive Two-Stage Cloud Resource Scaling via Hierarchical\n  Multi-Indicator Forecasting and Bayesian Decision-Making","summary":"  The surging demand for cloud computing resources, driven by the rapid growth\nof sophisticated large-scale models and data centers, underscores the critical\nimportance of efficient and adaptive resource allocation. As major tech\nenterprises deploy massive infrastructures with thousands of GPUs, existing\ncloud platforms still struggle with low resource utilization due to key\nchallenges: capturing hierarchical indicator structures, modeling non-Gaussian\ndistributions, and decision-making under uncertainty. To address these\nchallenges, we propose HRAMONY, an adaptive Hierarchical Attention-based\nResource Modeling and Decision-Making System. HARMONY combines hierarchical\nmulti-indicator distribution forecasting and uncertainty-aware Bayesian\ndecision-making. It introduces a novel hierarchical attention mechanism that\ncomprehensively models complex inter-indicator dependencies, enabling accurate\npredictions that can adapt to evolving environment states. By transforming\nGaussian projections into adaptive non-Gaussian distributions via Normalizing\nFlows. Crucially, HARMONY leverages the full predictive distributions in an\nadaptive Bayesian process, proactively incorporating uncertainties to optimize\nresource allocation while robustly meeting SLA constraints under varying\nconditions. Extensive evaluations across four large-scale cloud datasets\ndemonstrate HARMONY's state-of-the-art performance, significantly outperforming\nnine established methods. A month-long real-world deployment validated\nHARMONY's substantial practical impact, realizing over 35,000 GPU hours in\nsavings and translating to $100K+ in cost reduction, showcasing its remarkable\neconomic value through adaptive, uncertainty-aware scaling. Our code is\navailable at https://github.com/Floating-LY/HARMONY1.\n","authors":["Yang Luo","Shiyu Wang","Zhemeng Yu","Wei Lu","Xiaofeng Gao","Lintao Ma","Guihai Chen"],"pdf_url":"https://arxiv.org/pdf/2408.01000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00996v1","updated":"2024-08-02T04:09:15Z","published":"2024-08-02T04:09:15Z","title":"IncidentNet: Traffic Incident Detection, Localization and Severity\n  Estimation with Sparse Sensing","summary":"  Prior art in traffic incident detection relies on high sensor coverage and is\nprimarily based on decision-tree and random forest models that have limited\nrepresentation capacity and, as a result, cannot detect incidents with high\naccuracy. This paper presents IncidentNet - a novel approach for classifying,\nlocalizing, and estimating the severity of traffic incidents using deep\nlearning models trained on data captured from sparsely placed sensors in urban\nenvironments. Our model works on microscopic traffic data that can be collected\nusing cameras installed at traffic intersections. Due to the unavailability of\ndatasets that provide microscopic traffic details and traffic incident details\nsimultaneously, we also present a methodology to generate a synthetic\nmicroscopic traffic dataset that matches given macroscopic traffic data.\nIncidentNet achieves a traffic incident detection rate of 98%, with false alarm\nrates of less than 7% in 197 seconds on average in urban environments with\ncameras on less than 20% of the traffic intersections.\n","authors":["Sai Shashank Peddiraju","Kaustubh Harapanahalli","Edward Andert","Aviral Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2408.00996v1.pdf","comment":"6 pages, 6 figures, 2024 IEEE 27th International Conference on\n  Intelligent Transportation Systems (ITSC)"},{"id":"http://arxiv.org/abs/2303.12653v3","updated":"2024-08-02T04:02:26Z","published":"2023-03-09T05:30:53Z","title":"Robust Millimeter Beamforming via Self-Supervised Hybrid Deep Learning","summary":"  Beamforming with large-scale antenna arrays has been widely used in recent\nyears, which is acknowledged as an important part in 5G and incoming 6G. Thus,\nvarious techniques are leveraged to improve its performance, e.g., deep\nlearning, advanced optimization algorithms, etc. Although its performance in\nmany previous research scenarios with deep learning is quite attractive,\nusually it drops rapidly when the environment or dataset is changed. Therefore,\ndesigning effective beamforming network with strong robustness is an open issue\nfor the intelligent wireless communications. In this paper, we propose a robust\nbeamforming self-supervised network, and verify it in two kinds of different\ndatasets with various scenarios. Simulation results show that the proposed\nself-supervised network with hybrid learning performs well in both classic\nDeepMIMO and new WAIR-D dataset with the strong robustness under the various\nenvironments. Also, we present the principle to explain the rationality of this\nkind of hybrid learning, which is instructive to apply with more kinds of\ndatasets.\n","authors":["Fenghao Zhu","Bohao Wang","Zhaohui Yang","Chongwen Huang","Zhaoyang Zhang","George C. Alexandropoulos","Chau Yuen","Merouane Debbah"],"pdf_url":"https://arxiv.org/pdf/2303.12653v3.pdf","comment":"Accept by EUSIPCO 2023"},{"id":"http://arxiv.org/abs/2408.00992v1","updated":"2024-08-02T03:44:14Z","published":"2024-08-02T03:44:14Z","title":"Fairness in Large Language Models in Three Hour","summary":"  Large Language Models (LLMs) have demonstrated remarkable success across\nvarious domains but often lack fairness considerations, potentially leading to\ndiscriminatory outcomes against marginalized populations. Unlike fairness in\ntraditional machine learning, fairness in LLMs involves unique backgrounds,\ntaxonomies, and fulfillment techniques. This tutorial provides a systematic\noverview of recent advances in the literature concerning fair LLMs, beginning\nwith real-world case studies to introduce LLMs, followed by an analysis of bias\ncauses therein. The concept of fairness in LLMs is then explored, summarizing\nthe strategies for evaluating bias and the algorithms designed to promote\nfairness. Additionally, resources for assessing bias in LLMs, including\ntoolkits and datasets, are compiled, and current research challenges and open\nquestions in the field are discussed. The repository is available at\n\\url{https://github.com/LavinWong/Fairness-in-Large-Language-Models}.\n","authors":["Thang Doan Viet","Zichong Wang","Minh Nhat Nguyen","Wenbin Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01258v2","updated":"2024-08-02T03:39:18Z","published":"2024-01-02T15:59:00Z","title":"Towards Model-Free LQR Control over Rate-Limited Channels","summary":"  Given the success of model-free methods for control design in many problem\nsettings, it is natural to ask how things will change if realistic\ncommunication channels are utilized for the transmission of gradients or\npolicies. While the resulting problem has analogies with the formulations\nstudied under the rubric of networked control systems, the rich literature in\nthat area has typically assumed that the model of the system is known. As a\nstep towards bridging the fields of model-free control design and networked\ncontrol systems, we ask: \\textit{Is it possible to solve basic control problems\n- such as the linear quadratic regulator (LQR) problem - in a model-free manner\nover a rate-limited channel?} Toward answering this question, we study a\nsetting where a worker agent transmits quantized policy gradients (of the LQR\ncost) to a server over a noiseless channel with a finite bit-rate. We propose a\nnew algorithm titled Adaptively Quantized Gradient Descent (\\texttt{AQGD}), and\nprove that above a certain finite threshold bit-rate, \\texttt{AQGD} guarantees\nexponentially fast convergence to the globally optimal policy, with \\textit{no\ndeterioration of the exponent relative to the unquantized setting}. More\ngenerally, our approach reveals the benefits of adaptive quantization in\npreserving fast linear convergence rates, and, as such, may be of independent\ninterest to the literature on compressed optimization.\n","authors":["Aritra Mitra","Lintao Ye","Vijay Gupta"],"pdf_url":"https://arxiv.org/pdf/2401.01258v2.pdf","comment":"Accepted for an Oral Presentation at the 6th Annual Learning for\n  Dynamics & Control Conference"},{"id":"http://arxiv.org/abs/2407.11046v2","updated":"2024-08-02T03:22:22Z","published":"2024-07-08T12:32:10Z","title":"A Survey on LoRA of Large Language Models","summary":"  Low-Rank Adaptation~(LoRA), which updates the dense neural network layers\nwith pluggable low-rank matrices, is one of the best performed parameter\nefficient fine-tuning paradigms. Furthermore, it has significant advantages in\ncross-task generalization and privacy-preserving. Hence, LoRA has gained much\nattention recently, and the number of related literature demonstrates\nexponential growth. It is necessary to conduct a comprehensive overview of the\ncurrent progress on LoRA. This survey categorizes and reviews the progress from\nthe perspectives of (1) downstream adaptation improving variants that improve\nLoRA's performance on downstream tasks; (2) cross-task generalization methods\nthat mix multiple LoRA plugins to achieve cross-task generalization; (3)\nefficiency-improving methods that boost the computation-efficiency of LoRA; (4)\ndata privacy-preserving methods that use LoRA in federated learning; (5)\napplication. Besides, this survey also discusses the future directions in this\nfield. At last, we provide a Github page\n(https://github.com/ZJU-LLMs/Awesome-LoRAs.git) for readers to check the\nupdates and initiate discussions on this survey paper.\n","authors":["Yuren Mao","Yuhang Ge","Yijiang Fan","Wenyi Xu","Yu Mi","Zhonghao Hu","Yunjun Gao"],"pdf_url":"https://arxiv.org/pdf/2407.11046v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.15870v5","updated":"2024-08-02T03:16:07Z","published":"2023-07-29T02:35:37Z","title":"SemiSFL: Split Federated Learning on Unlabeled and Non-IID Data","summary":"  Federated Learning (FL) has emerged to allow multiple clients to\ncollaboratively train machine learning models on their private data at the\nnetwork edge. However, training and deploying large-scale models on\nresource-constrained devices is challenging. Fortunately, Split Federated\nLearning (SFL) offers a feasible solution by alleviating the computation and/or\ncommunication burden on clients. However, existing SFL works often assume\nsufficient labeled data on clients, which is usually impractical. Besides, data\nnon-IIDness poses another challenge to ensure efficient model training. To our\nbest knowledge, the above two issues have not been simultaneously addressed in\nSFL. Herein, we propose a novel Semi-supervised SFL system, termed SemiSFL,\nwhich incorporates clustering regularization to perform SFL with unlabeled and\nnon-IID client data. Moreover, our theoretical and experimental investigations\ninto model convergence reveal that the inconsistent training processes on\nlabeled and unlabeled data have an influence on the effectiveness of clustering\nregularization. To mitigate the training inconsistency, we develop an algorithm\nfor dynamically adjusting the global updating frequency, so as to improve\ntraining performance. Extensive experiments on benchmark models and datasets\nshow that our system provides a 3.8x speed-up in training time, reduces the\ncommunication cost by about 70.3% while reaching the target accuracy, and\nachieves up to 5.8% improvement in accuracy under non-IID scenarios compared to\nthe state-of-the-art baselines.\n","authors":["Yang Xu","Yunming Liao","Hongli Xu","Zhipeng Sun","Liusheng Huang","Chunming Qiao"],"pdf_url":"https://arxiv.org/pdf/2307.15870v5.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2408.00985v1","updated":"2024-08-02T03:02:39Z","published":"2024-08-02T03:02:39Z","title":"Reconstructing Richtmyer-Meshkov instabilities from noisy radiographs\n  using low dimensional features and attention-based neural networks","summary":"  A trained attention-based transformer network can robustly recover the\ncomplex topologies given by the Richtmyer-Meshkoff instability from a sequence\nof hydrodynamic features derived from radiographic images corrupted with blur,\nscatter, and noise. This approach is demonstrated on ICF-like double shell\nhydrodynamic simulations. The key component of this network is a transformer\nencoder that acts on a sequence of features extracted from noisy radiographs.\nThis encoder includes numerous self-attention layers that act to learn temporal\ndependencies in the input sequences and increase the expressiveness of the\nmodel. This approach is demonstrated to exhibit an excellent ability to\naccurately recover the Richtmyer-Meshkov instability growth rates, even despite\nthe gas-metal interface being greatly obscured by radiographic noise.\n","authors":["Daniel A. Serino","Marc L. Klasky","Balasubramanya T. Nadiga","Xiaojian Xu","Trevor Wilcox"],"pdf_url":"https://arxiv.org/pdf/2408.00985v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00973v1","updated":"2024-08-02T01:49:29Z","published":"2024-08-02T01:49:29Z","title":"META-ANOVA: Screening interactions for interpretable machine learning","summary":"  There are two things to be considered when we evaluate predictive models. One\nis prediction accuracy,and the other is interpretability. Over the recent\ndecades, many prediction models of high performance, such as ensemble-based\nmodels and deep neural networks, have been developed. However, these models are\noften too complex, making it difficult to intuitively interpret their\npredictions. This complexity in interpretation limits their use in many\nreal-world fields that require accountability, such as medicine, finance, and\ncollege admissions. In this study, we develop a novel method called Meta-ANOVA\nto provide an interpretable model for any given prediction model. The basic\nidea of Meta-ANOVA is to transform a given black-box prediction model to the\nfunctional ANOVA model. A novel technical contribution of Meta-ANOVA is a\nprocedure of screening out unnecessary interaction before transforming a given\nblack-box model to the functional ANOVA model. This screening procedure allows\nthe inclusion of higher order interactions in the transformed functional ANOVA\nmodel without computational difficulties. We prove that the screening procedure\nis asymptotically consistent. Through various experiments with synthetic and\nreal-world datasets, we empirically demonstrate the superiority of Meta-ANOVA\n","authors":["Yongchan Choi","Seokhun Park","Chanmoo Park","Dongha Kim","Yongdai Kim"],"pdf_url":"https://arxiv.org/pdf/2408.00973v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2308.14250v3","updated":"2024-08-02T01:38:16Z","published":"2023-08-28T01:57:38Z","title":"Rule-Based Error Detection and Correction to Operationalize Movement\n  Trajectory Classification","summary":"  Classification of movement trajectories has many applications in\ntransportation and is a key component for large-scale movement trajectory\ngeneration and anomaly detection which has key safety applications in the\naftermath of a disaster or other external shock. However, the current\nstate-of-the-art (SOTA) are based on supervised deep learning - which leads to\nchallenges when the distribution of trajectories changes due to such a shock.\nWe provide a neuro-symbolic rule-based framework to conduct error correction\nand detection of these models to integrate into our movement trajectory\nplatform. We provide a suite of experiments on several recent SOTA models where\nwe show highly accurate error detection, the ability to improve accuracy with a\nchanging test distribution, and accuracy improvement for the base use case in\naddition to a suite of theoretical properties that informed algorithm\ndevelopment. Specifically, we show an F1 scores for predicting errors of up to\n0.984, significant performance increase for out-of distribution accuracy (8.51%\nimprovement over SOTA for zero-shot accuracy), and accuracy improvement over\nthe SOTA model.\n","authors":["Bowen Xi","Kevin Scaria","Divyagna Bavikadi","Paulo Shakarian"],"pdf_url":"https://arxiv.org/pdf/2308.14250v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03452v2","updated":"2024-08-02T01:33:25Z","published":"2023-09-07T02:26:55Z","title":"Multimodal Guidance Network for Missing-Modality Inference in Content\n  Moderation","summary":"  Multimodal deep learning, especially vision-language models, have gained\nsignificant traction in recent years, greatly improving performance on many\ndownstream tasks, including content moderation and violence detection. However,\nstandard multimodal approaches often assume consistent modalities between\ntraining and inference, limiting applications in many real-world use cases, as\nsome modalities may not be available during inference. While existing research\nmitigates this problem through reconstructing the missing modalities, they\nunavoidably increase unnecessary computational cost, which could be just as\ncritical, especially for large, deployed infrastructures in industry. To this\nend, we propose a novel guidance network that promotes knowledge sharing during\ntraining, taking advantage of the multimodal representations to train better\nsingle-modality models to be used for inference. Real-world experiments in\nviolence detection shows that our proposed framework trains single-modality\nmodels that significantly outperform traditionally trained counterparts, while\navoiding increases in computational cost for inference.\n","authors":["Zhuokai Zhao","Harish Palani","Tianyi Liu","Lena Evans","Ruth Toner"],"pdf_url":"https://arxiv.org/pdf/2309.03452v2.pdf","comment":"ICME 2024 Camera Ready. Code is available at\n  https://github.com/zhuokaizhao/multimodal-guidance-network"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2408.01417v1","updated":"2024-08-02T17:51:57Z","published":"2024-08-02T17:51:57Z","title":"Talk Less, Interact Better: Evaluating In-context Conversational\n  Adaptation in Multimodal LLMs","summary":"  Humans spontaneously use increasingly efficient language as interactions\nprogress, by adapting and forming ad-hoc conventions. This phenomenon has been\nstudied extensively using reference games, showing properties of human language\nthat go beyond relaying intents. It remains unexplored whether multimodal large\nlanguage models (MLLMs) similarly increase communication efficiency during\ninteractions, and what mechanisms they may adopt for this purpose. We introduce\nICCA, an automated framework to evaluate such conversational adaptation as an\nin-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and\nobserve that while they may understand the increasingly efficient language of\ntheir interlocutor, they do not spontaneously make their own language more\nefficient over time. This latter ability can only be elicited in some models\n(e.g., GPT-4) with heavy-handed prompting. This shows that this property of\nlinguistic interaction does not arise from current training regimes, even\nthough it is a common hallmark of human language. ICCA is available at\nhttps://github.com/lil-lab/ICCA.\n","authors":["Yilun Hua","Yoav Artzi"],"pdf_url":"https://arxiv.org/pdf/2408.01417v1.pdf","comment":"Accepted to COLM 2024"},{"id":"http://arxiv.org/abs/2408.01384v1","updated":"2024-08-02T16:41:34Z","published":"2024-08-02T16:41:34Z","title":"NOLO: Navigate Only Look Once","summary":"  The in-context learning ability of Transformer models has brought new\npossibilities to visual navigation. In this paper, we focus on the video\nnavigation setting, where an in-context navigation policy needs to be learned\npurely from videos in an offline manner, without access to the actual\nenvironment. For this setting, we propose Navigate Only Look Once (NOLO), a\nmethod for learning a navigation policy that possesses the in-context ability\nand adapts to new scenes by taking corresponding context videos as input\nwithout finetuning or re-training. To enable learning from videos, we first\npropose a pseudo action labeling procedure using optical flow to recover the\naction label from egocentric videos. Then, offline reinforcement learning is\napplied to learn the navigation policy. Through extensive experiments on\ndifferent scenes, we show that our algorithm outperforms baselines by a large\nmargin, which demonstrates the in-context learning ability of the learned\npolicy.\n","authors":["Bohan Zhou","Jiangxing Wang","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2408.01384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11652v3","updated":"2024-08-02T16:30:55Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v3.pdf","comment":"I found critical errors in the manuscript affecting its validity. I\n  need to correct these before resubmitting. Major changes to methodology and\n  results are underway, significantly altering the content. I will resubmit the\n  revised version"},{"id":"http://arxiv.org/abs/2408.01372v1","updated":"2024-08-02T16:28:51Z","published":"2024-08-02T16:28:51Z","title":"Spatial-Spectral Morphological Mamba for Hyperspectral Image\n  Classification","summary":"  In recent years, Transformers have garnered significant attention for\nHyperspectral Image Classification (HSIC) due to their self-attention\nmechanism, which provides strong classification performance. However, these\nmodels face major challenges in computational efficiency, as their complexity\nincreases quadratically with the sequence length. The Mamba architecture,\nleveraging a State Space Model, offers a more efficient alternative to\nTransformers. This paper introduces the Spatial-Spectral Morphological Mamba\n(MorpMamba) model. In the MorpMamba model, a token generation module first\nconverts the Hyperspectral Image (HSI) patch into spatial-spectral tokens.\nThese tokens are then processed by a morphology block, which computes\nstructural and shape information using depthwise separable convolutional\noperations. The extracted information is enhanced in a feature enhancement\nmodule that adjusts the spatial and spectral tokens based on the center region\nof the HSI sample, allowing for effective information fusion within each block.\nSubsequently, the tokens are refined in a multi-head self-attention block to\nfurther improve the feature space. Finally, the combined information is fed\ninto the state space block for classification and the creation of the ground\ntruth map. Experiments on widely used Hyperspectral (HS) datasets demonstrate\nthat the MorpMamba model outperforms (parametric efficiency) both CNN and\nTransformer models.\n","authors":["Muhammad Ahmad","Muhammad Hassaan Farooq Butt","Muhammad Usama","Adil Mehmood Khan","Manual Mazzara","Salvatore Distenano"],"pdf_url":"https://arxiv.org/pdf/2408.01372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01370v1","updated":"2024-08-02T16:24:55Z","published":"2024-08-02T16:24:55Z","title":"EVIT: Event-based Visual-Inertial Tracking in Semi-Dense Maps Using\n  Windowed Nonlinear Optimization","summary":"  Event cameras are an interesting visual exteroceptive sensor that reacts to\nbrightness changes rather than integrating absolute image intensities. Owing to\nthis design, the sensor exhibits strong performance in situations of\nchallenging dynamics and illumination conditions. While event-based\nsimultaneous tracking and mapping remains a challenging problem, a number of\nrecent works have pointed out the sensor's suitability for prior map-based\ntracking. By making use of cross-modal registration paradigms, the camera's\nego-motion can be tracked across a large spectrum of illumination and dynamics\nconditions on top of accurate maps that have been created a priori by more\ntraditional sensors. The present paper follows up on a recently introduced\nevent-based geometric semi-dense tracking paradigm, and proposes the addition\nof inertial signals in order to robustify the estimation. More specifically,\nthe added signals provide strong cues for pose initialization as well as\nregularization during windowed, multi-frame tracking. As a result, the proposed\nframework achieves increased performance under challenging illumination\nconditions as well as a reduction of the rate at which intermediate event\nrepresentations need to be registered in order to maintain stable tracking\nacross highly dynamic sequences. Our evaluation focuses on a diverse set of\nreal world sequences and comprises a comparison of our proposed method against\na purely event-based alternative running at different rates.\n","authors":["Runze Yuan","Tao Liu","Zijia Dai","Yi-Fan Zuo","Laurent Kneip"],"pdf_url":"https://arxiv.org/pdf/2408.01370v1.pdf","comment":"8 pages, 5 figures, 3 tables, International Conference on Intelligent\n  Robots and Systems 2024"},{"id":"http://arxiv.org/abs/2408.01366v1","updated":"2024-08-02T16:20:56Z","published":"2024-08-02T16:20:56Z","title":"Play to the Score: Stage-Guided Dynamic Multi-Sensory Fusion for Robotic\n  Manipulation","summary":"  Humans possess a remarkable talent for flexibly alternating to different\nsenses when interacting with the environment. Picture a chef skillfully gauging\nthe timing of ingredient additions and controlling the heat according to the\ncolors, sounds, and aromas, seamlessly navigating through every stage of the\ncomplex cooking process. This ability is founded upon a thorough comprehension\nof task stages, as achieving the sub-goal within each stage can necessitate the\nutilization of different senses. In order to endow robots with similar ability,\nwe incorporate the task stages divided by sub-goals into the imitation learning\nprocess to accordingly guide dynamic multi-sensory fusion. We propose MS-Bot, a\nstage-guided dynamic multi-sensory fusion method with coarse-to-fine stage\nunderstanding, which dynamically adjusts the priority of modalities based on\nthe fine-grained state within the predicted current stage. We train a robot\nsystem equipped with visual, auditory, and tactile sensors to accomplish\nchallenging robotic manipulation tasks: pouring and peg insertion with keyway.\nExperimental results indicate that our approach enables more effective and\nexplainable dynamic fusion, aligning more closely with the human fusion process\nthan existing methods.\n","authors":["Ruoxuan Feng","Di Hu","Wenke Ma","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.01366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17804v3","updated":"2024-08-02T16:17:35Z","published":"2023-11-29T16:54:25Z","title":"The Importance of Downstream Networks in Digital Pathology Foundation\n  Models","summary":"  Digital pathology has significantly advanced disease detection and\npathologist efficiency through the analysis of gigapixel whole-slide images\n(WSI). In this process, WSIs are first divided into patches, for which a\nfeature extractor model is applied to obtain feature vectors, which are\nsubsequently processed by an aggregation model to predict the respective WSI\nlabel. With the rapid evolution of representation learning, numerous new\nfeature extractor models, often termed foundational models, have emerged.\nTraditional evaluation methods rely on a static downstream aggregation model\nsetup, encompassing a fixed architecture and hyperparameters, a practice we\nidentify as potentially biasing the results. Our study uncovers a sensitivity\nof feature extractor models towards aggregation model configurations,\nindicating that performance comparability can be skewed based on the chosen\nconfigurations. By accounting for this sensitivity, we find that the\nperformance of many current feature extractor models is notably similar. We\nsupport this insight by evaluating seven feature extractor models across three\ndifferent datasets with 162 different aggregation model configurations. This\ncomprehensive approach provides a more nuanced understanding of the feature\nextractors' sensitivity to various aggregation model configurations, leading to\na fairer and more accurate assessment of new foundation models in digital\npathology.\n","authors":["Gustav Bredell","Marcel Fischer","Przemyslaw Szostak","Samaneh Abbasi-Sureshjani","Alvaro Gomariz"],"pdf_url":"https://arxiv.org/pdf/2311.17804v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01363v1","updated":"2024-08-02T16:15:25Z","published":"2024-08-02T16:15:25Z","title":"Toward Automatic Relevance Judgment using Vision--Language Models for\n  Image--Text Retrieval Evaluation","summary":"  Vision--Language Models (VLMs) have demonstrated success across diverse\napplications, yet their potential to assist in relevance judgments remains\nuncertain. This paper assesses the relevance estimation capabilities of VLMs,\nincluding CLIP, LLaVA, and GPT-4V, within a large-scale \\textit{ad hoc}\nretrieval task tailored for multimedia content creation in a zero-shot fashion.\nPreliminary experiments reveal the following: (1) Both LLaVA and GPT-4V,\nencompassing open-source and closed-source visual-instruction-tuned Large\nLanguage Models (LLMs), achieve notable Kendall's $\\tau \\sim 0.4$ when compared\nto human relevance judgments, surpassing the CLIPScore metric. (2) While\nCLIPScore is strongly preferred, LLMs are less biased towards CLIP-based\nretrieval systems. (3) GPT-4V's score distribution aligns more closely with\nhuman judgments than other models, achieving a Cohen's $\\kappa$ value of around\n0.08, which outperforms CLIPScore at approximately -0.096. These findings\nunderscore the potential of LLM-powered VLMs in enhancing relevance judgments.\n","authors":["Jheng-Hong Yang","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2408.01363v1.pdf","comment":"Accepted by ACM SIGIR 2024 LLM4Eval Workshop:\n  https://llm4eval.github.io/papers"},{"id":"http://arxiv.org/abs/2404.13534v3","updated":"2024-08-02T16:14:46Z","published":"2024-04-21T05:09:56Z","title":"Motion-aware Latent Diffusion Models for Video Frame Interpolation","summary":"  With the advancement of AIGC, video frame interpolation (VFI) has become a\ncrucial component in existing video generation frameworks, attracting\nwidespread research interest. For the VFI task, the motion estimation between\nneighboring frames plays a crucial role in avoiding motion ambiguity. However,\nexisting VFI methods always struggle to accurately predict the motion\ninformation between consecutive frames, and this imprecise estimation leads to\nblurred and visually incoherent interpolated frames. In this paper, we propose\na novel diffusion framework, motion-aware latent diffusion models (MADiff),\nwhich is specifically designed for the VFI task. By incorporating motion priors\nbetween the conditional neighboring frames with the target interpolated frame\npredicted throughout the diffusion sampling procedure, MADiff progressively\nrefines the intermediate outcomes, culminating in generating both visually\nsmooth and realistic results. Extensive experiments conducted on benchmark\ndatasets demonstrate that our method achieves state-of-the-art performance\nsignificantly outperforming existing approaches, especially under challenging\nscenarios involving dynamic textures with complex motion.\n","authors":["Zhilin Huang","Yijie Yu","Ling Yang","Chujun Qin","Bing Zheng","Xiawu Zheng","Zikun Zhou","Yaowei Wang","Wenming Yang"],"pdf_url":"https://arxiv.org/pdf/2404.13534v3.pdf","comment":"17 pages, 4 figures"},{"id":"http://arxiv.org/abs/2404.18849v2","updated":"2024-08-02T16:13:40Z","published":"2024-04-29T16:42:58Z","title":"MiPa: Mixed Patch Infrared-Visible Modality Agnostic Object Detection","summary":"  In real-world scenarios, using multiple modalities like visible (RGB) and\ninfrared (IR) can greatly improve the performance of a predictive task such as\nobject detection (OD). Multimodal learning is a common way to leverage these\nmodalities, where multiple modality-specific encoders and a fusion module are\nused to improve performance. In this paper, we tackle a different way to employ\nRGB and IR modalities, where only one modality or the other is observed by a\nsingle shared vision encoder. This realistic setting requires a lower memory\nfootprint and is more suitable for applications such as autonomous driving and\nsurveillance, which commonly rely on RGB and IR data. However, when learning a\nsingle encoder on multiple modalities, one modality can dominate the other,\nproducing uneven recognition results. This work investigates how to efficiently\nleverage RGB and IR modalities to train a common transformer-based OD vision\nencoder, while countering the effects of modality imbalance. For this, we\nintroduce a novel training technique to Mix Patches (MiPa) from the two\nmodalities, in conjunction with a patch-wise modality agnostic module, for\nlearning a common representation of both modalities. Our experiments show that\nMiPa can learn a representation to reach competitive results on traditional\nRGB/IR benchmarks while only requiring a single modality during inference. Our\ncode is available at: https://github.com/heitorrapela/MiPa.\n","authors":["Heitor R. Medeiros","David Latortue","Eric Granger","Marco Pedersoli"],"pdf_url":"https://arxiv.org/pdf/2404.18849v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04551v2","updated":"2024-08-02T16:09:49Z","published":"2024-06-06T23:35:51Z","title":"Improving Geo-diversity of Generated Images with Contextualized Vendi\n  Score Guidance","summary":"  With the growing popularity of text-to-image generative models, there has\nbeen increasing focus on understanding their risks and biases. Recent work has\nfound that state-of-the-art models struggle to depict everyday objects with the\ntrue diversity of the real world and have notable gaps between geographic\nregions. In this work, we aim to increase the diversity of generated images of\ncommon objects such that per-region variations are representative of the real\nworld. We introduce an inference time intervention, contextualized Vendi Score\nGuidance (c-VSG), that guides the backwards steps of latent diffusion models to\nincrease the diversity of a sample as compared to a \"memory bank\" of previously\ngenerated images while constraining the amount of variation within that of an\nexemplar set of real-world contextualizing images. We evaluate c-VSG with two\ngeographically representative datasets and find that it substantially increases\nthe diversity of generated images, both for the worst performing regions and on\naverage, while simultaneously maintaining or improving image quality and\nconsistency. Additionally, qualitative analyses reveal that diversity of\ngenerated images is significantly improved, including along the lines of\nreductive region portrayals present in the original model. We hope that this\nwork is a step towards text-to-image generative models that reflect the true\ngeographic diversity of the world.\n","authors":["Reyhane Askari Hemmat","Melissa Hall","Alicia Sun","Candace Ross","Michal Drozdzal","Adriana Romero-Soriano"],"pdf_url":"https://arxiv.org/pdf/2406.04551v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01356v1","updated":"2024-08-02T16:09:06Z","published":"2024-08-02T16:09:06Z","title":"Balanced Residual Distillation Learning for 3D Point Cloud\n  Class-Incremental Semantic Segmentation","summary":"  Class-incremental learning (CIL) thrives due to its success in processing the\ninflux of information by learning from continuously added new classes while\npreventing catastrophic forgetting about the old ones. It is essential for the\nperformance breakthrough of CIL to effectively refine past knowledge from the\nbase model and balance it with new learning. However, such an issue has not yet\nbeen considered in current research. In this work, we explore the potential of\nCIL from these perspectives and propose a novel balanced residual distillation\nframework (BRD-CIL) to push the performance bar of CIL to a new higher level.\nSpecifically, BRD-CIL designs a residual distillation learning strategy, which\ncan dynamically expand the network structure to capture the residuals between\nthe base and target models, effectively refining the past knowledge.\nFurthermore, BRD-CIL designs a balanced pseudo-label learning strategy by\ngenerating a guidance mask to reduce the preference for old classes, ensuring\nbalanced learning from new and old classes. We apply the proposed BRD-CIL to a\nchallenging 3D point cloud semantic segmentation task where the data are\nunordered and unstructured. Extensive experimental results demonstrate that\nBRD-CIL sets a new benchmark with an outstanding balance capability in\nclass-biased scenarios.\n","authors":["Yuanzhi Su","Siyuan Chen","Yuan-Gen Wang"],"pdf_url":"https://arxiv.org/pdf/2408.01356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01355v1","updated":"2024-08-02T16:07:15Z","published":"2024-08-02T16:07:15Z","title":"Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models\n  within Perturbed Inputs","summary":"  Multi-modal Large Language Models (MLLMs) have demonstrated remarkable\nperformance on various visual-language understanding and generation tasks.\nHowever, MLLMs occasionally generate content inconsistent with the given\nimages, which is known as \"hallucination\". Prior works primarily center on\nevaluating hallucination using standard, unperturbed benchmarks, which overlook\nthe prevalent occurrence of perturbed inputs in real-world scenarios-such as\nimage cropping or blurring-that are critical for a comprehensive assessment of\nMLLMs' hallucination. In this paper, to bridge this gap, we propose Hallu-PI,\nthe first benchmark designed to evaluate Hallucination in MLLMs within\nPerturbed Inputs. Specifically, Hallu-PI consists of seven perturbed scenarios,\ncontaining 1,260 perturbed images from 11 object types. Each image is\naccompanied by detailed annotations, which include fine-grained hallucination\ntypes, such as existence, attribute, and relation. We equip these annotations\nwith a rich set of questions, making Hallu-PI suitable for both discriminative\nand generative tasks. Extensive experiments on 12 mainstream MLLMs, such as\nGPT-4V and Gemini-Pro Vision, demonstrate that these models exhibit significant\nhallucinations on Hallu-PI, which is not observed in unperturbed scenarios.\nFurthermore, our research reveals a severe bias in MLLMs' ability to handle\ndifferent types of hallucinations. We also design two baselines specifically\nfor perturbed scenarios, namely Perturbed-Reminder and Perturbed-ICL. We hope\nthat our study will bring researchers' attention to the limitations of MLLMs\nwhen dealing with perturbed inputs, and spur further investigations to address\nthis issue. Our code and datasets are publicly available at\nhttps://github.com/NJUNLP/Hallu-PI.\n","authors":["Peng Ding","Jingyu Wu","Jun Kuang","Dan Ma","Xuezhi Cao","Xunliang Cai","Shi Chen","Jiajun Chen","Shujian Huang"],"pdf_url":"https://arxiv.org/pdf/2408.01355v1.pdf","comment":"Acccepted by ACM MM 2024, 14 pages, 11 figures, 9 tables"},{"id":"http://arxiv.org/abs/2408.01349v1","updated":"2024-08-02T15:54:49Z","published":"2024-08-02T15:54:49Z","title":"PC$^2$: Pseudo-Classification Based Pseudo-Captioning for Noisy\n  Correspondence Learning in Cross-Modal Retrieval","summary":"  In the realm of cross-modal retrieval, seamlessly integrating diverse\nmodalities within multimedia remains a formidable challenge, especially given\nthe complexities introduced by noisy correspondence learning (NCL). Such noise\noften stems from mismatched data pairs, which is a significant obstacle\ndistinct from traditional noisy labels. This paper introduces\nPseudo-Classification based Pseudo-Captioning (PC$^2$) framework to address\nthis challenge. PC$^2$ offers a threefold strategy: firstly, it establishes an\nauxiliary \"pseudo-classification\" task that interprets captions as categorical\nlabels, steering the model to learn image-text semantic similarity through a\nnon-contrastive mechanism. Secondly, unlike prevailing margin-based techniques,\ncapitalizing on PC$^2$'s pseudo-classification capability, we generate\npseudo-captions to provide more informative and tangible supervision for each\nmismatched pair. Thirdly, the oscillation of pseudo-classification is borrowed\nto assistant the correction of correspondence. In addition to technical\ncontributions, we develop a realistic NCL dataset called Noise of Web (NoW),\nwhich could be a new powerful NCL benchmark where noise exists naturally.\nEmpirical evaluations of PC$^2$ showcase marked improvements over existing\nstate-of-the-art robust cross-modal retrieval techniques on both simulated and\nrealistic datasets with various NCL settings. The contributed dataset and\nsource code are released at https://github.com/alipay/PC2-NoiseofWeb.\n","authors":["Yue Duan","Zhangxuan Gu","Zhenzhe Ying","Lei Qi","Changhua Meng","Yinghuan Shi"],"pdf_url":"https://arxiv.org/pdf/2408.01349v1.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2304.11857v3","updated":"2024-08-02T15:43:33Z","published":"2023-04-24T07:12:50Z","title":"Accurate and Efficient Event-based Semantic Segmentation Using Adaptive\n  Spiking Encoder-Decoder Network","summary":"  Spiking neural networks (SNNs), known for their low-power, event-driven\ncomputation and intrinsic temporal dynamics, are emerging as promising\nsolutions for processing dynamic, asynchronous signals from event-based\nsensors. Despite their potential, SNNs face challenges in training and\narchitectural design, resulting in limited performance in challenging\nevent-based dense prediction tasks compared to artificial neural networks\n(ANNs). In this work, we develop an efficient spiking encoder-decoder network\n(SpikingEDN) for large-scale event-based semantic segmentation tasks. To\nenhance the learning efficiency from dynamic event streams, we harness the\nadaptive threshold which improves network accuracy, sparsity and robustness in\nstreaming inference. Moreover, we develop a dual-path Spiking\nSpatially-Adaptive Modulation module, which is specifically tailored to enhance\nthe representation of sparse events and multi-modal inputs, thereby\nconsiderably improving network performance. Our SpikingEDN attains a mean\nintersection over union (MIoU) of 72.57\\% on the DDD17 dataset and 58.32\\% on\nthe larger DSEC-Semantic dataset, showing competitive results to the\nstate-of-the-art ANNs while requiring substantially fewer computational\nresources. Our results shed light on the untapped potential of SNNs in\nevent-based vision applications. The source code will be made publicly\navailable.\n","authors":["Rui Zhang","Luziwei Leng","Kaiwei Che","Hu Zhang","Jie Cheng","Qinghai Guo","Jiangxing Liao","Ran Cheng"],"pdf_url":"https://arxiv.org/pdf/2304.11857v3.pdf","comment":"Accepted for publication in IEEE Transactions on Neural Networks and\n  Learning Systems"},{"id":"http://arxiv.org/abs/2408.01343v1","updated":"2024-08-02T15:41:16Z","published":"2024-08-02T15:41:16Z","title":"StitchFusion: Weaving Any Visual Modalities to Enhance Multimodal\n  Semantic Segmentation","summary":"  Multimodal semantic segmentation shows significant potential for enhancing\nsegmentation accuracy in complex scenes. However, current methods often\nincorporate specialized feature fusion modules tailored to specific modalities,\nthereby restricting input flexibility and increasing the number of training\nparameters. To address these challenges, we propose StitchFusion, a\nstraightforward yet effective modal fusion framework that integrates\nlarge-scale pre-trained models directly as encoders and feature fusers. This\napproach facilitates comprehensive multi-modal and multi-scale feature fusion,\naccommodating any visual modal inputs. Specifically, Our framework achieves\nmodal integration during encoding by sharing multi-modal visual information. To\nenhance information exchange across modalities, we introduce a\nmulti-directional adapter module (MultiAdapter) to enable cross-modal\ninformation transfer during encoding. By leveraging MultiAdapter to propagate\nmulti-scale information across pre-trained encoders during the encoding\nprocess, StitchFusion achieves multi-modal visual information integration\nduring encoding. Extensive comparative experiments demonstrate that our model\nachieves state-of-the-art performance on four multi-modal segmentation datasets\nwith minimal additional parameters. Furthermore, the experimental integration\nof MultiAdapter with existing Feature Fusion Modules (FFMs) highlights their\ncomplementary nature. Our code is available at StitchFusion_repo.\n","authors":["Bingyu Li","Da Zhang","Zhiyuan Zhao","Junyu Gao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.01343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01334v1","updated":"2024-08-02T15:32:42Z","published":"2024-08-02T15:32:42Z","title":"A Backbone for Long-Horizon Robot Task Understanding","summary":"  End-to-end robot learning, particularly for long-horizon tasks, often results\nin unpredictable outcomes and poor generalization. To address these challenges,\nwe propose a novel Therblig-based Backbone Framework (TBBF) to enhance robot\ntask understanding and transferability. This framework uses therbligs (basic\naction elements) as the backbone to decompose high-level robot tasks into\nelemental robot configurations, which are then integrated with current\nfoundation models to improve task understanding. The approach consists of two\nstages: offline training and online testing. During the offline training stage,\nwe developed the Meta-RGate SynerFusion (MGSF) network for accurate therblig\nsegmentation across various tasks. In the online testing stage, after a\none-shot demonstration of a new task is collected, our MGSF network extracts\nhigh-level knowledge, which is then encoded into the image using Action\nRegistration (ActionREG). Additionally, the Large Language Model\n(LLM)-Alignment Policy for Visual Correction (LAP-VC) is employed to ensure\nprecise action execution, facilitating trajectory transfer in novel robot\nscenarios. Experimental results validate these methods, achieving 94.37% recall\nin therblig segmentation and success rates of 94.4% and 80% in real-world\nonline robot testing for simple and complex scenarios, respectively.\nSupplementary material is available at:\nhttps://sites.google.com/view/therbligsbasedbackbone/home\n","authors":["Xiaoshuai Chen","Wei Chen","Dongmyoung Lee","Yukun Ge","Nicolas Rojas","Petar Kormushev"],"pdf_url":"https://arxiv.org/pdf/2408.01334v1.pdf","comment":"8 pages, 8 figures. This work is intended to be submitted to IEEE\n  Robotics and Automation Letters (RA-L) for possible publication"},{"id":"http://arxiv.org/abs/2404.12501v2","updated":"2024-08-02T15:28:19Z","published":"2024-04-18T20:43:33Z","title":"SPIdepth: Strengthened Pose Information for Self-supervised Monocular\n  Depth Estimation","summary":"  Self-supervised monocular depth estimation has garnered considerable\nattention for its applications in autonomous driving and robotics. While recent\nmethods have made strides in leveraging techniques like the Self Query Layer\n(SQL) to infer depth from motion, they often overlook the potential of\nstrengthening pose information. In this paper, we introduce SPIdepth, a novel\napproach that prioritizes enhancing the pose network for improved depth\nestimation. Building upon the foundation laid by SQL, SPIdepth emphasizes the\nimportance of pose information in capturing fine-grained scene structures. By\nenhancing the pose network's capabilities, SPIdepth achieves remarkable\nadvancements in scene understanding and depth estimation. Experimental results\non benchmark datasets such as KITTI, Cityscapes, and Make3D showcase SPIdepth's\nstate-of-the-art performance, surpassing previous methods by significant\nmargins. Specifically, SPIdepth tops the self-supervised KITTI benchmark.\nAdditionally, SPIdepth achieves the lowest AbsRel (0.029), SqRel (0.069), and\nRMSE (1.394) on KITTI, establishing new state-of-the-art results. On\nCityscapes, SPIdepth shows improvements over SQLdepth of 21.7% in AbsRel, 36.8%\nin SqRel, and 16.5% in RMSE, even without using motion masks. On Make3D,\nSPIdepth in zero-shot outperforms all other models. Remarkably, SPIdepth\nachieves these results using only a single image for inference, surpassing even\nmethods that utilize video sequences for inference, thus demonstrating its\nefficacy and efficiency in real-world applications. Our approach represents a\nsignificant leap forward in self-supervised monocular depth estimation,\nunderscoring the importance of strengthening pose information for advancing\nscene understanding in real-world applications.\n","authors":["Mykola Lavreniuk"],"pdf_url":"https://arxiv.org/pdf/2404.12501v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01322v1","updated":"2024-08-02T15:20:34Z","published":"2024-08-02T15:20:34Z","title":"A Robotics-Inspired Scanpath Model Reveals the Importance of Uncertainty\n  and Semantic Object Cues for Gaze Guidance in Dynamic Scenes","summary":"  How we perceive objects around us depends on what we actively attend to, yet\nour eye movements depend on the perceived objects. Still, object segmentation\nand gaze behavior are typically treated as two independent processes. Drawing\non an information processing pattern from robotics, we present a mechanistic\nmodel that simulates these processes for dynamic real-world scenes. Our\nimage-computable model uses the current scene segmentation for object-based\nsaccadic decision-making while using the foveated object to refine its scene\nsegmentation recursively. To model this refinement, we use a Bayesian filter,\nwhich also provides an uncertainty estimate for the segmentation that we use to\nguide active scene exploration. We demonstrate that this model closely\nresembles observers' free viewing behavior, measured by scanpath statistics,\nincluding foveation duration and saccade amplitude distributions used for\nparameter fitting and higher-level statistics not used for fitting. These\ninclude how object detections, inspections, and returns are balanced and a\ndelay of returning saccades without an explicit implementation of such temporal\ninhibition of return. Extensive simulations and ablation studies show that\nuncertainty promotes balanced exploration and that semantic object cues are\ncrucial to form the perceptual units used in object-based attention. Moreover,\nwe show how our model's modular design allows for extensions, such as\nincorporating saccadic momentum or pre-saccadic attention, to further align its\noutput with human scanpaths.\n","authors":["Vito Mengers","Nicolas Roth","Oliver Brock","Klaus Obermayer","Martin Rolfs"],"pdf_url":"https://arxiv.org/pdf/2408.01322v1.pdf","comment":"35+16 pages, 8+4 figures"},{"id":"http://arxiv.org/abs/2406.07867v2","updated":"2024-08-02T15:05:47Z","published":"2024-06-12T04:48:36Z","title":"Let's Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation","summary":"  In this paper, we introduce a novel Face-to-Face spoken dialogue model. It\nprocesses audio-visual speech from user input and generates audio-visual speech\nas the response, marking the initial step towards creating an avatar chatbot\nsystem without relying on intermediate text. To this end, we newly introduce\nMultiDialog, the first large-scale multimodal (i.e., audio and visual) spoken\ndialogue corpus containing 340 hours of approximately 9,000 dialogues, recorded\nbased on the open domain dialogue dataset, TopicalChat. The MultiDialog\ncontains parallel audio-visual recordings of conversation partners acting\naccording to the given script with emotion annotations, which we expect to open\nup research opportunities in multimodal synthesis. Our Face-to-Face spoken\ndialogue model incorporates a textually pretrained large language model and\nadapts it into the audio-visual spoken dialogue domain by incorporating\nspeech-text joint pretraining. Through extensive experiments, we validate the\neffectiveness of our model in facilitating a face-to-face conversation. Demo\nand data are available at https://multidialog.github.io and\nhttps://huggingface.co/datasets/IVLLab/MultiDialog, respectively.\n","authors":["Se Jin Park","Chae Won Kim","Hyeongseop Rha","Minsu Kim","Joanna Hong","Jeong Hun Yeo","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2406.07867v2.pdf","comment":"Accepted to ACL 2024 (Oral)"},{"id":"http://arxiv.org/abs/2408.01311v1","updated":"2024-08-02T15:01:29Z","published":"2024-08-02T15:01:29Z","title":"TopoNAS: Boosting Search Efficiency of Gradient-based NAS via\n  Topological Simplification","summary":"  Improving search efficiency serves as one of the crucial objectives of Neural\nArchitecture Search (NAS). However, many current approaches ignore the\nuniversality of the search strategy and fail to reduce the computational\nredundancy during the search process, especially in one-shot NAS architectures.\nBesides, current NAS methods show invalid reparameterization in non-linear\nsearch space, leading to poor efficiency in common search spaces like DARTS. In\nthis paper, we propose TopoNAS, a model-agnostic approach for gradient-based\none-shot NAS that significantly reduces searching time and memory usage by\ntopological simplification of searchable paths. Firstly, we model the\nnon-linearity in search spaces to reveal the parameterization difficulties. To\nimprove the search efficiency, we present a topological simplification method\nand iteratively apply module-sharing strategies to simplify the topological\nstructure of searchable paths. In addition, a kernel normalization technique is\nalso proposed to preserve the search accuracy. Experimental results on the\nNASBench201 benchmark with various search spaces demonstrate the effectiveness\nof our method. It proves the proposed TopoNAS enhances the performance of\nvarious architectures in terms of search efficiency while maintaining a high\nlevel of accuracy. The project page is available at\nhttps://xdedss.github.io/topo_simplification.\n","authors":["Danpei Zhao","Zhuoran Liu","Bo Yuan"],"pdf_url":"https://arxiv.org/pdf/2408.01311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01293v1","updated":"2024-08-02T14:28:49Z","published":"2024-08-02T14:28:49Z","title":"Underwater Object Detection Enhancement via Channel Stabilization","summary":"  The complex marine environment exacerbates the challenges of object detection\nmanifold. Marine trash endangers the aquatic ecosystem, presenting a persistent\nchallenge. Accurate detection of marine deposits is crucial for mitigating this\nharm. Our work addresses underwater object detection by enhancing image quality\nand evaluating detection methods. We use Detectron2's backbone with various\nbase models and configurations for this task.\n  We propose a novel channel stabilization technique alongside a simplified\nimage enhancement model to reduce haze and color cast in training images,\nimproving multi-scale object detection. Following image processing, we test\ndifferent Detectron2 backbones for optimal detection accuracy. Additionally, we\napply a sharpening filter with augmentation techniques to highlight object\nprofiles for easier recognition.\n  Results are demonstrated on the TrashCan Dataset, both instance and material\nversions. The best-performing backbone method incorporates our channel\nstabilization and augmentation techniques. We also compare our Detectron2\ndetection results with the Deformable Transformer. In the instance version of\nTrashCan 1.0, our method achieves a 9.53% absolute increase in average\nprecision for small objects and a 7% absolute gain in bounding box detection\ncompared to the baseline. The code will be available on Code:\nhttps://github.com/aliman80/Underwater-\nObject-Detection-via-Channel-Stablization\n","authors":["Muhammad Ali","Salman Khan"],"pdf_url":"https://arxiv.org/pdf/2408.01293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01292v1","updated":"2024-08-02T14:28:10Z","published":"2024-08-02T14:28:10Z","title":"3DPX: Progressive 2D-to-3D Oral Image Reconstruction with Hybrid MLP-CNN\n  Networks","summary":"  Panoramic X-ray (PX) is a prevalent modality in dental practice for its wide\navailability and low cost. However, as a 2D projection image, PX does not\ncontain 3D anatomical information, and therefore has limited use in dental\napplications that can benefit from 3D information, e.g., tooth angular\nmisa-lignment detection and classification. Reconstructing 3D structures\ndirectly from 2D PX has recently been explored to address limitations with\nexisting methods primarily reliant on Convolutional Neural Networks (CNNs) for\ndirect 2D-to-3D mapping. These methods, however, are unable to correctly infer\ndepth-axis spatial information. In addition, they are limited by the in-trinsic\nlocality of convolution operations, as the convolution kernels only capture the\ninformation of immediate neighborhood pixels. In this study, we propose a\nprogressive hybrid Multilayer Perceptron (MLP)-CNN pyra-mid network (3DPX) for\n2D-to-3D oral PX reconstruction. We introduce a progressive reconstruction\nstrategy, where 3D images are progressively re-constructed in the 3DPX with\nguidance imposed on the intermediate recon-struction result at each pyramid\nlevel. Further, motivated by the recent ad-vancement of MLPs that show promise\nin capturing fine-grained long-range dependency, our 3DPX integrates MLPs and\nCNNs to improve the semantic understanding during reconstruction. Extensive\nexperiments on two large datasets involving 464 studies demonstrate that our\n3DPX outperforms state-of-the-art 2D-to-3D oral reconstruction methods,\nincluding standalone MLP and transformers, in reconstruction quality, and also\nim-proves the performance of downstream angular misalignment classification\ntasks.\n","authors":["Xiaoshuang Li","Mingyuan Meng","Zimo Huang","Lei Bi","Eduardo Delamare","Dagan Feng","Bin Sheng","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2408.01292v1.pdf","comment":"accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2408.01291v1","updated":"2024-08-02T14:24:40Z","published":"2024-08-02T14:24:40Z","title":"TexGen: Text-Guided 3D Texture Generation with Multi-view Sampling and\n  Resampling","summary":"  Given a 3D mesh, we aim to synthesize 3D textures that correspond to\narbitrary textual descriptions. Current methods for generating and assembling\ntextures from sampled views often result in prominent seams or excessive\nsmoothing. To tackle these issues, we present TexGen, a novel multi-view\nsampling and resampling framework for texture generation leveraging a\npre-trained text-to-image diffusion model. For view consistent sampling, first\nof all we maintain a texture map in RGB space that is parameterized by the\ndenoising step and updated after each sampling step of the diffusion model to\nprogressively reduce the view discrepancy. An attention-guided multi-view\nsampling strategy is exploited to broadcast the appearance information across\nviews. To preserve texture details, we develop a noise resampling technique\nthat aids in the estimation of noise, generating inputs for subsequent\ndenoising steps, as directed by the text prompt and current texture map.\nThrough an extensive amount of qualitative and quantitative evaluations, we\ndemonstrate that our proposed method produces significantly better texture\nquality for diverse 3D objects with a high degree of view consistency and rich\nappearance details, outperforming current state-of-the-art methods.\nFurthermore, our proposed texture generation technique can also be applied to\ntexture editing while preserving the original identity. More experimental\nresults are available at https://dong-huo.github.io/TexGen/\n","authors":["Dong Huo","Zixin Guo","Xinxin Zuo","Zhihao Shi","Juwei Lu","Peng Dai","Songcen Xu","Li Cheng","Yee-Hong Yang"],"pdf_url":"https://arxiv.org/pdf/2408.01291v1.pdf","comment":"European Conference on Computer Vision (ECCV) 2024"},{"id":"http://arxiv.org/abs/2408.01287v1","updated":"2024-08-02T14:19:34Z","published":"2024-08-02T14:19:34Z","title":"Deep Learning based Visually Rich Document Content Understanding: A\n  Survey","summary":"  Visually Rich Documents (VRDs) are essential in academia, finance, medical\nfields, and marketing due to their multimodal information content. Traditional\nmethods for extracting information from VRDs depend on expert knowledge and\nmanual labor, making them costly and inefficient. The advent of deep learning\nhas revolutionized this process, introducing models that leverage multimodal\ninformation vision, text, and layout along with pretraining tasks to develop\ncomprehensive document representations. These models have achieved\nstate-of-the-art performance across various downstream tasks, significantly\nenhancing the efficiency and accuracy of information extraction from VRDs. In\nresponse to the growing demands and rapid developments in Visually Rich\nDocument Understanding (VRDU), this paper provides a comprehensive review of\ndeep learning-based VRDU frameworks. We systematically survey and analyze\nexisting methods and benchmark datasets, categorizing them based on adopted\nstrategies and downstream tasks. Furthermore, we compare different techniques\nused in VRDU models, focusing on feature representation and fusion, model\narchitecture, and pretraining methods, while highlighting their strengths,\nlimitations, and appropriate scenarios. Finally, we identify emerging trends\nand challenges in VRDU, offering insights into future research directions and\npractical applications. This survey aims to provide a thorough understanding of\nVRDU advancements, benefiting both academic and industrial sectors.\n","authors":["Yihao Ding","Jean Lee","Soyeon Caren Han"],"pdf_url":"https://arxiv.org/pdf/2408.01287v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2408.01284v1","updated":"2024-08-02T14:10:20Z","published":"2024-08-02T14:10:20Z","title":"Out-Of-Distribution Detection for Audio-visual Generalized Zero-Shot\n  Learning: A General Framework","summary":"  Generalized Zero-Shot Learning (GZSL) is a challenging task requiring\naccurate classification of both seen and unseen classes. Within this domain,\nAudio-visual GZSL emerges as an extremely exciting yet difficult task, given\nthe inclusion of both visual and acoustic features as multi-modal inputs.\nExisting efforts in this field mostly utilize either embedding-based or\ngenerative-based methods. However, generative training is difficult and\nunstable, while embedding-based methods often encounter domain shift problem.\nThus, we find it promising to integrate both methods into a unified framework\nto leverage their advantages while mitigating their respective disadvantages.\nOur study introduces a general framework employing out-of-distribution (OOD)\ndetection, aiming to harness the strengths of both approaches. We first employ\ngenerative adversarial networks to synthesize unseen features, enabling the\ntraining of an OOD detector alongside classifiers for seen and unseen classes.\nThis detector determines whether a test feature belongs to seen or unseen\nclasses, followed by classification utilizing separate classifiers for each\nfeature type. We test our framework on three popular audio-visual datasets and\nobserve a significant improvement comparing to existing state-of-the-art works.\nCodes can be found in https://github.com/liuyuan-wen/AV-OOD-GZSL.\n","authors":["Liuyuan Wen"],"pdf_url":"https://arxiv.org/pdf/2408.01284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01276v1","updated":"2024-08-02T14:01:34Z","published":"2024-08-02T14:01:34Z","title":"Wave-Mamba: Wavelet State Space Model for Ultra-High-Definition\n  Low-Light Image Enhancement","summary":"  Ultra-high-definition (UHD) technology has attracted widespread attention due\nto its exceptional visual quality, but it also poses new challenges for\nlow-light image enhancement (LLIE) techniques. UHD images inherently possess\nhigh computational complexity, leading existing UHD LLIE methods to employ\nhigh-magnification downsampling to reduce computational costs, which in turn\nresults in information loss. The wavelet transform not only allows downsampling\nwithout loss of information, but also separates the image content from the\nnoise. It enables state space models (SSMs) to avoid being affected by noise\nwhen modeling long sequences, thus making full use of the long-sequence\nmodeling capability of SSMs. On this basis, we propose Wave-Mamba, a novel\napproach based on two pivotal insights derived from the wavelet domain: 1) most\nof the content information of an image exists in the low-frequency component,\nless in the high-frequency component. 2) The high-frequency component exerts a\nminimal influence on the outcomes of low-light enhancement. Specifically, to\nefficiently model global content information on UHD images, we proposed a\nlow-frequency state space block (LFSSBlock) by improving SSMs to focus on\nrestoring the information of low-frequency sub-bands. Moreover, we propose a\nhigh-frequency enhance block (HFEBlock) for high-frequency sub-band\ninformation, which uses the enhanced low-frequency information to correct the\nhigh-frequency information and effectively restore the correct high-frequency\ndetails. Through comprehensive evaluation, our method has demonstrated superior\nperformance, significantly outshining current leading techniques while\nmaintaining a more streamlined architecture. The code is available at\nhttps://github.com/AlexZou14/Wave-Mamba.\n","authors":["Wenbin Zou","Hongxia Gao","Weipeng Yang","Tongtong Liu"],"pdf_url":"https://arxiv.org/pdf/2408.01276v1.pdf","comment":"10 pages, 8 figures, ACMMM2024 accepted"},{"id":"http://arxiv.org/abs/2408.01269v1","updated":"2024-08-02T13:46:15Z","published":"2024-08-02T13:46:15Z","title":"A General Framework to Boost 3D GS Initialization for Text-to-3D\n  Generation by Lexical Richness","summary":"  Text-to-3D content creation has recently received much attention, especially\nwith the prevalence of 3D Gaussians Splatting. In general, GS-based methods\ncomprise two key stages: initialization and rendering optimization. To achieve\ninitialization, existing works directly apply random sphere initialization or\n3D diffusion models, e.g., Point-E, to derive the initial shapes. However, such\nstrategies suffer from two critical yet challenging problems: 1) the final\nshapes are still similar to the initial ones even after training; 2) shapes can\nbe produced only from simple texts, e.g., \"a dog\", not for lexically richer\ntexts, e.g., \"a dog is sitting on the top of the airplane\". To address these\nproblems, this paper proposes a novel general framework to boost the 3D GS\nInitialization for text-to-3D generation upon the lexical richness. Our key\nidea is to aggregate 3D Gaussians into spatially uniform voxels to represent\ncomplex shapes while enabling the spatial interaction among the 3D Gaussians\nand semantic interaction between Gaussians and texts. Specifically, we first\nconstruct a voxelized representation, where each voxel holds a 3D Gaussian with\nits position, scale, and rotation fixed while setting opacity as the sole\nfactor to determine a position's occupancy. We then design an initialization\nnetwork mainly consisting of two novel components: 1) Global Information\nPerception (GIP) block and 2) Gaussians-Text Fusion (GTF) block. Such a design\nenables each 3D Gaussian to assimilate the spatial information from other areas\nand semantic information from texts. Extensive experiments show the superiority\nof our framework of high-quality 3D GS initialization against the existing\nmethods, e.g., Shap-E, by taking lexically simple, medium, and hard texts.\nAlso, our framework can be seamlessly plugged into SoTA training frameworks,\ne.g., LucidDreamer, for semantically consistent text-to-3D generation.\n","authors":["Lutao Jiang","Hangyu Li","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2408.01269v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09004v2","updated":"2024-08-02T13:27:09Z","published":"2023-11-15T14:46:20Z","title":"Incremental Object-Based Novelty Detection with Feedback Loop","summary":"  Object-based Novelty Detection (ND) aims to identify unknown objects that do\nnot belong to classes seen during training by an object detection model. The\ntask is particularly crucial in real-world applications, as it allows to avoid\npotentially harmful behaviours, e.g. as in the case of object detection models\nadopted in a self-driving car or in an autonomous robot. Traditional approaches\nto ND focus on one time offline post processing of the pretrained object\ndetection output, leaving no possibility to improve the model robustness after\ntraining and discarding the abundant amount of out-of-distribution data\nencountered during deployment. In this work, we propose a novel framework for\nobject-based ND, assuming that human feedback can be requested on the predicted\noutput and later incorporated to refine the ND model without negatively\naffecting the main object detection performance. This refinement operation is\nrepeated whenever new feedback is available. To tackle this new formulation of\nthe problem for object detection, we propose a lightweight ND module attached\non top of a pre-trained object detection model, which is incrementally updated\nthrough a feedback loop. We also propose a new benchmark to evaluate methods on\nthis new setting and test extensively our ND approach against baselines,\nshowing increased robustness and a successful incorporation of the received\nfeedback.\n","authors":["Simone Caldarella","Elisa Ricci","Rahaf Aljundi"],"pdf_url":"https://arxiv.org/pdf/2311.09004v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09339v4","updated":"2024-08-02T13:13:47Z","published":"2022-07-19T15:49:35Z","title":"Vision Transformers: From Semantic Segmentation to Dense Prediction","summary":"  The emergence of vision transformers (ViTs) in image classification has\nshifted the methodologies for visual representation learning. In particular,\nViTs learn visual representation at full receptive field per layer across all\nthe image patches, in comparison to the increasing receptive fields of CNNs\nacross layers and other alternatives (e.g., large kernels and atrous\nconvolution). In this work, for the first time we explore the global context\nlearning potentials of ViTs for dense visual prediction (e.g., semantic\nsegmentation). Our motivation is that through learning global context at full\nreceptive field layer by layer, ViTs may capture stronger long-range dependency\ninformation, critical for dense prediction tasks. We first demonstrate that\nencoding an image as a sequence of patches, a vanilla ViT without local\nconvolution and resolution reduction can yield stronger visual representation\nfor semantic segmentation. For example, our model, termed as SEgmentation\nTRansformer (SETR), excels on ADE20K (50.28% mIoU, the first position in the\ntest leaderboard on the day of submission) and performs competitively on\nCityscapes. However, the basic ViT architecture falls short in broader dense\nprediction applications, such as object detection and instance segmentation,\ndue to its lack of a pyramidal structure, high computational demand, and\ninsufficient local context. For tackling general dense visual prediction tasks\nin a cost-effective manner, we further formulate a family of Hierarchical\nLocal-Global (HLG) Transformers, characterized by local attention within\nwindows and global-attention across windows in a pyramidal architecture.\nExtensive experiments show that our methods achieve appealing performance on a\nvariety of dense prediction tasks (e.g., object detection and instance\nsegmentation and semantic segmentation) as well as image classification.\n","authors":["Li Zhang","Jiachen Lu","Sixiao Zheng","Xinxuan Zhao","Xiatian Zhu","Yanwei Fu","Tao Xiang","Jianfeng Feng","Philip H. S. Torr"],"pdf_url":"https://arxiv.org/pdf/2207.09339v4.pdf","comment":"Extended version of CVPR 2021 paper arXiv:2012.15840 Published on\n  International Journal of Computer Vision (2024)"},{"id":"http://arxiv.org/abs/2408.00374v2","updated":"2024-08-02T13:00:46Z","published":"2024-08-01T08:32:03Z","title":"Conformal Trajectory Prediction with Multi-View Data Integration in\n  Cooperative Driving","summary":"  Current research on trajectory prediction primarily relies on data collected\nby onboard sensors of an ego vehicle. With the rapid advancement in connected\ntechnologies, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure\n(V2I) communication, valuable information from alternate views becomes\naccessible via wireless networks. The integration of information from\nalternative views has the potential to overcome the inherent limitations\nassociated with a single viewpoint, such as occlusions and limited field of\nview. In this work, we introduce V2INet, a novel trajectory prediction\nframework designed to model multi-view data by extending existing single-view\nmodels. Unlike previous approaches where the multi-view data is manually fused\nor formulated as a separate training stage, our model supports end-to-end\ntraining, enhancing both flexibility and performance. Moreover, the predicted\nmultimodal trajectories are calibrated by a post-hoc conformal prediction\nmodule to get valid and efficient confidence regions. We evaluated the entire\nframework using the real-world V2I dataset V2X-Seq. Our results demonstrate\nsuperior performance in terms of Final Displacement Error (FDE) and Miss Rate\n(MR) using a single GPU. The code is publicly available at:\n\\url{https://github.com/xichennn/V2I_trajectory_prediction}.\n","authors":["Xi Chen","Rahul Bhadani","Larry Head"],"pdf_url":"https://arxiv.org/pdf/2408.00374v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01233v1","updated":"2024-08-02T12:48:36Z","published":"2024-08-02T12:48:36Z","title":"CLIP4Sketch: Enhancing Sketch to Mugshot Matching through Dataset\n  Augmentation using Diffusion Models","summary":"  Forensic sketch-to-mugshot matching is a challenging task in face\nrecognition, primarily hindered by the scarcity of annotated forensic sketches\nand the modality gap between sketches and photographs. To address this, we\npropose CLIP4Sketch, a novel approach that leverages diffusion models to\ngenerate a large and diverse set of sketch images, which helps in enhancing the\nperformance of face recognition systems in sketch-to-mugshot matching. Our\nmethod utilizes Denoising Diffusion Probabilistic Models (DDPMs) to generate\nsketches with explicit control over identity and style. We combine CLIP and\nAdaface embeddings of a reference mugshot, along with textual descriptions of\nstyle, as the conditions to the diffusion model. We demonstrate the efficacy of\nour approach by generating a comprehensive dataset of sketches corresponding to\nmugshots and training a face recognition model on our synthetic data. Our\nresults show significant improvements in sketch-to-mugshot matching accuracy\nover training on an existing, limited amount of real face sketch data,\nvalidating the potential of diffusion models in enhancing the performance of\nface recognition systems across modalities. We also compare our dataset with\ndatasets generated using GAN-based methods to show its superiority.\n","authors":["Kushal Kumar Jain","Steve Grosz","Anoop M. Namboodiri","Anil K. Jain"],"pdf_url":"https://arxiv.org/pdf/2408.01233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01231v1","updated":"2024-08-02T12:44:07Z","published":"2024-08-02T12:44:07Z","title":"WaveMamba: Spatial-Spectral Wavelet Mamba for Hyperspectral Image\n  Classification","summary":"  Hyperspectral Imaging (HSI) has proven to be a powerful tool for capturing\ndetailed spectral and spatial information across diverse applications. Despite\nthe advancements in Deep Learning (DL) and Transformer architectures for HSI\nClassification (HSIC), challenges such as computational efficiency and the need\nfor extensive labeled data persist. This paper introduces WaveMamba, a novel\napproach that integrates wavelet transformation with the Spatial-Spectral Mamba\narchitecture to enhance HSIC. WaveMamba captures both local texture patterns\nand global contextual relationships in an end-to-end trainable model. The\nWavelet-based enhanced features are then processed through the state-space\narchitecture to model spatial-spectral relationships and temporal dependencies.\nThe experimental results indicate that WaveMamba surpasses existing models,\nachieving an accuracy improvement of 4.5\\% on the University of Houston dataset\nand a 2.0\\% increase on the Pavia University dataset. These findings validate\nits effectiveness in addressing the complex data interactions inherent in HSIs.\n","authors":["Muhammad Ahmad","Muhammad Usama","Manual Mazzara"],"pdf_url":"https://arxiv.org/pdf/2408.01231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01228v1","updated":"2024-08-02T12:36:13Z","published":"2024-08-02T12:36:13Z","title":"The Phantom Menace: Unmasking Privacy Leakages in Vision-Language Models","summary":"  Vision-Language Models (VLMs) combine visual and textual understanding,\nrendering them well-suited for diverse tasks like generating image captions and\nanswering visual questions across various domains. However, these capabilities\nare built upon training on large amount of uncurated data crawled from the web.\nThe latter may include sensitive information that VLMs could memorize and leak,\nraising significant privacy concerns. In this paper, we assess whether these\nvulnerabilities exist, focusing on identity leakage. Our study leads to three\nkey findings: (i) VLMs leak identity information, even when the vision-language\nalignment and the fine-tuning use anonymized data; (ii) context has little\ninfluence on identity leakage; (iii) simple, widely used anonymization\ntechniques, like blurring, are not sufficient to address the problem. These\nfindings underscore the urgent need for robust privacy protection strategies\nwhen deploying VLMs. Ethical awareness and responsible development practices\nare essential to mitigate these risks.\n","authors":["Simone Caldarella","Massimiliano Mancini","Elisa Ricci","Rahaf Aljundi"],"pdf_url":"https://arxiv.org/pdf/2408.01228v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01224v1","updated":"2024-08-02T12:27:15Z","published":"2024-08-02T12:27:15Z","title":"Multi-head Spatial-Spectral Mamba for Hyperspectral Image Classification","summary":"  Spatial-Spectral Mamba (SSM) improves computational efficiency and captures\nlong-range dependencies, addressing Transformer limitations. However,\ntraditional Mamba models overlook rich spectral information in HSIs and\nstruggle with high dimensionality and sequential data. To address these issues,\nwe propose the SSM with multi-head self-attention and token enhancement\n(MHSSMamba). This model integrates spectral and spatial information by\nenhancing spectral tokens and using multi-head attention to capture complex\nrelationships between spectral bands and spatial locations. It also manages\nlong-range dependencies and the sequential nature of HSI data, preserving\ncontextual information across spectral bands. MHSSMamba achieved remarkable\nclassification accuracies of 97.62\\% on Pavia University, 96.92\\% on the\nUniversity of Houston, 96.85\\% on Salinas, and 99.49\\% on Wuhan-longKou\ndatasets.\n","authors":["Muhammad Ahmad","Muhammad Hassaan Farooq Butt","Muhammad Usama","Hamad Ahmed Altuwaijri","Manual Mazzara","Salvatore Distenano"],"pdf_url":"https://arxiv.org/pdf/2408.01224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01218v1","updated":"2024-08-02T12:16:07Z","published":"2024-08-02T12:16:07Z","title":"S2TD-Face: Reconstruct a Detailed 3D Face with Controllable Texture from\n  a Single Sketch","summary":"  3D textured face reconstruction from sketches applicable in many scenarios\nsuch as animation, 3D avatars, artistic design, missing people search, etc., is\na highly promising but underdeveloped research topic. On the one hand, the\nstylistic diversity of sketches leads to existing sketch-to-3D-face methods\nonly being able to handle pose-limited and realistically shaded sketches. On\nthe other hand, texture plays a vital role in representing facial appearance,\nyet sketches lack this information, necessitating additional texture control in\nthe reconstruction process. This paper proposes a novel method for\nreconstructing controllable textured and detailed 3D faces from sketches, named\nS2TD-Face. S2TD-Face introduces a two-stage geometry reconstruction framework\nthat directly reconstructs detailed geometry from the input sketch. To keep\ngeometry consistent with the delicate strokes of the sketch, we propose a novel\nsketch-to-geometry loss that ensures the reconstruction accurately fits the\ninput features like dimples and wrinkles. Our training strategies do not rely\non hard-to-obtain 3D face scanning data or labor-intensive hand-drawn sketches.\nFurthermore, S2TD-Face introduces a texture control module utilizing text\nprompts to select the most suitable textures from a library and seamlessly\nintegrate them into the geometry, resulting in a 3D detailed face with\ncontrollable texture. S2TD-Face surpasses existing state-of-the-art methods in\nextensive quantitative and qualitative experiments. Our project is available at\nhttps://github.com/wang-zidu/S2TD-Face .\n","authors":["Zidu Wang","Xiangyu Zhu","Jiang Yu","Tianshuo Zhang","Zhen Lei"],"pdf_url":"https://arxiv.org/pdf/2408.01218v1.pdf","comment":"ACM MM 2024"},{"id":"http://arxiv.org/abs/2408.01191v1","updated":"2024-08-02T11:18:32Z","published":"2024-08-02T11:18:32Z","title":"A Weakly Supervised and Globally Explainable Learning Framework for\n  Brain Tumor Segmentation","summary":"  Machine-based brain tumor segmentation can help doctors make better\ndiagnoses. However, the complex structure of brain tumors and expensive\npixel-level annotations present challenges for automatic tumor segmentation. In\nthis paper, we propose a counterfactual generation framework that not only\nachieves exceptional brain tumor segmentation performance without the need for\npixel-level annotations, but also provides explainability. Our framework\neffectively separates class-related features from class-unrelated features of\nthe samples, and generate new samples that preserve identity features while\naltering class attributes by embedding different class-related features. We\nperform topological data analysis on the extracted class-related features and\nobtain a globally explainable manifold, and for each abnormal sample to be\nsegmented, a meaningful normal sample could be effectively generated with the\nguidance of the rule-based paths designed within the manifold for comparison\nfor identifying the tumor regions. We evaluate our proposed method on two\ndatasets, which demonstrates superior performance of brain tumor segmentation.\nThe code is available at https://github.com/xrt11/tumor-segmentation.\n","authors":["Ruitao Xie","Limai Jiang","Xiaoxi He","Yi Pan","Yunpeng Cai"],"pdf_url":"https://arxiv.org/pdf/2408.01191v1.pdf","comment":"2024 IEEE International Conference on Multimedia and Expo"},{"id":"http://arxiv.org/abs/2408.01181v1","updated":"2024-08-02T11:03:22Z","published":"2024-08-02T11:03:22Z","title":"VAR-CLIP: Text-to-Image Generator with Visual Auto-Regressive Modeling","summary":"  VAR is a new generation paradigm that employs 'next-scale prediction' as\nopposed to 'next-token prediction'. This innovative transformation enables\nauto-regressive (AR) transformers to rapidly learn visual distributions and\nachieve robust generalization. However, the original VAR model is constrained\nto class-conditioned synthesis, relying solely on textual captions for\nguidance. In this paper, we introduce VAR-CLIP, a novel text-to-image model\nthat integrates Visual Auto-Regressive techniques with the capabilities of\nCLIP. The VAR-CLIP framework encodes captions into text embeddings, which are\nthen utilized as textual conditions for image generation. To facilitate\ntraining on extensive datasets, such as ImageNet, we have constructed a\nsubstantial image-text dataset leveraging BLIP2. Furthermore, we delve into the\nsignificance of word positioning within CLIP for the purpose of caption\nguidance. Extensive experiments confirm VAR-CLIP's proficiency in generating\nfantasy images with high fidelity, textual congruence, and aesthetic\nexcellence. Our project page are https://github.com/daixiangzi/VAR-CLIP\n","authors":["Qian Zhang","Xiangzi Dai","Ninghua Yang","Xiang An","Ziyong Feng","Xingyu Ren"],"pdf_url":"https://arxiv.org/pdf/2408.01181v1.pdf","comment":"total 10 pages, code:https://github.com/daixiangzi/VAR-CLIP"},{"id":"http://arxiv.org/abs/2407.19546v2","updated":"2024-08-02T10:53:37Z","published":"2024-07-28T17:38:21Z","title":"XLIP: Cross-modal Attention Masked Modelling for Medical Language-Image\n  Pre-Training","summary":"  Vision-and-language pretraining (VLP) in the medical field utilizes\ncontrastive learning on image-text pairs to achieve effective transfer across\ntasks. Yet, current VLP approaches with the masked modelling strategy face two\nchallenges when applied to the medical domain. First, current models struggle\nto accurately reconstruct key pathological features due to the scarcity of\nmedical data. Second, most methods only adopt either paired image-text or\nimage-only data, failing to exploit the combination of both paired and unpaired\ndata. To this end, this paper proposes a XLIP (Masked modelling for medical\nLanguage-Image Pre-training) framework to enhance pathological learning and\nfeature learning via unpaired data. First, we introduce the attention-masked\nimage modelling (AttMIM) and entity-driven masked language modelling module\n(EntMLM), which learns to reconstruct pathological visual and textual tokens\nvia multi-modal feature interaction, thus improving medical-enhanced features.\nThe AttMIM module masks a portion of the image features that are highly\nresponsive to textual features. This allows XLIP to improve the reconstruction\nof highly similar image data in medicine efficiency. Second, our XLIP\ncapitalizes unpaired data to enhance multimodal learning by introducing\ndisease-kind prompts. The experimental results show that XLIP achieves SOTA for\nzero-shot and fine-tuning classification performance on five datasets. Our code\nwill be available at https://github.com/White65534/XLIP\n","authors":["Biao Wu","Yutong Xie","Zeyu Zhang","Minh Hieu Phan","Qi Chen","Ling Chen","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2407.19546v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01167v1","updated":"2024-08-02T10:34:23Z","published":"2024-08-02T10:34:23Z","title":"Rethinking Pre-trained Feature Extractor Selection in Multiple Instance\n  Learning for Whole Slide Image Classification","summary":"  Multiple instance learning (MIL) has become a preferred method for\nclassifying gigapixel whole slide images (WSIs), without requiring patch label\nannotation. The focus of the current MIL research stream is on the\nembedding-based MIL approach, which involves extracting feature vectors from\npatches using a pre-trained feature extractor. These feature vectors are then\nfed into an MIL aggregator for slide-level prediction. Despite prior research\nsuggestions on enhancing the most commonly used ResNet50 supervised model\npre-trained on ImageNet-1K, there remains a lack of clear guidance on selecting\nthe optimal feature extractor to maximize WSI performance. This study aims at\naddressing this gap by examining MIL feature extractors across three\ndimensions: pre-training dataset, backbone model, and pre-training method.\nExtensive experiments were carried out on the two public WSI datasets\n(TCGA-NSCLC and Camelyon16) using four SOTA MIL models. The main findings\nindicate the following: 1) Performance significantly improves with larger and\nmore varied pre-training datasets in both CNN and Transformer backbones. 2)\n`Modern and deeper' backbones greatly outperform `standard' backbones (ResNet\nand ViT), with performance improvements more guaranteed in Transformer-based\nbackbones. 3) The choice of self-supervised learning (SSL) method is crucial,\nwith the most significant benefits observed when applied to the Transformer\n(ViT) backbone. The study findings have practical implications, including\ndesigning more effective pathological foundation models. Our code is available\nat: https://anonymous.4open.science/r/MIL-Feature-Extractor-Selection\n","authors":["Bryan Wong","Mun Yong Yi"],"pdf_url":"https://arxiv.org/pdf/2408.01167v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2408.01162v1","updated":"2024-08-02T10:24:35Z","published":"2024-08-02T10:24:35Z","title":"PreMix: Boosting Multiple Instance Learning in Digital Histopathology\n  through Pre-training with Intra-Batch Slide Mixing","summary":"  The classification of gigapixel-sized whole slide images (WSIs), digital\nrepresentations of histological slides obtained via a high-resolution scanner,\nfaces significant challenges associated with the meticulous and time-consuming\nnature of fine-grained labeling. While weakly-supervised multiple instance\nlearning (MIL) has emerged as a promising approach, current MIL methods are\nconstrained by their limited ability to leverage the wealth of information\nembedded within unlabeled WSIs. This limitation often necessitates training MIL\nfeature aggregators from scratch after the feature extraction process,\nhindering efficiency and accuracy. PreMix extends the general MIL framework by\npre-training the MIL aggregator with an intra-batch slide mixing approach.\nSpecifically, PreMix incorporates Barlow Twins Slide Mixing during\npre-training, enhancing its ability to handle diverse WSI sizes and maximizing\nthe utility of unlabeled WSIs. Combined with Mixup and Manifold Mixup during\nfine-tuning, PreMix achieves a mean of 4.7% performance improvement over the\nbaseline MIL framework, the hierarchical image pyramid transformer (HIPT), on\nthe Camelyon16 dataset. The observed improvement across a range of active\nlearning acquisition functions and WSI-labeled training budgets highlights the\nframework's adaptability to diverse datasets and varying resource constraints.\nUltimately, PreMix paves the way for more efficient and accurate WSI\nclassification under limited WSI-labeled datasets, encouraging the broader\nadoption of unlabeled WSI data in histopathological research. The code is\navailable at https://anonymous.4open.science/r/PreMix\n","authors":["Bryan Wong","Mun Yong Yi"],"pdf_url":"https://arxiv.org/pdf/2408.01162v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2408.01159v1","updated":"2024-08-02T10:21:10Z","published":"2024-08-02T10:21:10Z","title":"Robust Curve Detection in Volumetric Medical Imaging via Attraction\n  Field","summary":"  Understanding body part geometry is crucial for precise medical diagnostics.\nCurves effectively describe anatomical structures and are widely used in\nmedical imaging applications related to cardiovascular, respiratory, and\nskeletal diseases. Traditional curve detection methods are often task-specific,\nrelying heavily on domain-specific features, limiting their broader\napplicability. This paper introduces a novel approach for detecting\nnon-branching curves, which does not require prior knowledge of the object's\norientation, shape, or position. Our method uses neural networks to predict (1)\nan attraction field, which offers subpixel accuracy, and (2) a closeness map,\nwhich limits the region of interest and essentially eliminates outliers far\nfrom the desired curve. We tested our curve detector on several clinically\nrelevant tasks with diverse morphologies and achieved impressive subpixel-level\naccuracy results that surpass existing methods, highlighting its versatility\nand robustness. Additionally, to support further advancements in this field, we\nprovide our private annotations of aortic centerlines and masks, which can\nserve as a benchmark for future research. The dataset can be found at\nhttps://github.com/neuro-ml/curve-detection.\n","authors":["Farukh Yaushev","Daria Nogina","Valentin Samokhin","Mariya Dugova","Ekaterina Petrash","Dmitry Sevryukov","Mikhail Belyaev","Maxim Pisov"],"pdf_url":"https://arxiv.org/pdf/2408.01159v1.pdf","comment":"Accepted to ShapeMI MICCAI 2024"},{"id":"http://arxiv.org/abs/2403.15559v2","updated":"2024-08-02T10:19:34Z","published":"2024-03-22T18:28:04Z","title":"An Optimization Framework to Enforce Multi-View Consistency for\n  Texturing 3D Meshes","summary":"  A fundamental problem in the texturing of 3D meshes using pre-trained\ntext-to-image models is to ensure multi-view consistency. State-of-the-art\napproaches typically use diffusion models to aggregate multi-view inputs, where\ncommon issues are the blurriness caused by the averaging operation in the\naggregation step or inconsistencies in local features. This paper introduces an\noptimization framework that proceeds in four stages to achieve multi-view\nconsistency. Specifically, the first stage generates an over-complete set of 2D\ntextures from a predefined set of viewpoints using an MV-consistent diffusion\nprocess. The second stage selects a subset of views that are mutually\nconsistent while covering the underlying 3D model. We show how to achieve this\ngoal by solving semi-definite programs. The third stage performs non-rigid\nalignment to align the selected views across overlapping regions. The fourth\nstage solves an MRF problem to associate each mesh face with a selected view.\nIn particular, the third and fourth stages are iterated, with the cuts obtained\nin the fourth stage encouraging non-rigid alignment in the third stage to focus\non regions close to the cuts. Experimental results show that our approach\nsignificantly outperforms baseline approaches both qualitatively and\nquantitatively. Project page: https://aigc3d.github.io/ConsistenTex.\n","authors":["Zhengyi Zhao","Chen Song","Xiaodong Gu","Yuan Dong","Qi Zuo","Weihao Yuan","Liefeng Bo","Zilong Dong","Qixing Huang"],"pdf_url":"https://arxiv.org/pdf/2403.15559v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19394v3","updated":"2024-08-02T10:05:03Z","published":"2024-07-28T04:23:40Z","title":"Depth-Wise Convolutions in Vision Transformers for Efficient Training on\n  Small Datasets","summary":"  The Vision Transformer (ViT) leverages the Transformer's encoder to capture\nglobal information by dividing images into patches and achieves superior\nperformance across various computer vision tasks. However, the self-attention\nmechanism of ViT captures the global context from the outset, overlooking the\ninherent relationships between neighboring pixels in images or videos.\nTransformers mainly focus on global information while ignoring the fine-grained\nlocal details. Consequently, ViT lacks inductive bias during image or video\ndataset training. In contrast, convolutional neural networks (CNNs), with their\nreliance on local filters, possess an inherent inductive bias, making them more\nefficient and quicker to converge than ViT with less data. In this paper, we\npresent a lightweight Depth-Wise Convolution module as a shortcut in ViT\nmodels, bypassing entire Transformer blocks to ensure the models capture both\nlocal and global information with minimal overhead. Additionally, we introduce\ntwo architecture variants, allowing the Depth-Wise Convolution modules to be\napplied to multiple Transformer blocks for parameter savings, and incorporating\nindependent parallel Depth-Wise Convolution modules with different kernels to\nenhance the acquisition of local information. The proposed approach\nsignificantly boosts the performance of ViT models on image classification,\nobject detection and instance segmentation by a large margin, especially on\nsmall datasets, as evaluated on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet\nfor image classification, and COCO for object detection and instance\nsegmentation. The source code can be accessed at\nhttps://github.com/ZTX-100/Efficient_ViT_with_DW.\n","authors":["Tianxiao Zhang","Wenju Xu","Bo Luo","Guanghui Wang"],"pdf_url":"https://arxiv.org/pdf/2407.19394v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06542v2","updated":"2024-08-02T10:04:09Z","published":"2024-01-12T12:35:45Z","title":"Robustness-Aware 3D Object Detection in Autonomous Driving: A Review and\n  Outlook","summary":"  In the realm of modern autonomous driving, the perception system is\nindispensable for accurately assessing the state of the surrounding\nenvironment, thereby enabling informed prediction and planning. The key step to\nthis system is related to 3D object detection that utilizes vehicle-mounted\nsensors such as LiDAR and cameras to identify the size, the category, and the\nlocation of nearby objects. Despite the surge in 3D object detection methods\naimed at enhancing detection precision and efficiency, there is a gap in the\nliterature that systematically examines their resilience against environmental\nvariations, noise, and weather changes. This study emphasizes the importance of\nrobustness, alongside accuracy and latency, in evaluating perception systems\nunder practical scenarios. Our work presents an extensive survey of\ncamera-only, LiDAR-only, and multi-modal 3D object detection algorithms,\nthoroughly evaluating their trade-off between accuracy, latency, and\nrobustness, particularly on datasets like KITTI-C and nuScenes-C to ensure fair\ncomparisons. Among these, multi-modal 3D detection approaches exhibit superior\nrobustness, and a novel taxonomy is introduced to reorganize the literature for\nenhanced clarity. This survey aims to offer a more practical perspective on the\ncurrent capabilities and the constraints of 3D object detection algorithms in\nreal-world applications, thus steering future research towards\nrobustness-centric advancements.\n","authors":["Ziying Song","Lin Liu","Feiyang Jia","Yadan Luo","Guoxin Zhang","Lei Yang","Li Wang","Caiyan Jia"],"pdf_url":"https://arxiv.org/pdf/2401.06542v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08372v3","updated":"2024-08-02T09:56:14Z","published":"2023-12-13T18:59:58Z","title":"SAM-guided Graph Cut for 3D Instance Segmentation","summary":"  This paper addresses the challenge of 3D instance segmentation by\nsimultaneously leveraging 3D geometric and multi-view image information. Many\nprevious works have applied deep learning techniques to 3D point clouds for\ninstance segmentation. However, these methods often failed to generalize to\nvarious types of scenes due to the scarcity and low-diversity of labeled 3D\npoint cloud data. Some recent works have attempted to lift 2D instance\nsegmentations to 3D within a bottom-up framework. The inconsistency in 2D\ninstance segmentations among views can substantially degrade the performance of\n3D segmentation. In this work, we introduce a novel 3D-to-2D query framework to\neffectively exploit 2D segmentation models for 3D instance segmentation.\nSpecifically, we pre-segment the scene into several superpoints in 3D,\nformulating the task into a graph cut problem. The superpoint graph is\nconstructed based on 2D segmentation models, where node features are obtained\nfrom multi-view image features and edge weights are computed based on\nmulti-view segmentation results, enabling the better generalization ability. To\nprocess the graph, we train a graph neural network using pseudo 3D labels from\n2D segmentation models. Experimental results on the ScanNet, ScanNet++ and\nKITTI-360 datasets demonstrate that our method achieves robust segmentation\nperformance and can generalize across different types of scenes. Our project\npage is available at https://zju3dv.github.io/sam_graph.\n","authors":["Haoyu Guo","He Zhu","Sida Peng","Yuang Wang","Yujun Shen","Ruizhen Hu","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2312.08372v3.pdf","comment":"Project page: https://zju3dv.github.io/sam_graph"},{"id":"http://arxiv.org/abs/2408.01139v1","updated":"2024-08-02T09:35:06Z","published":"2024-08-02T09:35:06Z","title":"Interpreting Global Perturbation Robustness of Image Models using\n  Axiomatic Spectral Importance Decomposition","summary":"  Perturbation robustness evaluates the vulnerabilities of models, arising from\na variety of perturbations, such as data corruptions and adversarial attacks.\nUnderstanding the mechanisms of perturbation robustness is critical for global\ninterpretability. We present a model-agnostic, global mechanistic\ninterpretability method to interpret the perturbation robustness of image\nmodels. This research is motivated by two key aspects. First, previous global\ninterpretability works, in tandem with robustness benchmarks, e.g. mean\ncorruption error (mCE), are not designed to directly interpret the mechanisms\nof perturbation robustness within image models. Second, we notice that the\nspectral signal-to-noise ratios (SNR) of perturbed natural images exponentially\ndecay over the frequency. This power-law-like decay implies that: Low-frequency\nsignals are generally more robust than high-frequency signals -- yet high\nclassification accuracy can not be achieved by low-frequency signals alone. By\napplying Shapley value theory, our method axiomatically quantifies the\npredictive powers of robust features and non-robust features within an\ninformation theory framework. Our method, dubbed as \\textbf{I-ASIDE}\n(\\textbf{I}mage \\textbf{A}xiomatic \\textbf{S}pectral \\textbf{I}mportance\n\\textbf{D}ecomposition \\textbf{E}xplanation), provides a unique insight into\nmodel robustness mechanisms. We conduct extensive experiments over a variety of\nvision models pre-trained on ImageNet to show that \\textbf{I-ASIDE} can not\nonly \\textbf{measure} the perturbation robustness but also \\textbf{provide\ninterpretations} of its mechanisms.\n","authors":["Róisín Luo","James McDermott","Colm O'Riordan"],"pdf_url":"https://arxiv.org/pdf/2408.01139v1.pdf","comment":"Accepted by Transactions on Machine Learning Research (TMLR 2024)"},{"id":"http://arxiv.org/abs/2408.01137v1","updated":"2024-08-02T09:31:21Z","published":"2024-08-02T09:31:21Z","title":"PGNeXt: High-Resolution Salient Object Detection via Pyramid Grafting\n  Network","summary":"  We present an advanced study on more challenging high-resolution salient\nobject detection (HRSOD) from both dataset and network framework perspectives.\nTo compensate for the lack of HRSOD dataset, we thoughtfully collect a\nlarge-scale high resolution salient object detection dataset, called UHRSD,\ncontaining 5,920 images from real-world complex scenarios at 4K-8K resolutions.\nAll the images are finely annotated in pixel-level, far exceeding previous\nlow-resolution SOD datasets. Aiming at overcoming the contradiction between the\nsampling depth and the receptive field size in the past methods, we propose a\nnovel one-stage framework for HR-SOD task using pyramid grafting mechanism. In\ngeneral, transformer-based and CNN-based backbones are adopted to extract\nfeatures from different resolution images independently and then these features\nare grafted from transformer branch to CNN branch. An attention-based\nCross-Model Grafting Module (CMGM) is proposed to enable CNN branch to combine\nbroken detailed information more holistically, guided by different source\nfeature during decoding process. Moreover, we design an Attention Guided Loss\n(AGL) to explicitly supervise the attention matrix generated by CMGM to help\nthe network better interact with the attention from different branches.\nComprehensive experiments on UHRSD and widely-used SOD datasets demonstrate\nthat our method can simultaneously locate salient object and preserve rich\ndetails, outperforming state-of-the-art methods. To verify the generalization\nability of the proposed framework, we apply it to the camouflaged object\ndetection (COD) task. Notably, our method performs superior to most\nstate-of-the-art COD methods without bells and whistles.\n","authors":["Changqun Xia","Chenxi Xie","Zhentao He","Tianshu Yu","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2408.01137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01126v1","updated":"2024-08-02T09:07:31Z","published":"2024-08-02T09:07:31Z","title":"IG-SLAM: Instant Gaussian SLAM","summary":"  3D Gaussian Splatting has recently shown promising results as an alternative\nscene representation in SLAM systems to neural implicit representations.\nHowever, current methods either lack dense depth maps to supervise the mapping\nprocess or detailed training designs that consider the scale of the\nenvironment. To address these drawbacks, we present IG-SLAM, a dense RGB-only\nSLAM system that employs robust Dense-SLAM methods for tracking and combines\nthem with Gaussian Splatting. A 3D map of the environment is constructed using\naccurate pose and dense depth provided by tracking. Additionally, we utilize\ndepth uncertainty in map optimization to improve 3D reconstruction. Our decay\nstrategy in map optimization enhances convergence and allows the system to run\nat 10 fps in a single process. We demonstrate competitive performance with\nstate-of-the-art RGB-only SLAM systems while achieving faster operation speeds.\nWe present our experiments on the Replica, TUM-RGBD, ScanNet, and EuRoC\ndatasets. The system achieves photo-realistic 3D reconstruction in large-scale\nsequences, particularly in the EuRoC dataset.\n","authors":["Furkan Aykut Sarikamis","Abdullah Aydin Alatan"],"pdf_url":"https://arxiv.org/pdf/2408.01126v1.pdf","comment":"8 pages, 3 page ref, 5 figures, 3DV submission"},{"id":"http://arxiv.org/abs/2408.01120v1","updated":"2024-08-02T09:01:05Z","published":"2024-08-02T09:01:05Z","title":"An Efficient and Effective Transformer Decoder-Based Framework for\n  Multi-Task Visual Grounding","summary":"  Most advanced visual grounding methods rely on Transformers for\nvisual-linguistic feature fusion. However, these Transformer-based approaches\nencounter a significant drawback: the computational costs escalate\nquadratically due to the self-attention mechanism in the Transformer Encoder,\nparticularly when dealing with high-resolution images or long context\nsentences. This quadratic increase in computational burden restricts the\napplicability of visual grounding to more intricate scenes, such as\nconversation-based reasoning segmentation, which involves lengthy language\nexpressions. In this paper, we propose an efficient and effective multi-task\nvisual grounding (EEVG) framework based on Transformer Decoder to address this\nissue, which reduces the cost in both language and visual aspects. In the\nlanguage aspect, we employ the Transformer Decoder to fuse visual and\nlinguistic features, where linguistic features are input as memory and visual\nfeatures as queries. This allows fusion to scale linearly with language\nexpression length. In the visual aspect, we introduce a parameter-free approach\nto reduce computation by eliminating background visual tokens based on\nattention scores. We then design a light mask head to directly predict\nsegmentation masks from the remaining sparse feature maps. Extensive results\nand ablation studies on benchmarks demonstrate the efficiency and effectiveness\nof our approach. Code is available in https://github.com/chenwei746/EEVG.\n","authors":["Wei Chen","Long Chen","Yu Wu"],"pdf_url":"https://arxiv.org/pdf/2408.01120v1.pdf","comment":"21pages, 10 figures, 9 tables. Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2408.01099v1","updated":"2024-08-02T08:24:05Z","published":"2024-08-02T08:24:05Z","title":"Contribution-based Low-Rank Adaptation with Pre-training Model for Real\n  Image Restoration","summary":"  Recently, pre-trained model and efficient parameter tuning have achieved\nremarkable success in natural language processing and high-level computer\nvision with the aid of masked modeling and prompt tuning. In low-level computer\nvision, however, there have been limited investigations on pre-trained models\nand even efficient fine-tuning strategy has not yet been explored despite its\nimportance and benefit in various real-world tasks such as alleviating memory\ninflation issue when integrating new tasks on AI edge devices. Here, we propose\na novel efficient parameter tuning approach dubbed contribution-based low-rank\nadaptation (CoLoRA) for multiple image restorations along with effective\npre-training method with random order degradations (PROD). Unlike prior arts\nthat tune all network parameters, our CoLoRA effectively fine-tunes small\namount of parameters by leveraging LoRA (low-rank adaptation) for each new\nvision task with our contribution-based method to adaptively determine layer by\nlayer capacity for that task to yield comparable performance to full tuning.\nFurthermore, our PROD strategy allows to extend the capability of pre-trained\nmodels with improved performance as well as robustness to bridge synthetic\npre-training and real-world fine-tuning. Our CoLoRA with PROD has demonstrated\nits superior performance in various image restoration tasks across diverse\ndegradation types on both synthetic and real-world datasets for known and novel\ntasks.\n","authors":["Donwon Park","Hayeon Kim","Se Young Chun"],"pdf_url":"https://arxiv.org/pdf/2408.01099v1.pdf","comment":"33 pages, 15 figures, for homepage see this url :\n  https://janeyeon.github.io/colora/"},{"id":"http://arxiv.org/abs/2405.04788v3","updated":"2024-08-02T08:22:12Z","published":"2024-05-08T03:43:58Z","title":"SemiCD-VL: Visual-Language Model Guidance Makes Better Semi-supervised\n  Change Detector","summary":"  Change Detection (CD) aims to identify pixels with semantic changes between\nimages. However, annotating massive numbers of pixel-level images is\nlabor-intensive and costly, especially for multi-temporal images, which require\npixel-wise comparisons by human experts. Considering the excellent performance\nof visual language models (VLMs) for zero-shot, open-vocabulary, etc. with\nprompt-based reasoning, it is promising to utilize VLMs to make better CD under\nlimited labeled data. In this paper, we propose a VLM guidance-based\nsemi-supervised CD method, namely SemiCD-VL. The insight of SemiCD-VL is to\nsynthesize free change labels using VLMs to provide additional supervision\nsignals for unlabeled data. However, almost all current VLMs are designed for\nsingle-temporal images and cannot be directly applied to bi- or multi-temporal\nimages. Motivated by this, we first propose a VLM-based mixed change event\ngeneration (CEG) strategy to yield pseudo labels for unlabeled CD data. Since\nthe additional supervised signals provided by these VLM-driven pseudo labels\nmay conflict with the pseudo labels from the consistency regularization\nparadigm (e.g. FixMatch), we propose the dual projection head for de-entangling\ndifferent signal sources. Further, we explicitly decouple the bi-temporal\nimages semantic representation through two auxiliary segmentation decoders,\nwhich are also guided by VLM. Finally, to make the model more adequately\ncapture change representations, we introduce metric-aware supervision by\nfeature-level contrastive loss in auxiliary branches. Extensive experiments\nshow the advantage of SemiCD-VL. For instance, SemiCD-VL improves the FixMatch\nbaseline by +5.3 IoU on WHU-CD and by +2.4 IoU on LEVIR-CD with 5% labels. In\naddition, our CEG strategy, in an un-supervised manner, can achieve performance\nfar superior to state-of-the-art un-supervised CD methods.\n","authors":["Kaiyu Li","Xiangyong Cao","Yupeng Deng","Junmin Liu","Deyu Meng","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2405.04788v3.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.01089v1","updated":"2024-08-02T08:08:56Z","published":"2024-08-02T08:08:56Z","title":"Prototypical Partial Optimal Transport for Universal Domain Adaptation","summary":"  Universal domain adaptation (UniDA) aims to transfer knowledge from a labeled\nsource domain to an unlabeled target domain without requiring the same label\nsets of both domains. The existence of domain and category shift makes the task\nchallenging and requires us to distinguish \"known\" samples (i.e., samples whose\nlabels exist in both domains) and \"unknown\" samples (i.e., samples whose labels\nexist in only one domain) in both domains before reducing the domain gap. In\nthis paper, we consider the problem from the point of view of distribution\nmatching which we only need to align two distributions partially. A novel\napproach, dubbed mini-batch Prototypical Partial Optimal Transport (m-PPOT), is\nproposed to conduct partial distribution alignment for UniDA. In training\nphase, besides minimizing m-PPOT, we also leverage the transport plan of m-PPOT\nto reweight source prototypes and target samples, and design reweighted entropy\nloss and reweighted cross-entropy loss to distinguish \"known\" and \"unknown\"\nsamples. Experiments on four benchmarks show that our method outperforms the\nprevious state-of-the-art UniDA methods.\n","authors":["Yucheng Yang","Xiang Gu","Jian Sun"],"pdf_url":"https://arxiv.org/pdf/2408.01089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01085v1","updated":"2024-08-02T08:06:12Z","published":"2024-08-02T08:06:12Z","title":"Effect of Fog Particle Size Distribution on 3D Object Detection Under\n  Adverse Weather Conditions","summary":"  LiDAR-based sensors employing optical spectrum signals play a vital role in\nproviding significant information about the target objects in autonomous\ndriving vehicle systems. However, the presence of fog in the atmosphere\nseverely degrades the overall system's performance. This manuscript analyzes\nthe role of fog particle size distributions in 3D object detection under\nadverse weather conditions. We utilise Mie theory and meteorological optical\nrange (MOR) to calculate the attenuation and backscattering coefficient values\nfor point cloud generation and analyze the overall system's accuracy in Car,\nCyclist, and Pedestrian case scenarios under easy, medium and hard detection\ndifficulties. Gamma and Junge (Power-Law) distributions are employed to\nmathematically model the fog particle size distribution under strong and\nmoderate advection fog environments. Subsequently, we modified the KITTI\ndataset based on the backscattering coefficient values and trained it on the\nPV-RCNN++ deep neural network model for Car, Cyclist, and Pedestrian cases\nunder different detection difficulties. The result analysis shows a significant\nvariation in the system's accuracy concerning the changes in target object\ndimensionality, the nature of the fog environment and increasing detection\ndifficulties, with the Car exhibiting the highest accuracy of around 99% and\nthe Pedestrian showing the lowest accuracy of around 73%.\n","authors":["Ajinkya Shinde","Gaurav Sharma","Manisha Pattanaik","Sri Niwas Singh"],"pdf_url":"https://arxiv.org/pdf/2408.01085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01080v1","updated":"2024-08-02T07:57:06Z","published":"2024-08-02T07:57:06Z","title":"FCDFusion: a Fast, Low Color Deviation Method for Fusing Visible and\n  Infrared Image Pairs","summary":"  Visible and infrared image fusion (VIF) aims to combine information from\nvisible and infrared images into a single fused image. Previous VIF methods\nusually employ a color space transformation to keep the hue and saturation from\nthe original visible image. However, for fast VIF methods, this operation\naccounts for the majority of the calculation and is the bottleneck preventing\nfaster processing. In this paper, we propose a fast fusion method, FCDFusion,\nwith little color deviation. It preserves color information without color space\ntransformations, by directly operating in RGB color space. It incorporates\ngamma correction at little extra cost, allowing color and contrast to be\nrapidly improved. We regard the fusion process as a scaling operation on 3D\ncolor vectors, greatly simplifying the calculations. A theoretical analysis and\nexperiments show that our method can achieve satisfactory results in only 7\nFLOPs per pixel. Compared to state-of-the-art fast, color-preserving methods\nusing HSV color space, our method provides higher contrast at only half of the\ncomputational cost. We further propose a new metric, color deviation, to\nmeasure the ability of a VIF method to preserve color. It is specifically\ndesigned for VIF tasks with color visible-light images, and overcomes\ndeficiencies of existing VIF metrics used for this purpose. Our code is\navailable at https://github.com/HeasonLee/FCDFusion.\n","authors":["Hesong Li","Ying Fu"],"pdf_url":"https://arxiv.org/pdf/2408.01080v1.pdf","comment":"This article has been accepted by Computational Visual Media"},{"id":"http://arxiv.org/abs/2408.01077v1","updated":"2024-08-02T07:52:28Z","published":"2024-08-02T07:52:28Z","title":"PhysMamba: Leveraging Dual-Stream Cross-Attention SSD for Remote\n  Physiological Measurement","summary":"  Remote Photoplethysmography (rPPG) is a non-contact technique for extracting\nphysiological signals from facial videos, used in applications like emotion\nmonitoring, medical assistance, and anti-face spoofing. Unlike controlled\nlaboratory settings, real-world environments often contain motion artifacts and\nnoise, affecting the performance of existing methods. To address this, we\npropose PhysMamba, a dual-stream time-frequency interactive model based on\nMamba. PhysMamba integrates the state-of-the-art Mamba-2 model and employs a\ndual-stream architecture to learn diverse rPPG features, enhancing robustness\nin noisy conditions. Additionally, we designed the Cross-Attention State Space\nDuality (CASSD) module to improve information exchange and feature\ncomplementarity between the two streams. We validated PhysMamba using PURE,\nUBFC-rPPG and MMPD. Experimental results show that PhysMamba achieves\nstate-of-the-art performance across various scenarios, particularly in complex\nenvironments, demonstrating its potential in practical remote heart rate\nmonitoring applications.\n","authors":["Zhixin Yan","Yan Zhong","Wenjun Zhang","Lin Shu","Hongbin Xu","Wenxiong Kang"],"pdf_url":"https://arxiv.org/pdf/2408.01077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01076v1","updated":"2024-08-02T07:51:44Z","published":"2024-08-02T07:51:44Z","title":"Exploiting the Semantic Knowledge of Pre-trained Text-Encoders for\n  Continual Learning","summary":"  Deep neural networks (DNNs) excel on fixed datasets but struggle with\nincremental and shifting data in real-world scenarios. Continual learning\naddresses this challenge by allowing models to learn from new data while\nretaining previously learned knowledge. Existing methods mainly rely on visual\nfeatures, often neglecting the rich semantic information encoded in text. The\nsemantic knowledge available in the label information of the images, offers\nimportant semantic information that can be related with previously acquired\nknowledge of semantic classes. Consequently, effectively leveraging this\ninformation throughout continual learning is expected to be beneficial. To\naddress this, we propose integrating semantic guidance within and across tasks\nby capturing semantic similarity using text embeddings. We start from a\npre-trained CLIP model, employ the \\emph{Semantically-guided Representation\nLearning (SG-RL)} module for a soft-assignment towards all current task\nclasses, and use the Semantically-guided Knowledge Distillation (SG-KD) module\nfor enhanced knowledge transfer. Experimental results demonstrate the\nsuperiority of our method on general and fine-grained datasets. Our code can be\nfound in\nhttps://github.com/aprilsveryown/semantically-guided-continual-learning.\n","authors":["Lu Yu","Zhe Tao","Hantao Yao","Joost Van de Weijer","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2408.01076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01067v1","updated":"2024-08-02T07:40:34Z","published":"2024-08-02T07:40:34Z","title":"Amodal Segmentation for Laparoscopic Surgery Video Instruments","summary":"  Segmentation of surgical instruments is crucial for enhancing surgeon\nperformance and ensuring patient safety. Conventional techniques such as\nbinary, semantic, and instance segmentation share a common drawback: they do\nnot accommodate the parts of instruments obscured by tissues or other\ninstruments. Precisely predicting the full extent of these occluded instruments\ncan significantly improve laparoscopic surgeries by providing critical guidance\nduring operations and assisting in the analysis of potential surgical errors,\nas well as serving educational purposes. In this paper, we introduce Amodal\nSegmentation to the realm of surgical instruments in the medical field. This\ntechnique identifies both the visible and occluded parts of an object. To\nachieve this, we introduce a new Amoal Instruments Segmentation (AIS) dataset,\nwhich was developed by reannotating each instrument with its complete mask,\nutilizing the 2017 MICCAI EndoVis Robotic Instrument Segmentation Challenge\ndataset. Additionally, we evaluate several leading amodal segmentation methods\nto establish a benchmark for this new dataset.\n","authors":["Ruohua Shi","Zhaochen Liu","Lingyu Duan","Tingting Jiang"],"pdf_url":"https://arxiv.org/pdf/2408.01067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02371v2","updated":"2024-08-02T07:14:59Z","published":"2024-07-02T15:40:29Z","title":"OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video\n  Generation","summary":"  Text-to-video (T2V) generation has recently garnered significant attention\nthanks to the large multi-modality model Sora. However, T2V generation still\nfaces two important challenges: 1) Lacking a precise open sourced high-quality\ndataset. The previous popular video datasets, e.g. WebVid-10M and Panda-70M,\nare either with low quality or too large for most research institutions.\nTherefore, it is challenging but crucial to collect a precise high-quality\ntext-video pairs for T2V generation. 2) Ignoring to fully utilize textual\ninformation. Recent T2V methods have focused on vision transformers, using a\nsimple cross attention module for video generation, which falls short of\nthoroughly extracting semantic information from text prompt. To address these\nissues, we introduce OpenVid-1M, a precise high-quality dataset with expressive\ncaptions. This open-scenario dataset contains over 1 million text-video pairs,\nfacilitating research on T2V generation. Furthermore, we curate 433K 1080p\nvideos from OpenVid-1M to create OpenVidHD-0.4M, advancing high-definition\nvideo generation. Additionally, we propose a novel Multi-modal Video Diffusion\nTransformer (MVDiT) capable of mining both structure information from visual\ntokens and semantic information from text tokens. Extensive experiments and\nablation studies verify the superiority of OpenVid-1M over previous datasets\nand the effectiveness of our MVDiT.\n","authors":["Kepan Nan","Rui Xie","Penghao Zhou","Tiehan Fan","Zhenheng Yang","Zhijie Chen","Xiang Li","Jian Yang","Ying Tai"],"pdf_url":"https://arxiv.org/pdf/2407.02371v2.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2407.19397v2","updated":"2024-08-02T06:53:08Z","published":"2024-07-28T04:46:55Z","title":"Domain Adaptive Lung Nodule Detection in X-ray Image","summary":"  Medical images from different healthcare centers exhibit varied data\ndistributions, posing significant challenges for adapting lung nodule detection\ndue to the domain shift between training and application phases. Traditional\nunsupervised domain adaptive detection methods often struggle with this shift,\nleading to suboptimal outcomes. To overcome these challenges, we introduce a\nnovel domain adaptive approach for lung nodule detection that leverages mean\nteacher self-training and contrastive learning. First, we propose a\nhierarchical contrastive learning strategy to refine nodule representations and\nenhance the distinction between nodules and background. Second, we introduce a\nnodule-level domain-invariant feature learning (NDL) module to capture\ndomain-invariant features through adversarial learning across different\ndomains. Additionally, we propose a new annotated dataset of X-ray images to\naid in advancing lung nodule detection research. Extensive experiments\nconducted on multiple X-ray datasets demonstrate the efficacy of our approach\nin mitigating domain shift impacts.\n","authors":["Haifeng Zhao","Lixiang Jiang","Leilei Ma","Dengdi Sun","Yanping Fu"],"pdf_url":"https://arxiv.org/pdf/2407.19397v2.pdf","comment":"This paper will submit to IEEE SMC 2024"},{"id":"http://arxiv.org/abs/2408.01044v1","updated":"2024-08-02T06:32:45Z","published":"2024-08-02T06:32:45Z","title":"Boosting Gaze Object Prediction via Pixel-level Supervision from Vision\n  Foundation Model","summary":"  Gaze object prediction (GOP) aims to predict the category and location of the\nobject that a human is looking at. Previous methods utilized box-level\nsupervision to identify the object that a person is looking at, but struggled\nwith semantic ambiguity, ie, a single box may contain several items since\nobjects are close together. The Vision foundation model (VFM) has improved in\nobject segmentation using box prompts, which can reduce confusion by more\nprecisely locating objects, offering advantages for fine-grained prediction of\ngaze objects. This paper presents a more challenging gaze object segmentation\n(GOS) task, which involves inferring the pixel-level mask corresponding to the\nobject captured by human gaze behavior. In particular, we propose that the\npixel-level supervision provided by VFM can be integrated into gaze object\nprediction to mitigate semantic ambiguity. This leads to our gaze object\ndetection and segmentation framework that enables accurate pixel-level\npredictions. Different from previous methods that require additional head input\nor ignore head features, we propose to automatically obtain head features from\nscene features to ensure the model's inference efficiency and flexibility in\nthe real world. Moreover, rather than directly fuse features to predict gaze\nheatmap as in existing methods, which may overlook spatial location and subtle\ndetails of the object, we develop a space-to-object gaze regression method to\nfacilitate human-object gaze interaction. Specifically, it first constructs an\ninitial human-object spatial connection, then refines this connection by\ninteracting with semantically clear features in the segmentation branch,\nultimately predicting a gaze heatmap for precise localization. Extensive\nexperiments on GOO-Synth and GOO-Real datasets demonstrate the effectiveness of\nour method.\n","authors":["Yang Jin","Lei Zhang","Shi Yan","Bin Fan","Binglu Wang"],"pdf_url":"https://arxiv.org/pdf/2408.01044v1.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2408.01040v1","updated":"2024-08-02T06:24:39Z","published":"2024-08-02T06:24:39Z","title":"Privacy-Preserving Split Learning with Vision Transformers using\n  Patch-Wise Random and Noisy CutMix","summary":"  In computer vision, the vision transformer (ViT) has increasingly superseded\nthe convolutional neural network (CNN) for improved accuracy and robustness.\nHowever, ViT's large model sizes and high sample complexity make it difficult\nto train on resource-constrained edge devices. Split learning (SL) emerges as a\nviable solution, leveraging server-side resources to train ViTs while utilizing\nprivate data from distributed devices. However, SL requires additional\ninformation exchange for weight updates between the device and the server,\nwhich can be exposed to various attacks on private training data. To mitigate\nthe risk of data breaches in classification tasks, inspired from the CutMix\nregularization, we propose a novel privacy-preserving SL framework that injects\nGaussian noise into smashed data and mixes randomly chosen patches of smashed\ndata across clients, coined DP-CutMixSL. Our analysis demonstrates that\nDP-CutMixSL is a differentially private (DP) mechanism that strengthens privacy\nprotection against membership inference attacks during forward propagation.\nThrough simulations, we show that DP-CutMixSL improves privacy protection\nagainst membership inference attacks, reconstruction attacks, and label\ninference attacks, while also improving accuracy compared to DP-SL and\nDP-MixSL.\n","authors":["Seungeun Oh","Sihun Baek","Jihong Park","Hyelin Nam","Praneeth Vepakomma","Ramesh Raskar","Mehdi Bennis","Seong-Lyun Kim"],"pdf_url":"https://arxiv.org/pdf/2408.01040v1.pdf","comment":"23 pages, 11 figures, 8 tables, to be published in Transactions on\n  Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2408.01037v1","updated":"2024-08-02T06:20:48Z","published":"2024-08-02T06:20:48Z","title":"MambaST: A Plug-and-Play Cross-Spectral Spatial-Temporal Fuser for\n  Efficient Pedestrian Detection","summary":"  This paper proposes MambaST, a plug-and-play cross-spectral spatial-temporal\nfusion pipeline for efficient pedestrian detection. Several challenges exist\nfor pedestrian detection in autonomous driving applications. First, it is\ndifficult to perform accurate detection using RGB cameras under dark or\nlow-light conditions. Cross-spectral systems must be developed to integrate\ncomplementary information from multiple sensor modalities, such as thermal and\nvisible cameras, to improve the robustness of the detections. Second,\npedestrian detection models are latency-sensitive. Efficient and easy-to-scale\ndetection models with fewer parameters are highly desirable for real-time\napplications such as autonomous driving. Third, pedestrian video data provides\nspatial-temporal correlations of pedestrian movement. It is beneficial to\nincorporate temporal as well as spatial information to enhance pedestrian\ndetection. This work leverages recent advances in the state space model (Mamba)\nand proposes a novel Multi-head Hierarchical Patching and Aggregation (MHHPA)\nstructure to extract both fine-grained and coarse-grained information from both\nRGB and thermal imagery. Experimental results show that the proposed MHHPA is\nan effective and efficient alternative to a Transformer model for\ncross-spectral pedestrian detection. Our proposed model also achieves superior\nperformance on small-scale pedestrian detection. The code is available at\nhttps://github.com/XiangboGaoBarry/MambaST}{https://github.com/XiangboGaoBarry/MambaST.\n","authors":["Xiangbo Gao","Asiegbu Miracle Kanu-Asiegbu","Xiaoxiao Du"],"pdf_url":"https://arxiv.org/pdf/2408.01037v1.pdf","comment":"ITSC 2024 Accepted"},{"id":"http://arxiv.org/abs/2408.01035v1","updated":"2024-08-02T06:18:39Z","published":"2024-08-02T06:18:39Z","title":"Structure from Motion-based Motion Estimation and 3D Reconstruction of\n  Unknown Shaped Space Debris","summary":"  With the boost in the number of spacecraft launches in the current decades,\nthe space debris problem is daily becoming significantly crucial. For\nsustainable space utilization, the continuous removal of space debris is the\nmost severe problem for humanity. To maximize the reliability of the debris\ncapture mission in orbit, accurate motion estimation of the target is\nessential. Space debris has lost its attitude and orbit control capabilities,\nand its shape is unknown due to the break. This paper proposes the Structure\nfrom Motion-based algorithm to perform unknown shaped space debris motion\nestimation with limited resources, where only 2D images are required as input.\nThe method then outputs the reconstructed shape of the unknown object and the\nrelative pose trajectory between the target and the camera simultaneously,\nwhich are exploited to estimate the target's motion. The method is\nquantitatively validated with the realistic image dataset generated by the\nmicrogravity experiment in a 2D air-floating testbed and 3D kinematic\nsimulation.\n","authors":["Kentaro Uno","Takehiro Matsuoka","Akiyoshi Uchida","Kazuya Yoshida"],"pdf_url":"https://arxiv.org/pdf/2408.01035v1.pdf","comment":"6 pages, 10 figures. Manuscript accepted at the 2024 IEEE 20th\n  International Conference on Automation Science and Engineerin (CASE 2024)"},{"id":"http://arxiv.org/abs/2408.01031v1","updated":"2024-08-02T06:13:29Z","published":"2024-08-02T06:13:29Z","title":"POA: Pre-training Once for Models of All Sizes","summary":"  Large-scale self-supervised pre-training has paved the way for one foundation\nmodel to handle many different vision tasks. Most pre-training methodologies\ntrain a single model of a certain size at one time. Nevertheless, various\ncomputation or storage constraints in real-world scenarios require substantial\nefforts to develop a series of models with different sizes to deploy. Thus, in\nthis study, we propose a novel tri-branch self-supervised training framework,\ntermed as POA (Pre-training Once for All), to tackle this aforementioned issue.\nOur approach introduces an innovative elastic student branch into a modern\nself-distillation paradigm. At each pre-training step, we randomly sample a\nsub-network from the original student to form the elastic student and train all\nbranches in a self-distilling fashion. Once pre-trained, POA allows the\nextraction of pre-trained models of diverse sizes for downstream tasks.\nRemarkably, the elastic student facilitates the simultaneous pre-training of\nmultiple models with different sizes, which also acts as an additional ensemble\nof models of various sizes to enhance representation learning. Extensive\nexperiments, including k-nearest neighbors, linear probing evaluation and\nassessments on multiple downstream tasks demonstrate the effectiveness and\nadvantages of our POA. It achieves state-of-the-art performance using ViT, Swin\nTransformer and ResNet backbones, producing around a hundred models with\ndifferent sizes through a single pre-training session. The code is available\nat: https://github.com/Qichuzyy/POA.\n","authors":["Yingying Zhang","Xin Guo","Jiangwei Lao","Lei Yu","Lixiang Ru","Jian Wang","Guo Ye","Huimei He","Jingdong Chen","Ming Yang"],"pdf_url":"https://arxiv.org/pdf/2408.01031v1.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2408.01026v1","updated":"2024-08-02T05:50:49Z","published":"2024-08-02T05:50:49Z","title":"PINNs for Medical Image Analysis: A Survey","summary":"  The incorporation of physical information in machine learning frameworks is\ntransforming medical image analysis (MIA). By integrating fundamental knowledge\nand governing physical laws, these models achieve enhanced robustness and\ninterpretability. In this work, we explore the utility of physics-informed\napproaches for MIA (PIMIA) tasks such as registration, generation,\nclassification, and reconstruction. We present a systematic literature review\nof over 80 papers on physics-informed methods dedicated to MIA. We propose a\nunified taxonomy to investigate what physics knowledge and processes are\nmodelled, how they are represented, and the strategies to incorporate them into\nMIA models. We delve deep into a wide range of image analysis tasks, from\nimaging, generation, prediction, inverse imaging (super-resolution and\nreconstruction), registration, and image analysis (segmentation and\nclassification). For each task, we thoroughly examine and present in a tabular\nformat the central physics-guided operation, the region of interest (with\nrespect to human anatomy), the corresponding imaging modality, the dataset used\nfor model training, the deep network architecture employed, and the primary\nphysical process, equation, or principle utilized. Additionally, we also\nintroduce a novel metric to compare the performance of PIMIA methods across\ndifferent tasks and datasets. Based on this review, we summarize and distil our\nperspectives on the challenges, open research questions, and directions for\nfuture research. We highlight key open challenges in PIMIA, including selecting\nsuitable physics priors and establishing a standardized benchmarking platform.\n","authors":["Chayan Banerjee","Kien Nguyen","Olivier Salvado","Truyen Tran","Clinton Fookes"],"pdf_url":"https://arxiv.org/pdf/2408.01026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03298v2","updated":"2024-08-02T05:26:14Z","published":"2024-06-05T14:08:13Z","title":"L-PR: Exploiting LiDAR Fiducial Marker for Unordered Low Overlap\n  Multiview Point Cloud Registration","summary":"  Point cloud registration is a prerequisite for many applications in computer\nvision and robotics. Most existing methods focus on pairwise registration of\ntwo point clouds with high overlap. Although there have been some methods for\nlow overlap cases, they struggle in degraded scenarios. This paper introduces a\nnovel framework dubbed L-PR, designed to register unordered low overlap\nmultiview point clouds leveraging LiDAR fiducial markers. We refer to them as\nLiDAR fiducial markers, but they are the same as the popular AprilTag and ArUco\nmarkers, thin sheets of paper that do not affect the 3D geometry of the\nenvironment. We first propose an improved adaptive threshold marker detection\nmethod to provide robust detection results when the viewpoints among point\nclouds change dramatically. Then, we formulate the unordered multiview point\ncloud registration problem as a maximum a-posteriori (MAP) problem and develop\na framework consisting of two levels of graphs to address it. The first-level\ngraph, constructed as a weighted graph, is designed to efficiently and\noptimally infer initial values of scan poses from the unordered set. The\nsecond-level graph is constructed as a factor graph. By globally optimizing the\nvariables on the graph, including scan poses, marker poses, and marker corner\npositions, we tackle the MAP problem. We conduct both qualitative and\nquantitative experiments to demonstrate that the proposed method surpasses\nprevious state-of-the-art (SOTA) methods and to showcase that L-PR can serve as\na low-cost and efficient tool for 3D asset collection and training data\ncollection. In particular, we collect a new dataset named Livox-3DMatch using\nL-PR and incorporate it into the training of the SOTA learning-based method,\nSGHR, which brings evident improvements for SGHR on various benchmarks.\n","authors":["Yibo Liu","Jinjun Shan","Amaldev Haridevan","Shuo Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.03298v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2408.01014v1","updated":"2024-08-02T05:17:14Z","published":"2024-08-02T05:17:14Z","title":"EIUP: A Training-Free Approach to Erase Non-Compliant Concepts\n  Conditioned on Implicit Unsafe Prompts","summary":"  Text-to-image diffusion models have shown the ability to learn a diverse\nrange of concepts. However, it is worth noting that they may also generate\nundesirable outputs, consequently giving rise to significant security concerns.\nSpecifically, issues such as Not Safe for Work (NSFW) content and potential\nviolations of style copyright may be encountered. Since image generation is\nconditioned on text, prompt purification serves as a straightforward solution\nfor content safety. Similar to the approach taken by LLM, some efforts have\nbeen made to control the generation of safe outputs by purifying prompts.\nHowever, it is also important to note that even with these efforts, non-toxic\ntext still carries a risk of generating non-compliant images, which is referred\nto as implicit unsafe prompts. Furthermore, some existing works fine-tune the\nmodels to erase undesired concepts from model weights. This type of method\nnecessitates multiple training iterations whenever the concept is updated,\nwhich can be time-consuming and may potentially lead to catastrophic\nforgetting. To address these challenges, we propose a simple yet effective\napproach that incorporates non-compliant concepts into an erasure prompt. This\nerasure prompt proactively participates in the fusion of image spatial features\nand text embeddings. Through attention mechanisms, our method is capable of\nidentifying feature representations of non-compliant concepts in the image\nspace. We re-weight these features to effectively suppress the generation of\nunsafe images conditioned on original implicit unsafe prompts. Our method\nexhibits superior erasure effectiveness while achieving high scores in image\nfidelity compared to the state-of-the-art baselines. WARNING: This paper\ncontains model outputs that may be offensive.\n","authors":["Die Chen","Zhiwen Li","Mingyuan Fan","Cen Chen","Wenmeng Zhou","Yaliang Li"],"pdf_url":"https://arxiv.org/pdf/2408.01014v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00228v3","updated":"2024-08-02T05:17:10Z","published":"2024-03-01T02:19:40Z","title":"DISORF: A Distributed Online 3D Reconstruction Framework for Mobile\n  Robots","summary":"  We present a framework, DISORF, to enable online 3D reconstruction and\nvisualization of scenes captured by resource-constrained mobile robots and edge\ndevices. To address the limited computing capabilities of edge devices and\npotentially limited network availability, we design a framework that\nefficiently distributes computation between the edge device and the remote\nserver. We leverage on-device SLAM systems to generate posed keyframes and\ntransmit them to remote servers that can perform high-quality 3D reconstruction\nand visualization at runtime by leveraging recent advances in neural 3D\nmethods. We identify a key challenge with online training where naive image\nsampling strategies can lead to significant degradation in rendering quality.\nWe propose a novel shifted exponential frame sampling method that addresses\nthis challenge for online training. We demonstrate the effectiveness of our\nframework in enabling high-quality real-time reconstruction and visualization\nof unknown scenes as they are captured and streamed from cameras in mobile\nrobots and edge devices.\n","authors":["Chunlin Li","Hanrui Fan","Xiaorui Huang","Ruofan Liang","Sankeerth Durvasula","Nandita Vijaykumar"],"pdf_url":"https://arxiv.org/pdf/2403.00228v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21450v2","updated":"2024-08-02T05:01:38Z","published":"2024-07-31T08:54:50Z","title":"Forecasting Future Videos from Novel Views via Disentangled 3D Scene\n  Representation","summary":"  Video extrapolation in space and time (VEST) enables viewers to forecast a 3D\nscene into the future and view it from novel viewpoints. Recent methods propose\nto learn an entangled representation, aiming to model layered scene geometry,\nmotion forecasting and novel view synthesis together, while assuming simplified\naffine motion and homography-based warping at each scene layer, leading to\ninaccurate video extrapolation. Instead of entangled scene representation and\nrendering, our approach chooses to disentangle scene geometry from scene\nmotion, via lifting the 2D scene to 3D point clouds, which enables high quality\nrendering of future videos from novel views. To model future 3D scene motion,\nwe propose a disentangled two-stage approach that initially forecasts\nego-motion and subsequently the residual motion of dynamic objects (e.g., cars,\npeople). This approach ensures more precise motion predictions by reducing\ninaccuracies from entanglement of ego-motion with dynamic object motion, where\nbetter ego-motion forecasting could significantly enhance the visual outcomes.\nExtensive experimental analysis on two urban scene datasets demonstrate\nsuperior performance of our proposed method in comparison to strong baselines.\n","authors":["Sudhir Yarram","Junsong Yuan"],"pdf_url":"https://arxiv.org/pdf/2407.21450v2.pdf","comment":"Accepted to ECCV 2024. Project Page:\n  https://skrya.github.io/projects/ffn-dsr/"},{"id":"http://arxiv.org/abs/2408.00998v1","updated":"2024-08-02T04:13:38Z","published":"2024-08-02T04:13:38Z","title":"FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features\n  for Highly Controllable Text-Driven Image Translation","summary":"  Large-scale text-to-image diffusion models have been a revolutionary\nmilestone in the evolution of generative AI and multimodal technology, allowing\nextraordinary image generation based on natural-language text prompts. However,\nthe issue of lacking controllability of such models restricts their practical\napplicability for real-life content creation, for which attention has been\nfocused on leveraging a reference image to control text-to-image synthesis. Due\nto the close correlation between the reference image and the generated image,\nthis problem can also be regarded as the task of manipulating (or editing) the\nreference image as per the text, namely text-driven image-to-image translation.\nThis paper contributes a novel, concise, and efficient approach that adapts the\npre-trained large-scale text-to-image (T2I) diffusion model to the\nimage-to-image (I2I) paradigm in a plug-and-play manner, realizing high-quality\nand versatile text-driven I2I translation without any model training, model\nfine-tuning, or online optimization process. To guide T2I generation with a\nreference image, we propose to model diverse guiding factors with\ncorrespondingly different frequency bands of diffusion features in the DCT\nspectral space, and accordingly devise a novel frequency band substitution\nlayer that dynamically substitutes a certain DCT frequency band of the\ndiffusion features with the corresponding counterpart of the reference image\nalong the reverse sampling process. We demonstrate that our method flexibly\nenables highly controllable text-driven I2I translation both in the guiding\nfactor and guiding intensity of the reference image, simply by tuning the type\nand bandwidth of the substituted frequency band, respectively. Extensive\nqualitative and quantitative experiments verify the superiority of our approach\nover related methods in I2I translation visual quality, versatility, and\ncontrollability.\n","authors":["Xiang Gao","Jiaying Liu"],"pdf_url":"https://arxiv.org/pdf/2408.00998v1.pdf","comment":"Accepted conference paper of ACM MM 2024"},{"id":"http://arxiv.org/abs/2407.12939v3","updated":"2024-08-02T03:33:17Z","published":"2024-07-17T18:10:40Z","title":"GenRC: Generative 3D Room Completion from Sparse Image Collections","summary":"  Sparse RGBD scene completion is a challenging task especially when\nconsidering consistent textures and geometries throughout the entire scene.\nDifferent from existing solutions that rely on human-designed text prompts or\npredefined camera trajectories, we propose GenRC, an automated training-free\npipeline to complete a room-scale 3D mesh with high-fidelity textures. To\nachieve this, we first project the sparse RGBD images to a highly incomplete 3D\nmesh. Instead of iteratively generating novel views to fill in the void, we\nutilized our proposed E-Diffusion to generate a view-consistent panoramic RGBD\nimage which ensures global geometry and appearance consistency. Furthermore, we\nmaintain the input-output scene stylistic consistency through textual inversion\nto replace human-designed text prompts. To bridge the domain gap among\ndatasets, E-Diffusion leverages models trained on large-scale datasets to\ngenerate diverse appearances. GenRC outperforms state-of-the-art methods under\nmost appearance and geometric metrics on ScanNet and ARKitScenes datasets, even\nthough GenRC is not trained on these datasets nor using predefined camera\ntrajectories. Project page: https://minfenli.github.io/GenRC\n","authors":["Ming-Feng Li","Yueh-Feng Ku","Hong-Xuan Yen","Chi Liu","Yu-Lun Liu","Albert Y. C. Chen","Cheng-Hao Kuo","Min Sun"],"pdf_url":"https://arxiv.org/pdf/2407.12939v3.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2406.08782v2","updated":"2024-08-02T01:56:17Z","published":"2024-06-13T03:27:01Z","title":"Hybrid Spatial-spectral Neural Network for Hyperspectral Image Denoising","summary":"  Hyperspectral image (HSI) denoising is an essential procedure for HSI\napplications. Unfortunately, the existing Transformer-based methods mainly\nfocus on non-local modeling, neglecting the importance of locality in image\ndenoising. Moreover, deep learning methods employ complex spectral learning\nmechanisms, thus introducing large computation costs.\n  To address these problems, we propose a hybrid spatial-spectral denoising\nnetwork (HSSD), in which we design a novel hybrid dual-path network inspired by\nCNN and Transformer characteristics, leading to capturing both local and\nnon-local spatial details while suppressing noise efficiently. Furthermore, to\nreduce computational complexity, we adopt a simple but effective decoupling\nstrategy that disentangles the learning of space and spectral channels, where\nmultilayer perception with few parameters is utilized to learn the global\ncorrelations among spectra. The synthetic and real experiments demonstrate that\nour proposed method outperforms state-of-the-art methods on spatial and\nspectral reconstruction. The code and details are available on\nhttps://github.com/HLImg/HSSD.\n","authors":["Hao Liang"," Chengjie","Kun Li","Xin Tian"],"pdf_url":"https://arxiv.org/pdf/2406.08782v2.pdf","comment":"There are some errors in professional theory"},{"id":"http://arxiv.org/abs/2407.11590v3","updated":"2024-08-02T01:36:59Z","published":"2024-07-16T10:50:10Z","title":"Rethinking Learned Image Compression: Context is All You Need","summary":"  Since LIC has made rapid progress recently compared to traditional methods,\nthis paper attempts to discuss the question about 'Where is the boundary of\nLearned Image Compression(LIC)?'. Thus this paper splits the above problem into\ntwo sub-problems:1)Where is the boundary of rate-distortion performance of\nPSNR? 2)How to further improve the compression gain and achieve the boundary?\nTherefore this paper analyzes the effectiveness of scaling parameters for\nencoder, decoder and context model, which are the three components of LIC. Then\nwe conclude that scaling for LIC is to scale for context model and decoder\nwithin LIC. Extensive experiments demonstrate that overfitting can actually\nserve as an effective context. By optimizing the context, this paper further\nimproves PSNR and achieves state-of-the-art performance, showing a performance\ngain of 14.39% with BD-RATE over VVC.\n","authors":["Jixiang Luo"],"pdf_url":"https://arxiv.org/pdf/2407.11590v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03452v2","updated":"2024-08-02T01:33:25Z","published":"2023-09-07T02:26:55Z","title":"Multimodal Guidance Network for Missing-Modality Inference in Content\n  Moderation","summary":"  Multimodal deep learning, especially vision-language models, have gained\nsignificant traction in recent years, greatly improving performance on many\ndownstream tasks, including content moderation and violence detection. However,\nstandard multimodal approaches often assume consistent modalities between\ntraining and inference, limiting applications in many real-world use cases, as\nsome modalities may not be available during inference. While existing research\nmitigates this problem through reconstructing the missing modalities, they\nunavoidably increase unnecessary computational cost, which could be just as\ncritical, especially for large, deployed infrastructures in industry. To this\nend, we propose a novel guidance network that promotes knowledge sharing during\ntraining, taking advantage of the multimodal representations to train better\nsingle-modality models to be used for inference. Real-world experiments in\nviolence detection shows that our proposed framework trains single-modality\nmodels that significantly outperform traditionally trained counterparts, while\navoiding increases in computational cost for inference.\n","authors":["Zhuokai Zhao","Harish Palani","Tianyi Liu","Lena Evans","Ruth Toner"],"pdf_url":"https://arxiv.org/pdf/2309.03452v2.pdf","comment":"ICME 2024 Camera Ready. Code is available at\n  https://github.com/zhuokaizhao/multimodal-guidance-network"},{"id":"http://arxiv.org/abs/2408.00969v1","updated":"2024-08-02T01:29:43Z","published":"2024-08-02T01:29:43Z","title":"Visible-Thermal Multiple Object Tracking: Large-scale Video Dataset and\n  Progressive Fusion Approach","summary":"  The complementary benefits from visible and thermal infrared data are widely\nutilized in various computer vision task, such as visual tracking, semantic\nsegmentation and object detection, but rarely explored in Multiple Object\nTracking (MOT). In this work, we contribute a large-scale Visible-Thermal video\nbenchmark for MOT, called VT-MOT. VT-MOT has the following main advantages. 1)\nThe data is large scale and high diversity. VT-MOT includes 582 video sequence\npairs, 401k frame pairs from surveillance, drone, and handheld platforms. 2)\nThe cross-modal alignment is highly accurate. We invite several professionals\nto perform both spatial and temporal alignment frame by frame. 3) The\nannotation is dense and high-quality. VT-MOT has 3.99 million annotation boxes\nannotated and double-checked by professionals, including heavy occlusion and\nobject re-acquisition (object disappear and reappear) challenges. To provide a\nstrong baseline, we design a simple yet effective tracking framework, which\neffectively fuses temporal information and complementary information of two\nmodalities in a progressive manner, for robust visible-thermal MOT. A\ncomprehensive experiment are conducted on VT-MOT and the results prove the\nsuperiority and effectiveness of the proposed method compared with\nstate-of-the-art methods. From the evaluation results and analysis, we specify\nseveral potential future directions for visible-thermal MOT. The project is\nreleased in https://github.com/wqw123wqw/PFTrack.\n","authors":["Yabin Zhu","Qianwu Wang","Chenglong Li","Jin Tang","Zhixiang Huang"],"pdf_url":"https://arxiv.org/pdf/2408.00969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00967v1","updated":"2024-08-02T01:24:01Z","published":"2024-08-02T01:24:01Z","title":"Extracting Object Heights From LiDAR & Aerial Imagery","summary":"  This work shows a procedural method for extracting object heights from LiDAR\nand aerial imagery. We discuss how to get heights and the future of LiDAR and\nimagery processing. SOTA object segmentation allows us to take get object\nheights with no deep learning background. Engineers will be keeping track of\nworld data across generations and reprocessing them. They will be using older\nprocedural methods like this paper and newer ones discussed here. SOTA methods\nare going beyond analysis and into generative AI. We cover both a procedural\nmethodology and the newer ones performed with language models. These include\npoint cloud, imagery and text encoding allowing for spatially aware AI.\n","authors":["Jesus Guerrero"],"pdf_url":"https://arxiv.org/pdf/2408.00967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.09786v5","updated":"2024-08-02T01:22:46Z","published":"2024-01-18T08:10:34Z","title":"Adaptive Self-training Framework for Fine-grained Scene Graph Generation","summary":"  Scene graph generation (SGG) models have suffered from inherent problems\nregarding the benchmark datasets such as the long-tailed predicate distribution\nand missing annotation problems. In this work, we aim to alleviate the\nlong-tailed problem of SGG by utilizing unannotated triplets. To this end, we\nintroduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels\nfor unannotated triplets based on which the SGG models are trained. While there\nhas been significant progress in self-training for image recognition, designing\na self-training framework for the SGG task is more challenging due to its\ninherent nature such as the semantic ambiguity and the long-tailed distribution\nof predicate classes. Hence, we propose a novel pseudo-labeling technique for\nSGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is\na model-agnostic framework that can be applied to any existing SGG models.\nFurthermore, we devise a graph structure learner (GSL) that is beneficial when\nadopting our proposed self-training framework to the state-of-the-art\nmessage-passing neural network (MPNN)-based SGG models. Our extensive\nexperiments verify the effectiveness of ST-SGG on various SGG models,\nparticularly in enhancing the performance on fine-grained predicate classes.\n","authors":["Kibum Kim","Kanghoon Yoon","Yeonjun In","Jinyoung Moon","Donghyun Kim","Chanyoung Park"],"pdf_url":"https://arxiv.org/pdf/2401.09786v5.pdf","comment":"9 pages; ICLR 2024"},{"id":"http://arxiv.org/abs/2311.13186v2","updated":"2024-08-02T00:56:39Z","published":"2023-11-22T06:26:24Z","title":"Applications of Spiking Neural Networks in Visual Place Recognition","summary":"  In robotics, Spiking Neural Networks (SNNs) are increasingly recognized for\ntheir largely-unrealized potential energy efficiency and low latency\nparticularly when implemented on neuromorphic hardware. Our paper highlights\nthree advancements for SNNs in Visual Place Recognition (VPR). Firstly, we\npropose Modular SNNs, where each SNN represents a set of non-overlapping\ngeographically distinct places, enabling scalable networks for large\nenvironments. Secondly, we present Ensembles of Modular SNNs, where multiple\nnetworks represent the same place, significantly enhancing accuracy compared to\nsingle-network models. Each of our Modular SNN modules is compact, comprising\nonly 1500 neurons and 474k synapses, making them ideally suited for ensembling\ndue to their small size. Lastly, we investigate the role of sequence matching\nin SNN-based VPR, a technique where consecutive images are used to refine place\nrecognition. We analyze the responsiveness of SNNs to ensembling and sequence\nmatching compared to other VPR techniques. Our contributions highlight the\nviability of SNNs for VPR, offering scalable and robust solutions, and paving\nthe way for their application in various energy-sensitive robotic tasks.\n","authors":["Somayeh Hussaini","Michael Milford","Tobias Fischer"],"pdf_url":"https://arxiv.org/pdf/2311.13186v2.pdf","comment":"20 pages, 10 figures, under review"},{"id":"http://arxiv.org/abs/2408.00963v1","updated":"2024-08-02T00:35:18Z","published":"2024-08-02T00:35:18Z","title":"MIS-ME: A Multi-modal Framework for Soil Moisture Estimation","summary":"  Soil moisture estimation is an important task to enable precision agriculture\nin creating optimal plans for irrigation, fertilization, and harvest. It is\ncommon to utilize statistical and machine learning models to estimate soil\nmoisture from traditional data sources such as weather forecasts, soil\nproperties, and crop properties. However, there is a growing interest in\nutilizing aerial and geospatial imagery to estimate soil moisture. Although\nthese images capture high-resolution crop details, they are expensive to curate\nand challenging to interpret. Imagine, an AI-enhanced software tool that\npredicts soil moisture using visual cues captured by smartphones and\nstatistical data given by weather forecasts. This work is a first step towards\nthat goal of developing a multi-modal approach for soil moisture estimation. In\nparticular, we curate a dataset consisting of real-world images taken from\nground stations and their corresponding weather data. We also propose MIS-ME -\nMeteorological & Image based Soil Moisture Estimator, a multi-modal framework\nfor soil moisture estimation. Our extensive analysis shows that MIS-ME achieves\na MAPE of 10.79%, outperforming traditional unimodal approaches with a\nreduction of 2.6% in MAPE for meteorological data and 1.5% in MAPE for image\ndata, highlighting the effectiveness of tailored multi-modal approaches.\n","authors":["Mohammed Rakib","Adil Aman Mohammed","Cole Diggins","Sumit Sharma","Jeff Michael Sadler","Tyson Ochsner","Arun Bagavathi"],"pdf_url":"https://arxiv.org/pdf/2408.00963v1.pdf","comment":"Accepted by DSAA2024"},{"id":"http://arxiv.org/abs/2404.07514v2","updated":"2024-08-02T00:06:55Z","published":"2024-04-11T07:11:43Z","title":"Generalization Gap in Data Augmentation: Insights from Illumination","summary":"  In the field of computer vision, data augmentation is widely used to enrich\nthe feature complexity of training datasets with deep learning techniques.\nHowever, regarding the generalization capabilities of models, the difference in\nartificial features generated by data augmentation and natural visual features\nhas not been fully revealed. This study introduces the concept of \"visual\nrepresentation variables\" to define the possible visual variations in a task as\na joint distribution of these variables. We focus on the visual representation\nvariable \"illumination\", by simulating its distribution degradation and\nexamining how data augmentation techniques enhance model performance on a\nclassification task. Our goal is to investigate the differences in\ngeneralization between models trained with augmented data and those trained\nunder real-world illumination conditions. Results indicate that after applying\nvarious data augmentation methods, model performance has significantly\nimproved. Yet, a noticeable generalization gap still exists after utilizing\nvarious data augmentation methods, emphasizing the critical role of feature\ndiversity in the training set for enhancing model generalization.\n","authors":["Jianqiang Xiao","Weiwen Guo","Junfeng Liu","Mengze Li"],"pdf_url":"https://arxiv.org/pdf/2404.07514v2.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2408.01423v1","updated":"2024-08-02T17:59:42Z","published":"2024-08-02T17:59:42Z","title":"Prompt Recursive Search: A Living Framework with Adaptive Growth in LLM\n  Auto-Prompting","summary":"  Large Language Models (LLMs) exhibit remarkable proficiency in addressing a\ndiverse array of tasks within the Natural Language Processing (NLP) domain,\nwith various prompt design strategies significantly augmenting their\ncapabilities. However, these prompts, while beneficial, each possess inherent\nlimitations. The primary prompt design methodologies are twofold: The first,\nexemplified by the Chain of Thought (CoT), involves manually crafting prompts\nspecific to individual datasets, hence termed Expert-Designed Prompts (EDPs).\nOnce these prompts are established, they are unalterable, and their\neffectiveness is capped by the expertise of the human designers. When applied\nto LLMs, the static nature of EDPs results in a uniform approach to both simple\nand complex problems within the same dataset, leading to the inefficient use of\ntokens for straightforward issues. The second method involves prompts\nautonomously generated by the LLM, known as LLM-Derived Prompts (LDPs), which\nprovide tailored solutions to specific problems, mitigating the limitations of\nEDPs. However, LDPs may encounter a decline in performance when tackling\ncomplex problems due to the potential for error accumulation during the\nsolution planning process. To address these challenges, we have conceived a\nnovel Prompt Recursive Search (PRS) framework that leverages the LLM to\ngenerate solutions specific to the problem, thereby conserving tokens. The\nframework incorporates an assessment of problem complexity and an adjustable\nstructure, ensuring a reduction in the likelihood of errors. We have\nsubstantiated the efficacy of PRS framework through extensive experiments using\nLLMs with different numbers of parameters across a spectrum of datasets in\nvarious domains. Compared to the CoT method, the PRS method has increased the\naccuracy on the BBH dataset by 8% using Llama3-7B model, achieving a 22%\nimprovement.\n","authors":["Xiangyu Zhao","Chengqian Ma"],"pdf_url":"https://arxiv.org/pdf/2408.01423v1.pdf","comment":"8 pages,4 figures"},{"id":"http://arxiv.org/abs/2408.01420v1","updated":"2024-08-02T17:55:50Z","published":"2024-08-02T17:55:50Z","title":"Mission Impossible: A Statistical Perspective on Jailbreaking LLMs","summary":"  Large language models (LLMs) are trained on a deluge of text data with\nlimited quality control. As a result, LLMs can exhibit unintended or even\nharmful behaviours, such as leaking information, fake news or hate speech.\nCountermeasures, commonly referred to as preference alignment, include\nfine-tuning the pretrained LLMs with carefully crafted text examples of desired\nbehaviour. Even then, empirical evidence shows preference aligned LLMs can be\nenticed to harmful behaviour. This so called jailbreaking of LLMs is typically\nachieved by adversarially modifying the input prompt to the LLM. Our paper\nprovides theoretical insights into the phenomenon of preference alignment and\njailbreaking from a statistical perspective. Under our framework, we first show\nthat pretrained LLMs will mimic harmful behaviour if present in the training\ncorpus. Under that same framework, we then introduce a statistical notion of\nalignment, and lower-bound the jailbreaking probability, showing that it is\nunpreventable under reasonable assumptions. Based on our insights, we propose\nan alteration to the currently prevalent alignment strategy RLHF. Specifically,\nwe introduce a simple modification to the RLHF objective, we call E-RLHF, that\naims to increase the likelihood of safe responses. E-RLHF brings no additional\ntraining cost, and is compatible with other methods. Empirically, we\ndemonstrate that E-RLHF outperforms RLHF on all alignment problems put forward\nby the AdvBench and HarmBench project without sacrificing model performance as\nmeasured by the MT-Bench project.\n","authors":["Jingtong Su","Julia Kempe","Karen Ullrich"],"pdf_url":"https://arxiv.org/pdf/2408.01420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00118v2","updated":"2024-08-02T17:52:12Z","published":"2024-07-31T19:13:07Z","title":"Gemma 2: Improving Open Language Models at a Practical Size","summary":"  In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.\n","authors":[" Gemma Team","Morgane Riviere","Shreya Pathak","Pier Giuseppe Sessa","Cassidy Hardin","Surya Bhupatiraju","Léonard Hussenot","Thomas Mesnard","Bobak Shahriari","Alexandre Ramé","Johan Ferret","Peter Liu","Pouya Tafti","Abe Friesen","Michelle Casbon","Sabela Ramos","Ravin Kumar","Charline Le Lan","Sammy Jerome","Anton Tsitsulin","Nino Vieillard","Piotr Stanczyk","Sertan Girgin","Nikola Momchev","Matt Hoffman","Shantanu Thakoor","Jean-Bastien Grill","Behnam Neyshabur","Olivier Bachem","Alanna Walton","Aliaksei Severyn","Alicia Parrish","Aliya Ahmad","Allen Hutchison","Alvin Abdagic","Amanda Carl","Amy Shen","Andy Brock","Andy Coenen","Anthony Laforge","Antonia Paterson","Ben Bastian","Bilal Piot","Bo Wu","Brandon Royal","Charlie Chen","Chintu Kumar","Chris Perry","Chris Welty","Christopher A. Choquette-Choo","Danila Sinopalnikov","David Weinberger","Dimple Vijaykumar","Dominika Rogozińska","Dustin Herbison","Elisa Bandy","Emma Wang","Eric Noland","Erica Moreira","Evan Senter","Evgenii Eltyshev","Francesco Visin","Gabriel Rasskin","Gary Wei","Glenn Cameron","Gus Martins","Hadi Hashemi","Hanna Klimczak-Plucińska","Harleen Batra","Harsh Dhand","Ivan Nardini","Jacinda Mein","Jack Zhou","James Svensson","Jeff Stanway","Jetha Chan","Jin Peng Zhou","Joana Carrasqueira","Joana Iljazi","Jocelyn Becker","Joe Fernandez","Joost van Amersfoort","Josh Gordon","Josh Lipschultz","Josh Newlan","Ju-yeong Ji","Kareem Mohamed","Kartikeya Badola","Kat Black","Katie Millican","Keelin McDonell","Kelvin Nguyen","Kiranbir Sodhia","Kish Greene","Lars Lowe Sjoesund","Lauren Usui","Laurent Sifre","Lena Heuermann","Leticia Lago","Lilly McNealus","Livio Baldini Soares","Logan Kilpatrick","Lucas Dixon","Luciano Martins","Machel Reid","Manvinder Singh","Mark Iverson","Martin Görner","Mat Velloso","Mateo Wirth","Matt Davidow","Matt Miller","Matthew Rahtz","Matthew Watson","Meg Risdal","Mehran Kazemi","Michael Moynihan","Ming Zhang","Minsuk Kahng","Minwoo Park","Mofi Rahman","Mohit Khatwani","Natalie Dao","Nenshad Bardoliwalla","Nesh Devanathan","Neta Dumai","Nilay Chauhan","Oscar Wahltinez","Pankil Botarda","Parker Barnes","Paul Barham","Paul Michel","Pengchong Jin","Petko Georgiev","Phil Culliton","Pradeep Kuppala","Ramona Comanescu","Ramona Merhej","Reena Jana","Reza Ardeshir Rokni","Rishabh Agarwal","Ryan Mullins","Samaneh Saadat","Sara Mc Carthy","Sarah Perrin","Sébastien M. R. Arnold","Sebastian Krause","Shengyang Dai","Shruti Garg","Shruti Sheth","Sue Ronstrom","Susan Chan","Timothy Jordan","Ting Yu","Tom Eccles","Tom Hennigan","Tomas Kocisky","Tulsee Doshi","Vihan Jain","Vikas Yadav","Vilobh Meshram","Vishal Dharmadhikari","Warren Barkley","Wei Wei","Wenming Ye","Woohyun Han","Woosuk Kwon","Xiang Xu","Zhe Shen","Zhitao Gong","Zichuan Wei","Victor Cotruta","Phoebe Kirk","Anand Rao","Minh Giang","Ludovic Peran","Tris Warkentin","Eli Collins","Joelle Barral","Zoubin Ghahramani","Raia Hadsell","D. Sculley","Jeanine Banks","Anca Dragan","Slav Petrov","Oriol Vinyals","Jeff Dean","Demis Hassabis","Koray Kavukcuoglu","Clement Farabet","Elena Buchatskaya","Sebastian Borgeaud","Noah Fiedel","Armand Joulin","Kathleen Kenealy","Robert Dadashi","Alek Andreev"],"pdf_url":"https://arxiv.org/pdf/2408.00118v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01417v1","updated":"2024-08-02T17:51:57Z","published":"2024-08-02T17:51:57Z","title":"Talk Less, Interact Better: Evaluating In-context Conversational\n  Adaptation in Multimodal LLMs","summary":"  Humans spontaneously use increasingly efficient language as interactions\nprogress, by adapting and forming ad-hoc conventions. This phenomenon has been\nstudied extensively using reference games, showing properties of human language\nthat go beyond relaying intents. It remains unexplored whether multimodal large\nlanguage models (MLLMs) similarly increase communication efficiency during\ninteractions, and what mechanisms they may adopt for this purpose. We introduce\nICCA, an automated framework to evaluate such conversational adaptation as an\nin-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and\nobserve that while they may understand the increasingly efficient language of\ntheir interlocutor, they do not spontaneously make their own language more\nefficient over time. This latter ability can only be elicited in some models\n(e.g., GPT-4) with heavy-handed prompting. This shows that this property of\nlinguistic interaction does not arise from current training regimes, even\nthough it is a common hallmark of human language. ICCA is available at\nhttps://github.com/lil-lab/ICCA.\n","authors":["Yilun Hua","Yoav Artzi"],"pdf_url":"https://arxiv.org/pdf/2408.01417v1.pdf","comment":"Accepted to COLM 2024"},{"id":"http://arxiv.org/abs/2408.01416v1","updated":"2024-08-02T17:51:42Z","published":"2024-08-02T17:51:42Z","title":"The Quest for the Right Mediator: A History, Survey, and Theoretical\n  Grounding of Causal Interpretability","summary":"  Interpretability provides a toolset for understanding how and why neural\nnetworks behave in certain ways. However, there is little unity in the field:\nmost studies employ ad-hoc evaluations and do not share theoretical\nfoundations, making it difficult to measure progress and compare the pros and\ncons of different techniques. Furthermore, while mechanistic understanding is\nfrequently discussed, the basic causal units underlying these mechanisms are\noften not explicitly defined. In this paper, we propose a perspective on\ninterpretability research grounded in causal mediation analysis. Specifically,\nwe describe the history and current state of interpretability taxonomized\naccording to the types of causal units (mediators) employed, as well as methods\nused to search over mediators. We discuss the pros and cons of each mediator,\nproviding insights as to when particular kinds of mediators and search methods\nare most appropriate depending on the goals of a given study. We argue that\nthis framing yields a more cohesive narrative of the field, as well as\nactionable insights for future work. Specifically, we recommend a focus on\ndiscovering new mediators with better trade-offs between human-interpretability\nand compute-efficiency, and which can uncover more sophisticated abstractions\nfrom neural networks than the primarily linear mediators employed in current\nwork. We also argue for more standardized evaluations that enable principled\ncomparisons across mediator types, such that we can better understand when\nparticular causal units are better suited to particular use cases.\n","authors":["Aaron Mueller","Jannik Brinkmann","Millicent Li","Samuel Marks","Koyena Pal","Nikhil Prakash","Can Rager","Aruna Sankaranarayanan","Arnab Sen Sharma","Jiuding Sun","Eric Todd","David Bau","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2408.01416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01415v1","updated":"2024-08-02T17:43:34Z","published":"2024-08-02T17:43:34Z","title":"Conditional LoRA Parameter Generation","summary":"  Generative models have achieved remarkable success in image, video, and text\ndomains. Inspired by this, researchers have explored utilizing generative\nmodels to generate neural network parameters. However, these efforts have been\nlimited by the parameter size and the practicality of generating\nhigh-performance parameters. In this paper, we propose COND P-DIFF, a novel\napproach that demonstrates the feasibility of controllable high-performance\nparameter generation, particularly for LoRA (Low-Rank Adaptation) weights,\nduring the fine-tuning process. Specifically, we employ an autoencoder to\nextract efficient latent representations for parameters. We then train a\nconditional latent diffusion model to synthesize high-performing model\nparameters from random noise based on specific task conditions. Experimental\nresults in both computer vision and natural language processing domains\nconsistently demonstrate that COND P-DIFF can generate high-performance\nparameters conditioned on the given task. Moreover, we observe that the\nparameter distribution generated by COND P-DIFF exhibits differences compared\nto the distribution obtained through normal optimization methods, indicating a\ncertain level of generalization capability. Our work paves the way for further\nexploration of condition-driven parameter generation, offering a promising\ndirection for task-specific adaptation of neural networks.\n","authors":["Xiaolong Jin","Kai Wang","Dongwen Tang","Wangbo Zhao","Yukun Zhou","Junshu Tang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2408.01415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17513v2","updated":"2024-08-02T17:39:32Z","published":"2024-04-26T16:28:34Z","title":"A Comprehensive Evaluation on Event Reasoning of Large Language Models","summary":"  Event reasoning is a fundamental ability that underlies many applications. It\nrequires event schema knowledge to perform global reasoning and needs to deal\nwith the diversity of the inter-event relations and the reasoning paradigms.\nHow well LLMs accomplish event reasoning on various relations and reasoning\nparadigms remains unknown. To mitigate this disparity, we comprehensively\nevaluate the abilities of event reasoning of LLMs. We introduce a novel\nbenchmark EV2 for EValuation of EVent reasoning. EV2 consists of two levels of\nevaluation of schema and instance and is comprehensive in relations and\nreasoning paradigms. We conduct extensive experiments on EV2. We find that LLMs\nhave abilities to accomplish event reasoning but their performances are far\nfrom satisfactory. We also notice the imbalance of event reasoning abilities in\nLLMs. Besides, LLMs have event schema knowledge, however, they're not aligned\nwith humans on how to utilize the knowledge. Based on these findings, we guide\nthe LLMs in utilizing the event schema knowledge as memory leading to\nimprovements on event reasoning.\n","authors":["Zhengwei Tao","Zhi Jin","Yifan Zhang","Xiancai Chen","Haiyan Zhao","Jia Li","Bing Liang","Chongyang Tao","Qun Liu","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2404.17513v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01402v1","updated":"2024-08-02T17:25:34Z","published":"2024-08-02T17:25:34Z","title":"Pre-trained Language Models Improve the Few-shot Prompt Ability of\n  Decision Transformer","summary":"  Decision Transformer (DT) has emerged as a promising class of algorithms in\noffline reinforcement learning (RL) tasks, leveraging pre-collected datasets\nand Transformer's capability to model long sequences. Recent works have\ndemonstrated that using parts of trajectories from training tasks as prompts in\nDT enhances its performance on unseen tasks, giving rise to Prompt-DT methods.\nHowever, collecting data from specific environments can be both costly and\nunsafe in many scenarios, leading to suboptimal performance and limited\nfew-shot prompt abilities due to the data-hungry nature of Transformer-based\nmodels. Additionally, the limited datasets used in pre-training make it\nchallenging for Prompt-DT type of methods to distinguish between various RL\ntasks through prompts alone. To address these challenges, we introduce the\nLanguage model-initialized Prompt Decision Transformer (LPDT), which leverages\npre-trained language models for meta-RL tasks and fine-tunes the model using\nLow-rank Adaptation (LoRA). We further incorporate prompt regularization to\neffectively differentiate between tasks based on prompt feature\nrepresentations. Our approach integrates pre-trained language model and RL\ntasks seamlessly. Extensive empirical studies demonstrate that initializing\nwith a pre-trained language model significantly enhances the performance of\nPrompt-DT on unseen tasks compared to baseline methods.\n","authors":["Yu Yang","Pan Xu"],"pdf_url":"https://arxiv.org/pdf/2408.01402v1.pdf","comment":"2 figures, 8 tables. Accepted by the Training Agents with Foundation\n  Models Workshop at RLC 2024"},{"id":"http://arxiv.org/abs/2407.19631v2","updated":"2024-08-02T17:10:43Z","published":"2024-07-29T01:22:04Z","title":"\"A Good Bot Always Knows Its Limitations\": Assessing Autonomous System\n  Decision-making Competencies through Factorized Machine Self-confidence","summary":"  How can intelligent machines assess their competencies in completing tasks?\nThis question has come into focus for autonomous systems that algorithmically\nreason and make decisions under uncertainty. It is argued here that machine\nself-confidence - a form of meta-reasoning based on self-assessments of an\nagent's knowledge about the state of the world and itself, as well as its\nability to reason about and execute tasks - leads to many eminently computable\nand useful competency indicators for such agents. This paper presents a\nculmination of work on this concept in the form of a computational framework\ncalled Factorized Machine Self-confidence (FaMSeC), which provides a holistic\nengineering-focused description of factors driving an algorithmic\ndecision-making process, including: outcome assessment, solver quality, model\nquality, alignment quality, and past experience. In FaMSeC, self confidence\nindicators are derived from hierarchical `problem-solving statistics' embedded\nwithin broad classes of probabilistic decision-making algorithms such as Markov\ndecision processes. The problem-solving statistics are obtained by evaluating\nand grading probabilistic exceedance margins with respect to given competency\nstandards, which are specified for each of the various decision-making\ncompetency factors by the informee (e.g. a non-expert user or an expert system\ndesigner). This approach allows `algorithmic goodness of fit' evaluations to be\neasily incorporated into the design of many kinds of autonomous agents in the\nform of human-interpretable competency self-assessment reports. Detailed\ndescriptions and application examples for a Markov decision process agent show\nhow two of the FaMSeC factors (outcome assessment and solver quality) can be\ncomputed and reported for a range of possible tasking contexts through novel\nuse of meta-utility functions, behavior simulations, and surrogate prediction\nmodels.\n","authors":["Brett Israelsen","Nisar R. Ahmed","Matthew Aitken","Eric W. Frew","Dale A. Lawrence","Brian M. Argrow"],"pdf_url":"https://arxiv.org/pdf/2407.19631v2.pdf","comment":"59 pages, 22 figures, draft to be submitted for journal review"},{"id":"http://arxiv.org/abs/2407.21485v2","updated":"2024-08-02T16:58:02Z","published":"2024-07-31T09:50:22Z","title":"Parallel Strategies for Best-First Generalized Planning","summary":"  In recent years, there has been renewed interest in closing the performance\ngap between state-of-the-art planning solvers and generalized planning (GP), a\nresearch area of AI that studies the automated synthesis of algorithmic-like\nsolutions capable of solving multiple classical planning instances. One of the\ncurrent advancements has been the introduction of Best-First Generalized\nPlanning (BFGP), a GP algorithm based on a novel solution space that can be\nexplored with heuristic search, one of the foundations of modern planners. This\npaper evaluates the application of parallel search techniques to BFGP, another\ncritical component in closing the performance gap. We first discuss why BFGP is\nwell suited for parallelization and some of its differentiating characteristics\nfrom classical planners. Then, we propose two simple shared-memory parallel\nstrategies with good scaling with the number of cores.\n","authors":["Alejandro Fernández-Alburquerque","Javier Segovia-Aguas"],"pdf_url":"https://arxiv.org/pdf/2407.21485v2.pdf","comment":"3 pages"},{"id":"http://arxiv.org/abs/2407.11652v3","updated":"2024-08-02T16:30:55Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v3.pdf","comment":"I found critical errors in the manuscript affecting its validity. I\n  need to correct these before resubmitting. Major changes to methodology and\n  results are underway, significantly altering the content. I will resubmit the\n  revised version"},{"id":"http://arxiv.org/abs/2406.04551v2","updated":"2024-08-02T16:09:49Z","published":"2024-06-06T23:35:51Z","title":"Improving Geo-diversity of Generated Images with Contextualized Vendi\n  Score Guidance","summary":"  With the growing popularity of text-to-image generative models, there has\nbeen increasing focus on understanding their risks and biases. Recent work has\nfound that state-of-the-art models struggle to depict everyday objects with the\ntrue diversity of the real world and have notable gaps between geographic\nregions. In this work, we aim to increase the diversity of generated images of\ncommon objects such that per-region variations are representative of the real\nworld. We introduce an inference time intervention, contextualized Vendi Score\nGuidance (c-VSG), that guides the backwards steps of latent diffusion models to\nincrease the diversity of a sample as compared to a \"memory bank\" of previously\ngenerated images while constraining the amount of variation within that of an\nexemplar set of real-world contextualizing images. We evaluate c-VSG with two\ngeographically representative datasets and find that it substantially increases\nthe diversity of generated images, both for the worst performing regions and on\naverage, while simultaneously maintaining or improving image quality and\nconsistency. Additionally, qualitative analyses reveal that diversity of\ngenerated images is significantly improved, including along the lines of\nreductive region portrayals present in the original model. We hope that this\nwork is a step towards text-to-image generative models that reflect the true\ngeographic diversity of the world.\n","authors":["Reyhane Askari Hemmat","Melissa Hall","Alicia Sun","Candace Ross","Michal Drozdzal","Adriana Romero-Soriano"],"pdf_url":"https://arxiv.org/pdf/2406.04551v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01349v1","updated":"2024-08-02T15:54:49Z","published":"2024-08-02T15:54:49Z","title":"PC$^2$: Pseudo-Classification Based Pseudo-Captioning for Noisy\n  Correspondence Learning in Cross-Modal Retrieval","summary":"  In the realm of cross-modal retrieval, seamlessly integrating diverse\nmodalities within multimedia remains a formidable challenge, especially given\nthe complexities introduced by noisy correspondence learning (NCL). Such noise\noften stems from mismatched data pairs, which is a significant obstacle\ndistinct from traditional noisy labels. This paper introduces\nPseudo-Classification based Pseudo-Captioning (PC$^2$) framework to address\nthis challenge. PC$^2$ offers a threefold strategy: firstly, it establishes an\nauxiliary \"pseudo-classification\" task that interprets captions as categorical\nlabels, steering the model to learn image-text semantic similarity through a\nnon-contrastive mechanism. Secondly, unlike prevailing margin-based techniques,\ncapitalizing on PC$^2$'s pseudo-classification capability, we generate\npseudo-captions to provide more informative and tangible supervision for each\nmismatched pair. Thirdly, the oscillation of pseudo-classification is borrowed\nto assistant the correction of correspondence. In addition to technical\ncontributions, we develop a realistic NCL dataset called Noise of Web (NoW),\nwhich could be a new powerful NCL benchmark where noise exists naturally.\nEmpirical evaluations of PC$^2$ showcase marked improvements over existing\nstate-of-the-art robust cross-modal retrieval techniques on both simulated and\nrealistic datasets with various NCL settings. The contributed dataset and\nsource code are released at https://github.com/alipay/PC2-NoiseofWeb.\n","authors":["Yue Duan","Zhangxuan Gu","Zhenzhe Ying","Lei Qi","Changhua Meng","Yinghuan Shi"],"pdf_url":"https://arxiv.org/pdf/2408.01349v1.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2403.13940v2","updated":"2024-08-02T15:54:21Z","published":"2024-03-20T19:25:11Z","title":"A multi-criteria approach for selecting an explanation from the set of\n  counterfactuals produced by an ensemble of explainers","summary":"  Counterfactuals are widely used to explain ML model predictions by providing\nalternative scenarios for obtaining the more desired predictions. They can be\ngenerated by a variety of methods that optimize different, sometimes\nconflicting, quality measures and produce quite different solutions. However,\nchoosing the most appropriate explanation method and one of the generated\ncounterfactuals is not an easy task. Instead of forcing the user to test many\ndifferent explanation methods and analysing conflicting solutions, in this\npaper, we propose to use a multi-stage ensemble approach that will select\nsingle counterfactual based on the multiple-criteria analysis. It offers a\ncompromise solution that scores well on several popular quality measures. This\napproach exploits the dominance relation and the ideal point decision aid\nmethod, which selects one counterfactual from the Pareto front. The conducted\nexperiments demonstrated that the proposed approach generates fully actionable\ncounterfactuals with attractive compromise values of the considered quality\nmeasures.\n","authors":["Ignacy Stępka","Mateusz Lango","Jerzy Stefanowski"],"pdf_url":"https://arxiv.org/pdf/2403.13940v2.pdf","comment":"17 pages, 2 figures"},{"id":"http://arxiv.org/abs/2408.01343v1","updated":"2024-08-02T15:41:16Z","published":"2024-08-02T15:41:16Z","title":"StitchFusion: Weaving Any Visual Modalities to Enhance Multimodal\n  Semantic Segmentation","summary":"  Multimodal semantic segmentation shows significant potential for enhancing\nsegmentation accuracy in complex scenes. However, current methods often\nincorporate specialized feature fusion modules tailored to specific modalities,\nthereby restricting input flexibility and increasing the number of training\nparameters. To address these challenges, we propose StitchFusion, a\nstraightforward yet effective modal fusion framework that integrates\nlarge-scale pre-trained models directly as encoders and feature fusers. This\napproach facilitates comprehensive multi-modal and multi-scale feature fusion,\naccommodating any visual modal inputs. Specifically, Our framework achieves\nmodal integration during encoding by sharing multi-modal visual information. To\nenhance information exchange across modalities, we introduce a\nmulti-directional adapter module (MultiAdapter) to enable cross-modal\ninformation transfer during encoding. By leveraging MultiAdapter to propagate\nmulti-scale information across pre-trained encoders during the encoding\nprocess, StitchFusion achieves multi-modal visual information integration\nduring encoding. Extensive comparative experiments demonstrate that our model\nachieves state-of-the-art performance on four multi-modal segmentation datasets\nwith minimal additional parameters. Furthermore, the experimental integration\nof MultiAdapter with existing Feature Fusion Modules (FFMs) highlights their\ncomplementary nature. Our code is available at StitchFusion_repo.\n","authors":["Bingyu Li","Da Zhang","Zhiyuan Zhao","Junyu Gao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.01343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01342v1","updated":"2024-08-02T15:38:55Z","published":"2024-08-02T15:38:55Z","title":"Leveraging Knowledge Graph Embedding for Effective Conversational\n  Recommendation","summary":"  Conversational recommender system (CRS), which combines the techniques of\ndialogue system and recommender system, has obtained increasing interest\nrecently. In contrast to traditional recommender system, it learns the user\npreference better through interactions (i.e. conversations), and then further\nboosts the recommendation performance. However, existing studies on CRS ignore\nto address the relationship among attributes, users, and items effectively,\nwhich might lead to inappropriate questions and inaccurate recommendations. In\nthis view, we propose a knowledge graph based conversational recommender system\n(referred as KG-CRS). Specifically, we first integrate the user-item graph and\nitem-attribute graph into a dynamic graph, i.e., dynamically changing during\nthe dialogue process by removing negative items or attributes. We then learn\ninformative embedding of users, items, and attributes by also considering\npropagation through neighbors on the graph. Extensive experiments on three real\ndatasets validate the superiority of our method over the state-of-the-art\napproaches in terms of both the recommendation and conversation tasks.\n","authors":["Yunwen Xia","Hui Fang","Jie Zhang","Chong Long"],"pdf_url":"https://arxiv.org/pdf/2408.01342v1.pdf","comment":"26pages, 15figures"},{"id":"http://arxiv.org/abs/2407.00110v2","updated":"2024-08-02T15:34:22Z","published":"2024-06-27T12:08:21Z","title":"Chat AI: A Seamless Slurm-Native Solution for HPC-Based Services","summary":"  The widespread adoption of large language models (LLMs) has created a\npressing need for an efficient, secure and private serving infrastructure,\nwhich allows researchers to run open source or custom fine-tuned LLMs and\nensures users that their data remains private and is not stored without their\nconsent. While high-performance computing (HPC) systems equipped with\nstate-of-the-art GPUs are well-suited for training LLMs, their batch scheduling\nparadigm is not designed to support real-time serving of AI applications. Cloud\nsystems, on the other hand, are well suited for web services but commonly lack\naccess to the computational power of HPC clusters, especially expensive and\nscarce high-end GPUs, which are required for optimal inference speed. We\npropose an architecture with an implementation consisting of a web service that\nruns on a cloud VM with secure access to a scalable backend running a multitude\nof LLM models on HPC systems. By offering a web service using our HPC\ninfrastructure to host LLMs, we leverage the trusted environment of local\nuniversities and research centers to offer a private and secure alternative to\ncommercial LLM services. Our solution natively integrates with the HPC batch\nscheduler Slurm, enabling seamless deployment on HPC clusters, and is able to\nrun side by side with regular Slurm workloads, while utilizing gaps in the\nschedule created by Slurm. In order to ensure the security of the HPC system,\nwe use the SSH ForceCommand directive to construct a robust circuit breaker,\nwhich prevents successful attacks on the web-facing server from affecting the\ncluster. We have successfully deployed our system as a production service, and\nmade the source code available at \\url{https://github.com/gwdg/chat-ai}\n","authors":["Ali Doosthosseini","Jonathan Decker","Hendrik Nolte","Julian M. Kunkel"],"pdf_url":"https://arxiv.org/pdf/2407.00110v2.pdf","comment":"Various improvements to explanations and form and updated graphs to\n  include data points up to 30.07.2024"},{"id":"http://arxiv.org/abs/2408.01334v1","updated":"2024-08-02T15:32:42Z","published":"2024-08-02T15:32:42Z","title":"A Backbone for Long-Horizon Robot Task Understanding","summary":"  End-to-end robot learning, particularly for long-horizon tasks, often results\nin unpredictable outcomes and poor generalization. To address these challenges,\nwe propose a novel Therblig-based Backbone Framework (TBBF) to enhance robot\ntask understanding and transferability. This framework uses therbligs (basic\naction elements) as the backbone to decompose high-level robot tasks into\nelemental robot configurations, which are then integrated with current\nfoundation models to improve task understanding. The approach consists of two\nstages: offline training and online testing. During the offline training stage,\nwe developed the Meta-RGate SynerFusion (MGSF) network for accurate therblig\nsegmentation across various tasks. In the online testing stage, after a\none-shot demonstration of a new task is collected, our MGSF network extracts\nhigh-level knowledge, which is then encoded into the image using Action\nRegistration (ActionREG). Additionally, the Large Language Model\n(LLM)-Alignment Policy for Visual Correction (LAP-VC) is employed to ensure\nprecise action execution, facilitating trajectory transfer in novel robot\nscenarios. Experimental results validate these methods, achieving 94.37% recall\nin therblig segmentation and success rates of 94.4% and 80% in real-world\nonline robot testing for simple and complex scenarios, respectively.\nSupplementary material is available at:\nhttps://sites.google.com/view/therbligsbasedbackbone/home\n","authors":["Xiaoshuai Chen","Wei Chen","Dongmyoung Lee","Yukun Ge","Nicolas Rojas","Petar Kormushev"],"pdf_url":"https://arxiv.org/pdf/2408.01334v1.pdf","comment":"8 pages, 8 figures. This work is intended to be submitted to IEEE\n  Robotics and Automation Letters (RA-L) for possible publication"},{"id":"http://arxiv.org/abs/2404.02476v3","updated":"2024-08-02T15:30:14Z","published":"2024-04-03T05:32:10Z","title":"Deep Reinforcement Learning for Traveling Purchaser Problems","summary":"  The traveling purchaser problem (TPP) is an important combinatorial\noptimization problem with broad applications. Due to the coupling between\nrouting and purchasing, existing works on TPPs commonly address route\nconstruction and purchase planning simultaneously, which, however, leads to\nexact methods with high computational cost and heuristics with sophisticated\ndesign but limited performance. In sharp contrast, we propose a novel approach\nbased on deep reinforcement learning (DRL), which addresses route construction\nand purchase planning separately, while evaluating and optimizing the solution\nfrom a global perspective. The key components of our approach include a\nbipartite graph representation for TPPs to capture the market-product\nrelations, and a policy network that extracts information from the bipartite\ngraph and uses it to sequentially construct the route. One significant benefit\nof our framework is that we can efficiently construct the route using the\npolicy network, and once the route is determined, the associated purchasing\nplan can be easily derived through linear programming, while, leveraging DRL,\nwe can train the policy network to optimize the global solution objective.\nFurthermore, by introducing a meta-learning strategy, the policy network can be\ntrained stably on large-sized TPP instances, and generalize well across\ninstances of varying sizes and distributions, even to much larger instances\nthat are never seen during training. Experiments on various synthetic TPP\ninstances and the TPPLIB benchmark demonstrate that our DRL-based approach can\nsignificantly outperform well-established TPP heuristics, reducing the\noptimality gap by 40%-90%, and also showing an advantage in runtime, especially\non large-sized instances.\n","authors":["Haofeng Yuan","Rongping Zhu","Wanlu Yang","Shiji Song","Keyou You","Yuli Zhang","C. L. Philip Chen"],"pdf_url":"https://arxiv.org/pdf/2404.02476v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01322v1","updated":"2024-08-02T15:20:34Z","published":"2024-08-02T15:20:34Z","title":"A Robotics-Inspired Scanpath Model Reveals the Importance of Uncertainty\n  and Semantic Object Cues for Gaze Guidance in Dynamic Scenes","summary":"  How we perceive objects around us depends on what we actively attend to, yet\nour eye movements depend on the perceived objects. Still, object segmentation\nand gaze behavior are typically treated as two independent processes. Drawing\non an information processing pattern from robotics, we present a mechanistic\nmodel that simulates these processes for dynamic real-world scenes. Our\nimage-computable model uses the current scene segmentation for object-based\nsaccadic decision-making while using the foveated object to refine its scene\nsegmentation recursively. To model this refinement, we use a Bayesian filter,\nwhich also provides an uncertainty estimate for the segmentation that we use to\nguide active scene exploration. We demonstrate that this model closely\nresembles observers' free viewing behavior, measured by scanpath statistics,\nincluding foveation duration and saccade amplitude distributions used for\nparameter fitting and higher-level statistics not used for fitting. These\ninclude how object detections, inspections, and returns are balanced and a\ndelay of returning saccades without an explicit implementation of such temporal\ninhibition of return. Extensive simulations and ablation studies show that\nuncertainty promotes balanced exploration and that semantic object cues are\ncrucial to form the perceptual units used in object-based attention. Moreover,\nwe show how our model's modular design allows for extensions, such as\nincorporating saccadic momentum or pre-saccadic attention, to further align its\noutput with human scanpaths.\n","authors":["Vito Mengers","Nicolas Roth","Oliver Brock","Klaus Obermayer","Martin Rolfs"],"pdf_url":"https://arxiv.org/pdf/2408.01322v1.pdf","comment":"35+16 pages, 8+4 figures"},{"id":"http://arxiv.org/abs/2408.01319v1","updated":"2024-08-02T15:14:53Z","published":"2024-08-02T15:14:53Z","title":"A Comprehensive Review of Multimodal Large Language Models: Performance\n  and Challenges Across Different Tasks","summary":"  In an era defined by the explosive growth of data and rapid technological\nadvancements, Multimodal Large Language Models (MLLMs) stand at the forefront\nof artificial intelligence (AI) systems. Designed to seamlessly integrate\ndiverse data types-including text, images, videos, audio, and physiological\nsequences-MLLMs address the complexities of real-world applications far beyond\nthe capabilities of single-modality systems. In this paper, we systematically\nsort out the applications of MLLM in multimodal tasks such as natural language,\nvision, and audio. We also provide a comparative analysis of the focus of\ndifferent MLLMs in the tasks, and provide insights into the shortcomings of\ncurrent MLLMs, and suggest potential directions for future research. Through\nthese discussions, this paper hopes to provide valuable insights for the\nfurther development and application of MLLM.\n","authors":["Jiaqi Wang","Hanqi Jiang","Yiheng Liu","Chong Ma","Xu Zhang","Yi Pan","Mengyuan Liu","Peiran Gu","Sichen Xia","Wenjun Li","Yutong Zhang","Zihao Wu","Zhengliang Liu","Tianyang Zhong","Bao Ge","Tuo Zhang","Ning Qiang","Xintao Hu","Xi Jiang","Xin Zhang","Wei Zhang","Dinggang Shen","Tianming Liu","Shu Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.01319v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01316v1","updated":"2024-08-02T15:12:01Z","published":"2024-08-02T15:12:01Z","title":"Synergistic pathways of modulation enable robust task packing within\n  neural dynamics","summary":"  Understanding how brain networks learn and manage multiple tasks\nsimultaneously is of interest in both neuroscience and artificial intelligence.\nIn this regard, a recent research thread in theoretical neuroscience has\nfocused on how recurrent neural network models and their internal dynamics\nenact multi-task learning. To manage different tasks requires a mechanism to\nconvey information about task identity or context into the model, which from a\nbiological perspective may involve mechanisms of neuromodulation. In this\nstudy, we use recurrent network models to probe the distinctions between two\nforms of contextual modulation of neural dynamics, at the level of neuronal\nexcitability and at the level of synaptic strength. We characterize these\nmechanisms in terms of their functional outcomes, focusing on their robustness\nto context ambiguity and, relatedly, their efficiency with respect to packing\nmultiple tasks into finite size networks. We also demonstrate distinction\nbetween these mechanisms at the level of the neuronal dynamics they induce.\nTogether, these characterizations indicate complementarity and synergy in how\nthese mechanisms act, potentially over multiple time-scales, toward enhancing\nrobustness of multi-task learning.\n","authors":["Giacomo Vedovati","ShiNung Ching"],"pdf_url":"https://arxiv.org/pdf/2408.01316v1.pdf","comment":"24 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.07867v2","updated":"2024-08-02T15:05:47Z","published":"2024-06-12T04:48:36Z","title":"Let's Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation","summary":"  In this paper, we introduce a novel Face-to-Face spoken dialogue model. It\nprocesses audio-visual speech from user input and generates audio-visual speech\nas the response, marking the initial step towards creating an avatar chatbot\nsystem without relying on intermediate text. To this end, we newly introduce\nMultiDialog, the first large-scale multimodal (i.e., audio and visual) spoken\ndialogue corpus containing 340 hours of approximately 9,000 dialogues, recorded\nbased on the open domain dialogue dataset, TopicalChat. The MultiDialog\ncontains parallel audio-visual recordings of conversation partners acting\naccording to the given script with emotion annotations, which we expect to open\nup research opportunities in multimodal synthesis. Our Face-to-Face spoken\ndialogue model incorporates a textually pretrained large language model and\nadapts it into the audio-visual spoken dialogue domain by incorporating\nspeech-text joint pretraining. Through extensive experiments, we validate the\neffectiveness of our model in facilitating a face-to-face conversation. Demo\nand data are available at https://multidialog.github.io and\nhttps://huggingface.co/datasets/IVLLab/MultiDialog, respectively.\n","authors":["Se Jin Park","Chae Won Kim","Hyeongseop Rha","Minsu Kim","Joanna Hong","Jeong Hun Yeo","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2406.07867v2.pdf","comment":"Accepted to ACL 2024 (Oral)"},{"id":"http://arxiv.org/abs/2405.13586v2","updated":"2024-08-02T14:59:48Z","published":"2024-05-22T12:30:25Z","title":"Bond Graphs for multi-physics informed Neural Networks for multi-variate\n  time series","summary":"  In the trend of hybrid Artificial Intelligence techniques, Physical-Informed\nMachine Learning has seen a growing interest. It operates mainly by imposing\ndata, learning, or architecture bias with simulation data, Partial Differential\nEquations, or equivariance and invariance properties. While it has shown great\nsuccess on tasks involving one physical domain, such as fluid dynamics,\nexisting methods are not adapted to tasks with complex multi-physical and\nmulti-domain phenomena. In addition, it is mainly formulated as an end-to-end\nlearning scheme. To address these challenges, we propose to leverage Bond\nGraphs, a multi-physics modeling approach, together with Message Passing Graph\nNeural Networks. We propose a Neural Bond graph Encoder (NBgE) producing\nmulti-physics-informed representations that can be fed into any task-specific\nmodel. It provides a unified way to integrate both data and architecture biases\nin deep learning. Our experiments on two challenging multi-domain physical\nsystems - a Direct Current Motor and the Respiratory System - demonstrate the\neffectiveness of our approach on a multivariate time-series forecasting task.\n","authors":["Alexis-Raja Brachet","Pierre-Yves Richard","Céline Hudelot"],"pdf_url":"https://arxiv.org/pdf/2405.13586v2.pdf","comment":"9 pages, 3 figures, paper under review"},{"id":"http://arxiv.org/abs/2407.21043v2","updated":"2024-08-02T14:58:54Z","published":"2024-07-22T04:07:12Z","title":"CP-Prompt: Composition-Based Cross-modal Prompting for\n  Domain-Incremental Continual Learning","summary":"  The key challenge of cross-modal domain-incremental learning (DIL) is to\nenable the learning model to continuously learn from novel data with different\nfeature distributions under the same task without forgetting old ones. However,\nexisting top-performing methods still cause high forgetting rates, by lacking\nintra-domain knowledge extraction and inter-domain common prompting strategy.\nIn this paper, we propose a simple yet effective framework, CP-Prompt, by\ntraining limited parameters to instruct a pre-trained model to learn new\ndomains and avoid forgetting existing feature distributions. CP-Prompt captures\nintra-domain knowledge by compositionally inserting personalized prompts on\nmulti-head self-attention layers and then learns the inter-domain knowledge\nwith a common prompting strategy. CP-Prompt shows superiority compared with\nstate-of-the-art baselines among three widely evaluated DIL tasks. The source\ncode is available at https://github.com/dannis97500/CP_Prompt.\n","authors":["Yu Feng","Zhen Tian","Yifan Zhu","Zongfu Han","Haoran Luo","Guangwei Zhang","Meina Song"],"pdf_url":"https://arxiv.org/pdf/2407.21043v2.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2401.13979v2","updated":"2024-08-02T14:50:05Z","published":"2024-01-25T06:45:32Z","title":"Routoo: Learning to Route to Large Language Models Effectively","summary":"  Developing foundational large language models (LLMs) is becoming increasingly\ncostly and inefficient. Also, closed-source and larger open-source models\ngenerally offer better response quality but come with higher inference costs\nthan smaller models. In this paper, we introduce Routoo, an architecture\ndesigned to optimize the selection of LLMs for specific prompts based on\nperformance, cost, and efficiency. Routoo consists of two key components: a\nperformance predictor and a cost-aware decoding. The performance predictor is a\nlightweight LLM that estimates the performance of various underlying LLMs\nwithout needing to execute and evaluate them. The cost-aware decoding then\nselects the most suitable model based on these predictions and other\nconstraints like cost and latency. We evaluated Routoo using the MMLU benchmark\nacross 57 domains employing open-source models. Our results show that Routoo\nmatches the performance of the Mixtral 8x7b model while reducing inference\ncosts by one-third. Additionally, by allowing increased costs, Routoo surpasses\nMixtral's accuracy by over 5% at equivalent costs, achieving an accuracy of\n75.9%. When integrating GPT4 into our model pool, Routoo nearly matches GPT4's\nperformance at half the cost and exceeds it with a 25% cost reduction. These\noutcomes highlight Routoo's potential to create new SOTA in a cost-effective\nmanner by leveraging the collective knowledge of multiple LLMs.\n","authors":["Alireza Mohammadshahi","Arshad Rafiq Shaikh","Majid Yazdani"],"pdf_url":"https://arxiv.org/pdf/2401.13979v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01301v1","updated":"2024-08-02T14:43:45Z","published":"2024-08-02T14:43:45Z","title":"A Decision-driven Methodology for Designing Uncertainty-aware AI\n  Self-Assessment","summary":"  Artificial intelligence (AI) has revolutionized decision-making processes and\nsystems throughout society and, in particular, has emerged as a significant\ntechnology in high-impact scenarios of national interest. Yet, despite AI's\nimpressive predictive capabilities in controlled settings, it still suffers\nfrom a range of practical setbacks preventing its widespread use in various\ncritical scenarios. In particular, it is generally unclear if a given AI\nsystem's predictions can be trusted by decision-makers in downstream\napplications. To address the need for more transparent, robust, and trustworthy\nAI systems, a suite of tools has been developed to quantify the uncertainty of\nAI predictions and, more generally, enable AI to \"self-assess\" the reliability\nof its predictions. In this manuscript, we categorize methods for AI\nself-assessment along several key dimensions and provide guidelines for\nselecting and designing the appropriate method for a practitioner's needs. In\nparticular, we focus on uncertainty estimation techniques that consider the\nimpact of self-assessment on the choices made by downstream decision-makers and\non the resulting costs and benefits of decision outcomes. To demonstrate the\nutility of our methodology for self-assessment design, we illustrate its use\nfor two realistic national-interest scenarios. This manuscript is a practical\nguide for machine learning engineers and AI system users to select the ideal\nself-assessment techniques for each problem.\n","authors":["Gregory Canal","Vladimir Leung","Philip Sage","Eric Heim","I-Jeng Wang"],"pdf_url":"https://arxiv.org/pdf/2408.01301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.03389v2","updated":"2024-08-02T14:33:32Z","published":"2024-05-06T11:51:09Z","title":"Don't Waste Your Time: Early Stopping Cross-Validation","summary":"  State-of-the-art automated machine learning systems for tabular data often\nemploy cross-validation; ensuring that measured performances generalize to\nunseen data, or that subsequent ensembling does not overfit. However, using\nk-fold cross-validation instead of holdout validation drastically increases the\ncomputational cost of validating a single configuration. While ensuring better\ngeneralization and, by extension, better performance, the additional cost is\noften prohibitive for effective model selection within a time budget. We aim to\nmake model selection with cross-validation more effective. Therefore, we study\nearly stopping the process of cross-validation during model selection. We\ninvestigate the impact of early stopping on random search for two algorithms,\nMLP and random forest, across 36 classification datasets. We further analyze\nthe impact of the number of folds by considering 3-, 5-, and 10-folds. In\naddition, we investigate the impact of early stopping with Bayesian\noptimization instead of random search and also repeated cross-validation. Our\nexploratory study shows that even a simple-to-understand and easy-to-implement\nmethod consistently allows model selection to converge faster; in ~94% of all\ndatasets, on average by ~214%. Moreover, stopping cross-validation enables\nmodel selection to explore the search space more exhaustively by considering\n+167% configurations on average within one hour, while also obtaining better\noverall performance.\n","authors":["Edward Bergman","Lennart Purucker","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2405.03389v2.pdf","comment":"Accepted at Third International Conference on Automated Machine\n  Learning (AutoML 2024); for code, see\n  https://github.com/automl/DontWasteYourTime-early-stopping"},{"id":"http://arxiv.org/abs/2408.01292v1","updated":"2024-08-02T14:28:10Z","published":"2024-08-02T14:28:10Z","title":"3DPX: Progressive 2D-to-3D Oral Image Reconstruction with Hybrid MLP-CNN\n  Networks","summary":"  Panoramic X-ray (PX) is a prevalent modality in dental practice for its wide\navailability and low cost. However, as a 2D projection image, PX does not\ncontain 3D anatomical information, and therefore has limited use in dental\napplications that can benefit from 3D information, e.g., tooth angular\nmisa-lignment detection and classification. Reconstructing 3D structures\ndirectly from 2D PX has recently been explored to address limitations with\nexisting methods primarily reliant on Convolutional Neural Networks (CNNs) for\ndirect 2D-to-3D mapping. These methods, however, are unable to correctly infer\ndepth-axis spatial information. In addition, they are limited by the in-trinsic\nlocality of convolution operations, as the convolution kernels only capture the\ninformation of immediate neighborhood pixels. In this study, we propose a\nprogressive hybrid Multilayer Perceptron (MLP)-CNN pyra-mid network (3DPX) for\n2D-to-3D oral PX reconstruction. We introduce a progressive reconstruction\nstrategy, where 3D images are progressively re-constructed in the 3DPX with\nguidance imposed on the intermediate recon-struction result at each pyramid\nlevel. Further, motivated by the recent ad-vancement of MLPs that show promise\nin capturing fine-grained long-range dependency, our 3DPX integrates MLPs and\nCNNs to improve the semantic understanding during reconstruction. Extensive\nexperiments on two large datasets involving 464 studies demonstrate that our\n3DPX outperforms state-of-the-art 2D-to-3D oral reconstruction methods,\nincluding standalone MLP and transformers, in reconstruction quality, and also\nim-proves the performance of downstream angular misalignment classification\ntasks.\n","authors":["Xiaoshuang Li","Mingyuan Meng","Zimo Huang","Lei Bi","Eduardo Delamare","Dagan Feng","Bin Sheng","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2408.01292v1.pdf","comment":"accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.14962v3","updated":"2024-08-02T14:26:55Z","published":"2024-07-20T18:48:35Z","title":"Recent Advances in Generative AI and Large Language Models: Current\n  Status, Challenges, and Perspectives","summary":"  The emergence of Generative Artificial Intelligence (AI) and Large Language\nModels (LLMs) has marked a new era of Natural Language Processing (NLP),\nintroducing unprecedented capabilities that are revolutionizing various\ndomains. This paper explores the current state of these cutting-edge\ntechnologies, demonstrating their remarkable advancements and wide-ranging\napplications. Our paper contributes to providing a holistic perspective on the\ntechnical foundations, practical applications, and emerging challenges within\nthe evolving landscape of Generative AI and LLMs. We believe that understanding\nthe generative capabilities of AI systems and the specific context of LLMs is\ncrucial for researchers, practitioners, and policymakers to collaboratively\nshape the responsible and ethical integration of these technologies into\nvarious domains. Furthermore, we identify and address main research gaps,\nproviding valuable insights to guide future research endeavors within the AI\nresearch community.\n","authors":["Desta Haileselassie Hagos","Rick Battle","Danda B. Rawat"],"pdf_url":"https://arxiv.org/pdf/2407.14962v3.pdf","comment":"This version is accepted for publication in the journal of IEEE\n  Transactions on Artificial Intelligence (TAI)"},{"id":"http://arxiv.org/abs/2112.06311v4","updated":"2024-08-02T14:21:43Z","published":"2021-12-12T20:02:42Z","title":"Weakly Supervised Text-to-SQL Parsing through Question Decomposition","summary":"  Text-to-SQL parsers are crucial in enabling non-experts to effortlessly query\nrelational data. Training such parsers, by contrast, generally requires\nexpertise in annotating natural language (NL) utterances with corresponding SQL\nqueries. In this work, we propose a weak supervision approach for training\ntext-to-SQL parsers. We take advantage of the recently proposed question\nmeaning representation called QDMR, an intermediate between NL and formal query\nlanguages. Given questions, their QDMR structures (annotated by non-experts or\nautomatically predicted), and the answers, we are able to automatically\nsynthesize SQL queries that are used to train text-to-SQL models. We test our\napproach by experimenting on five benchmark datasets. Our results show that the\nweakly supervised models perform competitively with those trained on annotated\nNL-SQL data. Overall, we effectively train text-to-SQL parsers, while using\nzero SQL annotations.\n","authors":["Tomer Wolfson","Daniel Deutch","Jonathan Berant"],"pdf_url":"https://arxiv.org/pdf/2112.06311v4.pdf","comment":"Accepted for publication in Findings of NAACL 2022. Author's final\n  version"},{"id":"http://arxiv.org/abs/2304.13007v4","updated":"2024-08-02T14:18:51Z","published":"2023-04-25T17:27:37Z","title":"Answering Questions by Meta-Reasoning over Multiple Chains of Thought","summary":"  Modern systems for multi-hop question answering (QA) typically break\nquestions into a sequence of reasoning steps, termed chain-of-thought (CoT),\nbefore arriving at a final answer. Often, multiple chains are sampled and\naggregated through a voting mechanism over the final answers, but the\nintermediate steps themselves are discarded. While such approaches improve\nperformance, they do not consider the relations between intermediate steps\nacross chains and do not provide a unified explanation for the predicted\nanswer. We introduce Multi-Chain Reasoning (MCR), an approach which prompts\nlarge language models to meta-reason over multiple chains of thought, rather\nthan aggregating their answers. MCR examines different reasoning chains, mixes\ninformation between them and selects the most relevant facts in generating an\nexplanation and predicting the answer. MCR outperforms strong baselines on 7\nmulti-hop QA datasets. Moreover, our analysis reveals that MCR explanations\nexhibit high quality, enabling humans to verify its answers.\n","authors":["Ori Yoran","Tomer Wolfson","Ben Bogin","Uri Katz","Daniel Deutch","Jonathan Berant"],"pdf_url":"https://arxiv.org/pdf/2304.13007v4.pdf","comment":"Accepted for publication in The 2023 Conference on Empirical Methods\n  in Natural Language Processing (EMNLP 2023). Author's final version"},{"id":"http://arxiv.org/abs/2404.16054v2","updated":"2024-08-02T13:49:32Z","published":"2024-04-12T15:39:09Z","title":"LlamaTouch: A Faithful and Scalable Testbed for Mobile UI Task\n  Automation","summary":"  The emergent large language/multimodal models facilitate the evolution of\nmobile agents, especially in mobile UI task automation. However, existing\nevaluation approaches, which rely on human validation or established datasets\nto compare agent-predicted actions with predefined action sequences, are\nunscalable and unfaithful. To overcome these limitations, this paper presents\nLlamaTouch, a testbed for on-device mobile UI task execution and faithful,\nscalable task evaluation. By observing that the task execution process only\ntransfers UI states, LlamaTouch employs a novel evaluation approach that only\nassesses whether an agent traverses all manually annotated, essential\napplication/system states. LlamaTouch comprises three key techniques: (1)\nOn-device task execution that enables mobile agents to interact with realistic\nmobile environments for task execution. (2) Fine-grained UI component\nannotation that merges pixel-level screenshots and textual screen hierarchies\nto explicitly identify and precisely annotate essential UI components with a\nrich set of designed annotation primitives. (3) A multi-level application state\nmatching algorithm that utilizes exact and fuzzy matching to accurately detect\ncritical information in each screen, even with unpredictable UI layout/content\ndynamics. LlamaTouch currently incorporates four mobile agents and 496 tasks,\nencompassing both tasks in the widely-used datasets and our self-constructed\nones to cover more diverse mobile applications. Evaluation results demonstrate\nLlamaTouch's high faithfulness of evaluation in real-world mobile environments\nand its better scalability than human validation. LlamaTouch also enables easy\ntask annotation and integration of new mobile agents. Code and dataset are\npublicly available at https://github.com/LlamaTouch/LlamaTouch.\n","authors":["Li Zhang","Shihe Wang","Xianqing Jia","Zhihan Zheng","Yunhe Yan","Longxi Gao","Yuanchun Li","Mengwei Xu"],"pdf_url":"https://arxiv.org/pdf/2404.16054v2.pdf","comment":"Accepted at ACM UIST 2024"},{"id":"http://arxiv.org/abs/2407.02604v2","updated":"2024-08-02T13:45:53Z","published":"2024-07-02T18:43:10Z","title":"D-Rax: Domain-specific Radiologic assistant leveraging multi-modal data\n  and eXpert model predictions","summary":"  Large vision language models (VLMs) have progressed incredibly from research\nto applicability for general-purpose use cases. LLaVA-Med, a pioneering large\nlanguage and vision assistant for biomedicine, can perform multi-modal\nbiomedical image and data analysis to provide a natural language interface for\nradiologists. While it is highly generalizable and works with multi-modal data,\nit is currently limited by well-known challenges that exist in the large\nlanguage model space. Hallucinations and imprecision in responses can lead to\nmisdiagnosis which currently hinder the clinical adaptability of VLMs. To\ncreate precise, user-friendly models in healthcare, we propose D-Rax -- a\ndomain-specific, conversational, radiologic assistance tool that can be used to\ngain insights about a particular radiologic image. In this study, we enhance\nthe conversational analysis of chest X-ray (CXR) images to support radiological\nreporting, offering comprehensive insights from medical imaging and aiding in\nthe formulation of accurate diagnosis. D-Rax is achieved by fine-tuning the\nLLaVA-Med architecture on our curated enhanced instruction-following data,\ncomprising of images, instructions, as well as disease diagnosis and\ndemographic predictions derived from MIMIC-CXR imaging data, CXR-related visual\nquestion answer (VQA) pairs, and predictive outcomes from multiple expert AI\nmodels. We observe statistically significant improvement in responses when\nevaluated for both open and close-ended conversations. Leveraging the power of\nstate-of-the-art diagnostic models combined with VLMs, D-Rax empowers\nclinicians to interact with medical images using natural language, which could\npotentially streamline their decision-making process, enhance diagnostic\naccuracy, and conserve their time.\n","authors":["Hareem Nisar","Syed Muhammad Anwar","Zhifan Jiang","Abhijeet Parida","Ramon Sanchez-Jacob","Vishwesh Nath","Holger R. Roth","Marius George Linguraru"],"pdf_url":"https://arxiv.org/pdf/2407.02604v2.pdf","comment":"accepted to the MICCAI 2024 Second International Workshop on\n  Foundation Models for General Medical AI"},{"id":"http://arxiv.org/abs/2408.01263v1","updated":"2024-08-02T13:36:17Z","published":"2024-08-02T13:36:17Z","title":"The virtual CAT: A tool for algorithmic thinking assessment in Swiss\n  compulsory education","summary":"  In today's digital era, holding algorithmic thinking (AT) skills is crucial,\nnot only in computer science-related fields. These abilities enable individuals\nto break down complex problems into more manageable steps and create a sequence\nof actions to solve them. To address the increasing demand for AT assessments\nin educational settings and the limitations of current methods, this paper\nintroduces the virtual Cross Array Task (CAT), a digital adaptation of an\nunplugged assessment activity designed to evaluate algorithmic skills in Swiss\ncompulsory education. This tool offers scalable and automated assessment,\nreducing human involvement and mitigating potential data collection errors. The\nplatform features gesture-based and visual block-based programming interfaces,\nensuring its usability for diverse learners, further supported by multilingual\ncapabilities. To evaluate the virtual CAT platform, we conducted a pilot\nevaluation in Switzerland involving a heterogeneous group of students. The\nfindings show the platform's usability, proficiency and suitability for\nassessing AT skills among students of diverse ages, development stages, and\neducational backgrounds, as well as the feasibility of large-scale data\ncollection.\n","authors":["Giorgia Adorni","Alberto Piatti"],"pdf_url":"https://arxiv.org/pdf/2408.01263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01257v1","updated":"2024-08-02T13:27:56Z","published":"2024-08-02T13:27:56Z","title":"Detection and Characterization of Coordinated Online Behavior: A Survey","summary":"  Coordination is a fundamental aspect of life. The advent of social media has\nmade it integral also to online human interactions, such as those that\ncharacterize thriving online communities and social movements. At the same\ntime, coordination is also core to effective disinformation, manipulation, and\nhate campaigns. This survey collects, categorizes, and critically discusses the\nbody of work produced as a result of the growing interest on coordinated online\nbehavior. We reconcile industry and academic definitions, propose a\ncomprehensive framework to study coordinated online behavior, and review and\ncritically discuss the existing detection and characterization methods. Our\nanalysis identifies open challenges and promising directions of research,\nserving as a guide for scholars, practitioners, and policymakers in\nunderstanding and addressing the complexities inherent to online coordination.\n","authors":["Lorenzo Mannocci","Michele Mazza","Anna Monreale","Maurizio Tesconi","Stefano Cresci"],"pdf_url":"https://arxiv.org/pdf/2408.01257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16455v3","updated":"2024-08-02T13:27:16Z","published":"2024-04-25T09:34:49Z","title":"Canonical Decision Diagrams Modulo Theories","summary":"  Decision diagrams (DDs) are powerful tools to represent effectively\npropositional formulas, which are largely used in many domains, in particular\nin formal verification and in knowledge compilation. Some forms of DDs (e.g.,\nOBDDs, SDDs) are canonical, that is, (under given conditions on the atom list)\nthey univocally represent equivalence classes of formulas. Given the limited\nexpressiveness of propositional logic, a few attempts to leverage DDs to SMT\nlevel have been presented in the literature. Unfortunately, these techniques\nstill suffer from some limitations: most procedures are theory-specific; some\nproduce theory DDs (T-DDs) which do not univocally represent T-valid formulas\nor T-inconsistent formulas; none of these techniques provably produces\ntheory-canonical T-DDs, which (under given conditions on the T-atom list)\nunivocally represent T-equivalence classes of formulas. Also, these procedures\nare not easy to implement, and very few implementations are actually available.\nIn this paper, we present a novel very-general technique to leverage DDs to SMT\nlevel, which has several advantages: it is very easy to implement on top of an\nAllSMT solver and a DD package, which are used as blackboxes; it works for\nevery form of DDs and every theory, or combination thereof, supported by the\nAllSMT solver; it produces theory-canonical T-DDs if the propositional DD is\ncanonical. We have implemented a prototype tool for both T-OBDDs and T-SDDs on\ntop of OBDD and SDD packages and the MathSAT SMT solver. Some preliminary\nempirical evaluation supports the effectiveness of the approach.\n","authors":["Massimo Michelutti","Gabriele Masina","Giuseppe Spallitta","Roberto Sebastiani"],"pdf_url":"https://arxiv.org/pdf/2404.16455v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11053v3","updated":"2024-08-02T13:26:44Z","published":"2024-05-17T19:06:06Z","title":"The MovieLens Beliefs Dataset: Collecting Pre-Choice Data for Online\n  Recommender Systems","summary":"  An increasingly important aspect of designing recommender systems involves\nconsidering how recommendations will influence consumer choices. This paper\naddresses this issue by introducing a method for collecting user beliefs about\nun-experienced items - a critical predictor of choice behavior. We implemented\nthis method on the MovieLens platform, resulting in a rich dataset that\ncombines user ratings, beliefs, and observed recommendations. We document\nchallenges to such data collection, including selection bias in response and\nlimited coverage of the product space. This unique resource empowers\nresearchers to delve deeper into user behavior and analyze user choices absent\nrecommendations, measure the effectiveness of recommendations, and prototype\nalgorithms that leverage user belief data, ultimately leading to more impactful\nrecommender systems. The dataset can be found at\nhttps://grouplens.org/datasets/movielens/ml_belief_2024/.\n","authors":["Guy Aridor","Duarte Goncalves","Ruoyan Kong","Daniel Kluver","Joseph Konstan"],"pdf_url":"https://arxiv.org/pdf/2405.11053v3.pdf","comment":"To Appear in RecSys 2024"},{"id":"http://arxiv.org/abs/2401.04531v3","updated":"2024-08-02T13:23:18Z","published":"2024-01-09T12:55:21Z","title":"MERA: A Comprehensive LLM Evaluation in Russian","summary":"  Over the past few years, one of the most notable advancements in AI research\nhas been in foundation models (FMs), headlined by the rise of language models\n(LMs). As the models' size increases, LMs demonstrate enhancements in\nmeasurable aspects and the development of new qualitative features. However,\ndespite researchers' attention and the rapid growth in LM application, the\ncapabilities, limitations, and associated risks still need to be better\nunderstood. To address these issues, we introduce an open Multimodal Evaluation\nof Russian-language Architectures (MERA), a new instruction benchmark for\nevaluating foundation models oriented towards the Russian language. The\nbenchmark encompasses 21 evaluation tasks for generative models in 11 skill\ndomains and is designed as a black-box test to ensure the exclusion of data\nleakage. The paper introduces a methodology to evaluate FMs and LMs in zero-\nand few-shot fixed instruction settings that can be extended to other\nmodalities. We propose an evaluation methodology, an open-source code base for\nthe MERA assessment, and a leaderboard with a submission system. We evaluate\nopen LMs as baselines and find that they are still far behind the human level.\nWe publicly release MERA to guide forthcoming research, anticipate\ngroundbreaking model features, standardize the evaluation procedure, and\naddress potential societal drawbacks.\n","authors":["Alena Fenogenova","Artem Chervyakov","Nikita Martynov","Anastasia Kozlova","Maria Tikhonova","Albina Akhmetgareeva","Anton Emelyanov","Denis Shevelev","Pavel Lebedev","Leonid Sinev","Ulyana Isaeva","Katerina Kolomeytseva","Daniil Moskovskiy","Elizaveta Goncharova","Nikita Savushkin","Polina Mikhailova","Denis Dimitrov","Alexander Panchenko","Sergei Markov"],"pdf_url":"https://arxiv.org/pdf/2401.04531v3.pdf","comment":"The paper version comparable with the release code v.1.1.0 of the\n  benchmark MERA. ACL-2024 main track camera ready version"},{"id":"http://arxiv.org/abs/2408.01254v1","updated":"2024-08-02T13:15:17Z","published":"2024-08-02T13:15:17Z","title":"TrIM: Triangular Input Movement Systolic Array for Convolutional Neural\n  Networks -- Part I: Dataflow and Analytical Modelling","summary":"  In order to follow the ever-growing computational complexity and data\nintensity of state-of-the-art AI models, new computing paradigms are being\nproposed. These paradigms aim at achieving high energy efficiency, by\nmitigating the Von Neumann bottleneck that relates to the energy cost of moving\ndata between the processing cores and the memory. Convolutional Neural Networks\n(CNNs) are particularly susceptible to this bottleneck, given the massive data\nthey have to manage. Systolic Arrays (SAs) are promising architectures to\nmitigate the data transmission cost, thanks to high data utilization carried\nout by an array of Processing Elements (PEs). These PEs continuously exchange\nand process data locally based on specific dataflows (like weight stationary\nand row stationary), in turn reducing the number of memory accesses to the main\nmemory. The hardware specialization of SAs can meet different workloads,\nranging from matrix multiplications to multi-dimensional convolutions. In this\npaper, we propose TrIM: a novel dataflow for SAs based on a Triangular Input\nMovement and compatible with CNN computing. When compared to state-of-the-art\nSA dataflows, like weight stationary and row stationary, the high data\nutilization offered by TrIM guarantees ~10x less memory access. Furthermore,\nconsidering that PEs continuously overlap multiplications and accumulations,\nTrIM achieves high throughput (up to 81.8% higher than row stationary), other\nthan requiring a limited number of registers (up to 15.6x fewer registers than\nrow stationary).\n","authors":["Cristian Sestito","Shady Agwa","Themis Prodromakis"],"pdf_url":"https://arxiv.org/pdf/2408.01254v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2408.01253v1","updated":"2024-08-02T13:15:01Z","published":"2024-08-02T13:15:01Z","title":"Metareasoning in uncertain environments: a meta-BAMDP framework","summary":"  In decision-making scenarios, \\textit{reasoning} can be viewed as an\nalgorithm $P$ that makes a choice of an action $a^* \\in \\mathcal{A}$, aiming to\noptimize some outcome such as maximizing the value function of a Markov\ndecision process (MDP). However, executing $P$ itself may bear some costs\n(time, energy, limited capacity, etc.) and needs to be considered alongside\nexplicit utility obtained by making the choice in the underlying decision\nproblem. Such costs need to be taken into account in order to accurately model\nhuman behavior, as well as optimizing AI planning, as all physical systems are\nbound to face resource constraints. Finding the right $P$ can itself be framed\nas an optimization problem over the space of reasoning processes $P$, generally\nreferred to as \\textit{metareasoning}. Conventionally, human metareasoning\nmodels assume that the agent knows the transition and reward distributions of\nthe underlying MDP. This paper generalizes such models by proposing a meta\nBayes-Adaptive MDP (meta-BAMDP) framework to handle metareasoning in\nenvironments with unknown reward/transition distributions, which encompasses a\nfar larger and more realistic set of planning problems that humans and AI\nsystems face. As a first step, we apply the framework to two-armed Bernoulli\nbandit (TABB) tasks, which have often been used to study human decision making.\nOwing to the meta problem's complexity, our solutions are necessarily\napproximate, but nevertheless robust within a range of assumptions that are\narguably realistic for human decision-making scenarios. These results offer a\nnormative framework for understanding human exploration under cognitive\nconstraints. This integration of Bayesian adaptive strategies with\nmetareasoning enriches both the theoretical landscape of decision-making\nresearch and practical applications in designing AI systems that plan under\nuncertainty and resource constraints.\n","authors":["Prakhar Godara","Tilman Diego Aléman","Angela J. Yu"],"pdf_url":"https://arxiv.org/pdf/2408.01253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01248v1","updated":"2024-08-02T13:10:33Z","published":"2024-08-02T13:10:33Z","title":"Deep progressive reinforcement learning-based flexible resource\n  scheduling framework for IRS and UAV-assisted MEC system","summary":"  The intelligent reflection surface (IRS) and unmanned aerial vehicle\n(UAV)-assisted mobile edge computing (MEC) system is widely used in temporary\nand emergency scenarios. Our goal is to minimize the energy consumption of the\nMEC system by jointly optimizing UAV locations, IRS phase shift, task\noffloading, and resource allocation with a variable number of UAVs. To this\nend, we propose a Flexible REsource Scheduling (FRES) framework by employing a\nnovel deep progressive reinforcement learning which includes the following\ninnovations: Firstly, a novel multi-task agent is presented to deal with the\nmixed integer nonlinear programming (MINLP) problem. The multi-task agent has\ntwo output heads designed for different tasks, in which a classified head is\nemployed to make offloading decisions with integer variables while a fitting\nhead is applied to solve resource allocation with continuous variables.\nSecondly, a progressive scheduler is introduced to adapt the agent to the\nvarying number of UAVs by progressively adjusting a part of neurons in the\nagent. This structure can naturally accumulate experiences and be immune to\ncatastrophic forgetting. Finally, a light taboo search (LTS) is introduced to\nenhance the global search of the FRES. The numerical results demonstrate the\nsuperiority of the FRES framework which can make real-time and optimal resource\nscheduling even in dynamic MEC systems.\n","authors":["Li Dong","Feibo Jiang","Minjie Wang","Yubo Peng","Xiaolong Li"],"pdf_url":"https://arxiv.org/pdf/2408.01248v1.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2408.00374v2","updated":"2024-08-02T13:00:46Z","published":"2024-08-01T08:32:03Z","title":"Conformal Trajectory Prediction with Multi-View Data Integration in\n  Cooperative Driving","summary":"  Current research on trajectory prediction primarily relies on data collected\nby onboard sensors of an ego vehicle. With the rapid advancement in connected\ntechnologies, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure\n(V2I) communication, valuable information from alternate views becomes\naccessible via wireless networks. The integration of information from\nalternative views has the potential to overcome the inherent limitations\nassociated with a single viewpoint, such as occlusions and limited field of\nview. In this work, we introduce V2INet, a novel trajectory prediction\nframework designed to model multi-view data by extending existing single-view\nmodels. Unlike previous approaches where the multi-view data is manually fused\nor formulated as a separate training stage, our model supports end-to-end\ntraining, enhancing both flexibility and performance. Moreover, the predicted\nmultimodal trajectories are calibrated by a post-hoc conformal prediction\nmodule to get valid and efficient confidence regions. We evaluated the entire\nframework using the real-world V2I dataset V2X-Seq. Our results demonstrate\nsuperior performance in terms of Final Displacement Error (FDE) and Miss Rate\n(MR) using a single GPU. The code is publicly available at:\n\\url{https://github.com/xichennn/V2I_trajectory_prediction}.\n","authors":["Xi Chen","Rahul Bhadani","Larry Head"],"pdf_url":"https://arxiv.org/pdf/2408.00374v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01239v1","updated":"2024-08-02T12:58:08Z","published":"2024-08-02T12:58:08Z","title":"Tailoring Graph Neural Network-based Flow-guided Localization to\n  Individual Bloodstreams and Activities","summary":"  Flow-guided localization using in-body nanodevices in the bloodstream is\nexpected to be beneficial for early disease detection, continuous monitoring of\nbiological conditions, and targeted treatment. The nanodevices face size and\npower constraints that produce erroneous raw data for localization purposes.\nOn-body anchors receive this data, and use it to derive the locations of\ndiagnostic events of interest. Different Machine Learning (ML) approaches have\nbeen recently proposed for this task, yet they are currently restricted to a\nreference bloodstream of a resting patient. As such, they are unable to deal\nwith the physical diversity of patients' bloodstreams and cannot provide\ncontinuous monitoring due to changes in individual patient's activities. Toward\naddressing these issues for the current State-of-the-Art (SotA) flow-guided\nlocalization approach based on Graph Neural Networks (GNNs), we propose a\npipeline for GNN adaptation based on individual physiological indicators\nincluding height, weight, and heart rate. Our results indicate that the\nproposed adaptions are beneficial in reconciling the individual differences\nbetween bloodstreams and activities.\n","authors":["Pablo Galván","Filip Lemic","Gerard Calvo Bartra","Sergi Abadal","Xavier Costa Pérez"],"pdf_url":"https://arxiv.org/pdf/2408.01239v1.pdf","comment":"7 pages, 9 figures, 2 tables, 16 references, accepted at ACM\n  NanoCom'25"},{"id":"http://arxiv.org/abs/2308.01154v4","updated":"2024-08-02T12:39:17Z","published":"2023-08-02T13:58:37Z","title":"Arithmetic with Language Models: from Memorization to Computation","summary":"  A better understanding of the emergent computation and problem-solving\ncapabilities of recent large language models is of paramount importance to\nfurther improve them and broaden their applicability. This work investigates\nhow a language model, trained to predict the next token, can perform arithmetic\ncomputations generalizing beyond training data. Binary addition and\nmultiplication constitute a good testbed for this purpose, since they require a\nvery small vocabulary and exhibit relevant input/output discontinuities making\nsmooth input interpolation ineffective for novel data. We successfully trained\na light language model to learn these tasks and ran a number of experiments to\ninvestigate the extrapolation capabilities and internal information processing.\nOur findings support the hypothesis that the language model works as an\nEncoding-Regression-Decoding machine where the computation takes place in the\nvalue space once the input token representation is mapped to an appropriate\ninternal representation.\n","authors":["Davide Maltoni","Matteo Ferrara"],"pdf_url":"https://arxiv.org/pdf/2308.01154v4.pdf","comment":"The article has been accepted for publication in Elsevier Neural\n  Networks journal. The final version is available on the Elsevier\n  ScienceDirect platform"},{"id":"http://arxiv.org/abs/2209.05467v3","updated":"2024-08-02T12:27:17Z","published":"2022-09-07T10:09:12Z","title":"Modelling Assessment Rubrics through Bayesian Networks: a Pragmatic\n  Approach","summary":"  Automatic assessment of learner competencies is a fundamental task in\nintelligent tutoring systems. An assessment rubric typically and effectively\ndescribes relevant competencies and competence levels. This paper presents an\napproach to deriving a learner model directly from an assessment rubric\ndefining some (partial) ordering of competence levels. The model is based on\nBayesian networks and exploits logical gates with uncertainty (often referred\nto as noisy gates) to reduce the number of parameters of the model, so to\nsimplify their elicitation by experts and allow real-time inference in\nintelligent tutoring systems. We illustrate how the approach can be applied to\nautomatize the human assessment of an activity developed for testing\ncomputational thinking skills. The simple elicitation of the model starting\nfrom the assessment rubric opens up the possibility of quickly automating the\nassessment of several tasks, making them more easily exploitable in the context\nof adaptive assessment tools and intelligent tutoring systems.\n","authors":["Francesca Mangili","Giorgia Adorni","Alberto Piatti","Claudio Bonesana","Alessandro Antonucci"],"pdf_url":"https://arxiv.org/pdf/2209.05467v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01221v1","updated":"2024-08-02T12:21:05Z","published":"2024-08-02T12:21:05Z","title":"Rubric-based Learner Modelling via Noisy Gates Bayesian Networks for\n  Computational Thinking Skills Assessment","summary":"  In modern and personalised education, there is a growing interest in\ndeveloping learners' competencies and accurately assessing them. In a previous\nwork, we proposed a procedure for deriving a learner model for automatic skill\nassessment from a task-specific competence rubric, thus simplifying the\nimplementation of automated assessment tools. The previous approach, however,\nsuffered two main limitations: (i) the ordering between competencies defined by\nthe assessment rubric was only indirectly modelled; (ii) supplementary skills,\nnot under assessment but necessary for accomplishing the task, were not\nincluded in the model. In this work, we address issue (i) by introducing dummy\nobserved nodes, strictly enforcing the skills ordering without changing the\nnetwork's structure. In contrast, for point (ii), we design a network with two\nlayers of gates, one performing disjunctive operations by noisy-OR gates and\nthe other conjunctive operations through logical ANDs. Such changes improve the\nmodel outcomes' coherence and the modelling tool's flexibility without\ncompromising the model's compact parametrisation, interpretability and simple\nexperts' elicitation. We used this approach to develop a learner model for\nComputational Thinking (CT) skills assessment. The CT-cube skills assessment\nframework and the Cross Array Task (CAT) are used to exemplify it and\ndemonstrate its feasibility.\n","authors":["Giorgia Adorni","Francesca Mangili","Alberto Piatti","Claudio Bonesana","Alessandro Antonucci"],"pdf_url":"https://arxiv.org/pdf/2408.01221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19813v2","updated":"2024-08-02T12:11:17Z","published":"2024-07-29T09:05:10Z","title":"Improving Retrieval Augmented Language Model with Self-Reasoning","summary":"  The Retrieval-Augmented Language Model (RALM) has shown remarkable\nperformance on knowledge-intensive tasks by incorporating external knowledge\nduring inference, which mitigates the factual hallucinations inherited in large\nlanguage models (LLMs). Despite these advancements, challenges persist in the\nimplementation of RALMs, particularly concerning their reliability and\ntraceability. To be specific, the irrelevant document retrieval may result in\nunhelpful response generation or even deteriorate the performance of LLMs,\nwhile the lack of proper citations in generated outputs complicates efforts to\nverify the trustworthiness of the models. To this end, we propose a novel\nself-reasoning framework aimed at improving the reliability and traceability of\nRALMs, whose core idea is to leverage reasoning trajectories generated by the\nLLM itself. The framework involves constructing self-reason trajectories with\nthree processes: a relevance-aware process, an evidence-aware selective\nprocess, and a trajectory analysis process. We have evaluated our framework\nacross four public datasets (two short-form QA datasets, one long-form QA\ndataset, and one fact verification dataset) to demonstrate the superiority of\nour method, which can outperform existing state-of-art models and can achieve\ncomparable performance with GPT-4, while only using 2,000 training samples.\n","authors":["Yuan Xia","Jingbo Zhou","Zhenhui Shi","Jun Chen","Haifeng Huang"],"pdf_url":"https://arxiv.org/pdf/2407.19813v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01214v1","updated":"2024-08-02T12:00:00Z","published":"2024-08-02T12:00:00Z","title":"High-Throughput Phenotyping of Clinical Text Using Large Language Models","summary":"  High-throughput phenotyping automates the mapping of patient signs to\nstandardized ontology concepts and is essential for precision medicine. This\nstudy evaluates the automation of phenotyping of clinical summaries from the\nOnline Mendelian Inheritance in Man (OMIM) database using large language\nmodels. Due to their rich phenotype data, these summaries can be surrogates for\nphysician notes. We conduct a performance comparison of GPT-4 and\nGPT-3.5-Turbo. Our results indicate that GPT-4 surpasses GPT-3.5-Turbo in\nidentifying, categorizing, and normalizing signs, achieving concordance with\nmanual annotators comparable to inter-rater agreement. Despite some limitations\nin sign normalization, the extensive pre-training of GPT-4 results in high\nperformance and generalizability across several phenotyping tasks while\nobviating the need for manually annotated training data. Large language models\nare expected to be the dominant method for automating high-throughput\nphenotyping of clinical text.\n","authors":["Daniel B. Hier","S. Ilyas Munzir","Anne Stahlfeld","Tayo Obafemi-Ajayi","Michael D. Carrithers"],"pdf_url":"https://arxiv.org/pdf/2408.01214v1.pdf","comment":"Submitted to IEEE-EMBS International Conference on Biomedical and\n  Health Informatics (BHI), Houston TX"},{"id":"http://arxiv.org/abs/2401.03469v3","updated":"2024-08-02T11:39:03Z","published":"2024-01-07T12:31:36Z","title":"Efficient Test Data Generation for MC/DC with OCL and Search","summary":"  System-level testing of avionics software systems requires compliance with\ndifferent international safety standards such as DO-178C. An important\nconsideration of the avionics industry is automated test data generation\naccording to the criteria suggested by safety standards. One of the recommended\ncriteria by DO-178C is the modified condition/decision coverage (MC/DC)\ncriterion. The current model-based test data generation approaches use\nconstraints written in Object Constraint Language (OCL), and apply search\ntechniques to generate test data. These approaches either do not support MC/DC\ncriterion or suffer from performance issues while generating test data for\nlarge-scale avionics systems. In this paper, we propose an effective way to\nautomate MC/DC test data generation during model-based testing. We develop a\nstrategy that utilizes case-based reasoning (CBR) and range reduction\nheuristics designed to solve MC/DC-tailored OCL constraints. We performed an\nempirical study to compare our proposed strategy for MC/DC test data generation\nusing CBR, range reduction, both CBR and range reduction, with an original\nsearch algorithm, and random search. We also empirically compared our strategy\nwith existing constraint-solving approaches. The results show that both CBR and\nrange reduction for MC/DC test data generation outperform the baseline\napproach. Moreover, the combination of both CBR and range reduction for MC/DC\ntest data generation is an effective approach compared to existing constraint\nsolvers.\n","authors":["Hassan Sartaj","Muhammad Zohaib Iqbal","Atif Aftab Ahmed Jilani","Muhammad Uzair Khan"],"pdf_url":"https://arxiv.org/pdf/2401.03469v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15857v2","updated":"2024-08-02T11:36:14Z","published":"2024-03-23T14:47:26Z","title":"Automated System-level Testing of Unmanned Aerial Systems","summary":"  Unmanned aerial systems (UAS) rely on various avionics systems that are\nsafety-critical and mission-critical. A major requirement of international\nsafety standards is to perform rigorous system-level testing of avionics\nsoftware systems. The current industrial practice is to manually create test\nscenarios, manually/automatically execute these scenarios using simulators, and\nmanually evaluate outcomes. The test scenarios typically consist of setting\ncertain flight or environment conditions and testing the system under test in\nthese settings. The state-of-the-art approaches for this purpose also require\nmanual test scenario development and evaluation. In this paper, we propose a\nnovel approach to automate the system-level testing of the UAS. The proposed\napproach (AITester) utilizes model-based testing and artificial intelligence\n(AI) techniques to automatically generate, execute, and evaluate various test\nscenarios. The test scenarios are generated on the fly, i.e., during test\nexecution based on the environmental context at runtime. The approach is\nsupported by a toolset. We empirically evaluate the proposed approach on two\ncore components of UAS, an autopilot system of an unmanned aerial vehicle (UAV)\nand cockpit display systems (CDS) of the ground control station (GCS). The\nresults show that the AITester effectively generates test scenarios causing\ndeviations from the expected behavior of the UAV autopilot and reveals\npotential flaws in the GCS-CDS.\n","authors":["Hassan Sartaj","Asmar Muqeet","Muhammad Zohaib Iqbal","Muhammad Uzair Khan"],"pdf_url":"https://arxiv.org/pdf/2403.15857v2.pdf","comment":"Published in Automated Software Engineering"},{"id":"http://arxiv.org/abs/2407.19630v2","updated":"2024-08-02T11:26:12Z","published":"2024-07-29T01:21:11Z","title":"LLMs' Understanding of Natural Language Revealed","summary":"  Large language models (LLMs) are the result of a massive experiment in\nbottom-up, data-driven reverse engineering of language at scale. Despite their\nutility in a number of downstream NLP tasks, ample research has shown that LLMs\nare incapable of performing reasoning in tasks that require quantification over\nand the manipulation of symbolic variables (e.g., planning and problem\nsolving); see for example [25][26]. In this document, however, we will focus on\ntesting LLMs for their language understanding capabilities, their supposed\nforte. As we will show here, the language understanding capabilities of LLMs\nhave been widely exaggerated. While LLMs have proven to generate human-like\ncoherent language (since that's how they were designed), their language\nunderstanding capabilities have not been properly tested. In particular, we\nbelieve that the language understanding capabilities of LLMs should be tested\nby performing an operation that is the opposite of 'text generation' and\nspecifically by giving the LLM snippets of text as input and then querying what\nthe LLM \"understood\". As we show here, when doing so it will become apparent\nthat LLMs do not truly understand language, beyond very superficial inferences\nthat are essentially the byproduct of the memorization of massive amounts of\ningested text.\n","authors":["Walid S. Saba"],"pdf_url":"https://arxiv.org/pdf/2407.19630v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01188v1","updated":"2024-08-02T11:16:09Z","published":"2024-08-02T11:16:09Z","title":"Multi-Objective Deep Reinforcement Learning for Optimisation in\n  Autonomous Systems","summary":"  Reinforcement Learning (RL) is used extensively in Autonomous Systems (AS) as\nit enables learning at runtime without the need for a model of the environment\nor predefined actions. However, most applications of RL in AS, such as those\nbased on Q-learning, can only optimize one objective, making it necessary in\nmulti-objective systems to combine multiple objectives in a single objective\nfunction with predefined weights. A number of Multi-Objective Reinforcement\nLearning (MORL) techniques exist but they have mostly been applied in RL\nbenchmarks rather than real-world AS systems. In this work, we use a MORL\ntechnique called Deep W-Learning (DWN) and apply it to the Emergent Web Servers\nexemplar, a self-adaptive server, to find the optimal configuration for runtime\nperformance optimization. We compare DWN to two single-objective optimization\nimplementations: {\\epsilon}-greedy algorithm and Deep Q-Networks. Our initial\nevaluation shows that DWN optimizes multiple objectives simultaneously with\nsimilar results than DQN and {\\epsilon}-greedy approaches, having a better\nperformance for some metrics, and avoids issues associated with combining\nmultiple objectives into a single utility function.\n","authors":["Juan C. Rosero","Ivana Dusparic","Nicolás Cardozo"],"pdf_url":"https://arxiv.org/pdf/2408.01188v1.pdf","comment":"pages, Accepted to AI4AS 2024 workshop"},{"id":"http://arxiv.org/abs/2408.01187v1","updated":"2024-08-02T11:14:41Z","published":"2024-08-02T11:14:41Z","title":"Optimizing Variational Quantum Circuits Using Metaheuristic Strategies\n  in Reinforcement Learning","summary":"  Quantum Reinforcement Learning (QRL) offers potential advantages over\nclassical Reinforcement Learning, such as compact state space representation\nand faster convergence in certain scenarios. However, practical benefits\nrequire further validation. QRL faces challenges like flat solution landscapes,\nwhere traditional gradient-based methods are inefficient, necessitating the use\nof gradient-free algorithms. This work explores the integration of\nmetaheuristic algorithms -- Particle Swarm Optimization, Ant Colony\nOptimization, Tabu Search, Genetic Algorithm, Simulated Annealing, and Harmony\nSearch -- into QRL. These algorithms provide flexibility and efficiency in\nparameter optimization. Evaluations in $5\\times5$ MiniGrid Reinforcement\nLearning environments show that, all algorithms yield near-optimal results,\nwith Simulated Annealing and Particle Swarm Optimization performing best. In\nthe Cart Pole environment, Simulated Annealing, Genetic Algorithms, and\nParticle Swarm Optimization achieve optimal results, while the others perform\nslightly better than random action selection. These findings demonstrate the\npotential of Particle Swarm Optimization and Simulated Annealing for efficient\nQRL learning, emphasizing the need for careful algorithm selection and\nadaptation.\n","authors":["Michael Kölle","Daniel Seidl","Maximilian Zorn","Philipp Altmann","Jonas Stein","Thomas Gabor"],"pdf_url":"https://arxiv.org/pdf/2408.01187v1.pdf","comment":"Accepted at QCE24 - QCRL24 Workshop"},{"id":"http://arxiv.org/abs/2408.01168v1","updated":"2024-08-02T10:35:49Z","published":"2024-08-02T10:35:49Z","title":"Misinforming LLMs: vulnerabilities, challenges and opportunities","summary":"  Large Language Models (LLMs) have made significant advances in natural\nlanguage processing, but their underlying mechanisms are often misunderstood.\nDespite exhibiting coherent answers and apparent reasoning behaviors, LLMs rely\non statistical patterns in word embeddings rather than true cognitive\nprocesses. This leads to vulnerabilities such as \"hallucination\" and\nmisinformation. The paper argues that current LLM architectures are inherently\nuntrustworthy due to their reliance on correlations of sequential patterns of\nword embedding vectors. However, ongoing research into combining generative\ntransformer-based models with fact bases and logic programming languages may\nlead to the development of trustworthy LLMs capable of generating statements\nbased on given truth and explaining their self-reasoning process.\n","authors":["Bo Zhou","Daniel Geißler","Paul Lukowicz"],"pdf_url":"https://arxiv.org/pdf/2408.01168v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15559v2","updated":"2024-08-02T10:19:34Z","published":"2024-03-22T18:28:04Z","title":"An Optimization Framework to Enforce Multi-View Consistency for\n  Texturing 3D Meshes","summary":"  A fundamental problem in the texturing of 3D meshes using pre-trained\ntext-to-image models is to ensure multi-view consistency. State-of-the-art\napproaches typically use diffusion models to aggregate multi-view inputs, where\ncommon issues are the blurriness caused by the averaging operation in the\naggregation step or inconsistencies in local features. This paper introduces an\noptimization framework that proceeds in four stages to achieve multi-view\nconsistency. Specifically, the first stage generates an over-complete set of 2D\ntextures from a predefined set of viewpoints using an MV-consistent diffusion\nprocess. The second stage selects a subset of views that are mutually\nconsistent while covering the underlying 3D model. We show how to achieve this\ngoal by solving semi-definite programs. The third stage performs non-rigid\nalignment to align the selected views across overlapping regions. The fourth\nstage solves an MRF problem to associate each mesh face with a selected view.\nIn particular, the third and fourth stages are iterated, with the cuts obtained\nin the fourth stage encouraging non-rigid alignment in the third stage to focus\non regions close to the cuts. Experimental results show that our approach\nsignificantly outperforms baseline approaches both qualitatively and\nquantitatively. Project page: https://aigc3d.github.io/ConsistenTex.\n","authors":["Zhengyi Zhao","Chen Song","Xiaodong Gu","Yuan Dong","Qi Zuo","Weihao Yuan","Liefeng Bo","Zilong Dong","Qixing Huang"],"pdf_url":"https://arxiv.org/pdf/2403.15559v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01156v1","updated":"2024-08-02T10:16:28Z","published":"2024-08-02T10:16:28Z","title":"TCR-GPT: Integrating Autoregressive Model and Reinforcement Learning for\n  T-Cell Receptor Repertoires Generation","summary":"  T-cell receptors (TCRs) play a crucial role in the immune system by\nrecognizing and binding to specific antigens presented by infected or cancerous\ncells. Understanding the sequence patterns of TCRs is essential for developing\ntargeted immune therapies and designing effective vaccines. Language models,\nsuch as auto-regressive transformers, offer a powerful solution to this problem\nby learning the probability distributions of TCR repertoires, enabling the\ngeneration of new TCR sequences that inherit the underlying patterns of the\nrepertoire. We introduce TCR-GPT, a probabilistic model built on a decoder-only\ntransformer architecture, designed to uncover and replicate sequence patterns\nin TCR repertoires. TCR-GPT demonstrates an accuracy of 0.953 in inferring\nsequence probability distributions measured by Pearson correlation coefficient.\nFurthermore, by leveraging Reinforcement Learning(RL), we adapted the\ndistribution of TCR sequences to generate TCRs capable of recognizing specific\npeptides, offering significant potential for advancing targeted immune\ntherapies and vaccine development. With the efficacy of RL, fine-tuned\npretrained TCR-GPT models demonstrated the ability to produce TCR repertoires\nlikely to bind specific peptides, illustrating RL's efficiency in enhancing the\nmodel's adaptability to the probability distributions of biologically relevant\nTCR sequences.\n","authors":["Yicheng Lin","Dandan Zhang","Yun Liu"],"pdf_url":"https://arxiv.org/pdf/2408.01156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01154v1","updated":"2024-08-02T10:12:42Z","published":"2024-08-02T10:12:42Z","title":"DERA: Dense Entity Retrieval for Entity Alignment in Knowledge Graphs","summary":"  Entity Alignment (EA) aims to match equivalent entities in different\nKnowledge Graphs (KGs), which is essential for knowledge fusion and\nintegration. Recently, embedding-based EA has attracted significant attention\nand many approaches have been proposed. Early approaches primarily focus on\nlearning entity embeddings from the structural features of KGs, defined by\nrelation triples. Later methods incorporated entities' names and attributes as\nauxiliary information to enhance embeddings for EA. However, these approaches\noften used different techniques to encode structural and attribute information,\nlimiting their interaction and mutual enhancement. In this work, we propose a\ndense entity retrieval framework for EA, leveraging language models to\nuniformly encode various features of entities and facilitate nearest entity\nsearch across KGs. Alignment candidates are first generated through entity\nretrieval, which are subsequently reranked to determine the final alignments.\nWe conduct comprehensive experiments on both cross-lingual and monolingual EA\ndatasets, demonstrating that our approach achieves state-of-the-art performance\ncompared to existing EA methods.\n","authors":["Zhichun Wang","Xuan Chen"],"pdf_url":"https://arxiv.org/pdf/2408.01154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01139v1","updated":"2024-08-02T09:35:06Z","published":"2024-08-02T09:35:06Z","title":"Interpreting Global Perturbation Robustness of Image Models using\n  Axiomatic Spectral Importance Decomposition","summary":"  Perturbation robustness evaluates the vulnerabilities of models, arising from\na variety of perturbations, such as data corruptions and adversarial attacks.\nUnderstanding the mechanisms of perturbation robustness is critical for global\ninterpretability. We present a model-agnostic, global mechanistic\ninterpretability method to interpret the perturbation robustness of image\nmodels. This research is motivated by two key aspects. First, previous global\ninterpretability works, in tandem with robustness benchmarks, e.g. mean\ncorruption error (mCE), are not designed to directly interpret the mechanisms\nof perturbation robustness within image models. Second, we notice that the\nspectral signal-to-noise ratios (SNR) of perturbed natural images exponentially\ndecay over the frequency. This power-law-like decay implies that: Low-frequency\nsignals are generally more robust than high-frequency signals -- yet high\nclassification accuracy can not be achieved by low-frequency signals alone. By\napplying Shapley value theory, our method axiomatically quantifies the\npredictive powers of robust features and non-robust features within an\ninformation theory framework. Our method, dubbed as \\textbf{I-ASIDE}\n(\\textbf{I}mage \\textbf{A}xiomatic \\textbf{S}pectral \\textbf{I}mportance\n\\textbf{D}ecomposition \\textbf{E}xplanation), provides a unique insight into\nmodel robustness mechanisms. We conduct extensive experiments over a variety of\nvision models pre-trained on ImageNet to show that \\textbf{I-ASIDE} can not\nonly \\textbf{measure} the perturbation robustness but also \\textbf{provide\ninterpretations} of its mechanisms.\n","authors":["Róisín Luo","James McDermott","Colm O'Riordan"],"pdf_url":"https://arxiv.org/pdf/2408.01139v1.pdf","comment":"Accepted by Transactions on Machine Learning Research (TMLR 2024)"},{"id":"http://arxiv.org/abs/2405.13071v2","updated":"2024-08-02T09:30:03Z","published":"2024-05-20T20:55:07Z","title":"A Novel Method for News Article Event-Based Embedding","summary":"  Embedding news articles is a crucial tool for multiple fields, such as media\nbias detection, identifying fake news, and making news recommendations.\nHowever, existing news embedding methods are not optimized to capture the\nlatent context of news events. Most embedding methods rely on full-text\ninformation and neglect time-relevant embedding generation. In this paper, we\npropose a novel lightweight method that optimizes news embedding generation by\nfocusing on entities and themes mentioned in articles and their historical\nconnections to specific events. We suggest a method composed of three stages.\nFirst, we process and extract events, entities, and themes from the given news\narticles. Second, we generate periodic time embeddings for themes and entities\nby training time-separated GloVe models on current and historical data. Lastly,\nwe concatenate the news embeddings generated by two distinct approaches: Smooth\nInverse Frequency (SIF) for article-level vectors and Siamese Neural Networks\nfor embeddings with nuanced event-related information. We leveraged over\n850,000 news articles and 1,000,000 events from the GDELT project to test and\nevaluate our method. We conducted a comparative analysis of different news\nembedding generation methods for validation. Our experiments demonstrate that\nour approach can both improve and outperform state-of-the-art methods on shared\nevent detection tasks.\n","authors":["Koren Ishlach","Itzhak Ben-David","Michael Fire","Lior Rokach"],"pdf_url":"https://arxiv.org/pdf/2405.13071v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01129v1","updated":"2024-08-02T09:18:41Z","published":"2024-08-02T09:18:41Z","title":"A Survey of Mamba","summary":"  Deep learning, as a vital technique, has sparked a notable revolution in\nartificial intelligence. As the most representative architecture, Transformers\nhave empowered numerous advanced models, especially the large language models\nthat comprise billions of parameters, becoming a cornerstone in deep learning.\nDespite the impressive achievements, Transformers still face inherent\nlimitations, particularly the time-consuming inference resulting from the\nquadratic computation complexity of attention calculation. Recently, a novel\narchitecture named Mamba, drawing inspiration from classical state space\nmodels, has emerged as a promising alternative for building foundation models,\ndelivering comparable modeling abilities to Transformers while preserving\nnear-linear scalability concerning sequence length. This has sparked an\nincreasing number of studies actively exploring Mamba's potential to achieve\nimpressive performance across diverse domains. Given such rapid evolution,\nthere is a critical need for a systematic review that consolidates existing\nMamba-empowered models, offering a comprehensive understanding of this emerging\nmodel architecture. In this survey, we therefore conduct an in-depth\ninvestigation of recent Mamba-associated studies, covering from three main\naspects: the advancements of Mamba-based models, the techniques of adapting\nMamba to diverse data, and the applications where Mamba can excel.\nSpecifically, we first recall the foundational knowledge of various\nrepresentative deep learning models and the details of Mamba as preliminaries.\nThen, to showcase the significance of Mamba, we comprehensively review the\nrelated studies focusing on Mamba models' architecture design, data\nadaptability, and applications. Finally, we present an discussion of current\nlimitations and explore various promising research directions to provide deeper\ninsights for future investigations.\n","authors":["Haohao Qu","Liangbo Ning","Rui An","Wenqi Fan","Tyler Derr","Xin Xu","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2408.01129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19108v2","updated":"2024-08-02T09:10:54Z","published":"2024-06-27T11:34:35Z","title":"Computational Life: How Well-formed, Self-replicating Programs Emerge\n  from Simple Interaction","summary":"  The fields of Origin of Life and Artificial Life both question what life is\nand how it emerges from a distinct set of \"pre-life\" dynamics. One common\nfeature of most substrates where life emerges is a marked shift in dynamics\nwhen self-replication appears. While there are some hypotheses regarding how\nself-replicators arose in nature, we know very little about the general\ndynamics, computational principles, and necessary conditions for\nself-replicators to emerge. This is especially true on \"computational\nsubstrates\" where interactions involve logical, mathematical, or programming\nrules. In this paper we take a step towards understanding how self-replicators\narise by studying several computational substrates based on various simple\nprogramming languages and machine instruction sets. We show that when random,\nnon self-replicating programs are placed in an environment lacking any explicit\nfitness landscape, self-replicators tend to arise. We demonstrate how this\noccurs due to random interactions and self-modification, and can happen with\nand without background random mutations. We also show how increasingly complex\ndynamics continue to emerge following the rise of self-replicators. Finally, we\nshow a counterexample of a minimalistic programming language where\nself-replicators are possible, but so far have not been observed to arise.\n","authors":["Blaise Agüera y Arcas","Jyrki Alakuijala","James Evans","Ben Laurie","Alexander Mordvintsev","Eyvind Niklasson","Ettore Randazzo","Luca Versari"],"pdf_url":"https://arxiv.org/pdf/2406.19108v2.pdf","comment":"20 pages; updated introduction with further related work"},{"id":"http://arxiv.org/abs/2403.17247v3","updated":"2024-08-02T09:03:09Z","published":"2024-03-25T22:49:56Z","title":"DASA: Delay-Adaptive Multi-Agent Stochastic Approximation","summary":"  We consider a setting in which $N$ agents aim to speedup a common Stochastic\nApproximation (SA) problem by acting in parallel and communicating with a\ncentral server. We assume that the up-link transmissions to the server are\nsubject to asynchronous and potentially unbounded time-varying delays. To\nmitigate the effect of delays and stragglers while reaping the benefits of\ndistributed computation, we propose \\texttt{DASA}, a Delay-Adaptive algorithm\nfor multi-agent Stochastic Approximation. We provide a finite-time analysis of\n\\texttt{DASA} assuming that the agents' stochastic observation processes are\nindependent Markov chains. Significantly advancing existing results,\n\\texttt{DASA} is the first algorithm whose convergence rate depends only on the\nmixing time $\\tau_{mix}$ and on the average delay $\\tau_{avg}$ while jointly\nachieving an $N$-fold convergence speedup under Markovian sampling. Our work is\nrelevant for various SA applications, including multi-agent and distributed\ntemporal difference (TD) learning, Q-learning and stochastic optimization with\ncorrelated data.\n","authors":["Nicolò Dal Fabbro","Arman Adibi","H. Vincent Poor","Sanjeev R. Kulkarni","Aritra Mitra","George J. Pappas"],"pdf_url":"https://arxiv.org/pdf/2403.17247v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01121v1","updated":"2024-08-02T09:02:42Z","published":"2024-08-02T09:02:42Z","title":"Being Accountable is Smart: Navigating the Technical and Regulatory\n  Landscape of AI-based Services for Power Grid","summary":"  The emergence of artificial intelligence and digitization of the power grid\nintroduced numerous effective application scenarios for AI-based services for\nthe smart grid. Nevertheless, adopting AI in critical infrastructures presents\nchallenges due to unclear regulations and lacking risk quantification\ntechniques. Regulated and accountable approaches for integrating AI-based\nservices into the smart grid could accelerate the adoption of innovative\nmethods in daily practices and address society's general safety concerns. This\npaper contributes to this objective by defining accountability and highlighting\nits importance for AI-based services in the energy sector. It underlines the\ncurrent shortcomings of the AI Act and proposes an approach to address these\nissues in a potential delegated act. The proposed technical approach for\ndeveloping and operating accountable AI-based smart grid services allows for\nassessing different service life cycle phases and identifying related\naccountability risks.\n","authors":["Anna Volkova","Mahdieh Hatamian","Alina Anapyanova","Hermann de Meer"],"pdf_url":"https://arxiv.org/pdf/2408.01121v1.pdf","comment":"Author's version of the paper for International Conference on\n  Information Technology for Social Good (GoodIT '24), September 4--6, 2024,\n  Bremen, Germany. It is posted here for your personal use. Not for\n  redistribution"},{"id":"http://arxiv.org/abs/2408.01107v1","updated":"2024-08-02T08:37:03Z","published":"2024-08-02T08:37:03Z","title":"BioRAG: A RAG-LLM Framework for Biological Question Reasoning","summary":"  The question-answering system for Life science research, which is\ncharacterized by the rapid pace of discovery, evolving insights, and complex\ninteractions among knowledge entities, presents unique challenges in\nmaintaining a comprehensive knowledge warehouse and accurate information\nretrieval. To address these issues, we introduce BioRAG, a novel\nRetrieval-Augmented Generation (RAG) with the Large Language Models (LLMs)\nframework. Our approach starts with parsing, indexing, and segmenting an\nextensive collection of 22 million scientific papers as the basic knowledge,\nfollowed by training a specialized embedding model tailored to this domain.\nAdditionally, we enhance the vector retrieval process by incorporating a\ndomain-specific knowledge hierarchy, which aids in modeling the intricate\ninterrelationships among each query and context. For queries requiring the most\ncurrent information, BioRAG deconstructs the question and employs an iterative\nretrieval process incorporated with the search engine for step-by-step\nreasoning. Rigorous experiments have demonstrated that our model outperforms\nfine-tuned LLM, LLM with search engines, and other scientific RAG frameworks\nacross multiple life science question-answering tasks.\n","authors":["Chengrui Wang","Qingqing Long","Xiao Meng","Xunxin Cai","Chengjun Wu","Zhen Meng","Xuezhi Wang","Yuanchun Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.01107v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.00655v2","updated":"2024-08-02T08:27:08Z","published":"2024-08-01T15:45:19Z","title":"SentenceVAE: Faster, Longer and More Accurate Inference with\n  Next-sentence Prediction for Large Language Models","summary":"  Contemporary large language models (LLMs) primarily rely on next-token\nprediction method for inference, which significantly impedes their processing\nspeed. In this paper, we introduce a novel inference methodology termed\nnext-sentence prediction, aimed at enhancing the inference efficiency of LLMs.\nWe present Sentence Variational Autoencoder (SentenceVAE), a tiny model\nconsisting of a Sentence Encoder and a Sentence Decoder. The encoder\neffectively condenses the information within a sentence into a singular token,\nwhile the decoder reconstructs this compressed data back into its original\nsentential form. By integrating SentenceVAE into the input and output layers of\nLLMs, we develop Sentence-level LLMs (SLLMs) that employ a sentence-by-sentence\ninference approach, markedly accelerating inference speeds. SentenceVAE also\nmaintains the integrity of the original semantic content by segmenting the text\ninto sentences, thereby improving accuracy while boosting inference speeds.\nCompared to published LLMs, SLLMs process fewer tokens over equivalent context\nlengths, significantly reducing memory demands for self-attention computations\nand facilitating the handling of longer contexts. Our experimental findings\nreveal that this method can accelerate inference speeds by 204~365%, reduce\nperplexity (PPL) to 46~75% of its original metric, and decrease memory overhead\nby 86~91% for the same context length, compared to the token-by-token method.\nMoreover, the benefits of this approach become even more pronounced as model\nparameters increase.\n","authors":["Hongjun An","Yifan Chen","Xiaozhen Qiao","Zhe Sun","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.00655v2.pdf","comment":"Modified some of the expression details and optimized the charts"},{"id":"http://arxiv.org/abs/2408.01099v1","updated":"2024-08-02T08:24:05Z","published":"2024-08-02T08:24:05Z","title":"Contribution-based Low-Rank Adaptation with Pre-training Model for Real\n  Image Restoration","summary":"  Recently, pre-trained model and efficient parameter tuning have achieved\nremarkable success in natural language processing and high-level computer\nvision with the aid of masked modeling and prompt tuning. In low-level computer\nvision, however, there have been limited investigations on pre-trained models\nand even efficient fine-tuning strategy has not yet been explored despite its\nimportance and benefit in various real-world tasks such as alleviating memory\ninflation issue when integrating new tasks on AI edge devices. Here, we propose\na novel efficient parameter tuning approach dubbed contribution-based low-rank\nadaptation (CoLoRA) for multiple image restorations along with effective\npre-training method with random order degradations (PROD). Unlike prior arts\nthat tune all network parameters, our CoLoRA effectively fine-tunes small\namount of parameters by leveraging LoRA (low-rank adaptation) for each new\nvision task with our contribution-based method to adaptively determine layer by\nlayer capacity for that task to yield comparable performance to full tuning.\nFurthermore, our PROD strategy allows to extend the capability of pre-trained\nmodels with improved performance as well as robustness to bridge synthetic\npre-training and real-world fine-tuning. Our CoLoRA with PROD has demonstrated\nits superior performance in various image restoration tasks across diverse\ndegradation types on both synthetic and real-world datasets for known and novel\ntasks.\n","authors":["Donwon Park","Hayeon Kim","Se Young Chun"],"pdf_url":"https://arxiv.org/pdf/2408.01099v1.pdf","comment":"33 pages, 15 figures, for homepage see this url :\n  https://janeyeon.github.io/colora/"},{"id":"http://arxiv.org/abs/2408.01096v1","updated":"2024-08-02T08:16:55Z","published":"2024-08-02T08:16:55Z","title":"Six Dragons Fly Again: Reviving 15th-Century Korean Court Music with\n  Transformers and Novel Encoding","summary":"  We introduce a project that revives a piece of 15th-century Korean court\nmusic, Chihwapyeong and Chwipunghyeong, composed upon the poem Songs of the\nDragon Flying to Heaven. One of the earliest examples of Jeongganbo, a Korean\nmusical notation system, the remaining version only consists of a rudimentary\nmelody. Our research team, commissioned by the National Gugak (Korean\nTraditional Music) Center, aimed to transform this old melody into a\nperformable arrangement for a six-part ensemble. Using Jeongganbo data acquired\nthrough bespoke optical music recognition, we trained a BERT-like masked\nlanguage model and an encoder-decoder transformer model. We also propose an\nencoding scheme that strictly follows the structure of Jeongganbo and denotes\nnote durations as positions. The resulting machine-transformed version of\nChihwapyeong and Chwipunghyeong were evaluated by experts and performed by the\nCourt Music Orchestra of National Gugak Center. Our work demonstrates that\ngenerative models can successfully be applied to traditional music with limited\ntraining data if combined with careful design.\n","authors":["Danbinaerin Han","Mark Gotham","Dongmin Kim","Hannah Park","Sihun Lee","Dasaem Jeong"],"pdf_url":"https://arxiv.org/pdf/2408.01096v1.pdf","comment":"Accepted at the 25th International Society for Music Information\n  Retrieval Conference (ISMIR 2024)"},{"id":"http://arxiv.org/abs/2408.01091v1","updated":"2024-08-02T08:11:11Z","published":"2024-08-02T08:11:11Z","title":"Dissecting Dissonance: Benchmarking Large Multimodal Models Against\n  Self-Contradictory Instructions","summary":"  Large multimodal models (LMMs) excel in adhering to human instructions.\nHowever, self-contradictory instructions may arise due to the increasing trend\nof multimodal interaction and context length, which is challenging for language\nbeginners and vulnerable populations. We introduce the Self-Contradictory\nInstructions benchmark to evaluate the capability of LMMs in recognizing\nconflicting commands. It comprises 20,000 conflicts, evenly distributed between\nlanguage and vision paradigms. It is constructed by a novel automatic dataset\ncreation framework, which expedites the process and enables us to encompass a\nwide range of instruction forms. Our comprehensive evaluation reveals current\nLMMs consistently struggle to identify multimodal instruction discordance due\nto a lack of self-awareness. Hence, we propose the Cognitive Awakening\nPrompting to inject cognition from external, largely enhancing dissonance\ndetection. The dataset and code are here: https://selfcontradiction.github.io/.\n","authors":["Jin Gao","Lei Gan","Yuankai Li","Yixin Ye","Dequan Wang"],"pdf_url":"https://arxiv.org/pdf/2408.01091v1.pdf","comment":"Accepted by the 18th European Conference on Computer Vision ECCV 2024"},{"id":"http://arxiv.org/abs/2408.01075v1","updated":"2024-08-02T07:51:29Z","published":"2024-08-02T07:51:29Z","title":"The EAP-AIAS: Adapting the AI Assessment Scale for English for Academic\n  Purposes","summary":"  The rapid advancement of Generative Artificial Intelligence (GenAI) presents\nboth opportunities and challenges for English for Academic Purposes (EAP)\ninstruction. This paper proposes an adaptation of the AI Assessment Scale\n(AIAS) specifically tailored for EAP contexts, termed the EAP-AIAS.\n  This framework aims to provide a structured approach for integrating GenAI\ntools into EAP assessment practices while maintaining academic integrity and\nsupporting language development. The EAP-AIAS consists of five levels, ranging\nfrom \"No AI\" to \"Full AI\", each delineating appropriate GenAI usage in EAP\ntasks. We discuss the rationale behind this adaptation, considering the unique\nneeds of language learners and the dual focus of EAP on language proficiency\nand academic acculturation.\n  This paper explores potential applications of the EAP-AIAS across various EAP\nassessment types, including writing tasks, presentations, and research\nprojects. By offering a flexible framework, the EAP-AIAS seeks to empower EAP\npractitioners seeking to deal with the complexities of GenAI integration in\neducation and prepare students for an AI-enhanced academic and professional\nfuture. This adaptation represents a step towards addressing the pressing need\nfor ethical and pedagogically sound AI integration in language education.\n","authors":["Jasper Roe","Mike Perkins","Yulia Tregubova"],"pdf_url":"https://arxiv.org/pdf/2408.01075v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01072v1","updated":"2024-08-02T07:47:51Z","published":"2024-08-02T07:47:51Z","title":"A Survey on Self-play Methods in Reinforcement Learning","summary":"  Self-play, characterized by agents' interactions with copies or past versions\nof itself, has recently gained prominence in reinforcement learning. This paper\nfirst clarifies the preliminaries of self-play, including the multi-agent\nreinforcement learning framework and basic game theory concepts. Then it\nprovides a unified framework and classifies existing self-play algorithms\nwithin this framework. Moreover, the paper bridges the gap between the\nalgorithms and their practical implications by illustrating the role of\nself-play in different scenarios. Finally, the survey highlights open\nchallenges and future research directions in self-play. This paper is an\nessential guide map for understanding the multifaceted landscape of self-play\nin RL.\n","authors":["Ruize Zhang","Zelai Xu","Chengdong Ma","Chao Yu","Wei-Wei Tu","Shiyu Huang","Deheng Ye","Wenbo Ding","Yaodong Yang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2408.01072v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11660v2","updated":"2024-08-02T07:42:37Z","published":"2024-01-22T02:33:38Z","title":"Differentiable Tree Search Network","summary":"  In decision-making problems with limited training data, policy functions\napproximated using deep neural networks often exhibit suboptimal performance.\nAn alternative approach involves learning a world model from the limited data\nand determining actions through online search. However, the performance is\nadversely affected by compounding errors arising from inaccuracies in the\nlearned world model. While methods like TreeQN have attempted to address these\ninaccuracies by incorporating algorithmic inductive biases into the neural\nnetwork architectures, the biases they introduce are often weak and\ninsufficient for complex decision-making tasks. In this work, we introduce\nDifferentiable Tree Search Network (D-TSN), a novel neural network architecture\nthat significantly strengthens the inductive bias by embedding the algorithmic\nstructure of a best-first online search algorithm. D-TSN employs a learned\nworld model to conduct a fully differentiable online search. The world model is\njointly optimized with the search algorithm, enabling the learning of a robust\nworld model and mitigating the effect of prediction inaccuracies. Further, we\nnote that a naive incorporation of best-first search could lead to a\ndiscontinuous loss function in the parameter space. We address this issue by\nadopting a stochastic tree expansion policy, formulating search tree expansion\nas another decision-making task, and introducing an effective variance\nreduction technique for the gradient computation. We evaluate D-TSN in an\noffline-RL setting with a limited training data scenario on Procgen games and\ngrid navigation task, and demonstrate that D-TSN outperforms popular model-free\nand model-based baselines.\n","authors":["Dixant Mittal","Wee Sun Lee"],"pdf_url":"https://arxiv.org/pdf/2401.11660v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12631v2","updated":"2024-08-02T07:04:42Z","published":"2024-04-19T05:14:47Z","title":"Breaching the Bottleneck: Evolutionary Transition from Reward-Driven\n  Learning to Reward-Agnostic Domain-Adapted Learning in Neuromodulated Neural\n  Nets","summary":"  Advanced biological intelligence learns efficiently from an information-rich\nstream of stimulus information, even when feedback on behaviour quality is\nsparse or absent. Such learning exploits implicit assumptions about task\ndomains. We refer to such learning as Domain-Adapted Learning (DAL). In\ncontrast, AI learning algorithms rely on explicit externally provided measures\nof behaviour quality to acquire fit behaviour. This imposes an information\nbottleneck that precludes learning from diverse non-reward stimulus\ninformation, limiting learning efficiency. We consider the question of how\nbiological evolution circumvents this bottleneck to produce DAL. We propose\nthat species first evolve the ability to learn from reward signals, providing\ninefficient (bottlenecked) but broad adaptivity. From there, integration of\nnon-reward information into the learning process can proceed via gradual\naccumulation of biases induced by such information on specific task domains.\nThis scenario provides a biologically plausible pathway towards\nbottleneck-free, domain-adapted learning. Focusing on the second phase of this\nscenario, we set up a population of NNs with reward-driven learning modelled as\nReinforcement Learning (A2C), and allow evolution to improve learning\nefficiency by integrating non-reward information into the learning process\nusing a neuromodulatory update mechanism. On a navigation task in continuous 2D\nspace, evolved DAL agents show a 300-fold increase in learning speed compared\nto pure RL agents. Evolution is found to eliminate reliance on reward\ninformation altogether, allowing DAL agents to learn from non-reward\ninformation exclusively, using local neuromodulation-based connection weight\nupdates only. Code available at github.com/aislab/dal.\n","authors":["Solvi Arnold","Reiji Suzuki","Takaya Arita","Kimitoshi Yamazaki"],"pdf_url":"https://arxiv.org/pdf/2404.12631v2.pdf","comment":"Camera ready version. 9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.01055v1","updated":"2024-08-02T07:03:00Z","published":"2024-08-02T07:03:00Z","title":"LLM as Runtime Error Handler: A Promising Pathway to Adaptive\n  Self-Healing of Software Systems","summary":"  Unanticipated runtime errors, lacking predefined handlers, can abruptly\nterminate execution and lead to severe consequences, such as data loss or\nsystem crashes. Despite extensive efforts to identify potential errors during\nthe development phase, such unanticipated errors remain a challenge to to be\nentirely eliminated, making the runtime mitigation measurements still\nindispensable to minimize their impact. Automated self-healing techniques, such\nas reusing existing handlers, have been investigated to reduce the loss coming\nthrough with the execution termination. However, the usability of existing\nmethods is retained by their predefined heuristic rules and they fail to handle\ndiverse runtime errors adaptively. Recently, the advent of Large Language\nModels (LLMs) has opened new avenues for addressing this problem. Inspired by\ntheir remarkable capabilities in understanding and generating code, we propose\nto deal with the runtime errors in a real-time manner using LLMs.\n  Specifically, we propose Healer, the first LLM-assisted self-healing\nframework for handling runtime errors. When an unhandled runtime error occurs,\nHealer will be activated to generate a piece of error-handling code with the\nhelp of its internal LLM and the code will be executed inside the runtime\nenvironment owned by the framework to obtain a rectified program state from\nwhich the program should continue its execution. Our exploratory study\nevaluates the performance of Healer using four different code benchmarks and\nthree state-of-the-art LLMs, GPT-3.5, GPT-4, and CodeQwen-7B. Results show\nthat, without the need for any fine-tuning, GPT-4 can successfully help\nprograms recover from 72.8% of runtime errors, highlighting the potential of\nLLMs in handling runtime errors.\n","authors":["Zhensu Sun","Haotian Zhu","Bowen Xu","Xiaoning Du","Li Li","David Lo"],"pdf_url":"https://arxiv.org/pdf/2408.01055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01051v1","updated":"2024-08-02T06:57:52Z","published":"2024-08-02T06:57:52Z","title":"From Stem to Stern: Contestability Along AI Value Chains","summary":"  This workshop will grow and consolidate a community of interdisciplinary CSCW\nresearchers focusing on the topic of contestable AI. As an outcome of the\nworkshop, we will synthesize the most pressing opportunities and challenges for\ncontestability along AI value chains in the form of a research roadmap. This\nroadmap will help shape and inspire imminent work in this field. Considering\nthe length and depth of AI value chains, it will especially spur discussions\naround the contestability of AI systems along various sites of such chains. The\nworkshop will serve as a platform for dialogue and demonstrations of concrete,\nsuccessful, and unsuccessful examples of AI systems that (could or should) have\nbeen contested, to identify requirements, obstacles, and opportunities for\ndesigning and deploying contestable AI in various contexts. This will be held\nprimarily as an in-person workshop, with some hybrid accommodation. The day\nwill consist of individual presentations and group activities to stimulate\nideation and inspire broad reflections on the field of contestable AI. Our aim\nis to facilitate interdisciplinary dialogue by bringing together researchers,\npractitioners, and stakeholders to foster the design and deployment of\ncontestable AI.\n","authors":["Agathe Balayn","Yulu Pi","David Gray Widder","Kars Alfrink","Mireia Yurrita","Sohini Upadhyay","Naveena Karusala","Henrietta Lyons","Cagatay Turkay","Christelle Tessono","Blair Attard-Frost","Ujwal Gadiraju"],"pdf_url":"https://arxiv.org/pdf/2408.01051v1.pdf","comment":"5 pages, 0 figure, to be held as a workshop at CSCW'24"},{"id":"http://arxiv.org/abs/2408.01024v1","updated":"2024-08-02T05:50:31Z","published":"2024-08-02T05:50:31Z","title":"Semantic Skill Grounding for Embodied Instruction-Following in\n  Cross-Domain Environments","summary":"  In embodied instruction-following (EIF), the integration of pretrained\nlanguage models (LMs) as task planners emerges as a significant branch, where\ntasks are planned at the skill level by prompting LMs with pretrained skills\nand user instructions. However, grounding these pretrained skills in different\ndomains remains challenging due to their intricate entanglement with the\ndomain-specific knowledge. To address this challenge, we present a semantic\nskill grounding (SemGro) framework that leverages the hierarchical nature of\nsemantic skills. SemGro recognizes the broad spectrum of these skills, ranging\nfrom short-horizon low-semantic skills that are universally applicable across\ndomains to long-horizon rich-semantic skills that are highly specialized and\ntailored for particular domains. The framework employs an iterative skill\ndecomposition approach, starting from the higher levels of semantic skill\nhierarchy and then moving downwards, so as to ground each planned skill to an\nexecutable level within the target domain. To do so, we use the reasoning\ncapabilities of LMs for composing and decomposing semantic skills, as well as\ntheir multi-modal extension for assessing the skill feasibility in the target\ndomain. Our experiments in the VirtualHome benchmark show the efficacy of\nSemGro in 300 cross-domain EIF scenarios.\n","authors":["Sangwoo Shin","Seunghyun Kim","Youngsoo Jang","Moontae Lee","Honguk Woo"],"pdf_url":"https://arxiv.org/pdf/2408.01024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01018v1","updated":"2024-08-02T05:36:14Z","published":"2024-08-02T05:36:14Z","title":"GNN-MolKAN: Harnessing the Power of KAN to Advance Molecular\n  Representation Learning with GNNs","summary":"  Effective molecular representation learning is crucial for molecular property\nprediction and drug design. However, existing approaches struggle with\nlimitations in insufficient annotations and suboptimal architecture design. For\ninstance, Graph Neural Networks (GNNs) suffer from over-squashing, causing the\nloss of important structural details in molecules, thus impairing molecular\nrepresentations. In this work, we propose a new class of GNNs, GNN-MolKAN and\nits augmented variant, GNN-MolKAN+, that integrate the Kolmogorov-Arnold\nNetworks (KAN) architecture from AI + Science into GNNs to address these\nchallenges. Additionally, we introduce Adaptive FastKAN (AdFastKAN), an\nadvanced KAN that offers increased stability and speed, further enhancing the\nperformance of standard GNNs. Notably, our approach holds three key benefits:\n1) Superior Performance: GNN-MolKAN and GNN-MolKAN+ demonstrate superior\nprediction ability, robust generalization to unseen scaffolds, and versatile\ntransferability across different GNN architectures. 2) Efficiency: These models\nrequire less computational time and fewer parameters while matching or\nsurpassing the state-of-the-art (SOTA) self-supervised methods. 3) Few-shot\nLearning Ability: GNN-MolKAN demonstrates great potential in few-shot learning\nscenarios, achieving an average improvement of 6.97% across few-shot\nbenchmarks. Overall, we validate our architecture on 6 classification datasets,\n6 regression datasets, and 4 few-shot learning datasets, consistently achieving\nhighly competitive results across all of them.\n","authors":["Ruifeng Li"],"pdf_url":"https://arxiv.org/pdf/2408.01018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01016v1","updated":"2024-08-02T05:23:19Z","published":"2024-08-02T05:23:19Z","title":"IBB Traffic Graph Data: Benchmarking and Road Traffic Prediction Model","summary":"  Road traffic congestion prediction is a crucial component of intelligent\ntransportation systems, since it enables proactive traffic management, enhances\nsuburban experience, reduces environmental impact, and improves overall safety\nand efficiency. Although there are several public datasets, especially for\nmetropolitan areas, these datasets may not be applicable to practical scenarios\ndue to insufficiency in the scale of data (i.e. number of sensors and road\nlinks) and several external factors like different characteristics of the\ntarget area such as urban, highways and the data collection location. To\naddress this, this paper introduces a novel IBB Traffic graph dataset as an\nalternative benchmark dataset to mitigate these limitations and enrich the\nliterature with new geographical characteristics. IBB Traffic graph dataset\ncovers the sensor data collected at 2451 distinct locations. Moreover, we\npropose a novel Road Traffic Prediction Model that strengthens temporal links\nthrough feature engineering, node embedding with GLEE to represent\ninter-related relationships within the traffic network, and traffic prediction\nwith ExtraTrees. The results indicate that the proposed model consistently\noutperforms the baseline models, demonstrating an average accuracy improvement\nof 4%.\n","authors":["Eren Olug","Kiymet Kaya","Resul Tugay","Sule Gunduz Oguducu"],"pdf_url":"https://arxiv.org/pdf/2408.01016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01008v1","updated":"2024-08-02T04:45:58Z","published":"2024-08-02T04:45:58Z","title":"Tensor Train Low-rank Approximation (TT-LoRA): Democratizing AI with\n  Accelerated LLMs","summary":"  In recent years, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities across a wide range of natural language processing (NLP) tasks,\nsuch as question-answering, sentiment analysis, text summarization, and machine\ntranslation. However, the ever-growing complexity of LLMs demands immense\ncomputational resources, hindering the broader research and application of\nthese models. To address this, various parameter-efficient fine-tuning\nstrategies, such as Low-Rank Approximation (LoRA) and Adapters, have been\ndeveloped. Despite their potential, these methods often face limitations in\ncompressibility. Specifically, LoRA struggles to scale effectively with the\nincreasing number of trainable parameters in modern large scale LLMs.\nAdditionally, Low-Rank Economic Tensor-Train Adaptation (LoRETTA), which\nutilizes tensor train decomposition, has not yet achieved the level of\ncompression necessary for fine-tuning very large scale models with limited\nresources. This paper introduces Tensor Train Low-Rank Approximation (TT-LoRA),\na novel parameter-efficient fine-tuning (PEFT) approach that extends LoRETTA\nwith optimized tensor train (TT) decomposition integration. By eliminating\nAdapters and traditional LoRA-based structures, TT-LoRA achieves greater model\ncompression without compromising downstream task performance, along with\nreduced inference latency and computational overhead. We conduct an exhaustive\nparameter search to establish benchmarks that highlight the trade-off between\nmodel compression and performance. Our results demonstrate significant\ncompression of LLMs while maintaining comparable performance to larger models,\nfacilitating their deployment on resource-constraint platforms.\n","authors":["Afia Anjum","Maksim E. Eren","Ismael Boureima","Boian Alexandrov","Manish Bhattarai"],"pdf_url":"https://arxiv.org/pdf/2408.01008v1.pdf","comment":"LA-UR-24-28177"},{"id":"http://arxiv.org/abs/2403.02504v3","updated":"2024-08-02T04:44:29Z","published":"2024-03-04T21:51:11Z","title":"A Tutorial on the Pretrain-Finetune Paradigm for Natural Language\n  Processing","summary":"  Given that natural language serves as the primary conduit for expressing\nthoughts and emotions, text analysis has become a key technique in\npsychological research. It enables the extraction of valuable insights from\nnatural language, facilitating endeavors like personality traits assessment,\nmental health monitoring, and sentiment analysis in interpersonal\ncommunications. In text analysis, existing studies often resort to either human\ncoding, which is time-consuming, using pre-built dictionaries, which often\nfails to cover all possible scenarios, or training models from scratch, which\nrequires large amounts of labeled data. In this tutorial, we introduce the\npretrain-finetune paradigm. The pretrain-finetune paradigm represents a\ntransformative approach in text analysis and natural language processing. This\nparadigm distinguishes itself through the use of large pretrained language\nmodels, demonstrating remarkable efficiency in finetuning tasks, even with\nlimited training data. This efficiency is especially beneficial for research in\nsocial sciences, where the number of annotated samples is often quite limited.\nOur tutorial offers a comprehensive introduction to the pretrain-finetune\nparadigm. We first delve into the fundamental concepts of pretraining and\nfinetuning, followed by practical exercises using real-world applications. We\ndemonstrate the application of the paradigm across various tasks, including\nmulti-class classification and regression. Emphasizing its efficacy and\nuser-friendliness, the tutorial aims to encourage broader adoption of this\nparadigm. To this end, we have provided open access to all our code and\ndatasets. The tutorial is highly beneficial across various psychology\ndisciplines, providing a comprehensive guide to employing text analysis in\ndiverse research settings.\n","authors":["Yu Wang","Wen Qu"],"pdf_url":"https://arxiv.org/pdf/2403.02504v3.pdf","comment":"29 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2408.01003v1","updated":"2024-08-02T04:34:37Z","published":"2024-08-02T04:34:37Z","title":"Piculet: Specialized Models-Guided Hallucination Decrease for MultiModal\n  Large Language Models","summary":"  Multimodal Large Language Models (MLLMs) have made significant progress in\nbridging the gap between visual and language modalities. However,\nhallucinations in MLLMs, where the generated text does not align with image\ncontent, continue to be a major challenge. Existing methods for addressing\nhallucinations often rely on instruction-tuning, which requires retraining the\nmodel with specific data, which increases the cost of utilizing MLLMs further.\nIn this paper, we introduce a novel training-free method, named Piculet, for\nenhancing the input representation of MLLMs. Piculet leverages multiple\nspecialized models to extract descriptions of visual information from the input\nimage and combine these descriptions with the original image and query as input\nto the MLLM. We evaluate our method both quantitively and qualitatively, and\nthe results demonstrate that Piculet greatly decreases hallucinations of MLLMs.\nOur method can be easily extended to different MLLMs while being universal.\n","authors":["Kohou Wang","Xiang Liu","Zhaoxiang Liu","Kai Wang","Shiguo Lian"],"pdf_url":"https://arxiv.org/pdf/2408.01003v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.00998v1","updated":"2024-08-02T04:13:38Z","published":"2024-08-02T04:13:38Z","title":"FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features\n  for Highly Controllable Text-Driven Image Translation","summary":"  Large-scale text-to-image diffusion models have been a revolutionary\nmilestone in the evolution of generative AI and multimodal technology, allowing\nextraordinary image generation based on natural-language text prompts. However,\nthe issue of lacking controllability of such models restricts their practical\napplicability for real-life content creation, for which attention has been\nfocused on leveraging a reference image to control text-to-image synthesis. Due\nto the close correlation between the reference image and the generated image,\nthis problem can also be regarded as the task of manipulating (or editing) the\nreference image as per the text, namely text-driven image-to-image translation.\nThis paper contributes a novel, concise, and efficient approach that adapts the\npre-trained large-scale text-to-image (T2I) diffusion model to the\nimage-to-image (I2I) paradigm in a plug-and-play manner, realizing high-quality\nand versatile text-driven I2I translation without any model training, model\nfine-tuning, or online optimization process. To guide T2I generation with a\nreference image, we propose to model diverse guiding factors with\ncorrespondingly different frequency bands of diffusion features in the DCT\nspectral space, and accordingly devise a novel frequency band substitution\nlayer that dynamically substitutes a certain DCT frequency band of the\ndiffusion features with the corresponding counterpart of the reference image\nalong the reverse sampling process. We demonstrate that our method flexibly\nenables highly controllable text-driven I2I translation both in the guiding\nfactor and guiding intensity of the reference image, simply by tuning the type\nand bandwidth of the substituted frequency band, respectively. Extensive\nqualitative and quantitative experiments verify the superiority of our approach\nover related methods in I2I translation visual quality, versatility, and\ncontrollability.\n","authors":["Xiang Gao","Jiaying Liu"],"pdf_url":"https://arxiv.org/pdf/2408.00998v1.pdf","comment":"Accepted conference paper of ACM MM 2024"},{"id":"http://arxiv.org/abs/2408.00997v1","updated":"2024-08-02T04:09:30Z","published":"2024-08-02T04:09:30Z","title":"A Safe Exploration Strategy for Model-free Task Adaptation in\n  Safety-constrained Grid Environments","summary":"  Training a model-free reinforcement learning agent requires allowing the\nagent to sufficiently explore the environment to search for an optimal policy.\nIn safety-constrained environments, utilizing unsupervised exploration or a\nnon-optimal policy may lead the agent to undesirable states, resulting in\noutcomes that are potentially costly or hazardous for both the agent and the\nenvironment. In this paper, we introduce a new exploration framework for\nnavigating the grid environments that enables model-free agents to interact\nwith the environment while adhering to safety constraints. Our framework\nincludes a pre-training phase, during which the agent learns to identify\npotentially unsafe states based on both observable features and specified\nsafety constraints in the environment. Subsequently, a binary classification\nmodel is trained to predict those unsafe states in new environments that\nexhibit similar dynamics. This trained classifier empowers model-free agents to\ndetermine situations in which employing random exploration or a suboptimal\npolicy may pose safety risks, in which case our framework prompts the agent to\nfollow a predefined safe policy to mitigate the potential for hazardous\nconsequences. We evaluated our framework on three randomly generated grid\nenvironments and demonstrated how model-free agents can safely adapt to new\ntasks and learn optimal policies for new environments. Our results indicate\nthat by defining an appropriate safe policy and utilizing a well-trained model\nto detect unsafe states, our framework enables a model-free agent to adapt to\nnew tasks and environments with significantly fewer safety violations.\n","authors":["Erfan Entezami","Mahsa Sahebdel","Dhawal Gupta"],"pdf_url":"https://arxiv.org/pdf/2408.00997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00996v1","updated":"2024-08-02T04:09:15Z","published":"2024-08-02T04:09:15Z","title":"IncidentNet: Traffic Incident Detection, Localization and Severity\n  Estimation with Sparse Sensing","summary":"  Prior art in traffic incident detection relies on high sensor coverage and is\nprimarily based on decision-tree and random forest models that have limited\nrepresentation capacity and, as a result, cannot detect incidents with high\naccuracy. This paper presents IncidentNet - a novel approach for classifying,\nlocalizing, and estimating the severity of traffic incidents using deep\nlearning models trained on data captured from sparsely placed sensors in urban\nenvironments. Our model works on microscopic traffic data that can be collected\nusing cameras installed at traffic intersections. Due to the unavailability of\ndatasets that provide microscopic traffic details and traffic incident details\nsimultaneously, we also present a methodology to generate a synthetic\nmicroscopic traffic dataset that matches given macroscopic traffic data.\nIncidentNet achieves a traffic incident detection rate of 98%, with false alarm\nrates of less than 7% in 197 seconds on average in urban environments with\ncameras on less than 20% of the traffic intersections.\n","authors":["Sai Shashank Peddiraju","Kaustubh Harapanahalli","Edward Andert","Aviral Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2408.00996v1.pdf","comment":"6 pages, 6 figures, 2024 IEEE 27th International Conference on\n  Intelligent Transportation Systems (ITSC)"},{"id":"http://arxiv.org/abs/2312.01700v3","updated":"2024-08-02T03:56:35Z","published":"2023-12-04T07:42:16Z","title":"Data Management For Training Large Language Models: A Survey","summary":"  Data plays a fundamental role in training Large Language Models (LLMs).\nEfficient data management, particularly in formulating a well-suited training\ndataset, is significant for enhancing model performance and improving training\nefficiency during pretraining and supervised fine-tuning stages. Despite the\nconsiderable importance of data management, the underlying mechanism of current\nprominent practices are still unknown. Consequently, the exploration of data\nmanagement has attracted more and more attention among the research community.\nThis survey aims to provide a comprehensive overview of current research in\ndata management within both the pretraining and supervised fine-tuning stages\nof LLMs, covering various aspects of data management strategy design. Looking\ninto the future, we extrapolate existing challenges and outline promising\ndirections for development in this field. Therefore, this survey serves as a\nguiding resource for practitioners aspiring to construct powerful LLMs through\nefficient data management practices. The collection of the latest papers is\navailable at https://github.com/ZigeW/data_management_LLM.\n","authors":["Zige Wang","Wanjun Zhong","Yufei Wang","Qi Zhu","Fei Mi","Baojun Wang","Lifeng Shang","Xin Jiang","Qun Liu"],"pdf_url":"https://arxiv.org/pdf/2312.01700v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2408.00994v1","updated":"2024-08-02T03:54:36Z","published":"2024-08-02T03:54:36Z","title":"ArchCode: Incorporating Software Requirements in Code Generation with\n  Large Language Models","summary":"  This paper aims to extend the code generation capability of large language\nmodels (LLMs) to automatically manage comprehensive software requirements from\ngiven textual descriptions. Such requirements include both functional (i.e.\nachieving expected behavior for inputs) and non-functional (e.g., time/space\nperformance, robustness, maintainability) requirements. However, textual\ndescriptions can either express requirements verbosely or may even omit some of\nthem. We introduce ARCHCODE, a novel framework that leverages in-context\nlearning to organize requirements observed in descriptions and to extrapolate\nunexpressed requirements from them. ARCHCODE generates requirements from given\ndescriptions, conditioning them to produce code snippets and test cases. Each\ntest case is tailored to one of the requirements, allowing for the ranking of\ncode snippets based on the compliance of their execution results with the\nrequirements. Public benchmarks show that ARCHCODE enhances to satisfy\nfunctional requirements, significantly improving Pass@k scores. Furthermore, we\nintroduce HumanEval-NFR, the first evaluation of LLMs' non-functional\nrequirements in code generation, demonstrating ARCHCODE's superiority over\nbaseline methods. The implementation of ARCHCODE and the HumanEval-NFR\nbenchmark are both publicly accessible.\n","authors":["Hojae Han","Jaejin Kim","Jaeseok Yoo","Youngwon Lee","Seung-won Hwang"],"pdf_url":"https://arxiv.org/pdf/2408.00994v1.pdf","comment":"Accepted by ACL 2024 main conference"},{"id":"http://arxiv.org/abs/2405.15077v3","updated":"2024-08-02T03:38:58Z","published":"2024-05-23T21:56:12Z","title":"Eliciting Informative Text Evaluations with Large Language Models","summary":"  Peer prediction mechanisms motivate high-quality feedback with provable\nguarantees. However, current methods only apply to rather simple reports, like\nmultiple-choice or scalar numbers. We aim to broaden these techniques to the\nlarger domain of text-based reports, drawing on the recent developments in\nlarge language models. This vastly increases the applicability of peer\nprediction mechanisms as textual feedback is the norm in a large variety of\nfeedback channels: peer reviews, e-commerce customer reviews, and comments on\nsocial media.\n  We introduce two mechanisms, the Generative Peer Prediction Mechanism (GPPM)\nand the Generative Synopsis Peer Prediction Mechanism (GSPPM). These mechanisms\nutilize LLMs as predictors, mapping from one agent's report to a prediction of\nher peer's report. Theoretically, we show that when the LLM prediction is\nsufficiently accurate, our mechanisms can incentivize high effort and\ntruth-telling as an (approximate) Bayesian Nash equilibrium. Empirically, we\nconfirm the efficacy of our mechanisms through experiments conducted on two\nreal datasets: the Yelp review dataset and the ICLR OpenReview dataset. We\nhighlight the results that on the ICLR dataset, our mechanisms can\ndifferentiate three quality levels -- human-written reviews, GPT-4-generated\nreviews, and GPT-3.5-generated reviews in terms of expected scores.\nAdditionally, GSPPM penalizes LLM-generated reviews more effectively than GPPM.\n","authors":["Yuxuan Lu","Shengwei Xu","Yichi Zhang","Yuqing Kong","Grant Schoenebeck"],"pdf_url":"https://arxiv.org/pdf/2405.15077v3.pdf","comment":"Accepted by the Twenty-Fifth ACM Conference on Economics and\n  Computation (EC'24)"},{"id":"http://arxiv.org/abs/2408.00989v1","updated":"2024-08-02T03:25:20Z","published":"2024-08-02T03:25:20Z","title":"On the Resilience of Multi-Agent Systems with Malicious Agents","summary":"  Multi-agent systems, powered by large language models, have shown great\nabilities across various tasks due to the collaboration of expert agents, each\nfocusing on a specific domain. However, when agents are deployed separately,\nthere is a risk that malicious users may introduce malicious agents who\ngenerate incorrect or irrelevant results that are too stealthy to be identified\nby other non-specialized agents. Therefore, this paper investigates two\nessential questions: (1) What is the resilience of various multi-agent system\nstructures (e.g., A$\\rightarrow$B$\\rightarrow$C,\nA$\\leftrightarrow$B$\\leftrightarrow$C) under malicious agents, on different\ndownstream tasks? (2) How can we increase system resilience to defend against\nmalicious agents? To simulate malicious agents, we devise two methods,\nAutoTransform and AutoInject, to transform any agent into a malicious one while\npreserving its functional integrity. We run comprehensive experiments on four\ndownstream multi-agent systems tasks, namely code generation, math problems,\ntranslation, and text evaluation. Results suggest that the \"hierarchical\"\nmulti-agent structure, i.e., A$\\rightarrow$(B$\\leftrightarrow$C), exhibits\nsuperior resilience with the lowest performance drop of $23.6\\%$, compared to\n$46.4\\%$ and $49.8\\%$ of other two structures. Additionally, we show the\npromise of improving multi-agent system resilience by demonstrating that two\ndefense methods, introducing an additional agent to review and correct messages\nor mechanisms for each agent to challenge others' outputs, can enhance system\nresilience. Our code and data are available at\nhttps://github.com/CUHK-ARISE/MAS-Resilience.\n","authors":["Jen-tse Huang","Jiaxu Zhou","Tailin Jin","Xuhui Zhou","Zixi Chen","Wenxuan Wang","Youliang Yuan","Maarten Sap","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2408.00989v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2407.11046v2","updated":"2024-08-02T03:22:22Z","published":"2024-07-08T12:32:10Z","title":"A Survey on LoRA of Large Language Models","summary":"  Low-Rank Adaptation~(LoRA), which updates the dense neural network layers\nwith pluggable low-rank matrices, is one of the best performed parameter\nefficient fine-tuning paradigms. Furthermore, it has significant advantages in\ncross-task generalization and privacy-preserving. Hence, LoRA has gained much\nattention recently, and the number of related literature demonstrates\nexponential growth. It is necessary to conduct a comprehensive overview of the\ncurrent progress on LoRA. This survey categorizes and reviews the progress from\nthe perspectives of (1) downstream adaptation improving variants that improve\nLoRA's performance on downstream tasks; (2) cross-task generalization methods\nthat mix multiple LoRA plugins to achieve cross-task generalization; (3)\nefficiency-improving methods that boost the computation-efficiency of LoRA; (4)\ndata privacy-preserving methods that use LoRA in federated learning; (5)\napplication. Besides, this survey also discusses the future directions in this\nfield. At last, we provide a Github page\n(https://github.com/ZJU-LLMs/Awesome-LoRAs.git) for readers to check the\nupdates and initiate discussions on this survey paper.\n","authors":["Yuren Mao","Yuhang Ge","Yijiang Fan","Wenyi Xu","Yu Mi","Zhonghao Hu","Yunjun Gao"],"pdf_url":"https://arxiv.org/pdf/2407.11046v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00986v1","updated":"2024-08-02T03:06:51Z","published":"2024-08-02T03:06:51Z","title":"A SAT-based approach to rigorous verification of Bayesian networks","summary":"  Recent advancements in machine learning have accelerated its widespread\nadoption across various real-world applications. However, in safety-critical\ndomains, the deployment of machine learning models is riddled with challenges\ndue to their complexity, lack of interpretability, and absence of formal\nguarantees regarding their behavior. In this paper, we introduce a verification\nframework tailored for Bayesian networks, designed to address these drawbacks.\nOur framework comprises two key components: (1) a two-step compilation and\nencoding scheme that translates Bayesian networks into Boolean logic literals,\nand (2) formal verification queries that leverage these literals to verify\nvarious properties encoded as constraints. Specifically, we introduce two\nverification queries: if-then rules (ITR) and feature monotonicity (FMO). We\nbenchmark the efficiency of our verification scheme and demonstrate its\npractical utility in real-world scenarios.\n","authors":["Ignacy Stępka","Nicholas Gisolfi","Artur Dubrawski"],"pdf_url":"https://arxiv.org/pdf/2408.00986v1.pdf","comment":"Workshop on Explainable and Robust AI for Industry 4.0 & 5.0 (X-RAI)\n  at European Conference on Machine Learning and Principles and Practice of\n  Knowledge Discovery in Databases (2024)"},{"id":"http://arxiv.org/abs/2308.14250v3","updated":"2024-08-02T01:38:16Z","published":"2023-08-28T01:57:38Z","title":"Rule-Based Error Detection and Correction to Operationalize Movement\n  Trajectory Classification","summary":"  Classification of movement trajectories has many applications in\ntransportation and is a key component for large-scale movement trajectory\ngeneration and anomaly detection which has key safety applications in the\naftermath of a disaster or other external shock. However, the current\nstate-of-the-art (SOTA) are based on supervised deep learning - which leads to\nchallenges when the distribution of trajectories changes due to such a shock.\nWe provide a neuro-symbolic rule-based framework to conduct error correction\nand detection of these models to integrate into our movement trajectory\nplatform. We provide a suite of experiments on several recent SOTA models where\nwe show highly accurate error detection, the ability to improve accuracy with a\nchanging test distribution, and accuracy improvement for the base use case in\naddition to a suite of theoretical properties that informed algorithm\ndevelopment. Specifically, we show an F1 scores for predicting errors of up to\n0.984, significant performance increase for out-of distribution accuracy (8.51%\nimprovement over SOTA for zero-shot accuracy), and accuracy improvement over\nthe SOTA model.\n","authors":["Bowen Xi","Kevin Scaria","Divyagna Bavikadi","Paulo Shakarian"],"pdf_url":"https://arxiv.org/pdf/2308.14250v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.09786v5","updated":"2024-08-02T01:22:46Z","published":"2024-01-18T08:10:34Z","title":"Adaptive Self-training Framework for Fine-grained Scene Graph Generation","summary":"  Scene graph generation (SGG) models have suffered from inherent problems\nregarding the benchmark datasets such as the long-tailed predicate distribution\nand missing annotation problems. In this work, we aim to alleviate the\nlong-tailed problem of SGG by utilizing unannotated triplets. To this end, we\nintroduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels\nfor unannotated triplets based on which the SGG models are trained. While there\nhas been significant progress in self-training for image recognition, designing\na self-training framework for the SGG task is more challenging due to its\ninherent nature such as the semantic ambiguity and the long-tailed distribution\nof predicate classes. Hence, we propose a novel pseudo-labeling technique for\nSGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is\na model-agnostic framework that can be applied to any existing SGG models.\nFurthermore, we devise a graph structure learner (GSL) that is beneficial when\nadopting our proposed self-training framework to the state-of-the-art\nmessage-passing neural network (MPNN)-based SGG models. Our extensive\nexperiments verify the effectiveness of ST-SGG on various SGG models,\nparticularly in enhancing the performance on fine-grained predicate classes.\n","authors":["Kibum Kim","Kanghoon Yoon","Yeonjun In","Jinyoung Moon","Donghyun Kim","Chanyoung Park"],"pdf_url":"https://arxiv.org/pdf/2401.09786v5.pdf","comment":"9 pages; ICLR 2024"},{"id":"http://arxiv.org/abs/2408.00965v1","updated":"2024-08-02T00:58:01Z","published":"2024-08-02T00:58:01Z","title":"Integrating ESG and AI: A Comprehensive Responsible AI Assessment\n  Framework","summary":"  Artificial Intelligence (AI) is a widely developed and adopted technology\nacross entire industry sectors. Integrating environmental, social, and\ngovernance (ESG) considerations with AI investments is crucial for ensuring\nethical and sustainable technological advancement. Particularly from an\ninvestor perspective, this integration not only mitigates risks but also\nenhances long-term value creation by aligning AI initiatives with broader\nsocietal goals. Yet, this area has been less explored in both academia and\nindustry. To bridge the gap, we introduce a novel ESG-AI framework, which is\ndeveloped based on insights from engagements with 28 companies and comprises\nthree key components. The framework provides a structured approach to this\nintegration, developed in collaboration with industry practitioners. The ESG-AI\nframework provides an overview of the environmental and social impacts of AI\napplications, helping users such as investors assess the materiality of AI use.\nMoreover, it enables investors to evaluate a company's commitment to\nresponsible AI through structured engagements and thorough assessment of\nspecific risk areas. We have publicly released the framework and toolkit in\nApril 2024, which has received significant attention and positive feedback from\nthe investment community. This paper details each component of the framework,\ndemonstrating its applicability in real-world contexts and its potential to\nguide ethical AI investments.\n","authors":["Sung Une Lee","Harsha Perera","Yue Liu","Boming Xia","Qinghua Lu","Liming Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.00965v1.pdf","comment":"23 pages, 8 tables, 10 figures"},{"id":"http://arxiv.org/abs/2408.00960v1","updated":"2024-08-02T00:24:22Z","published":"2024-08-02T00:24:22Z","title":"PERSOMA: PERsonalized SOft ProMpt Adapter Architecture for Personalized\n  Language Prompting","summary":"  Understanding the nuances of a user's extensive interaction history is key to\nbuilding accurate and personalized natural language systems that can adapt to\nevolving user preferences. To address this, we introduce PERSOMA, Personalized\nSoft Prompt Adapter architecture. Unlike previous personalized prompting\nmethods for large language models, PERSOMA offers a novel approach to\nefficiently capture user history. It achieves this by resampling and\ncompressing interactions as free form text into expressive soft prompt\nembeddings, building upon recent research utilizing embedding representations\nas input for LLMs. We rigorously validate our approach by evaluating various\nadapter architectures, first-stage sampling strategies, parameter-efficient\ntuning techniques like LoRA, and other personalization methods. Our results\ndemonstrate PERSOMA's superior ability to handle large and complex user\nhistories compared to existing embedding-based and text-prompt-based\ntechniques.\n","authors":["Liam Hebert","Krishna Sayana","Ambarish Jash","Alexandros Karatzoglou","Sukhdeep Sodhi","Sumanth Doddapaneni","Yanli Cai","Dima Kuzmin"],"pdf_url":"https://arxiv.org/pdf/2408.00960v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16522v4","updated":"2024-08-02T00:21:41Z","published":"2024-05-26T11:17:49Z","title":"Multi-State TD Target for Model-Free Reinforcement Learning","summary":"  Temporal difference (TD) learning is a fundamental technique in reinforcement\nlearning that updates value estimates for states or state-action pairs using a\nTD target. This target represents an improved estimate of the true value by\nincorporating both immediate rewards and the estimated value of subsequent\nstates. Traditionally, TD learning relies on the value of a single subsequent\nstate. We propose an enhanced multi-state TD (MSTD) target that utilizes the\nestimated values of multiple subsequent states. Building on this new MSTD\nconcept, we develop complete actor-critic algorithms that include management of\nreplay buffers in two modes, and integrate with deep deterministic policy\noptimization (DDPG) and soft actor-critic (SAC). Experimental results\ndemonstrate that algorithms employing the MSTD target significantly improve\nlearning performance compared to traditional methods.The code is provided on\nGitHub.\n","authors":["Wuhao Wang","Zhiyong Chen","Lepeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.16522v4.pdf","comment":"8 pages, 16 figures"}]},"2024-08-05T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2408.02662v1","updated":"2024-08-05T17:55:23Z","published":"2024-08-05T17:55:23Z","title":"Integrating Model-Based Footstep Planning with Model-Free Reinforcement\n  Learning for Dynamic Legged Locomotion","summary":"  In this work, we introduce a control framework that combines model-based\nfootstep planning with Reinforcement Learning (RL), leveraging desired footstep\npatterns derived from the Linear Inverted Pendulum (LIP) dynamics. Utilizing\nthe LIP model, our method forward predicts robot states and determines the\ndesired foot placement given the velocity commands. We then train an RL policy\nto track the foot placements without following the full reference motions\nderived from the LIP model. This partial guidance from the physics model allows\nthe RL policy to integrate the predictive capabilities of the physics-informed\ndynamics and the adaptability characteristics of the RL controller without\noverfitting the policy to the template model. Our approach is validated on the\nMIT Humanoid, demonstrating that our policy can achieve stable yet dynamic\nlocomotion for walking and turning. We further validate the adaptability and\ngeneralizability of our policy by extending the locomotion task to unseen,\nuneven terrain. During the hardware deployment, we have achieved forward\nwalking speeds of up to 1.5 m/s on a treadmill and have successfully performed\ndynamic locomotion maneuvers such as 90-degree and 180-degree turns.\n","authors":["Ho Jae Lee","Seungwoo Hong","Sangbae Kim"],"pdf_url":"https://arxiv.org/pdf/2408.02662v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2408.02661v1","updated":"2024-08-05T17:55:20Z","published":"2024-08-05T17:55:20Z","title":"Context-aware Mamba-based Reinforcement Learning for social robot\n  navigation","summary":"  Social robot navigation (SRN) is a relevant problem that involves navigating\na pedestrian-rich environment in a socially acceptable manner. It is an\nessential part of making social robots effective in pedestrian-rich settings.\nThe use cases of such robots could vary from companion robots to warehouse\nrobots to autonomous wheelchairs. In recent years, deep reinforcement learning\nhas been increasingly used in research on social robot navigation. Our work\nintroduces CAMRL (Context-Aware Mamba-based Reinforcement Learning). Mamba is a\nnew deep learning-based State Space Model (SSM) that has achieved results\ncomparable to transformers in sequencing tasks. CAMRL uses Mamba to determine\nthe robot's next action, which maximizes the value of the next state predicted\nby the neural network, enabling the robot to navigate effectively based on the\nrewards assigned. We evaluate CAMRL alongside existing solutions (CADRL,\nLSTM-RL, SARL) using a rigorous testing dataset which involves a variety of\ndensities and environment behaviors based on ORCA and SFM, thus, demonstrating\nthat CAMRL achieves higher success rates, minimizes collisions, and maintains\nsafer distances from pedestrians. This work introduces a new SRN planner,\nshowcasing the potential for deep-state space models for robot navigation.\n","authors":["Syed Muhammad Mustafa","Omema Rizvi","Zain Ahmed Usmani","Abdul Basit Memon"],"pdf_url":"https://arxiv.org/pdf/2408.02661v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06406v2","updated":"2024-08-05T17:00:00Z","published":"2023-12-11T14:27:10Z","title":"Partial End-to-end Reinforcement Learning for Robustness Against\n  Modelling Error in Autonomous Racing","summary":"  In this paper, we address the issue of increasing the performance of\nreinforcement learning (RL) solutions for autonomous racing cars when\nnavigating under conditions where practical vehicle modelling errors (commonly\nknown as \\emph{model mismatches}) are present. To address this challenge, we\npropose a partial end-to-end algorithm that decouples the planning and control\ntasks. Within this framework, an RL agent generates a trajectory comprising a\npath and velocity, which is subsequently tracked using a pure pursuit steering\ncontroller and a proportional velocity controller, respectively. In contrast,\nmany current learning-based (i.e., reinforcement and imitation learning)\nalgorithms utilise an end-to-end approach whereby a deep neural network\ndirectly maps from sensor data to control commands. By leveraging the\nrobustness of a classical controller, our partial end-to-end driving algorithm\nexhibits better robustness towards model mismatches than standard end-to-end\nalgorithms.\n","authors":["Andrew Murdoch","Johannes Cornelius Schoeman","Hendrik Willem Jordaan"],"pdf_url":"https://arxiv.org/pdf/2312.06406v2.pdf","comment":"Submitted to IEEE Transactions on Intelligent Transport Systems"},{"id":"http://arxiv.org/abs/2312.02396v3","updated":"2024-08-05T16:49:51Z","published":"2023-12-04T23:26:12Z","title":"Unsupervised Change Detection for Space Habitats Using 3D Point Clouds","summary":"  This work presents an algorithm for scene change detection from point clouds\nto enable autonomous robotic caretaking in future space habitats. Autonomous\nrobotic systems will help maintain future deep-space habitats, such as the\nGateway space station, which will be uncrewed for extended periods. Existing\nscene analysis software used on the International Space Station (ISS) relies on\nmanually-labeled images for detecting changes. In contrast, the algorithm\npresented in this work uses raw, unlabeled point clouds as inputs. The\nalgorithm first applies modified Expectation-Maximization Gaussian Mixture\nModel (GMM) clustering to two input point clouds. It then performs change\ndetection by comparing the GMMs using the Earth Mover's Distance. The algorithm\nis validated quantitatively and qualitatively using a test dataset collected by\nan Astrobee robot in the NASA Ames Granite Lab comprising single frame depth\nimages taken directly by Astrobee and full-scene reconstructed maps built with\nRGB-D and pose data from Astrobee. The runtimes of the approach are also\nanalyzed in depth. The source code is publicly released to promote further\ndevelopment.\n","authors":["Jamie Santos","Holly Dinkel","Julia Di","Paulo V. K. Borges","Marina Moreira","Oleg Alexandrov","Brian Coltin","Trey Smith"],"pdf_url":"https://arxiv.org/pdf/2312.02396v3.pdf","comment":"15 pages, 7 figures, Manuscript was presented at the AIAA SciTech\n  Forum in Orlando, FL, USA, 8 - 12 January 2024. Video presentation:\n  [https://www.youtube.com/watch?v=7WHp0dQYG4Y]. Code:\n  [https://github.com/nasa/isaac/tree/master/anomaly/gmm-change-detection]"},{"id":"http://arxiv.org/abs/2408.02619v1","updated":"2024-08-05T16:43:11Z","published":"2024-08-05T16:43:11Z","title":"Mastering Agile Jumping Skills from Simple Practices with Iterative\n  Learning Control","summary":"  Achieving precise target jumping with legged robots poses a significant\nchallenge due to the long flight phase and the uncertainties inherent in\ncontact dynamics and hardware. Forcefully attempting these agile motions on\nhardware could result in severe failures and potential damage. Motivated by\nthese challenging problems, we propose an Iterative Learning Control (ILC)\napproach that aims to learn and refine jumping skills from easy to difficult,\ninstead of directly learning these challenging tasks. We verify that learning\nfrom simplicity can enhance safety and target jumping accuracy over trials.\nCompared to other ILC approaches for legged locomotion, our method can tackle\nthe problem of a long flight phase where control input is not available. In\naddition, our approach allows the robot to apply what it learns from a simple\njumping task to accomplish more challenging tasks within a few trials directly\nin hardware, instead of learning from scratch. We validate the method via\nextensive experiments in the A1 model and hardware for various jumping tasks.\nStarting from a small jump (e.g., a forward leap of 40cm), our learning\napproach empowers the robot to accomplish a variety of challenging targets,\nincluding jumping onto a 20cm high box, jumping to a greater distance of up to\n60cm, as well as performing jumps while carrying an unknown payload of 2kg. Our\nframework can allow the robot to reach the desired position and orientation\ntargets with approximate errors of 1cm and 1 degree within a few trials.\n","authors":["Chuong Nguyen","Lingfan Bao","Quan Nguyen"],"pdf_url":"https://arxiv.org/pdf/2408.02619v1.pdf","comment":"Legged Robots, Dynamic Jumping, Iterative Learning"},{"id":"http://arxiv.org/abs/2403.11515v2","updated":"2024-08-05T16:39:15Z","published":"2024-03-18T07:01:21Z","title":"SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption\n  of Monocular Depth Estimation in Autonomous Navigation Applications","summary":"  Monocular depth estimation (MDE) has advanced significantly, primarily\nthrough the integration of convolutional neural networks (CNNs) and more\nrecently, Transformers. However, concerns about their susceptibility to\nadversarial attacks have emerged, especially in safety-critical domains like\nautonomous driving and robotic navigation. Existing approaches for assessing\nCNN-based depth prediction methods have fallen short in inducing comprehensive\ndisruptions to the vision system, often limited to specific local areas. In\nthis paper, we introduce SSAP (Shape-Sensitive Adversarial Patch), a novel\napproach designed to comprehensively disrupt monocular depth estimation (MDE)\nin autonomous navigation applications. Our patch is crafted to selectively\nundermine MDE in two distinct ways: by distorting estimated distances or by\ncreating the illusion of an object disappearing from the system's perspective.\nNotably, our patch is shape-sensitive, meaning it considers the specific shape\nand scale of the target object, thereby extending its influence beyond\nimmediate proximity. Furthermore, our patch is trained to effectively address\ndifferent scales and distances from the camera. Experimental results\ndemonstrate that our approach induces a mean depth estimation error surpassing\n0.5, impacting up to 99% of the targeted region for CNN-based MDE models.\nAdditionally, we investigate the vulnerability of Transformer-based MDE models\nto patch-based attacks, revealing that SSAP yields a significant error of 0.59\nand exerts substantial influence over 99% of the target region on these models.\n","authors":["Amira Guesmi","Muhammad Abdullah Hanif","Ihsen Alouani","Bassem Ouni","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2403.11515v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2303.01351"},{"id":"http://arxiv.org/abs/2303.01351v3","updated":"2024-08-05T16:37:37Z","published":"2023-03-02T15:31:53Z","title":"APARATE: Adaptive Adversarial Patch for CNN-based Monocular Depth\n  Estimation for Autonomous Navigation","summary":"  In recent times, monocular depth estimation (MDE) has experienced significant\nadvancements in performance, largely attributed to the integration of\ninnovative architectures, i.e., convolutional neural networks (CNNs) and\nTransformers. Nevertheless, the susceptibility of these models to adversarial\nattacks has emerged as a noteworthy concern, especially in domains where safety\nand security are paramount. This concern holds particular weight for MDE due to\nits critical role in applications like autonomous driving and robotic\nnavigation, where accurate scene understanding is pivotal. To assess the\nvulnerability of CNN-based depth prediction methods, recent work tries to\ndesign adversarial patches against MDE. However, the existing approaches fall\nshort of inducing a comprehensive and substantially disruptive impact on the\nvision system. Instead, their influence is partial and confined to specific\nlocal areas. These methods lead to erroneous depth predictions only within the\noverlapping region with the input image, without considering the\ncharacteristics of the target object, such as its size, shape, and position. In\nthis paper, we introduce a novel adversarial patch named APARATE. This patch\npossesses the ability to selectively undermine MDE in two distinct ways: by\ndistorting the estimated distances or by creating the illusion of an object\ndisappearing from the perspective of the autonomous system. Notably, APARATE is\ndesigned to be sensitive to the shape and scale of the target object, and its\ninfluence extends beyond immediate proximity. APARATE, results in a mean depth\nestimation error surpassing $0.5$, significantly impacting as much as $99\\%$ of\nthe targeted region when applied to CNN-based MDE models. Furthermore, it\nyields a significant error of $0.34$ and exerts substantial influence over\n$94\\%$ of the target region in the context of Transformer-based MDE.\n","authors":["Amira Guesmi","Muhammad Abdullah Hanif","Ihsen Alouani","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2303.01351v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02605v1","updated":"2024-08-05T16:31:28Z","published":"2024-08-05T16:31:28Z","title":"Trade-offs of Dynamic Control Structure in Human-swarm Systems","summary":"  Swarm robotics is a study of simple robots that exhibit complex behaviour\nonly by interacting locally with other robots and their environment. The\ncontrol in swarm robotics is mainly distributed whereas centralised control is\nwidely used in other fields of robotics. Centralised and decentralised control\nstrategies both pose a unique set of benefits and drawbacks for the control of\nmulti-robot systems. While decentralised systems are more scalable and\nresilient, they are less efficient compared to the centralised systems and they\nlead to excessive data transmissions to the human operators causing cognitive\noverload. We examine the trade-offs of each of these approaches in a\nhuman-swarm system to perform an environmental monitoring task and propose a\nflexible hybrid approach, which combines elements of hierarchical and\ndecentralised systems. We find that a flexible hybrid system can outperform a\ncentralised system (in our environmental monitoring task by 19.2%) while\nreducing the number of messages sent to a human operator (here by 23.1%). We\nconclude that establishing centralisation for a system is not always optimal\nfor performance and that utilising aspects of centralised and decentralised\nsystems can keep the swarm from hindering its performance.\n","authors":["Thomas G. Kelly","Mohammad D. Soorati","Klaus-Peter Zauner","Sarvapali D. Ramchurn","and Danesh Tarapore"],"pdf_url":"https://arxiv.org/pdf/2408.02605v1.pdf","comment":"The International Symposium on Distributed Autonomous Robotic Systems\n  (DARS 2024)"},{"id":"http://arxiv.org/abs/2402.18088v2","updated":"2024-08-05T16:00:25Z","published":"2024-02-28T06:20:30Z","title":"Bimanual Manipulation of Steady Hand Eye Robots with Adaptive Sclera\n  Force Control: Cooperative vs. Teleoperation Strategies","summary":"  Performing retinal vein cannulation (RVC) as a potential treatment for\nretinal vein occlusion (RVO) without the assistance of a surgical robotic\nsystem is very challenging to do safely. The main limitation is the\nphysiological hand tremor of surgeons. Robot-assisted eye surgery technology\nmay resolve the problems of hand tremors and fatigue and improve the safety and\nprecision of RVC. The Steady-Hand Eye Robot (SHER) is an admittance-based\nrobotic system that can filter out hand tremors and enables ophthalmologists to\nmanipulate a surgical instrument inside the eye cooperatively. However, the\nadmittance-based cooperative control mode does not safely minimize the contact\nforce between the surgical instrument and the sclera to prevent tissue damage.\nAdditionally, features like haptic feedback or hand motion scaling, which can\nimprove the safety and precision of surgery, require a teleoperation control\nframework. This work presents a bimanual adaptive teleoperation (BMAT) control\nframework using SHER 2.0 and SHER 2.1 robotic systems. We integrate them with\nan adaptive force control (AFC) algorithm to automatically minimize the\ntool-sclera interaction force. The scleral forces are measured using two fiber\nBragg grating (FBG)-based force-sensing tools. We compare the performance of\nthe BMAT mode with a bimanual adaptive cooperative (BMAC) mode in a\nvessel-following experiment under a surgical microscope. Experimental results\ndemonstrate the effectiveness of the proposed BMAT control framework in\nperforming a safe bimanual telemanipulation of the eye without over-stretching\nit, even in the absence of registration between the two robots.\n","authors":["Mojtaba Esfandiari","Peter Gehlbach","Russell H. Taylor","Iulian Iordachita"],"pdf_url":"https://arxiv.org/pdf/2402.18088v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02547v1","updated":"2024-08-05T15:17:34Z","published":"2024-08-05T15:17:34Z","title":"The Role of Functional Muscle Networks in Improving Hand Gesture\n  Perception for Human-Machine Interfaces","summary":"  Developing accurate hand gesture perception models is critical for various\nrobotic applications, enabling effective communication between humans and\nmachines and directly impacting neurorobotics and interactive robots. Recently,\nsurface electromyography (sEMG) has been explored for its rich informational\ncontext and accessibility when combined with advanced machine learning\napproaches and wearable systems. The literature presents numerous approaches to\nboost performance while ensuring robustness for neurorobots using sEMG, often\nresulting in models requiring high processing power, large datasets, and less\nscalable solutions. This paper addresses this challenge by proposing the\ndecoding of muscle synchronization rather than individual muscle activation. We\nstudy coherence-based functional muscle networks as the core of our perception\nmodel, proposing that functional synchronization between muscles and the\ngraph-based network of muscle connectivity encode contextual information about\nintended hand gestures. This can be decoded using shallow machine learning\napproaches without the need for deep temporal networks. Our technique could\nimpact myoelectric control of neurorobots by reducing computational burdens and\nenhancing efficiency. The approach is benchmarked on the Ninapro database,\nwhich contains 12 EMG signals from 40 subjects performing 17 hand gestures. It\nachieves an accuracy of 85.1%, demonstrating improved performance compared to\nexisting methods while requiring much less computational power. The results\nsupport the hypothesis that a coherence-based functional muscle network encodes\ncritical information related to gesture execution, significantly enhancing hand\ngesture perception with potential applications for neurorobotic systems and\ninteractive machines.\n","authors":["Costanza Armanini","Tuka Alhanai","Farah E. Shamout","S. Farokh Atashzar"],"pdf_url":"https://arxiv.org/pdf/2408.02547v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11592v2","updated":"2024-08-05T15:09:27Z","published":"2024-07-16T10:50:39Z","title":"Learning to Imitate Spatial Organization in Multi-robot Systems","summary":"  Understanding collective behavior and how it evolves is important to ensure\nthat robot swarms can be trusted in a shared environment. One way to understand\nthe behavior of the swarm is through collective behavior reconstruction using\nprior demonstrations. Existing approaches often require access to the swarm\ncontroller which may not be available. We reconstruct collective behaviors in\ndistinct swarm scenarios involving shared environments without using swarm\ncontroller information. We achieve this by transforming prior demonstrations\ninto features that describe multi-agent interactions before behavior\nreconstruction with multi-agent generative adversarial imitation learning\n(MA-GAIL). We show that our approach outperforms existing algorithms in spatial\norganization, and can be used to observe and reconstruct a swarm's behavior for\nfurther analysis and testing, which might be impractical or undesirable on the\noriginal robot swarm.\n","authors":["Ayomide O. Agunloye","Sarvapali D. Ramchurn","Mohammad D. Soorati"],"pdf_url":"https://arxiv.org/pdf/2407.11592v2.pdf","comment":"6 pages, 4 figures. Accepted for presentation at the IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2024)"},{"id":"http://arxiv.org/abs/2408.02535v1","updated":"2024-08-05T15:08:26Z","published":"2024-08-05T15:08:26Z","title":"Towards Coarse-grained Visual Language Navigation Task Planning Enhanced\n  by Event Knowledge Graph","summary":"  Visual language navigation (VLN) is one of the important research in embodied\nAI. It aims to enable an agent to understand the surrounding environment and\ncomplete navigation tasks. VLN instructions could be categorized into\ncoarse-grained and fine-grained commands. Fine-grained command describes a\nwhole task with subtasks step-by-step. In contrast, coarse-grained command\ngives an abstract task description, which more suites human habits. Most\nexisting work focuses on the former kind of instruction in VLN tasks, ignoring\nthe latter abstract instructions belonging to daily life scenarios. To overcome\nthe above challenge in abstract instruction, we attempt to consider\ncoarse-grained instruction in VLN by event knowledge enhancement. Specifically,\nwe first propose a prompt-based framework to extract an event knowledge graph\n(named VLN-EventKG) for VLN integrally over multiple mainstream benchmark\ndatasets. Through small and large language model collaboration, we realize\nknowledge-enhanced navigation planning (named EventNav) for VLN tasks with\ncoarse-grained instruction input. Additionally, we design a novel dynamic\nhistory backtracking module to correct potential error action planning in real\ntime. Experimental results in various public benchmarks show our\nknowledge-enhanced method has superiority in coarse-grained-instruction VLN\nusing our proposed VLN-EventKG with over $5\\%$ improvement in success rate. Our\nproject is available at https://sites.google.com/view/vln-eventkg\n","authors":["Zhao Kaichen","Song Yaoxian","Zhao Haiquan","Liu Haoyu","Li Tiefeng","Li Zhixu"],"pdf_url":"https://arxiv.org/pdf/2408.02535v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2405.01316v2","updated":"2024-08-05T13:40:19Z","published":"2024-05-02T14:23:07Z","title":"LOG-LIO2: A LiDAR-Inertial Odometry with Efficient Uncertainty Analysis","summary":"  Uncertainty in LiDAR measurements, stemming from factors such as range\nsensing, is crucial for LIO (LiDAR-Inertial Odometry) systems as it affects the\naccurate weighting in the loss function. While recent LIO systems address\nuncertainty related to range sensing, the impact of incident angle on\nuncertainty is often overlooked by the community. Moreover, the existing\nuncertainty propagation methods suffer from computational inefficiency. This\npaper proposes a comprehensive point uncertainty model that accounts for both\nthe uncertainties from LiDAR measurements and surface characteristics, along\nwith an efficient local uncertainty analytical method for LiDAR-based state\nestimation problem. We employ a projection operator that separates the\nuncertainty into the ray direction and its orthogonal plane. Then, we derive\nincremental Jacobian matrices of eigenvalues and eigenvectors w.r.t. points,\nwhich enables a fast approximation of uncertainty propagation. This approach\neliminates the requirement for redundant traversal of points, significantly\nreducing the time complexity of uncertainty propagation from $\\mathcal{O} (n)$\nto $\\mathcal{O} (1)$ when a new point is added. Simulations and experiments on\npublic datasets are conducted to validate the accuracy and efficiency of our\nformulations. The proposed methods have been integrated into a LIO system,\nwhich is available at https://github.com/tiev-tongji/LOG-LIO2.\n","authors":["Kai Huang","Junqiao Zhao","Jiaye Lin","Zhongyang Zhu","Shuangfu Song","Chen Ye","Tiantian Feng"],"pdf_url":"https://arxiv.org/pdf/2405.01316v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02455v1","updated":"2024-08-05T13:25:29Z","published":"2024-08-05T13:25:29Z","title":"A Surprisingly Efficient Representation for Multi-Finger Grasping","summary":"  The problem of grasping objects using a multi-finger hand has received\nsignificant attention in recent years. However, it remains challenging to\nhandle a large number of unfamiliar objects in real and cluttered environments.\nIn this work, we propose a representation that can be effectively mapped to the\nmulti-finger grasp space. Based on this representation, we develop a simple\ndecision model that generates accurate grasp quality scores for different\nmulti-finger grasp poses using only hundreds to thousands of training samples.\nWe demonstrate that our representation performs well on a real robot and\nachieves a success rate of 78.64% after training with only 500 real-world grasp\nattempts and 87% with 4500 grasp attempts. Additionally, we achieve a success\nrate of 84.51% in a dynamic human-robot handover scenario using a multi-finger\nhand.\n","authors":["Hengxu Yan","Hao-Shu Fang","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2408.02455v1.pdf","comment":"Published at International Conference on Robotics and Automation\n  (ICRA) 2024"},{"id":"http://arxiv.org/abs/2408.02454v1","updated":"2024-08-05T13:25:27Z","published":"2024-08-05T13:25:27Z","title":"TGS: Trajectory Generation and Selection using Vision Language Models in\n  Mapless Outdoor Environments","summary":"  We present a multi-modal trajectory generation and selection algorithm for\nreal-world mapless outdoor navigation in challenging scenarios with\nunstructured off-road features like buildings, grass, and curbs. Our goal is to\ncompute suitable trajectories that (1) satisfy the environment-specific\ntraversability constraints and (2) match human-like paths while navigating in\ncrosswalks, sidewalks, etc. Our formulation uses a Conditional Variational\nAutoencoder (CVAE) generative model enhanced with traversability constraints to\ngenerate multiple candidate trajectories for global navigation. We use VLMs and\na visual prompting approach with their zero-shot ability of semantic\nunderstanding and logical reasoning to choose the best trajectory given the\ncontextual information about the task. We evaluate our methods in various\noutdoor scenes with wheeled robots and compare the performance with other\nglobal navigation algorithms. In practice, we observe at least 3.35%\nimprovement in the traversability and 20.61% improvement in terms of human-like\nnavigation in generated trajectories in challenging outdoor navigation\nscenarios, such as sidewalks, crosswalks, etc.\n","authors":["Daeun Song","Jing Liang","Xuesu Xiao","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2408.02454v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02444v1","updated":"2024-08-05T13:08:53Z","published":"2024-08-05T13:08:53Z","title":"RIs-Calib: An Open-Source Spatiotemporal Calibrator for Multiple 3D\n  Radars and IMUs Based on Continuous-Time Estimation","summary":"  Aided inertial navigation system (INS), typically consisting of an inertial\nmeasurement unit (IMU) and an exteroceptive sensor, has been widely accepted as\na feasible solution for navigation. Compared with vision-aided and LiDAR-aided\nINS, radar-aided INS could achieve better performance in adverse weather\nconditions since the radar utilizes low-frequency measuring signals with less\nattenuation effect in atmospheric gases and rain. For such a radar-aided INS,\naccurate spatiotemporal transformation is a fundamental prerequisite to\nachieving optimal information fusion. In this work, we present RIs-Calib: a\nspatiotemporal calibrator for multiple 3D radars and IMUs based on\ncontinuous-time estimation, which enables accurate spatiotemporal calibration\nand does not require any additional artificial infrastructure or prior\nknowledge. Our approach starts with a rigorous and robust procedure for state\ninitialization, followed by batch optimizations, where all parameters can be\nrefined to global optimal states steadily. We validate and evaluate RIs-Calib\non both simulated and real-world experiments, and the results demonstrate that\nRIs-Calib is capable of accurate and consistent calibration. We open-source our\nimplementations at (https://github.com/Unsigned-Long/RIs-Calib) to benefit the\nresearch community.\n","authors":["Shuolong Chen","Xingxing Li","Shengyu Li","Yuxuan Zhou","Shiwen Wang"],"pdf_url":"https://arxiv.org/pdf/2408.02444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00606v2","updated":"2024-08-05T12:13:21Z","published":"2024-08-01T14:43:20Z","title":"U2UData: A Large-scale Cooperative Perception Dataset for Swarm UAVs\n  Autonomous Flight","summary":"  Modern perception systems for autonomous flight are sensitive to occlusion\nand have limited long-range capability, which is a key bottleneck in improving\nlow-altitude economic task performance. Recent research has shown that the\nUAV-to-UAV (U2U) cooperative perception system has great potential to\nrevolutionize the autonomous flight industry. However, the lack of a\nlarge-scale dataset is hindering progress in this area. This paper presents\nU2UData, the first large-scale cooperative perception dataset for swarm UAVs\nautonomous flight. The dataset was collected by three UAVs flying autonomously\nin the U2USim, covering a 9 km$^2$ flight area. It comprises 315K LiDAR frames,\n945K RGB and depth frames, and 2.41M annotated 3D bounding boxes for 3 classes.\nIt also includes brightness, temperature, humidity, smoke, and airflow values\ncovering all flight routes. U2USim is the first real-world mapping swarm UAVs\nsimulation environment. It takes Yunnan Province as the prototype and includes\n4 terrains, 7 weather conditions, and 8 sensor types. U2UData introduces two\nperception tasks: cooperative 3D object detection and cooperative 3D object\ntracking. This paper provides comprehensive benchmarks of recent cooperative\nperception algorithms on these tasks.\n","authors":["Tongtong Feng","Xin Wang","Feilin Han","Leping Zhang","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.00606v2.pdf","comment":"Accepted by ACM MM24"},{"id":"http://arxiv.org/abs/2408.02394v1","updated":"2024-08-05T11:40:59Z","published":"2024-08-05T11:40:59Z","title":"CMR-Agent: Learning a Cross-Modal Agent for Iterative Image-to-Point\n  Cloud Registration","summary":"  Image-to-point cloud registration aims to determine the relative camera pose\nof an RGB image with respect to a point cloud. It plays an important role in\ncamera localization within pre-built LiDAR maps. Despite the modality gaps,\nmost learning-based methods establish 2D-3D point correspondences in feature\nspace without any feedback mechanism for iterative optimization, resulting in\npoor accuracy and interpretability. In this paper, we propose to reformulate\nthe registration procedure as an iterative Markov decision process, allowing\nfor incremental adjustments to the camera pose based on each intermediate\nstate. To achieve this, we employ reinforcement learning to develop a\ncross-modal registration agent (CMR-Agent), and use imitation learning to\ninitialize its registration policy for stability and quick-start of the\ntraining. According to the cross-modal observations, we propose a 2D-3D hybrid\nstate representation that fully exploits the fine-grained features of RGB\nimages while reducing the useless neutral states caused by the spatial\ntruncation of camera frustum. Additionally, the overall framework is\nwell-designed to efficiently reuse one-shot cross-modal embeddings, avoiding\nrepetitive and time-consuming feature extraction. Extensive experiments on the\nKITTI-Odometry and NuScenes datasets demonstrate that CMR-Agent achieves\ncompetitive accuracy and efficiency in registration. Once the one-shot\nembeddings are completed, each iteration only takes a few milliseconds.\n","authors":["Gongxin Yao","Yixin Xuan","Xinyang Li","Yu Pan"],"pdf_url":"https://arxiv.org/pdf/2408.02394v1.pdf","comment":"Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2024"},{"id":"http://arxiv.org/abs/2402.06046v3","updated":"2024-08-05T10:48:32Z","published":"2024-02-08T20:37:51Z","title":"Anatomy of a Robotaxi Crash: Lessons from the Cruise Pedestrian Dragging\n  Mishap","summary":"  An October 2023 crash between a GM Cruise robotaxi and a pedestrian in San\nFrancisco resulted not only in a severe injury, but also dramatic upheaval at\nthat company that will likely have lasting effects throughout the industry.\nIs-sues stem not just from the loss events themselves, but also from how Cruise\nmishandled dealing with their robotaxi dragging a pedestrian under the vehicle\nafter the initial post-crash stop. External investigation reports provide raw\nmaterial describing the incident and critique the company's response from a\nregulatory point of view, but exclude safety engineering recommendations from\nscope. We highlight specific facts and relationships among events by tying\ntogether different pieces of the external report material. We then explore\nsafety lessons that might be learned related to: recognizing and responding to\nnearby mishaps, building an accurate world model of a post-collision scenario,\nthe in-adequacy of a so-called \"minimal risk condition\" strategy in complex\nsituations, poor organizational discipline in responding to a mishap, overly\naggressive post-collision automation choices that made a bad situation worse,\nand a reluctance to admit to a mishap causing much worse organizational harm\ndown-stream.\n","authors":["Philip Koopman"],"pdf_url":"https://arxiv.org/pdf/2402.06046v3.pdf","comment":"15 pages, 2 figures"},{"id":"http://arxiv.org/abs/2408.02319v1","updated":"2024-08-05T09:01:45Z","published":"2024-08-05T09:01:45Z","title":"Self-centering 3-DOF feet controller for hands-free locomotion control\n  in telepresence and virtual reality","summary":"  We present a novel seated foot controller for handling 3-DOF aimed to control\nlocomotion for telepresence robotics and virtual reality environments. Tilting\nthe feet on two axes yields in forward, backward and sideways motion. In\naddition, a separate rotary joint allows for rotation around the vertical axis.\nAttached springs on all joints self-center the controller. The HTC Vive tracker\nis used to translate the trackers' orientation into locomotion commands. The\nproposed self-centering foot controller was used successfully for the ANA\nAvatar XPRIZE competition, where a naive operator traversed the robot through a\nlonger distance, surpassing obstacles while solving various interaction and\nmanipulation tasks in between. We publicly provide the models of the mostly\n3D-printed feet controller for reproduction.\n","authors":["Raphael Memmesheimer","Christian Lenz","Max Schwarz","Michael Schreiber","Sven Behnke"],"pdf_url":"https://arxiv.org/pdf/2408.02319v1.pdf","comment":"4 pages, 7 figures, submitted to 2024 IEEE International Conference\n  on Telepresence (Tele 2024)"},{"id":"http://arxiv.org/abs/2408.02297v1","updated":"2024-08-05T08:14:28Z","published":"2024-08-05T08:14:28Z","title":"Perception Matters: Enhancing Embodied AI with Uncertainty-Aware\n  Semantic Segmentation","summary":"  Embodied AI has made significant progress acting in unexplored environments.\nHowever, tasks such as object search have largely focused on efficient policy\nlearning. In this work, we identify several gaps in current search methods:\nThey largely focus on dated perception models, neglect temporal aggregation,\nand transfer from ground truth directly to noisy perception at test time,\nwithout accounting for the resulting overconfidence in the perceived state. We\naddress the identified problems through calibrated perception probabilities and\nuncertainty across aggregation and found decisions, thereby adapting the models\nfor sequential tasks. The resulting methods can be directly integrated with\npretrained models across a wide family of existing search approaches at no\nadditional training cost. We perform extensive evaluations of aggregation\nmethods across both different semantic perception models and policies,\nconfirming the importance of calibrated uncertainties in both the aggregation\nand found decisions. We make the code and trained models available at\nhttp://semantic-search.cs.uni-freiburg.de.\n","authors":["Sai Prasanna","Daniel Honerkamp","Kshitij Sirohi","Tim Welschehold","Wolfram Burgard","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2408.02297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02293v1","updated":"2024-08-05T08:08:54Z","published":"2024-08-05T08:08:54Z","title":"OPENGRASP-LITE Version 1.0: A Tactile Artificial Hand with a Compliant\n  Linkage Mechanism","summary":"  Recent research has seen notable progress in the development of linkage-based\nartificial hands. While previous designs have focused on adaptive grasping,\ndexterity and biomimetic artificial skin, only a few systems have proposed a\nlightweight, accessible solution integrating tactile sensing with a compliant\nlinkage-based mechanism. This paper introduces OPENGRASP LITE, an open-source,\nhighly integrated, tactile, and lightweight artificial hand. Leveraging\ncompliant linkage systems and MEMS barometer-based tactile sensing, it offers\nversatile grasping capabilities with six degrees of actuation. By providing\ntactile sensors and enabling soft grasping, it serves as an accessible platform\nfor further research in tactile artificial hands.\n","authors":["Sonja Groß","Michael Ratzel","Edgar Welte","Diego Hidalgo-Carvajal","Lingyun Chen","Edmundo Pozo Fortunić","Amartya Ganguly","Abdalla Swikir","Sami Haddadin"],"pdf_url":"https://arxiv.org/pdf/2408.02293v1.pdf","comment":"Accepted at IEEE/RSJ International Conference on Intelligent Robots\n  and Systems, 14-18 October 2024"},{"id":"http://arxiv.org/abs/2408.02277v1","updated":"2024-08-05T07:14:47Z","published":"2024-08-05T07:14:47Z","title":"Integrating a Digital Twin Concept in the Zero Emission Sea Transporter\n  (ZEST) Project for Sustainable Maritime Transport using Stonefish Simulator","summary":"  In response to stringent emission reduction targets imposed by the\nInternational Maritime Organization (IMO) and the European Green Deal's Fit for\n55 legislation package, the maritime industry has shifted its focus towards\ndecarbonization. While significant attention has been placed on vessels\nexceeding 5,000 gross tons (GT), emissions from coastal and short sea shipping,\namounting to approximately 13% of global shipping transportation and 15% within\nthe European Union (EU), have not received adequate consideration. This\nabstract introduces the Zero Emission Sea Transporter (ZEST) project, designed\nto address this issue by developing a zero-emissions multi-purpose catamaran\nfor short sea route\n","authors":["Michele Grimaldi","Carlo Cernicchiaro","George Rossides","Angelos Ktoris","Elias Yfantis","Ioannis Kyriakides"],"pdf_url":"https://arxiv.org/pdf/2408.02277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05433v6","updated":"2024-08-05T06:54:54Z","published":"2024-07-07T16:36:43Z","title":"An efficient algorithm for solving linear equality-constrained LQR\n  problems","summary":"  We present a new algorithm for solving linear-quadratic regulator (LQR)\nproblems with linear equality constraints, also known as constrained LQR (CLQR)\nproblems. Our method's sequential runtime is linear in the number of stages and\nconstraints, and its parallel runtime is logarithmic in the number of stages.\nThe main technical contribution of this paper is the derivation of\nparallelizable techniques for eliminating the linear equality constraints while\npreserving the standard positive (semi-)definiteness requirements of LQR\nproblems.\n","authors":["João Sousa-Pinto","Dominique Orban"],"pdf_url":"https://arxiv.org/pdf/2407.05433v6.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2403.09990v3","updated":"2024-08-05T04:02:29Z","published":"2024-03-15T03:20:10Z","title":"CLOSURE: Fast Quantification of Pose Uncertainty Sets","summary":"  We investigate uncertainty quantification of 6D pose estimation from learned\nnoisy measurements (e.g. keypoints and pose hypotheses). Assuming\nunknown-but-bounded measurement noises, a pose uncertainty set (PURSE) is a\nsubset of SE(3) that contains all possible 6D poses compatible with the\nmeasurements. Despite being simple to formulate and its ability to embed\nuncertainty, the PURSE is difficult to manipulate and interpret due to the many\nabstract nonconvex polynomial constraints. An appealing simplification of PURSE\nis to find its minimum enclosing geodesic ball (MEGB), i.e., a point pose\nestimation with minimum worst-case error bound. We contribute (i) a geometric\ninterpretation of the nonconvex PURSE, and (ii) a fast algorithm to inner\napproximate the MEGB. Particularly, we show the PURSE corresponds to the\nfeasible set of a constrained dynamical system or the intersection of multiple\ngeodesic balls, and this perspective allows us to design an algorithm to\ndensely sample the boundary of the PURSE through strategic random walks. We\nthen use the miniball algorithm to compute the MEGB of PURSE samples, leading\nto an inner approximation. Our algorithm is named CLOSURE (enClosing baLl frOm\npurSe boUndaRy samplEs) and it enables computing a certificate of approximation\ntightness by calculating the relative size ratio between the inner\napproximation and the outer approximation. Running on a single RTX 3090 GPU,\nCLOSURE achieves the relative ratio of 92.8% on the LM-O dataset, 91.4% on the\n3DMatch dataset and 96.6% on the LM dataset with the average runtime less than\n0.3 second. Obtaining comparable worst-case error bound but 398x 833x and 23.6x\nfaster than the outer approximation GRCC, CLOSURE enables uncertainty\nquantification of 6D pose estimation to be implemented in real-time robot\nperception applications.\n","authors":["Yihuai Gao","Yukai Tang","Han Qi","Heng Yang"],"pdf_url":"https://arxiv.org/pdf/2403.09990v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02206v1","updated":"2024-08-05T03:12:30Z","published":"2024-08-05T03:12:30Z","title":"Large-scale Deployment of Vision-based Tactile Sensors on Multi-fingered\n  Grippers","summary":"  Vision-based Tactile Sensors (VBTSs) show significant promise in that they\ncan leverage image measurements to provide high-spatial-resolution human-like\nperformance. However, current VBTS designs, typically confined to the\nfingertips of robotic grippers, prove somewhat inadequate, as many grasping and\nmanipulation tasks require multiple contact points with the object. With an end\ngoal of enabling large-scale, multi-surface tactile sensing via VBTSs, our\nresearch (i) develops a synchronized image acquisition system with minimal\nlatency,(ii) proposes a modularized VBTS design for easy integration into\nfinger phalanges, and (iii) devises a zero-shot calibration approach to improve\ndata efficiency in the simultaneous calibration of multiple VBTSs. In\nvalidating the system within a miniature 3-fingered robotic gripper equipped\nwith 7 VBTSs we demonstrate improved tactile perception performance by covering\nthe contact surfaces of both gripper fingers and palm. Additionally, we show\nthat our VBTS design can be seamlessly integrated into various end-effector\nmorphologies significantly reducing the data requirements for calibration.\n","authors":["Meng Wang","Wanlin Li","Hao Liang","Boren Li","Kaspar Althoefer","Yao Su","Hangxin Liu"],"pdf_url":"https://arxiv.org/pdf/2408.02206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18294v3","updated":"2024-08-05T02:42:07Z","published":"2024-02-28T12:38:49Z","title":"Whole-body Humanoid Robot Locomotion with Human Reference","summary":"  Recently, humanoid robots have made significant advances in their ability to\nperform challenging tasks due to the deployment of Reinforcement Learning (RL),\nhowever, the inherent complexity of humanoid robots, including the difficulty\nof designing complicated reward functions and training entire sophisticated\nsystems, still poses a notable challenge. To conquer these challenges, after\nmany iterations and in-depth investigations, we have meticulously developed a\nfull-size humanoid robot, \"Adam\", whose innovative structural design greatly\nimproves the efficiency and effectiveness of the imitation learning process. In\naddition, we have developed a novel imitation learning framework based on an\nadversarial motion prior, which applies not only to Adam but also to humanoid\nrobots in general. Using the framework, Adam can exhibit unprecedented\nhuman-like characteristics in locomotion tasks. Our experimental results\ndemonstrate that the proposed framework enables Adam to achieve\nhuman-comparable performance in complex locomotion tasks, marking the first\ntime that human locomotion data has been used for imitation learning in a\nfull-size humanoid robot.\n","authors":["Qiang Zhang","Peter Cui","David Yan","Jingkai Sun","Yiqun Duan","Gang Han","Wen Zhao","Weining Zhang","Yijie Guo","Arthur Zhang","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2402.18294v3.pdf","comment":"7pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.02184v1","updated":"2024-08-05T02:11:12Z","published":"2024-08-05T02:11:12Z","title":"RoPotter: Toward Robotic Pottery and Deformable Object Manipulation with\n  Structural Priors","summary":"  Humans are capable of continuously manipulating a wide variety of deformable\nobjects into complex shapes. This is made possible by our intuitive\nunderstanding of material properties and mechanics of the object, for reasoning\nabout object states even when visual perception is occluded. These capabilities\nallow us to perform diverse tasks ranging from cooking with dough to expressing\nourselves with pottery-making. However, developing robotic systems to robustly\nperform similar tasks remains challenging, as current methods struggle to\neffectively model volumetric deformable objects and reason about the complex\nbehavior they typically exhibit. To study the robotic systems and algorithms\ncapable of deforming volumetric objects, we introduce a novel robotics task of\ncontinuously deforming clay on a pottery wheel. We propose a pipeline for\nperception and pottery skill-learning, called RoPotter, wherein we demonstrate\nthat structural priors specific to the task of pottery-making can be exploited\nto simplify the pottery skill-learning process. Namely, we can project the\ncross-section of the clay to a plane to represent the state of the clay,\nreducing dimensionality. We also demonstrate a mesh-based method of occluded\nclay state recovery, toward robotic agents capable of continuously deforming\nclay. Our experiments show that by using the reduced representation with\nstructural priors based on the deformation behaviors of the clay, RoPotter can\nperform the long-horizon pottery task with 44.4% lower final shape error\ncompared to the state-of-the-art baselines.\n","authors":["Uksang Yoo","Adam Hung","Jonathan Francis","Jean Oh","Jeffrey Ichnowski"],"pdf_url":"https://arxiv.org/pdf/2408.02184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18302v2","updated":"2024-08-05T23:48:12Z","published":"2024-02-28T12:50:16Z","title":"EchoTrack: Auditory Referring Multi-Object Tracking for Autonomous\n  Driving","summary":"  This paper introduces the task of Auditory Referring Multi-Object Tracking\n(AR-MOT), which dynamically tracks specific objects in a video sequence based\non audio expressions and appears as a challenging problem in autonomous\ndriving. Due to the lack of semantic modeling capacity in audio and video,\nexisting works have mainly focused on text-based multi-object tracking, which\noften comes at the cost of tracking quality, interaction efficiency, and even\nthe safety of assistance systems, limiting the application of such methods in\nautonomous driving. In this paper, we delve into the problem of AR-MOT from the\nperspective of audio-video fusion and audio-video tracking. We put forward\nEchoTrack, an end-to-end AR-MOT framework with dual-stream vision transformers.\nThe dual streams are intertwined with our Bidirectional Frequency-domain\nCross-attention Fusion Module (Bi-FCFM), which bidirectionally fuses audio and\nvideo features from both frequency- and spatiotemporal domains. Moreover, we\npropose the Audio-visual Contrastive Tracking Learning (ACTL) regime to extract\nhomogeneous semantic features between expressions and visual objects by\nlearning homogeneous features between different audio and video objects\neffectively. Aside from the architectural design, we establish the first set of\nlarge-scale AR-MOT benchmarks, including Echo-KITTI, Echo-KITTI+, and Echo-BDD.\nExtensive experiments on the established benchmarks demonstrate the\neffectiveness of the proposed EchoTrack and its components. The source code and\ndatasets are available at https://github.com/lab206/EchoTrack.\n","authors":["Jiacheng Lin","Jiajun Chen","Kunyu Peng","Xuan He","Zhiyong Li","Rainer Stiefelhagen","Kailun Yang"],"pdf_url":"https://arxiv.org/pdf/2402.18302v2.pdf","comment":"Accepted to IEEE Transactions on Intelligent Transportation Systems\n  (T-ITS). The source code and datasets are available at\n  https://github.com/lab206/EchoTrack"},{"id":"http://arxiv.org/abs/2408.02786v1","updated":"2024-08-05T19:05:52Z","published":"2024-08-05T19:05:52Z","title":"Multi-Scale Cell Decomposition for Path Planning using Restrictive\n  Routing Potential Fields","summary":"  In burgeoning domains, like urban goods distribution, the advent of aerial\ncargo transportation necessitates the development of routing solutions that\nprioritize safety. This paper introduces Larp, a novel path planning framework\nthat leverages the concept of restrictive potential fields to forge routes\ndemonstrably safer than those derived from existing methods. The algorithm\nachieves it by segmenting a potential field into a hierarchy of cells, each\nwith a designated restriction zone determined by obstacle proximity. While the\nprimary impetus behind Larp is to enhance the safety of aerial pathways for\ncargo-carrying Unmanned Aerial Vehicles (UAVs), its utility extends to a wide\narray of path planning scenarios. Comparative analyses with both established\nand contemporary potential field-based methods reveal Larp's proficiency in\nmaintaining a safe distance from restrictions and its adeptness in\ncircumventing local minima.\n","authors":["Josue N. Rivera","Dengfeng Sun"],"pdf_url":"https://arxiv.org/pdf/2408.02786v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2408.02672v1","updated":"2024-08-05T17:59:51Z","published":"2024-08-05T17:59:51Z","title":"Latent-INR: A Flexible Framework for Implicit Representations of Videos\n  with Discriminative Semantics","summary":"  Implicit Neural Networks (INRs) have emerged as powerful representations to\nencode all forms of data, including images, videos, audios, and scenes. With\nvideo, many INRs for video have been proposed for the compression task, and\nrecent methods feature significant improvements with respect to encoding time,\nstorage, and reconstruction quality. However, these encoded representations\nlack semantic meaning, so they cannot be used for any downstream tasks that\nrequire such properties, such as retrieval. This can act as a barrier for\nadoption of video INRs over traditional codecs as they do not offer any\nsignificant edge apart from compression. To alleviate this, we propose a\nflexible framework that decouples the spatial and temporal aspects of the video\nINR. We accomplish this with a dictionary of per-frame latents that are learned\njointly with a set of video specific hypernetworks, such that given a latent,\nthese hypernetworks can predict the INR weights to reconstruct the given frame.\nThis framework not only retains the compression efficiency, but the learned\nlatents can be aligned with features from large vision models, which grants\nthem discriminative properties. We align these latents with CLIP and show good\nperformance for both compression and video retrieval tasks. By aligning with\nVideoLlama, we are able to perform open-ended chat with our learned latents as\nthe visual inputs. Additionally, the learned latents serve as a proxy for the\nunderlying weights, allowing us perform tasks like video interpolation. These\nsemantic properties and applications, existing simultaneously with ability to\nperform compression, interpolation, and superresolution properties, are a first\nin this field of work.\n","authors":["Shishira R Maiya","Anubhav Gupta","Matthew Gwilliam","Max Ehrlich","Abhinav Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2408.02672v1.pdf","comment":"equal contribution for first two authors; accepted to ECCV2024; 14\n  pages, 4 tables, 10 figures in main paper, supplementary after bibliography"},{"id":"http://arxiv.org/abs/2405.04634v3","updated":"2024-08-05T17:53:28Z","published":"2024-05-07T19:37:22Z","title":"FRACTAL: An Ultra-Large-Scale Aerial Lidar Dataset for 3D Semantic\n  Segmentation of Diverse Landscapes","summary":"  Mapping agencies are increasingly adopting Aerial Lidar Scanning (ALS) as a\nnew tool to monitor territory and support public policies. Processing ALS data\nat scale requires efficient point classification methods that perform well over\nhighly diverse territories. To evaluate them, researchers need large annotated\nLidar datasets, however, current Lidar benchmark datasets have restricted scope\nand often cover a single urban area. To bridge this data gap, we present the\nFRench ALS Clouds from TArgeted Landscapes (FRACTAL) dataset: an\nultra-large-scale aerial Lidar dataset made of 100,000 dense point clouds with\nhigh-quality labels for 7 semantic classes and spanning 250 km$^2$. FRACTAL is\nbuilt upon France's nationwide open Lidar data. It achieves spatial and\nsemantic diversity via a sampling scheme that explicitly concentrates rare\nclasses and challenging landscapes from five French regions. It should support\nthe development of 3D deep learning approaches for large-scale land monitoring.\nWe describe the nature of the source data, the sampling workflow, the content\nof the resulting dataset, and provide an initial evaluation of segmentation\nperformance using a performant 3D neural architecture.\n","authors":["Charles Gaydon","Michel Daab","Floryne Roche"],"pdf_url":"https://arxiv.org/pdf/2405.04634v3.pdf","comment":"15 pages | 9 figures | 8 tables | Dataset is available at\n  https://huggingface.co/datasets/IGNF/FRACTAL | Trained model is available at\n  https://huggingface.co/IGNF/FRACTAL-LidarHD_7cl_randlanet | Deep learning\n  code repository is on Gihtub at https://github.com/IGNF/myria3d | Data\n  engineering code repository is on Github at https://github.com/IGNF/pacasam"},{"id":"http://arxiv.org/abs/2407.11913v2","updated":"2024-08-05T17:50:03Z","published":"2024-07-16T17:05:20Z","title":"Quantised Global Autoencoder: A Holistic Approach to Representing Visual\n  Data","summary":"  In quantised autoencoders, images are usually split into local patches, each\nencoded by one token. This representation is redundant in the sense that the\nsame number of tokens is spend per region, regardless of the visual information\ncontent in that region. Adaptive discretisation schemes like quadtrees are\napplied to allocate tokens for patches with varying sizes, but this just varies\nthe region of influence for a token which nevertheless remains a local\ndescriptor. Modern architectures add an attention mechanism to the autoencoder\nwhich infuses some degree of global information into the local tokens. Despite\nthe global context, tokens are still associated with a local image region. In\ncontrast, our method is inspired by spectral decompositions which transform an\ninput signal into a superposition of global frequencies. Taking the data-driven\nperspective, we learn custom basis functions corresponding to the codebook\nentries in our VQ-VAE setup. Furthermore, a decoder combines these basis\nfunctions in a non-linear fashion, going beyond the simple linear superposition\nof spectral decompositions. We can achieve this global description with an\nefficient transpose operation between features and channels and demonstrate our\nperformance on compression.\n","authors":["Tim Elsner","Paula Usinger","Victor Czech","Gregor Kobsik","Yanjiang He","Isaak Lim","Leif Kobbelt"],"pdf_url":"https://arxiv.org/pdf/2407.11913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02657v1","updated":"2024-08-05T17:46:53Z","published":"2024-08-05T17:46:53Z","title":"Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation\n  with Multimodal Generative Pretraining","summary":"  We present Lumina-mGPT, a family of multimodal autoregressive models capable\nof various vision and language tasks, particularly excelling in generating\nflexible photorealistic images from text descriptions. Unlike existing\nautoregressive image generation approaches, Lumina-mGPT employs a pretrained\ndecoder-only transformer as a unified framework for modeling multimodal token\nsequences. Our key insight is that a simple decoder-only transformer with\nmultimodal Generative PreTraining (mGPT), utilizing the next-token prediction\nobjective on massive interleaved text-image sequences, can learn broad and\ngeneral multimodal capabilities, thereby illuminating photorealistic\ntext-to-image generation. Building on these pretrained models, we propose\nFlexible Progressive Supervised Finetuning (FP-SFT) on high-quality image-text\npairs to fully unlock their potential for high-aesthetic image synthesis at any\nresolution while maintaining their general multimodal capabilities.\nFurthermore, we introduce Ominiponent Supervised Finetuning (Omni-SFT),\ntransforming Lumina-mGPT into a foundation model that seamlessly achieves\nomnipotent task unification. The resulting model demonstrates versatile\nmultimodal capabilities, including visual generation tasks like flexible\ntext-to-image generation and controllable generation, visual recognition tasks\nlike segmentation and depth estimation, and vision-language tasks like\nmultiturn visual question answering. Additionally, we analyze the differences\nand similarities between diffusion-based and autoregressive methods in a direct\ncomparison.\n","authors":["Dongyang Liu","Shitian Zhao","Le Zhuo","Weifeng Lin","Yu Qiao","Hongsheng Li","Peng Gao"],"pdf_url":"https://arxiv.org/pdf/2408.02657v1.pdf","comment":"Code available at: https://github.com/Alpha-VLLM/Lumina-mGPT"},{"id":"http://arxiv.org/abs/2408.02654v1","updated":"2024-08-05T17:33:09Z","published":"2024-08-05T17:33:09Z","title":"On Using Quasirandom Sequences in Machine Learning for Model Weight\n  Initialization","summary":"  The effectiveness of training neural networks directly impacts computational\ncosts, resource allocation, and model development timelines in machine learning\napplications. An optimizer's ability to train the model adequately (in terms of\ntrained model performance) depends on the model's initial weights. Model weight\ninitialization schemes use pseudorandom number generators (PRNGs) as a source\nof randomness.\n  We investigate whether substituting PRNGs for low-discrepancy quasirandom\nnumber generators (QRNGs) -- namely Sobol' sequences -- as a source of\nrandomness for initializers can improve model performance. We examine\nMulti-Layer Perceptrons (MLP), Convolutional Neural Networks (CNN), Long\nShort-Term Memory (LSTM), and Transformer architectures trained on MNIST,\nCIFAR-10, and IMDB datasets using SGD and Adam optimizers. Our analysis uses\nten initialization schemes: Glorot, He, Lecun (both Uniform and Normal);\nOrthogonal, Random Normal, Truncated Normal, and Random Uniform. Models with\nweights set using PRNG- and QRNG-based initializers are compared pairwise for\neach combination of dataset, architecture, optimizer, and initialization\nscheme.\n  Our findings indicate that QRNG-based neural network initializers either\nreach a higher accuracy or achieve the same accuracy more quickly than\nPRNG-based initializers in 60% of the 120 experiments conducted. Thus, using\nQRNG-based initializers instead of PRNG-based initializers can speed up and\nimprove model training.\n","authors":["Andriy Miranskyy","Adam Sorrenti","Viral Thakar"],"pdf_url":"https://arxiv.org/pdf/2408.02654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02635v1","updated":"2024-08-05T16:58:56Z","published":"2024-08-05T16:58:56Z","title":"Interactive 3D Medical Image Segmentation with SAM 2","summary":"  Interactive medical image segmentation (IMIS) has shown significant potential\nin enhancing segmentation accuracy by integrating iterative feedback from\nmedical professionals. However, the limited availability of enough 3D medical\ndata restricts the generalization and robustness of most IMIS methods. The\nSegment Anything Model (SAM), though effective for 2D images, requires\nexpensive semi-auto slice-by-slice annotations for 3D medical images. In this\npaper, we explore the zero-shot capabilities of SAM 2, the next-generation Meta\nSAM model trained on videos, for 3D medical image segmentation. By treating\nsequential 2D slices of 3D images as video frames, SAM 2 can fully\nautomatically propagate annotations from a single frame to the entire 3D\nvolume. We propose a practical pipeline for using SAM 2 in 3D medical image\nsegmentation and present key findings highlighting its efficiency and potential\nfor further optimization. Concretely, numerical experiments on the BraTS2020\nand the medical segmentation decathlon datasets demonstrate that SAM 2 still\nhas a gap with supervised methods but can narrow the gap in specific settings\nand organ types, significantly reducing the annotation burden on medical\nprofessionals. Our code will be open-sourced and available at\nhttps://github.com/Chuyun-Shen/SAM_2_Medical_3D.\n","authors":["Chuyun Shen","Wenhao Li","Yuhang Shi","Xiangfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2408.02635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02629v1","updated":"2024-08-05T16:53:23Z","published":"2024-08-05T16:53:23Z","title":"VidGen-1M: A Large-Scale Dataset for Text-to-video Generation","summary":"  The quality of video-text pairs fundamentally determines the upper bound of\ntext-to-video models. Currently, the datasets used for training these models\nsuffer from significant shortcomings, including low temporal consistency,\npoor-quality captions, substandard video quality, and imbalanced data\ndistribution. The prevailing video curation process, which depends on image\nmodels for tagging and manual rule-based curation, leads to a high\ncomputational load and leaves behind unclean data. As a result, there is a lack\nof appropriate training datasets for text-to-video models. To address this\nproblem, we present VidGen-1M, a superior training dataset for text-to-video\nmodels. Produced through a coarse-to-fine curation strategy, this dataset\nguarantees high-quality videos and detailed captions with excellent temporal\nconsistency. When used to train the video generation model, this dataset has\nled to experimental results that surpass those obtained with other models.\n","authors":["Zhiyu Tan","Xiaomeng Yang","Luozheng Qin","Hao Li"],"pdf_url":"https://arxiv.org/pdf/2408.02629v1.pdf","comment":"project page: https://sais-fuxi.github.io/projects/vidgen-1m"},{"id":"http://arxiv.org/abs/2312.02396v3","updated":"2024-08-05T16:49:51Z","published":"2023-12-04T23:26:12Z","title":"Unsupervised Change Detection for Space Habitats Using 3D Point Clouds","summary":"  This work presents an algorithm for scene change detection from point clouds\nto enable autonomous robotic caretaking in future space habitats. Autonomous\nrobotic systems will help maintain future deep-space habitats, such as the\nGateway space station, which will be uncrewed for extended periods. Existing\nscene analysis software used on the International Space Station (ISS) relies on\nmanually-labeled images for detecting changes. In contrast, the algorithm\npresented in this work uses raw, unlabeled point clouds as inputs. The\nalgorithm first applies modified Expectation-Maximization Gaussian Mixture\nModel (GMM) clustering to two input point clouds. It then performs change\ndetection by comparing the GMMs using the Earth Mover's Distance. The algorithm\nis validated quantitatively and qualitatively using a test dataset collected by\nan Astrobee robot in the NASA Ames Granite Lab comprising single frame depth\nimages taken directly by Astrobee and full-scene reconstructed maps built with\nRGB-D and pose data from Astrobee. The runtimes of the approach are also\nanalyzed in depth. The source code is publicly released to promote further\ndevelopment.\n","authors":["Jamie Santos","Holly Dinkel","Julia Di","Paulo V. K. Borges","Marina Moreira","Oleg Alexandrov","Brian Coltin","Trey Smith"],"pdf_url":"https://arxiv.org/pdf/2312.02396v3.pdf","comment":"15 pages, 7 figures, Manuscript was presented at the AIAA SciTech\n  Forum in Orlando, FL, USA, 8 - 12 January 2024. Video presentation:\n  [https://www.youtube.com/watch?v=7WHp0dQYG4Y]. Code:\n  [https://github.com/nasa/isaac/tree/master/anomaly/gmm-change-detection]"},{"id":"http://arxiv.org/abs/2408.02623v1","updated":"2024-08-05T16:48:03Z","published":"2024-08-05T16:48:03Z","title":"YOWOv3: An Efficient and Generalized Framework for Human Action\n  Detection and Recognition","summary":"  In this paper, we propose a new framework called YOWOv3, which is an improved\nversion of YOWOv2, designed specifically for the task of Human Action Detection\nand Recognition. This framework is designed to facilitate extensive\nexperimentation with different configurations and supports easy customization\nof various components within the model, reducing efforts required for\nunderstanding and modifying the code. YOWOv3 demonstrates its superior\nperformance compared to YOWOv2 on two widely used datasets for Human Action\nDetection and Recognition: UCF101-24 and AVAv2.2. Specifically, the predecessor\nmodel YOWOv2 achieves an mAP of 85.2% and 20.3% on UCF101-24 and AVAv2.2,\nrespectively, with 109.7M parameters and 53.6 GFLOPs. In contrast, our model -\nYOWOv3, with only 59.8M parameters and 39.8 GFLOPs, achieves an mAP of 88.33%\nand 20.31% on UCF101-24 and AVAv2.2, respectively. The results demonstrate that\nYOWOv3 significantly reduces the number of parameters and GFLOPs while still\nachieving comparable performance.\n","authors":["Duc Manh Nguyen Dang","Viet Hang Duong","Jia Ching Wang","Nhan Bui Duc"],"pdf_url":"https://arxiv.org/pdf/2408.02623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02615v1","updated":"2024-08-05T16:39:39Z","published":"2024-08-05T16:39:39Z","title":"LaMamba-Diff: Linear-Time High-Fidelity Diffusion Models Based on Local\n  Attention and Mamba","summary":"  Recent Transformer-based diffusion models have shown remarkable performance,\nlargely attributed to the ability of the self-attention mechanism to accurately\ncapture both global and local contexts by computing all-pair interactions among\ninput tokens. However, their quadratic complexity poses significant\ncomputational challenges for long-sequence inputs. Conversely, a recent state\nspace model called Mamba offers linear complexity by compressing a filtered\nglobal context into a hidden state. Despite its efficiency, compression\ninevitably leads to information loss of fine-grained local dependencies among\ntokens, which are crucial for effective visual generative modeling. Motivated\nby these observations, we introduce Local Attentional Mamba (LaMamba) blocks\nthat combine the strengths of self-attention and Mamba, capturing both global\ncontexts and local details with linear complexity. Leveraging the efficient\nU-Net architecture, our model exhibits exceptional scalability and surpasses\nthe performance of DiT across various model scales on ImageNet at 256x256\nresolution, all while utilizing substantially fewer GFLOPs and a comparable\nnumber of parameters. Compared to state-of-the-art diffusion models on ImageNet\n256x256 and 512x512, our largest model presents notable advantages, such as a\nreduction of up to 62\\% GFLOPs compared to DiT-XL/2, while achieving superior\nperformance with comparable or fewer parameters.\n","authors":["Yunxiang Fu","Chaoqi Chen","Yizhou Yu"],"pdf_url":"https://arxiv.org/pdf/2408.02615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11515v2","updated":"2024-08-05T16:39:15Z","published":"2024-03-18T07:01:21Z","title":"SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption\n  of Monocular Depth Estimation in Autonomous Navigation Applications","summary":"  Monocular depth estimation (MDE) has advanced significantly, primarily\nthrough the integration of convolutional neural networks (CNNs) and more\nrecently, Transformers. However, concerns about their susceptibility to\nadversarial attacks have emerged, especially in safety-critical domains like\nautonomous driving and robotic navigation. Existing approaches for assessing\nCNN-based depth prediction methods have fallen short in inducing comprehensive\ndisruptions to the vision system, often limited to specific local areas. In\nthis paper, we introduce SSAP (Shape-Sensitive Adversarial Patch), a novel\napproach designed to comprehensively disrupt monocular depth estimation (MDE)\nin autonomous navigation applications. Our patch is crafted to selectively\nundermine MDE in two distinct ways: by distorting estimated distances or by\ncreating the illusion of an object disappearing from the system's perspective.\nNotably, our patch is shape-sensitive, meaning it considers the specific shape\nand scale of the target object, thereby extending its influence beyond\nimmediate proximity. Furthermore, our patch is trained to effectively address\ndifferent scales and distances from the camera. Experimental results\ndemonstrate that our approach induces a mean depth estimation error surpassing\n0.5, impacting up to 99% of the targeted region for CNN-based MDE models.\nAdditionally, we investigate the vulnerability of Transformer-based MDE models\nto patch-based attacks, revealing that SSAP yields a significant error of 0.59\nand exerts substantial influence over 99% of the target region on these models.\n","authors":["Amira Guesmi","Muhammad Abdullah Hanif","Ihsen Alouani","Bassem Ouni","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2403.11515v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2303.01351"},{"id":"http://arxiv.org/abs/2303.01351v3","updated":"2024-08-05T16:37:37Z","published":"2023-03-02T15:31:53Z","title":"APARATE: Adaptive Adversarial Patch for CNN-based Monocular Depth\n  Estimation for Autonomous Navigation","summary":"  In recent times, monocular depth estimation (MDE) has experienced significant\nadvancements in performance, largely attributed to the integration of\ninnovative architectures, i.e., convolutional neural networks (CNNs) and\nTransformers. Nevertheless, the susceptibility of these models to adversarial\nattacks has emerged as a noteworthy concern, especially in domains where safety\nand security are paramount. This concern holds particular weight for MDE due to\nits critical role in applications like autonomous driving and robotic\nnavigation, where accurate scene understanding is pivotal. To assess the\nvulnerability of CNN-based depth prediction methods, recent work tries to\ndesign adversarial patches against MDE. However, the existing approaches fall\nshort of inducing a comprehensive and substantially disruptive impact on the\nvision system. Instead, their influence is partial and confined to specific\nlocal areas. These methods lead to erroneous depth predictions only within the\noverlapping region with the input image, without considering the\ncharacteristics of the target object, such as its size, shape, and position. In\nthis paper, we introduce a novel adversarial patch named APARATE. This patch\npossesses the ability to selectively undermine MDE in two distinct ways: by\ndistorting the estimated distances or by creating the illusion of an object\ndisappearing from the perspective of the autonomous system. Notably, APARATE is\ndesigned to be sensitive to the shape and scale of the target object, and its\ninfluence extends beyond immediate proximity. APARATE, results in a mean depth\nestimation error surpassing $0.5$, significantly impacting as much as $99\\%$ of\nthe targeted region when applied to CNN-based MDE models. Furthermore, it\nyields a significant error of $0.34$ and exerts substantial influence over\n$94\\%$ of the target region in the context of Transformer-based MDE.\n","authors":["Amira Guesmi","Muhammad Abdullah Hanif","Ihsen Alouani","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2303.01351v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07338v2","updated":"2024-08-05T16:34:43Z","published":"2023-03-13T17:59:02Z","title":"Revisiting Class-Incremental Learning with Pre-Trained Models:\n  Generalizability and Adaptivity are All You Need","summary":"  Class-incremental learning (CIL) aims to adapt to emerging new classes\nwithout forgetting old ones. Traditional CIL models are trained from scratch to\ncontinually acquire knowledge as data evolves. Recently, pre-training has\nachieved substantial progress, making vast pre-trained models (PTMs) accessible\nfor CIL. Contrary to traditional methods, PTMs possess generalizable\nembeddings, which can be easily transferred for CIL. In this work, we revisit\nCIL with PTMs and argue that the core factors in CIL are adaptivity for model\nupdating and generalizability for knowledge transferring. 1) We first reveal\nthat frozen PTM can already provide generalizable embeddings for CIL.\nSurprisingly, a simple baseline (SimpleCIL) which continually sets the\nclassifiers of PTM to prototype features can beat state-of-the-art even without\ntraining on the downstream task. 2) Due to the distribution gap between\npre-trained and downstream datasets, PTM can be further cultivated with\nadaptivity via model adaptation. We propose AdaPt and mERge (APER), which\naggregates the embeddings of PTM and adapted models for classifier\nconstruction. APER is a general framework that can be orthogonally combined\nwith any parameter-efficient tuning method, which holds the advantages of PTM's\ngeneralizability and adapted model's adaptivity. 3) Additionally, considering\nprevious ImageNet-based benchmarks are unsuitable in the era of PTM due to data\noverlapping, we propose four new benchmarks for assessment, namely ImageNet-A,\nObjectNet, OmniBenchmark, and VTAB. Extensive experiments validate the\neffectiveness of APER with a unified and concise framework. Code is available\nat https://github.com/zhoudw-zdw/RevisitingCIL\n","authors":["Da-Wei Zhou","Zi-Wen Cai","Han-Jia Ye","De-Chuan Zhan","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2303.07338v2.pdf","comment":"Accepted to IJCV. Code is available at:\n  https://github.com/zhoudw-zdw/RevisitingCIL"},{"id":"http://arxiv.org/abs/2408.02595v1","updated":"2024-08-05T16:07:31Z","published":"2024-08-05T16:07:31Z","title":"Modelling Visual Semantics via Image Captioning to extract Enhanced\n  Multi-Level Cross-Modal Semantic Incongruity Representation with Attention\n  for Multimodal Sarcasm Detection","summary":"  Sarcasm is a type of irony, characterized by an inherent mismatch between the\nliteral interpretation and the intended connotation. Though sarcasm detection\nin text has been extensively studied, there are situations in which textual\ninput alone might be insufficient to perceive sarcasm. The inclusion of\nadditional contextual cues, such as images, is essential to recognize sarcasm\nin social media data effectively. This study presents a novel framework for\nmultimodal sarcasm detection that can process input triplets. Two components of\nthese triplets comprise the input text and its associated image, as provided in\nthe datasets. Additionally, a supplementary modality is introduced in the form\nof descriptive image captions. The motivation behind incorporating this visual\nsemantic representation is to more accurately capture the discrepancies between\nthe textual and visual content, which are fundamental to the sarcasm detection\ntask. The primary contributions of this study are: (1) a robust textual feature\nextraction branch that utilizes a cross-lingual language model; (2) a visual\nfeature extraction branch that incorporates a self-regulated residual ConvNet\nintegrated with a lightweight spatially aware attention module; (3) an\nadditional modality in the form of image captions generated using an\nencoder-decoder architecture capable of reading text embedded in images; (4)\ndistinct attention modules to effectively identify the incongruities between\nthe text and two levels of image representations; (5) multi-level cross-domain\nsemantic incongruity representation achieved through feature fusion. Compared\nwith cutting-edge baselines, the proposed model achieves the best accuracy of\n92.89% and 64.48%, respectively, on the Twitter multimodal sarcasm and\nMultiBully datasets.\n","authors":["Sajal Aggarwal","Ananya Pandey","Dinesh Kumar Vishwakarma"],"pdf_url":"https://arxiv.org/pdf/2408.02595v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02571v1","updated":"2024-08-05T15:45:59Z","published":"2024-08-05T15:45:59Z","title":"Contrastive Learning-based Multi Modal Architecture for Emoticon\n  Prediction by Employing Image-Text Pairs","summary":"  The emoticons are symbolic representations that generally accompany the\ntextual content to visually enhance or summarize the true intention of a\nwritten message. Although widely utilized in the realm of social media, the\ncore semantics of these emoticons have not been extensively explored based on\nmultiple modalities. Incorporating textual and visual information within a\nsingle message develops an advanced way of conveying information. Hence, this\nresearch aims to analyze the relationship among sentences, visuals, and\nemoticons. For an orderly exposition, this paper initially provides a detailed\nexamination of the various techniques for extracting multimodal features,\nemphasizing the pros and cons of each method. Through conducting a\ncomprehensive examination of several multimodal algorithms, with specific\nemphasis on the fusion approaches, we have proposed a novel contrastive\nlearning based multimodal architecture. The proposed model employs the joint\ntraining of dual-branch encoder along with the contrastive learning to\naccurately map text and images into a common latent space. Our key finding is\nthat by integrating the principle of contrastive learning with that of the\nother two branches yields superior results. The experimental results\ndemonstrate that our suggested methodology surpasses existing multimodal\napproaches in terms of accuracy and robustness. The proposed model attained an\naccuracy of 91% and an MCC-score of 90% while assessing emoticons using the\nMultimodal-Twitter Emoticon dataset acquired from Twitter. We provide evidence\nthat deep features acquired by contrastive learning are more efficient,\nsuggesting that the proposed fusion technique also possesses strong\ngeneralisation capabilities for recognising emoticons across several modes.\n","authors":["Ananya Pandey","Dinesh Kumar Vishwakarma"],"pdf_url":"https://arxiv.org/pdf/2408.02571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02568v1","updated":"2024-08-05T15:43:56Z","published":"2024-08-05T15:43:56Z","title":"Cross-Modality Clustering-based Self-Labeling for Multimodal Data\n  Classification","summary":"  Technological advances facilitate the ability to acquire multimodal data,\nposing a challenge for recognition systems while also providing an opportunity\nto use the heterogeneous nature of the information to increase the\ngeneralization capability of models. An often overlooked issue is the cost of\nthe labeling process, which is typically high due to the need for a significant\ninvestment in time and money associated with human experts. Existing\nsemi-supervised learning methods often focus on operating in the feature space\ncreated by the fusion of available modalities, neglecting the potential for\ncross-utilizing complementary information available in each modality. To\naddress this problem, we propose Cross-Modality Clustering-based Self-Labeling\n(CMCSL). Based on a small set of pre-labeled data, CMCSL groups instances\nbelonging to each modality in the deep feature space and then propagates known\nlabels within the resulting clusters. Next, information about the instances'\nclass membership in each modality is exchanged based on the Euclidean distance\nto ensure more accurate labeling. Experimental evaluation conducted on 20\ndatasets derived from the MM-IMDb dataset indicates that cross-propagation of\nlabels between modalities -- especially when the number of pre-labeled\ninstances is small -- can allow for more reliable labeling and thus increase\nthe classification performance in each modality.\n","authors":["Paweł Zyblewski","Leandro L. Minku"],"pdf_url":"https://arxiv.org/pdf/2408.02568v1.pdf","comment":"10 pages, 5 figures, 9 tables"},{"id":"http://arxiv.org/abs/2301.07088v3","updated":"2024-08-05T15:38:05Z","published":"2023-01-17T18:53:24Z","title":"Vision Learners Meet Web Image-Text Pairs","summary":"  Many self-supervised learning methods are pre-trained on the well-curated\nImageNet-1K dataset. In this work, given the excellent scalability of web data,\nwe consider self-supervised pre-training on noisy web sourced image-text paired\ndata. First, we conduct a benchmark study of representative self-supervised\npre-training methods on large-scale web data in a like-for-like setting. We\ncompare a range of methods, including single-modal ones that use masked\ntraining objectives and multi-modal ones that use image-text constrastive\ntraining. We observe that existing multi-modal methods do not outperform their\nsingle-modal counterparts on vision transfer learning tasks. We derive an\ninformation-theoretical view to explain these benchmark results, which provides\ninsight into how to design a novel vision learner. Inspired by this insight, we\npresent a new visual representation pre-training method, MUlti-modal\nGenerator~(MUG), that learns from scalable web sourced image-text data. MUG\nachieves state-of-the-art transfer performance on a variety of tasks and\ndemonstrates promising scaling properties. Pre-trained models and code will be\nmade public upon acceptance.\n","authors":["Bingchen Zhao","Quan Cui","Hao Wu","Osamu Yoshie","Cheng Yang","Oisin Mac Aodha"],"pdf_url":"https://arxiv.org/pdf/2301.07088v3.pdf","comment":"Project page: https://bzhao.me/MUG/"},{"id":"http://arxiv.org/abs/2408.02561v1","updated":"2024-08-05T15:37:18Z","published":"2024-08-05T15:37:18Z","title":"HQOD: Harmonious Quantization for Object Detection","summary":"  Task inharmony problem commonly occurs in modern object detectors, leading to\ninconsistent qualities between classification and regression tasks. The\npredicted boxes with high classification scores but poor localization positions\nor low classification scores but accurate localization positions will worsen\nthe performance of detectors after Non-Maximum Suppression. Furthermore, when\nobject detectors collaborate with Quantization-Aware Training (QAT), we observe\nthat the task inharmony problem will be further exacerbated, which is\nconsidered one of the main causes of the performance degradation of quantized\ndetectors. To tackle this issue, we propose the Harmonious Quantization for\nObject Detection (HQOD) framework, which consists of two components. Firstly,\nwe propose a task-correlated loss to encourage detectors to focus on improving\nsamples with lower task harmony quality during QAT. Secondly, a harmonious\nIntersection over Union (IoU) loss is incorporated to balance the optimization\nof the regression branch across different IoU levels. The proposed HQOD can be\neasily integrated into different QAT algorithms and detectors. Remarkably, on\nthe MS COCO dataset, our 4-bit ATSS with ResNet-50 backbone achieves a\nstate-of-the-art mAP of 39.6%, even surpassing the full-precision one.\n","authors":["Long Huang","Zhiwei Dong","Song-Lu Chen","Ruiyao Zhang","Shutong Ti","Feng Chen","Xu-Cheng Yin"],"pdf_url":"https://arxiv.org/pdf/2408.02561v1.pdf","comment":"2024 IEEE International Conference on Multimedia and Expo (ICME),\n  July 15 - July 19, 2024, Niagra Falls, Ontario, Canada"},{"id":"http://arxiv.org/abs/2408.02555v1","updated":"2024-08-05T15:33:45Z","published":"2024-08-05T15:33:45Z","title":"MeshAnything V2: Artist-Created Mesh Generation With Adjacent Mesh\n  Tokenization","summary":"  We introduce MeshAnything V2, an autoregressive transformer that generates\nArtist-Created Meshes (AM) aligned to given shapes. It can be integrated with\nvarious 3D asset production pipelines to achieve high-quality, highly\ncontrollable AM generation. MeshAnything V2 surpasses previous methods in both\nefficiency and performance using models of the same size. These improvements\nare due to our newly proposed mesh tokenization method: Adjacent Mesh\nTokenization (AMT). Different from previous methods that represent each face\nwith three vertices, AMT uses a single vertex whenever possible. Compared to\nprevious methods, AMT requires about half the token sequence length to\nrepresent the same mesh in average. Furthermore, the token sequences from AMT\nare more compact and well-structured, fundamentally benefiting AM generation.\nOur extensive experiments show that AMT significantly improves the efficiency\nand performance of AM generation. Project Page:\nhttps://buaacyw.github.io/meshanything-v2/\n","authors":["Yiwen Chen","Yikai Wang","Yihao Luo","Zhengyi Wang","Zilong Chen","Jun Zhu","Chi Zhang","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2408.02555v1.pdf","comment":"Project Page: https://buaacyw.github.io/meshanything-v2/ Github:\n  https://github.com/buaacyw/MeshAnythingV2"},{"id":"http://arxiv.org/abs/2408.02507v1","updated":"2024-08-05T14:31:09Z","published":"2024-08-05T14:31:09Z","title":"Estimating Pore Location of PBF-LB/M Processes with Segmentation Models","summary":"  Reliably manufacturing defect free products is still an open challenge for\nLaser Powder Bed Fusion processes. Particularly, pores that occur frequently\nhave a negative impact on mechanical properties like fatigue performance.\nTherefore, an accurate localisation of pores is mandatory for quality\nassurance, but requires time-consuming post-processing steps like computer\ntomography scans. Although existing solutions using in-situ monitoring data can\ndetect pore occurrence within a layer, they are limited in their localisation\nprecision. Therefore, we propose a pore localisation approach that estimates\ntheir position within a single layer using a Gaussian kernel density\nestimation. This allows segmentation models to learn the correlation between\nin-situ monitoring data and the derived probability distribution of pore\noccurrence. Within our experiments, we compare the prediction performance of\ndifferent segmentation models depending on machine parameter configuration and\ngeometry features. From our results, we conclude that our approach allows a\nprecise localisation of pores that requires minimal data preprocessing. Our\nresearch extends the literature by providing a foundation for more precise pore\ndetection systems.\n","authors":["Hans Aoyang Zhou","Jan Theunissen","Marco Kemmerling","Anas Abdelrazeq","Johannes Henrich Schleifenbaum","Robert H. Schmitt"],"pdf_url":"https://arxiv.org/pdf/2408.02507v1.pdf","comment":"20 pages, 7 figures, This work has been submitted to the Journal\n  Progress in Additive Manufacturing"},{"id":"http://arxiv.org/abs/2408.02496v1","updated":"2024-08-05T14:19:03Z","published":"2024-08-05T14:19:03Z","title":"Automatic rating of incomplete hippocampal inversions evaluated across\n  multiple cohorts","summary":"  Incomplete Hippocampal Inversion (IHI), sometimes called hippocampal\nmalrotation, is an atypical anatomical pattern of the hippocampus found in\nabout 20% of the general population. IHI can be visually assessed on coronal\nslices of T1 weighted MR images, using a composite score that combines four\nanatomical criteria. IHI has been associated with several brain disorders\n(epilepsy, schizophrenia). However, these studies were based on small samples.\nFurthermore, the factors (genetic or environmental) that contribute to the\ngenesis of IHI are largely unknown. Large-scale studies are thus needed to\nfurther understand IHI and their potential relationships to neurological and\npsychiatric disorders. However, visual evaluation is long and tedious,\njustifying the need for an automatic method. In this paper, we propose, for the\nfirst time, to automatically rate IHI. We proceed by predicting four anatomical\ncriteria, which are then summed up to form the IHI score, providing the\nadvantage of an interpretable score. We provided an extensive experimental\ninvestigation of different machine learning methods and training strategies. We\nperformed automatic rating using a variety of deep learning models (conv5-FC3,\nResNet and SECNN) as well as a ridge regression. We studied the generalization\nof our models using different cohorts and performed multi-cohort learning. We\nrelied on a large population of 2,008 participants from the IMAGEN study, 993\nand 403 participants from the QTIM/QTAB studies as well as 985 subjects from\nthe UKBiobank. We showed that deep learning models outperformed a ridge\nregression. We demonstrated that the performances of the conv5-FC3 network were\nat least as good as more complex networks while maintaining a low complexity\nand computation time. We showed that training on a single cohort may lack in\nvariability while training on several cohorts improves generalization.\n","authors":["Lisa Hemforth","Baptiste Couvy-Duchesne","Kevin De Matos","Camille Brianceau","Matthieu Joulot","Tobias Banaschewski","Arun L. W. Bokde","Sylvane Desrivières","Herta Flor","Antoine Grigis","Hugh Garavan","Penny Gowland","Andreas Heinz","Rüdiger Brühl","Jean-Luc Martinot","Marie-Laure Paillère Martinot","Eric Artiges","Dimitri Papadopoulos","Herve Lemaitre","Tomas Paus","Luise Poustka","Sarah Hohmann","Nathalie Holz","Juliane H. Fröhner","Michael N. Smolka","Nilakshi Vaidya","Henrik Walter","Robert Whelan","Gunter Schumann","Christian Büchel","JB Poline","Bernd Itterman","Vincent Frouin","Alexandre Martin","IMAGEN study group","Claire Cury","Olivier Colliot"],"pdf_url":"https://arxiv.org/pdf/2408.02496v1.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2024:016"},{"id":"http://arxiv.org/abs/2408.02494v1","updated":"2024-08-05T14:18:29Z","published":"2024-08-05T14:18:29Z","title":"HyperSpaceX: Radial and Angular Exploration of HyperSpherical Dimensions","summary":"  Traditional deep learning models rely on methods such as softmax\ncross-entropy and ArcFace loss for tasks like classification and face\nrecognition. These methods mainly explore angular features in a hyperspherical\nspace, often resulting in entangled inter-class features due to dense angular\ndata across many classes. In this paper, a new field of feature exploration is\nproposed known as HyperSpaceX which enhances class discrimination by exploring\nboth angular and radial dimensions in multi-hyperspherical spaces, facilitated\nby a novel DistArc loss. The proposed DistArc loss encompasses three feature\narrangement components: two angular and one radial, enforcing intra-class\nbinding and inter-class separation in multi-radial arrangement, improving\nfeature discriminability. Evaluation of HyperSpaceX framework for the novel\nrepresentation utilizes a proposed predictive measure that accounts for both\nangular and radial elements, providing a more comprehensive assessment of model\naccuracy beyond standard metrics. Experiments across seven object\nclassification and six face recognition datasets demonstrate state-of-the-art\n(SoTA) results obtained from HyperSpaceX, achieving up to a 20% performance\nimprovement on large-scale object datasets in lower dimensions and up to 6%\ngain in higher dimensions.\n","authors":["Chiranjeev Chiranjeev","Muskan Dosi","Kartik Thakral","Mayank Vatsa","Richa Singh"],"pdf_url":"https://arxiv.org/pdf/2408.02494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10107v2","updated":"2024-08-05T14:06:35Z","published":"2024-06-14T15:08:04Z","title":"Annotation Cost-Efficient Active Learning for Deep Metric Learning\n  Driven Remote Sensing Image Retrieval","summary":"  Deep metric learning (DML) has shown to be effective for content-based image\nretrieval (CBIR) in remote sensing (RS). Most of DML methods for CBIR rely on a\nhigh number of annotated images to accurately learn model parameters of deep\nneural networks (DNNs). However, gathering such data is time-consuming and\ncostly. To address this, we propose an annotation cost-efficient active\nlearning (ANNEAL) method tailored to DML-driven CBIR in RS. ANNEAL aims to\ncreate a small but informative training set made up of similar and dissimilar\nimage pairs to be utilized for accurately learning a metric space. The\ninformativeness of image pairs is evaluated by combining uncertainty and\ndiversity criteria. To assess the uncertainty of image pairs, we introduce two\nalgorithms: 1) metric-guided uncertainty estimation (MGUE); and 2) binary\nclassifier guided uncertainty estimation (BCGUE). MGUE algorithm automatically\nestimates a threshold value that acts as a boundary between similar and\ndissimilar image pairs based on the distances in the metric space. The closer\nthe similarity between image pairs is to the estimated threshold value the\nhigher their uncertainty. BCGUE algorithm estimates the uncertainty of the\nimage pairs based on the confidence of the classifier in assigning correct\nsimilarity labels. The diversity criterion is assessed through a\nclustering-based strategy. ANNEAL combines either MGUE or BCGUE algorithm with\nthe clustering-based strategy to select the most informative image pairs, which\nare then labelled by expert annotators as similar or dissimilar. This way of\nannotating images significantly reduces the annotation cost compared to\nannotating images with land-use land-cover class labels. Experimental results\non two RS benchmark datasets demonstrate the effectiveness of our method. The\ncode of this work is publicly available at\n\\url{https://git.tu-berlin.de/rsim/anneal_tgrs}.\n","authors":["Genc Hoxha","Gencer Sumbul","Julia Henkel","Lars Möllenbrok","Begüm Demir"],"pdf_url":"https://arxiv.org/pdf/2406.10107v2.pdf","comment":"Accepted for publication in the IEEE Transactions on Geoscience and\n  Remote Sensing (TGRS)"},{"id":"http://arxiv.org/abs/2408.02484v1","updated":"2024-08-05T14:05:25Z","published":"2024-08-05T14:05:25Z","title":"Exploring Conditional Multi-Modal Prompts for Zero-shot HOI Detection","summary":"  Zero-shot Human-Object Interaction (HOI) detection has emerged as a frontier\ntopic due to its capability to detect HOIs beyond a predefined set of\ncategories. This task entails not only identifying the interactiveness of\nhuman-object pairs and localizing them but also recognizing both seen and\nunseen interaction categories. In this paper, we introduce a novel framework\nfor zero-shot HOI detection using Conditional Multi-Modal Prompts, namely CMMP.\nThis approach enhances the generalization of large foundation models, such as\nCLIP, when fine-tuned for HOI detection. Unlike traditional prompt-learning\nmethods, we propose learning decoupled vision and language prompts for\ninteractiveness-aware visual feature extraction and generalizable interaction\nclassification, respectively. Specifically, we integrate prior knowledge of\ndifferent granularity into conditional vision prompts, including an\ninput-conditioned instance prior and a global spatial pattern prior. The former\nencourages the image encoder to treat instances belonging to seen or\npotentially unseen HOI concepts equally while the latter provides\nrepresentative plausible spatial configuration of the human and object under\ninteraction. Besides, we employ language-aware prompt learning with a\nconsistency constraint to preserve the knowledge of the large foundation model\nto enable better generalization in the text branch. Extensive experiments\ndemonstrate the efficacy of our detector with conditional multi-modal prompts,\noutperforming previous state-of-the-art on unseen classes of various zero-shot\nsettings. The code and models are available at\n\\url{https://github.com/ltttpku/CMMP}.\n","authors":["Ting Lei","Shaofeng Yin","Yuxin Peng","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2408.02484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02875v2","updated":"2024-08-05T14:01:26Z","published":"2024-03-05T11:38:48Z","title":"Enhancing Conceptual Understanding in Multimodal Contrastive Learning\n  through Hard Negative Samples","summary":"  Current multimodal models leveraging contrastive learning often face\nlimitations in developing fine-grained conceptual understanding. This is due to\nrandom negative samples during pretraining, causing almost exclusively very\ndissimilar concepts to be compared in the loss function. Consequently, the\nmodels struggle with fine-grained semantic differences. To address this\nproblem, we introduce a novel pretraining method incorporating synthetic hard\nnegative text examples. The hard negatives permute terms corresponding to\nvisual concepts, leading to a more fine-grained visual and textual concept\nalignment. Further, we introduce InpaintCOCO, a new challenging dataset for\nassessing the fine-grained alignment of colors, objects, and sizes in\nvision-language models. We created the dataset using generative inpainting from\nCOCO images by changing the visual concepts so that the images no longer match\ntheir original captions. Our results show significant improvements in\nfine-grained concept understanding across a wide range of vision-language\ndatasets, including our InpaintCOCO dataset.\n","authors":["Philipp J. Rösch","Norbert Oswald","Michaela Geierhos","Jindřich Libovický"],"pdf_url":"https://arxiv.org/pdf/2403.02875v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02464v1","updated":"2024-08-05T13:44:22Z","published":"2024-08-05T13:44:22Z","title":"Fairness and Bias Mitigation in Computer Vision: A Survey","summary":"  Computer vision systems have witnessed rapid progress over the past two\ndecades due to multiple advances in the field. As these systems are\nincreasingly being deployed in high-stakes real-world applications, there is a\ndire need to ensure that they do not propagate or amplify any discriminatory\ntendencies in historical or human-curated data or inadvertently learn biases\nfrom spurious correlations. This paper presents a comprehensive survey on\nfairness that summarizes and sheds light on ongoing trends and successes in the\ncontext of computer vision. The topics we discuss include 1) The origin and\ntechnical definitions of fairness drawn from the wider fair machine learning\nliterature and adjacent disciplines. 2) Work that sought to discover and\nanalyze biases in computer vision systems. 3) A summary of methods proposed to\nmitigate bias in computer vision systems in recent years. 4) A comprehensive\nsummary of resources and datasets produced by researchers to measure, analyze,\nand mitigate bias and enhance fairness. 5) Discussion of the field's success,\ncontinuing trends in the context of multimodal foundation and generative\nmodels, and gaps that still need to be addressed. The presented\ncharacterization should help researchers understand the importance of\nidentifying and mitigating bias in computer vision and the state of the field\nand identify potential directions for future research.\n","authors":["Sepehr Dehdashtian","Ruozhen He","Yi Li","Guha Balakrishnan","Nuno Vasconcelos","Vicente Ordonez","Vishnu Naresh Boddeti"],"pdf_url":"https://arxiv.org/pdf/2408.02464v1.pdf","comment":"20 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.02462v1","updated":"2024-08-05T13:40:33Z","published":"2024-08-05T13:40:33Z","title":"An investigation into the causes of race bias in AI-based cine CMR\n  segmentation","summary":"  Artificial intelligence (AI) methods are being used increasingly for the\nautomated segmentation of cine cardiac magnetic resonance (CMR) imaging.\nHowever, these methods have been shown to be subject to race bias, i.e. they\nexhibit different levels of performance for different races depending on the\n(im)balance of the data used to train the AI model. In this paper we\ninvestigate the source of this bias, seeking to understand its root cause(s) so\nthat it can be effectively mitigated. We perform a series of classification and\nsegmentation experiments on short-axis cine CMR images acquired from Black and\nWhite subjects from the UK Biobank and apply AI interpretability methods to\nunderstand the results. In the classification experiments, we found that race\ncan be predicted with high accuracy from the images alone, but less accurately\nfrom ground truth segmentations, suggesting that the distributional shift\nbetween races, which is often the cause of AI bias, is mostly image-based\nrather than segmentation-based. The interpretability methods showed that most\nattention in the classification models was focused on non-heart regions, such\nas subcutaneous fat. Cropping the images tightly around the heart reduced\nclassification accuracy to around chance level. Similarly, race can be\npredicted from the latent representations of a biased segmentation model,\nsuggesting that race information is encoded in the model. Cropping images\ntightly around the heart reduced but did not eliminate segmentation bias. We\nalso investigate the influence of possible confounders on the bias observed.\n","authors":["Tiarna Lee","Esther Puyol-Anton","Bram Ruijsink","Sebastien Roujol","Theodore Barfoot","Shaheim Ogbomo-Harmitt","Miaojing Shi","Andrew P. King"],"pdf_url":"https://arxiv.org/pdf/2408.02462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15269v2","updated":"2024-08-05T13:23:17Z","published":"2024-06-21T16:04:14Z","title":"You Only Acquire Sparse-channel (YOAS): A Unified Framework for\n  Dense-channel EEG Generation","summary":"  High-precision acquisition of dense-channel electroencephalogram (EEG)\nsignals is often impeded by the costliness and lack of portability of\nequipment. In contrast, generating dense-channel EEG signals effectively from\nsparse channels shows promise and economic viability. However, sparse-channel\nEEG poses challenges such as reduced spatial resolution, information loss,\nsignal mixing, and heightened susceptibility to noise and interference. To\naddress these challenges, we first theoretically formulate the dense-channel\nEEG generation problem as by optimizing a set of cross-channel EEG signal\ngeneration problems. Then, we propose the YOAS framework for generating\ndense-channel data from sparse-channel EEG signals. The YOAS totally consists\nof four sequential stages: Data Preparation, Data Preprocessing, Biased-EEG\nGeneration, and Synthetic EEG Generation. Data Preparation and Preprocessing\ncarefully consider the distribution of EEG electrodes and low signal-to-noise\nratio problem of EEG signals. Biased-EEG Generation includes sub-modules of\nBiasEEGGanFormer and BiasEEGDiffFormer, which facilitate long-term feature\nextraction with attention and generate signals by combining electrode position\nalignment with diffusion model, respectively. Synthetic EEG Generation\nsynthesizes the final signals, employing a deduction paradigm for multi-channel\nEEG generation. Extensive experiments confirmed YOAS's feasibility, efficiency,\nand theoretical validity, even remarkably enhancing data discernibility. This\nbreakthrough in dense-channel EEG signal generation from sparse-channel data\nopens new avenues for exploration in EEG signal processing and application.\n","authors":["Hongyu Chen","Weiming Zeng","Luhui Cai","Lei Wang","Jia Lu","Yueyang Li","Hongjie Yan","Wai Ting Siok","Nizhuan Wang"],"pdf_url":"https://arxiv.org/pdf/2406.15269v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12890v3","updated":"2024-08-05T13:10:02Z","published":"2023-11-21T06:24:09Z","title":"De-fine: Decomposing and Refining Visual Programs with Auto-Feedback","summary":"  Visual programming, a modular and generalizable paradigm, integrates\ndifferent modules and Python operators to solve various vision-language tasks.\nUnlike end-to-end models that need task-specific data, it advances in\nperforming visual processing and reasoning in an unsupervised manner. Current\nvisual programming methods generate programs in a single pass for each task\nwhere the ability to evaluate and optimize based on feedback, unfortunately, is\nlacking, which consequentially limits their effectiveness for complex,\nmulti-step problems. Drawing inspiration from benders decomposition, we\nintroduce De-fine, a training-free framework that automatically decomposes\ncomplex tasks into simpler subtasks and refines programs through auto-feedback.\nThis model-agnostic approach can improve logical reasoning performance by\nintegrating the strengths of multiple models. Our experiments across various\nvisual tasks show that De-fine creates more robust programs. Moreover, viewing\neach feedback module as an independent agent will yield fresh prospects for the\nfield of agent research.\n","authors":["Minghe Gao","Juncheng Li","Hao Fei","Liang Pang","Wei Ji","Guoming Wang","Zheqi Lv","Wenqiao Zhang","Siliang Tang","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2311.12890v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07990v2","updated":"2024-08-05T12:55:47Z","published":"2024-04-11T17:59:56Z","title":"OpenBias: Open-set Bias Detection in Text-to-Image Generative Models","summary":"  Text-to-image generative models are becoming increasingly popular and\naccessible to the general public. As these models see large-scale deployments,\nit is necessary to deeply investigate their safety and fairness to not\ndisseminate and perpetuate any kind of biases. However, existing works focus on\ndetecting closed sets of biases defined a priori, limiting the studies to\nwell-known concepts. In this paper, we tackle the challenge of open-set bias\ndetection in text-to-image generative models presenting OpenBias, a new\npipeline that identifies and quantifies the severity of biases agnostically,\nwithout access to any precompiled set. OpenBias has three stages. In the first\nphase, we leverage a Large Language Model (LLM) to propose biases given a set\nof captions. Secondly, the target generative model produces images using the\nsame set of captions. Lastly, a Vision Question Answering model recognizes the\npresence and extent of the previously proposed biases. We study the behavior of\nStable Diffusion 1.5, 2, and XL emphasizing new biases, never investigated\nbefore. Via quantitative experiments, we demonstrate that OpenBias agrees with\ncurrent closed-set bias detection methods and human judgement.\n","authors":["Moreno D'Incà","Elia Peruzzo","Massimiliano Mancini","Dejia Xu","Vidit Goel","Xingqian Xu","Zhangyang Wang","Humphrey Shi","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2404.07990v2.pdf","comment":"CVPR 2024 Highlight - Code:\n  https://github.com/Picsart-AI-Research/OpenBias"},{"id":"http://arxiv.org/abs/2303.10571v2","updated":"2024-08-05T12:44:04Z","published":"2023-03-19T05:20:52Z","title":"Reinforcement Learning Friendly Vision-Language Model for Minecraft","summary":"  One of the essential missions in the AI research community is to build an\nautonomous embodied agent that can achieve high-level performance across a wide\nspectrum of tasks. However, acquiring or manually designing rewards for all\nopen-ended tasks is unrealistic. In this paper, we propose a novel cross-modal\ncontrastive learning framework architecture, CLIP4MC, aiming to learn a\nreinforcement learning (RL) friendly vision-language model (VLM) that serves as\nan intrinsic reward function for open-ended tasks. Simply utilizing the\nsimilarity between the video snippet and the language prompt is not RL-friendly\nsince standard VLMs may only capture the similarity at a coarse level. To\nachieve RL-friendliness, we incorporate the task completion degree into the VLM\ntraining objective, as this information can assist agents in distinguishing the\nimportance between different states. Moreover, we provide neat YouTube datasets\nbased on the large-scale YouTube database provided by MineDojo. Specifically,\ntwo rounds of filtering operations guarantee that the dataset covers enough\nessential information and that the video-text pair is highly correlated.\nEmpirically, we demonstrate that the proposed method achieves better\nperformance on RL tasks compared with baselines. The code and datasets are\navailable at https://github.com/PKU-RL/CLIP4MC.\n","authors":["Haobin Jiang","Junpeng Yue","Hao Luo","Ziluo Ding","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2303.10571v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2404.11129v2","updated":"2024-08-05T12:39:06Z","published":"2024-04-17T07:20:56Z","title":"Fact :Teaching MLLMs with Faithful, Concise and Transferable Rationales","summary":"  The remarkable performance of Multimodal Large Language Models (MLLMs) has\nunequivocally demonstrated their proficient understanding capabilities in\nhandling a wide array of visual tasks. Nevertheless, the opaque nature of their\nblack-box reasoning processes persists as an enigma, rendering them\nuninterpretable and struggling with hallucination. Their ability to execute\nintricate compositional reasoning tasks is also constrained, culminating in a\nstagnation of learning progression for these models. In this work, we introduce\nFact, a novel paradigm designed to generate multimodal rationales that are\nfaithful, concise, and transferable for teaching MLLMs. This paradigm utilizes\nverifiable visual programming to generate executable code guaranteeing\nfaithfulness and precision. Subsequently, through a series of operations\nincluding pruning, merging, and bridging, the rationale enhances its\nconciseness. Furthermore, we filter rationales that can be transferred to\nend-to-end paradigms from programming paradigms to guarantee transferability.\nEmpirical evidence from experiments demonstrates the superiority of our method\nacross models of varying parameter sizes, significantly enhancing their\ncompositional reasoning and generalization ability. Our approach also reduces\nhallucinations owing to its high correlation between images and text.\n","authors":["Minghe Gao","Shuang Chen","Liang Pang","Yuan Yao","Jisheng Dang","Wenqiao Zhang","Juncheng Li","Siliang Tang","Yueting Zhuang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2404.11129v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02427v1","updated":"2024-08-05T12:34:49Z","published":"2024-08-05T12:34:49Z","title":"Attenuation-adjusted deep learning of pore defects in 2D radiographs of\n  additive manufacturing powders","summary":"  The presence of gas pores in metal feedstock powder for additive\nmanufacturing greatly affects the final AM product. Since current porosity\nanalysis often involves lengthy X-ray computed tomography (XCT) scans with a\nfull rotation around the sample, motivation exists to explore methods that\nallow for high throughput -- possibly enabling in-line porosity analysis during\nmanufacturing. Through labelling pore pixels on single 2D radiographs of\npowders, this work seeks to simulate such future efficient setups. High\nsegmentation accuracy is achieved by combining a model of X-ray attenuation\nthrough particles with a variant of the widely applied UNet architecture;\nnotably, F1-score increases by $11.4\\%$ compared to the baseline UNet. The\nproposed pore segmentation is enabled by: 1) pretraining on synthetic data, 2)\nmaking tight particle cutouts, and 3) subtracting an ideal particle without\npores generated from a distance map inspired by Lambert-Beers law. This paper\nexplores four image processing methods, where the fastest (yet still\nunoptimized) segments a particle in mean $0.014s$ time with F1-score $0.78$,\nand the most accurate in $0.291s$ with F1-score $0.87$. Due to their scalable\nnature, these strategies can be involved in making high throughput porosity\nanalysis of metal feedstock powder for additive manufacturing.\n","authors":["Andreas Bjerregaard","David Schumacher","Jon Sporring"],"pdf_url":"https://arxiv.org/pdf/2408.02427v1.pdf","comment":"Implementation on https://github.com/yhsure/porosity"},{"id":"http://arxiv.org/abs/2408.02426v1","updated":"2024-08-05T12:33:07Z","published":"2024-08-05T12:33:07Z","title":"FPT+: A Parameter and Memory Efficient Transfer Learning Method for\n  High-resolution Medical Image Classification","summary":"  The success of large-scale pre-trained models has established fine-tuning as\na standard method for achieving significant improvements in downstream tasks.\nHowever, fine-tuning the entire parameter set of a pre-trained model is costly.\nParameter-efficient transfer learning (PETL) has recently emerged as a\ncost-effective alternative for adapting pre-trained models to downstream tasks.\nDespite its advantages, the increasing model size and input resolution present\nchallenges for PETL, as the training memory consumption is not reduced as\neffectively as the parameter usage. In this paper, we introduce Fine-grained\nPrompt Tuning plus (FPT+), a PETL method designed for high-resolution medical\nimage classification, which significantly reduces memory consumption compared\nto other PETL methods. FPT+ performs transfer learning by training a\nlightweight side network and accessing pre-trained knowledge from a large\npre-trained model (LPM) through fine-grained prompts and fusion modules.\nSpecifically, we freeze the LPM and construct a learnable lightweight side\nnetwork. The frozen LPM processes high-resolution images to extract\nfine-grained features, while the side network employs the corresponding\ndown-sampled low-resolution images to minimize the memory usage. To enable the\nside network to leverage pre-trained knowledge, we propose fine-grained prompts\nand fusion modules, which collaborate to summarize information through the\nLPM's intermediate activations. We evaluate FPT+ on eight medical image\ndatasets of varying sizes, modalities, and complexities. Experimental results\ndemonstrate that FPT+ outperforms other PETL methods, using only 1.03% of the\nlearnable parameters and 3.18% of the memory required for fine-tuning an entire\nViT-B model. Our code is available at https://github.com/YijinHuang/FPT.\n","authors":["Yijin Huang","Pujin Cheng","Roger Tam","Xiaoying Tang"],"pdf_url":"https://arxiv.org/pdf/2408.02426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19719v2","updated":"2024-08-05T12:29:47Z","published":"2024-07-29T06:03:13Z","title":"Revolutionizing Urban Safety Perception Assessments: Integrating\n  Multimodal Large Language Models with Street View Images","summary":"  Measuring urban safety perception is an important and complex task that\ntraditionally relies heavily on human resources. This process often involves\nextensive field surveys, manual data collection, and subjective assessments,\nwhich can be time-consuming, costly, and sometimes inconsistent. Street View\nImages (SVIs), along with deep learning methods, provide a way to realize\nlarge-scale urban safety detection. However, achieving this goal often requires\nextensive human annotation to train safety ranking models, and the\narchitectural differences between cities hinder the transferability of these\nmodels. Thus, a fully automated method for conducting safety evaluations is\nessential. Recent advances in multimodal large language models (MLLMs) have\ndemonstrated powerful reasoning and analytical capabilities. Cutting-edge\nmodels, e.g., GPT-4 have shown surprising performance in many tasks. We\nemployed these models for urban safety ranking on a human-annotated anchor set\nand validated that the results from MLLMs align closely with human perceptions.\nAdditionally, we proposed a method based on the pre-trained Contrastive\nLanguage-Image Pre-training (CLIP) feature and K-Nearest Neighbors (K-NN)\nretrieval to quickly assess the safety index of the entire city. Experimental\nresults show that our method outperforms existing training needed deep learning\napproaches, achieving efficient and accurate urban safety evaluations. The\nproposed automation for urban safety perception assessment is a valuable tool\nfor city planners, policymakers, and researchers aiming to improve urban\nenvironments.\n","authors":["Jiaxin Zhang","Yunqin Li","Tomohiro Fukuda","Bowen Wang"],"pdf_url":"https://arxiv.org/pdf/2407.19719v2.pdf","comment":"13 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.02421v1","updated":"2024-08-05T12:27:28Z","published":"2024-08-05T12:27:28Z","title":"FE-Adapter: Adapting Image-based Emotion Classifiers to Videos","summary":"  Utilizing large pre-trained models for specific tasks has yielded impressive\nresults. However, fully fine-tuning these increasingly large models is becoming\nprohibitively resource-intensive. This has led to a focus on more\nparameter-efficient transfer learning, primarily within the same modality. But\nthis approach has limitations, particularly in video understanding where\nsuitable pre-trained models are less common. Addressing this, our study\nintroduces a novel cross-modality transfer learning approach from images to\nvideos, which we call parameter-efficient image-to-video transfer learning. We\npresent the Facial-Emotion Adapter (FE-Adapter), designed for efficient\nfine-tuning in video tasks. This adapter allows pre-trained image models, which\ntraditionally lack temporal processing capabilities, to analyze dynamic video\ncontent efficiently. Notably, it uses about 15 times fewer parameters than\nprevious methods, while improving accuracy. Our experiments in video emotion\nrecognition demonstrate that the FE-Adapter can match or even surpass existing\nfine-tuning and video emotion models in both performance and efficiency. This\nbreakthrough highlights the potential for cross-modality approaches in\nenhancing the capabilities of AI models, particularly in fields like video\nemotion analysis where the demand for efficiency and accuracy is constantly\nrising.\n","authors":["Shreyank N Gowda","Boyan Gao","David A. Clifton"],"pdf_url":"https://arxiv.org/pdf/2408.02421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12198v2","updated":"2024-08-05T12:20:49Z","published":"2024-02-19T15:03:04Z","title":"Zero shot VLMs for hate meme detection: Are we there yet?","summary":"  Multimedia content on social media is rapidly evolving, with memes gaining\nprominence as a distinctive form. Unfortunately, some malicious users exploit\nmemes to target individuals or vulnerable communities, making it imperative to\nidentify and address such instances of hateful memes. Extensive research has\nbeen conducted to address this issue by developing hate meme detection models.\nHowever, a notable limitation of traditional machine/deep learning models is\nthe requirement for labeled datasets for accurate classification. Recently, the\nresearch community has witnessed the emergence of several visual language\nmodels that have exhibited outstanding performance across various tasks. In\nthis study, we aim to investigate the efficacy of these visual language models\nin handling intricate tasks such as hate meme detection. We use various prompt\nsettings to focus on zero-shot classification of hateful/harmful memes. Through\nour analysis, we observe that large VLMs are still vulnerable for zero-shot\nhate meme detection.\n","authors":["Naquee Rizwan","Paramananda Bhaskar","Mithun Das","Swadhin Satyaprakash Majhi","Punyajoy Saha","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2402.12198v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19905v2","updated":"2024-08-05T12:12:48Z","published":"2024-06-28T13:20:17Z","title":"Solving Token Gradient Conflict in Mixture-of-Experts for Large\n  Vision-Language Model","summary":"  The Mixture-of-Experts (MoE) has gained increasing attention in studying\nLarge Vision-Language Models (LVLMs). It uses a sparse model to replace the\ndense model, achieving comparable performance while activating fewer parameters\nduring inference, thus significantly reducing the inference cost. Existing MoE\nmethods in LVLMs encourage different experts to handle different tokens, and\nthey usually employ a router to predict the routing of each token. However, the\npredictions are based solely on sample features and do not truly reveal the\noptimization directions of tokens. This may lead to severe optimization\ninterference between different tokens assigned to an expert. To address this\nproblem, this paper proposes a novel method based on token-level gradient\nanalysis, i.e., Solving Token Gradient Conflict (STGC). Specifically, we first\nuse token-level gradients to identify conflicting tokens in experts. After\nthat, we add a specialized loss tailored to eliminate conflicts among tokens\nwithin each expert. Our method can serve as a plug-in for diverse Large\nVision-Language Models, and extensive experimental results demonstrate its\neffectiveness. The code will be publicly available at\nhttps://github.com/longrongyang/STGC.\n","authors":["Longrong Yang","Dong Shen","Chaoxiang Cai","Fan Yang","Size Li","Di Zhang","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2406.19905v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02408v1","updated":"2024-08-05T12:09:38Z","published":"2024-08-05T12:09:38Z","title":"Multi-weather Cross-view Geo-localization Using Denoising Diffusion\n  Models","summary":"  Cross-view geo-localization in GNSS-denied environments aims to determine an\nunknown location by matching drone-view images with the correct geo-tagged\nsatellite-view images from a large gallery. Recent research shows that learning\ndiscriminative image representations under specific weather conditions can\nsignificantly enhance performance. However, the frequent occurrence of unseen\nextreme weather conditions hinders progress. This paper introduces MCGF, a\nMulti-weather Cross-view Geo-localization Framework designed to dynamically\nadapt to unseen weather conditions. MCGF establishes a joint optimization\nbetween image restoration and geo-localization using denoising diffusion\nmodels. For image restoration, MCGF incorporates a shared encoder and a\nlightweight restoration module to help the backbone eliminate weather-specific\ninformation. For geo-localization, MCGF uses EVA-02 as a backbone for feature\nextraction, with cross-entropy loss for training and cosine distance for\ntesting. Extensive experiments on University160k-WX demonstrate that MCGF\nachieves competitive results for geo-localization in varying weather\nconditions.\n","authors":["Tongtong Feng","Qing Li","Xin Wang","Mingzi Wang","Guangyao Li","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.02408v1.pdf","comment":"Accepted by ACM MM24 workshop"},{"id":"http://arxiv.org/abs/2408.02398v1","updated":"2024-08-05T11:42:41Z","published":"2024-08-05T11:42:41Z","title":"Tensorial template matching for fast cross-correlation with rotations\n  and its application for tomography","summary":"  Object detection is a main task in computer vision. Template matching is the\nreference method for detecting objects with arbitrary templates. However,\ntemplate matching computational complexity depends on the rotation accuracy,\nbeing a limiting factor for large 3D images (tomograms). Here, we implement a\nnew algorithm called tensorial template matching, based on a mathematical\nframework that represents all rotations of a template with a tensor field.\nContrary to standard template matching, the computational complexity of the\npresented algorithm is independent of the rotation accuracy. Using both,\nsynthetic and real data from tomography, we demonstrate that tensorial template\nmatching is much faster than template matching and has the potential to improve\nits accuracy\n","authors":["Antonio Martinez-Sanchez","Ulrike Homberg","José María Almira","Harold Phelippeau"],"pdf_url":"https://arxiv.org/pdf/2408.02398v1.pdf","comment":"Accepted in The 18th European Conference on Computer Vision ECCV 2024"},{"id":"http://arxiv.org/abs/2408.02394v1","updated":"2024-08-05T11:40:59Z","published":"2024-08-05T11:40:59Z","title":"CMR-Agent: Learning a Cross-Modal Agent for Iterative Image-to-Point\n  Cloud Registration","summary":"  Image-to-point cloud registration aims to determine the relative camera pose\nof an RGB image with respect to a point cloud. It plays an important role in\ncamera localization within pre-built LiDAR maps. Despite the modality gaps,\nmost learning-based methods establish 2D-3D point correspondences in feature\nspace without any feedback mechanism for iterative optimization, resulting in\npoor accuracy and interpretability. In this paper, we propose to reformulate\nthe registration procedure as an iterative Markov decision process, allowing\nfor incremental adjustments to the camera pose based on each intermediate\nstate. To achieve this, we employ reinforcement learning to develop a\ncross-modal registration agent (CMR-Agent), and use imitation learning to\ninitialize its registration policy for stability and quick-start of the\ntraining. According to the cross-modal observations, we propose a 2D-3D hybrid\nstate representation that fully exploits the fine-grained features of RGB\nimages while reducing the useless neutral states caused by the spatial\ntruncation of camera frustum. Additionally, the overall framework is\nwell-designed to efficiently reuse one-shot cross-modal embeddings, avoiding\nrepetitive and time-consuming feature extraction. Extensive experiments on the\nKITTI-Odometry and NuScenes datasets demonstrate that CMR-Agent achieves\ncompetitive accuracy and efficiency in registration. Once the one-shot\nembeddings are completed, each iteration only takes a few milliseconds.\n","authors":["Gongxin Yao","Yixin Xuan","Xinyang Li","Yu Pan"],"pdf_url":"https://arxiv.org/pdf/2408.02394v1.pdf","comment":"Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2024"},{"id":"http://arxiv.org/abs/2408.02392v1","updated":"2024-08-05T11:39:22Z","published":"2024-08-05T11:39:22Z","title":"MaFreeI2P: A Matching-Free Image-to-Point Cloud Registration Paradigm\n  with Active Camera Pose Retrieval","summary":"  Image-to-point cloud registration seeks to estimate their relative camera\npose, which remains an open question due to the data modality gaps. The recent\nmatching-based methods tend to tackle this by building 2D-3D correspondences.\nIn this paper, we reveal the information loss inherent in these methods and\npropose a matching-free paradigm, named MaFreeI2P. Our key insight is to\nactively retrieve the camera pose in SE(3) space by contrasting the geometric\nfeatures between the point cloud and the query image. To achieve this, we first\nsample a set of candidate camera poses and construct their cost volume using\nthe cross-modal features. Superior to matching, cost volume can preserve more\ninformation and its feature similarity implicitly reflects the confidence level\nof the sampled poses. Afterwards, we employ a convolutional network to\nadaptively formulate a similarity assessment function, where the input cost\nvolume is further improved by filtering and pose-based weighting. Finally, we\nupdate the camera pose based on the similarity scores, and adopt a heuristic\nstrategy to iteratively shrink the pose sampling space for convergence. Our\nMaFreeI2P achieves a very competitive registration accuracy and recall on the\nKITTI-Odometry and Apollo-DaoxiangLake datasets.\n","authors":["Gongxin Yao","Xinyang Li","Yixin Xuan","Yu Pan"],"pdf_url":"https://arxiv.org/pdf/2408.02392v1.pdf","comment":"Accepted to IEEE Conference on Multimedia Expo 2024"},{"id":"http://arxiv.org/abs/2309.01446v4","updated":"2024-08-05T11:34:10Z","published":"2023-09-04T08:54:20Z","title":"Open Sesame! Universal Black Box Jailbreaking of Large Language Models","summary":"  Large language models (LLMs), designed to provide helpful and safe responses,\noften rely on alignment techniques to align with user intent and social\nguidelines. Unfortunately, this alignment can be exploited by malicious actors\nseeking to manipulate an LLM's outputs for unintended purposes. In this paper\nwe introduce a novel approach that employs a genetic algorithm (GA) to\nmanipulate LLMs when model architecture and parameters are inaccessible. The GA\nattack works by optimizing a universal adversarial prompt that -- when combined\nwith a user's query -- disrupts the attacked model's alignment, resulting in\nunintended and potentially harmful outputs. Our novel approach systematically\nreveals a model's limitations and vulnerabilities by uncovering instances where\nits responses deviate from expected behavior. Through extensive experiments we\ndemonstrate the efficacy of our technique, thus contributing to the ongoing\ndiscussion on responsible AI development by providing a diagnostic tool for\nevaluating and enhancing alignment of LLMs with human intent. To our knowledge\nthis is the first automated universal black box jailbreak attack.\n","authors":["Raz Lapid","Ron Langberg","Moshe Sipper"],"pdf_url":"https://arxiv.org/pdf/2309.01446v4.pdf","comment":"Accepted at SeT-LLM @ ICLR 2024"},{"id":"http://arxiv.org/abs/2408.02382v1","updated":"2024-08-05T11:14:23Z","published":"2024-08-05T11:14:23Z","title":"Cross Psuedo Supervision Framework for Sparsely Labelled Geo-spatial\n  Images","summary":"  Land Use Land Cover (LULC) mapping is essential for urban and resource\nplanning and is one of the key elements in developing smart and sustainable\ncities. This study introduces a semi-supervised segmentation model for LULC\nprediction using high-resolution satellite images with a huge diversity in data\ndistributions in different areas from the country of India. Our approach\nensures a robust generalization across different types of buildings, roads,\ntrees, and water bodies within these distinct areas. We propose a modified\nCross Pseudo Supervision framework to train image segmentation models on\nsparsely labelled data. The proposed framework addresses the limitations of the\npopular \"Cross Pseudo Supervision\" technique for semi-supervised learning.\nSpecifically, it tackles the challenges of training segmentation models on\nnoisy satellite image data with sparse and inaccurate labels. This\ncomprehensive approach enhances the accuracy and utility of LULC mapping for\nvarious urban planning applications.\n","authors":["Yash Dixit","Naman Srivastava","Joel D Joy","Rohan Olikara","Swarup E","Rakshit Ramesh"],"pdf_url":"https://arxiv.org/pdf/2408.02382v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02369v1","updated":"2024-08-05T10:38:50Z","published":"2024-08-05T10:38:50Z","title":"The NPU-ASLP System Description for Visual Speech Recognition in CNVSRC\n  2024","summary":"  This paper delineates the visual speech recognition (VSR) system introduced\nby the NPU-ASLP (Team 237) in the second Chinese Continuous Visual Speech\nRecognition Challenge (CNVSRC 2024), engaging in all four tracks, including the\nfixed and open tracks of Single-Speaker VSR Task and Multi-Speaker VSR Task. In\nterms of data processing, we leverage the lip motion extractor from the\nbaseline1 to produce multiscale video data. Besides, various augmentation\ntechniques are applied during training, encompassing speed perturbation, random\nrotation, horizontal flipping, and color transformation. The VSR model adopts\nan end-to-end architecture with joint CTC/attention loss, introducing Enhanced\nResNet3D visual frontend, E-Branchformer encoder, and Bi-directional\nTransformer decoder. Our approach yields a 30.47% CER for the Single-Speaker\nTask and 34.30% CER for the Multi-Speaker Task, securing second place in the\nopen track of the Single-Speaker Task and first place in the other three\ntracks.\n","authors":["He Wang","Lei Xie"],"pdf_url":"https://arxiv.org/pdf/2408.02369v1.pdf","comment":"2 pages, 2 figures, CNVSRC 2024 System Report"},{"id":"http://arxiv.org/abs/2408.02367v1","updated":"2024-08-05T10:32:06Z","published":"2024-08-05T10:32:06Z","title":"StoDIP: Efficient 3D MRF image reconstruction with deep image priors and\n  stochastic iterations","summary":"  Magnetic Resonance Fingerprinting (MRF) is a time-efficient approach to\nquantitative MRI for multiparametric tissue mapping. The reconstruction of\nquantitative maps requires tailored algorithms for removing aliasing artefacts\nfrom the compressed sampled MRF acquisitions. Within approaches found in the\nliterature, many focus solely on two-dimensional (2D) image reconstruction,\nneglecting the extension to volumetric (3D) scans despite their higher\nrelevance and clinical value. A reason for this is that transitioning to 3D\nimaging without appropriate mitigations presents significant challenges,\nincluding increased computational cost and storage requirements, and the need\nfor large amount of ground-truth (artefact-free) data for training. To address\nthese issues, we introduce StoDIP, a new algorithm that extends the\nground-truth-free Deep Image Prior (DIP) reconstruction to 3D MRF imaging.\nStoDIP employs memory-efficient stochastic updates across the multicoil MRF\ndata, a carefully selected neural network architecture, as well as faster\nnonuniform FFT (NUFFT) transformations. This enables a faster convergence\ncompared against a conventional DIP implementation without these features.\nTested on a dataset of whole-brain scans from healthy volunteers, StoDIP\ndemonstrated superior performance over the ground-truth-free reconstruction\nbaselines, both quantitatively and qualitatively.\n","authors":["Perla Mayo","Matteo Cencini","Carolin M. Pirkl","Marion I. Menzel","Michela Tosetti","Bjoern H. Menze","Mohammad Golbabaee"],"pdf_url":"https://arxiv.org/pdf/2408.02367v1.pdf","comment":"10 pages, 2 figures, 1 table, 1 algorithm"},{"id":"http://arxiv.org/abs/2407.08583v2","updated":"2024-08-05T10:31:24Z","published":"2024-07-11T15:08:11Z","title":"The Synergy between Data and Multi-Modal Large Language Models: A Survey\n  from Co-Development Perspective","summary":"  The rapid development of large language models (LLMs) has been witnessed in\nrecent years. Based on the powerful LLMs, multi-modal LLMs (MLLMs) extend the\nmodality from text to a broader spectrum of domains, attracting widespread\nattention due to the broader range of application scenarios. As LLMs and MLLMs\nrely on vast amounts of model parameters and data to achieve emergent\ncapabilities, the importance of data is receiving increasingly widespread\nattention and recognition. Tracing and analyzing recent data-oriented works for\nMLLMs, we find that the development of models and data is not two separate\npaths but rather interconnected. On the one hand, vaster and higher-quality\ndata contribute to better performance of MLLMs; on the other hand, MLLMs can\nfacilitate the development of data. The co-development of multi-modal data and\nMLLMs requires a clear view of 1) at which development stages of MLLMs specific\ndata-centric approaches can be employed to enhance certain MLLM capabilities,\nand 2) how MLLMs, utilizing those capabilities, can contribute to multi-modal\ndata in specific roles. To promote the data-model co-development for MLLM\ncommunity, we systematically review existing works related to MLLMs from the\ndata-model co-development perspective. A regularly maintained project\nassociated with this survey is accessible at\nhttps://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md.\n","authors":["Zhen Qin","Daoyuan Chen","Wenhao Zhang","Liuyi Yao","Yilun Huang","Bolin Ding","Yaliang Li","Shuiguang Deng"],"pdf_url":"https://arxiv.org/pdf/2407.08583v2.pdf","comment":"Ongoing work. 21 pages. Related materials are continually maintained\n  and available at\n  https://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md"},{"id":"http://arxiv.org/abs/2405.10802v2","updated":"2024-08-05T10:20:11Z","published":"2024-05-17T14:16:40Z","title":"Reduced storage direct tensor ring decomposition for convolutional\n  neural networks compression","summary":"  Convolutional neural networks (CNNs) are among the most widely used machine\nlearning models for computer vision tasks, such as image classification. To\nimprove the efficiency of CNNs, many CNNs compressing approaches have been\ndeveloped. Low-rank methods approximate the original convolutional kernel with\na sequence of smaller convolutional kernels, which leads to reduced storage and\ntime complexities. In this study, we propose a novel low-rank CNNs compression\nmethod that is based on reduced storage direct tensor ring decomposition\n(RSDTR). The proposed method offers a higher circular mode permutation\nflexibility, and it is characterized by large parameter and FLOPS compression\nrates, while preserving a good classification accuracy of the compressed\nnetwork. The experiments, performed on the CIFAR-10 and ImageNet datasets,\nclearly demonstrate the efficiency of RSDTR in comparison to other\nstate-of-the-art CNNs compression approaches.\n","authors":["Mateusz Gabor","Rafał Zdunek"],"pdf_url":"https://arxiv.org/pdf/2405.10802v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02348v1","updated":"2024-08-05T09:50:16Z","published":"2024-08-05T09:50:16Z","title":"Earth System Data Cubes: Avenues for advancing Earth system research","summary":"  Recent advancements in Earth system science have been marked by the\nexponential increase in the availability of diverse, multivariate datasets\ncharacterised by moderate to high spatio-temporal resolutions. Earth System\nData Cubes (ESDCs) have emerged as one suitable solution for transforming this\nflood of data into a simple yet robust data structure. ESDCs achieve this by\norganising data into an analysis-ready format aligned with a spatio-temporal\ngrid, facilitating user-friendly analysis and diminishing the need for\nextensive technical data processing knowledge. Despite these significant\nbenefits, the completion of the entire ESDC life cycle remains a challenging\ntask. Obstacles are not only of a technical nature but also relate to\ndomain-specific problems in Earth system research. There exist barriers to\nrealising the full potential of data collections in light of novel cloud-based\ntechnologies, particularly in curating data tailored for specific application\ndomains. These include transforming data to conform to a spatio-temporal grid\nwith minimum distortions and managing complexities such as spatio-temporal\nautocorrelation issues. Addressing these challenges is pivotal for the\neffective application of Artificial Intelligence (AI) approaches. Furthermore,\nadhering to open science principles for data dissemination, reproducibility,\nvisualisation, and reuse is crucial for fostering sustainable research.\nOvercoming these challenges offers a substantial opportunity to advance\ndata-driven Earth system research, unlocking the full potential of an\nintegrated, multidimensional view of Earth system processes. This is\nparticularly true when such research is coupled with innovative research\nparadigms and technological progress.\n","authors":["David Montero","Guido Kraemer","Anca Anghelea","César Aybar","Gunnar Brandt","Gustau Camps-Valls","Felix Cremer","Ida Flik","Fabian Gans","Sarah Habershon","Chaonan Ji","Teja Kattenborn","Laura Martínez-Ferrer","Francesco Martinuzzi","Martin Reinhardt","Maximilian Söchting","Khalil Teber","Miguel D. Mahecha"],"pdf_url":"https://arxiv.org/pdf/2408.02348v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00938v2","updated":"2024-08-05T09:32:30Z","published":"2024-08-01T22:01:42Z","title":"CIResDiff: A Clinically-Informed Residual Diffusion Model for Predicting\n  Idiopathic Pulmonary Fibrosis Progression","summary":"  The progression of Idiopathic Pulmonary Fibrosis (IPF) significantly\ncorrelates with higher patient mortality rates. Early detection of IPF\nprogression is critical for initiating timely treatment, which can effectively\nslow down the advancement of the disease. However, the current clinical\ncriteria define disease progression requiring two CT scans with a one-year\ninterval, presenting a dilemma: a disease progression is identified only after\nthe disease has already progressed. To this end, in this paper, we develop a\nnovel diffusion model to accurately predict the progression of IPF by\ngenerating patient's follow-up CT scan from the initial CT scan. Specifically,\nfrom the clinical prior knowledge, we tailor improvements to the traditional\ndiffusion model and propose a Clinically-Informed Residual Diffusion model,\ncalled CIResDiff. The key innovations of CIResDiff include 1) performing the\ntarget region pre-registration to align the lung regions of two CT scans at\ndifferent time points for reducing the generation difficulty, 2) adopting the\nresidual diffusion instead of traditional diffusion to enable the model focus\nmore on differences (i.e., lesions) between the two CT scans rather than the\nlargely identical anatomical content, and 3) designing the clinically-informed\nprocess based on CLIP technology to integrate lung function information which\nis highly relevant to diagnosis into the reverse process for assisting\ngeneration. Extensive experiments on clinical data demonstrate that our\napproach can outperform state-of-the-art methods and effectively predict the\nprogression of IPF.\n","authors":["Caiwen Jiang","Xiaodan Xing","Zaixin Ou","Mianxin Liu","Walsh Simon","Guang Yang","Dinggang Shen"],"pdf_url":"https://arxiv.org/pdf/2408.00938v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02336v1","updated":"2024-08-05T09:19:52Z","published":"2024-08-05T09:19:52Z","title":"Infusing Environmental Captions for Long-Form Video Language Grounding","summary":"  In this work, we tackle the problem of long-form video-language grounding\n(VLG). Given a long-form video and a natural language query, a model should\ntemporally localize the precise moment that answers the query. Humans can\neasily solve VLG tasks, even with arbitrarily long videos, by discarding\nirrelevant moments using extensive and robust knowledge gained from experience.\nUnlike humans, existing VLG methods are prone to fall into superficial cues\nlearned from small-scale datasets, even when they are within irrelevant frames.\nTo overcome this challenge, we propose EI-VLG, a VLG method that leverages\nricher textual information provided by a Multi-modal Large Language Model\n(MLLM) as a proxy for human experiences, helping to effectively exclude\nirrelevant frames. We validate the effectiveness of the proposed method via\nextensive experiments on a challenging EgoNLQ benchmark.\n","authors":["Hyogun Lee","Soyeon Hong","Mujeen Sung","Jinwoo Choi"],"pdf_url":"https://arxiv.org/pdf/2408.02336v1.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2407.16248v3","updated":"2024-08-05T09:05:59Z","published":"2024-07-23T07:36:54Z","title":"Spatiotemporal Graph Guided Multi-modal Network for Livestreaming\n  Product Retrieval","summary":"  With the rapid expansion of e-commerce, more consumers have become accustomed\nto making purchases via livestreaming. Accurately identifying the products\nbeing sold by salespeople, i.e., livestreaming product retrieval (LPR), poses a\nfundamental and daunting challenge. The LPR task encompasses three primary\ndilemmas in real-world scenarios: 1) the recognition of intended products from\ndistractor products present in the background; 2) the video-image heterogeneity\nthat the appearance of products showcased in live streams often deviates\nsubstantially from standardized product images in stores; 3) there are numerous\nconfusing products with subtle visual nuances in the shop. To tackle these\nchallenges, we propose the Spatiotemporal Graphing Multi-modal Network (SGMN).\nFirst, we employ a text-guided attention mechanism that leverages the spoken\ncontent of salespeople to guide the model to focus toward intended products,\nemphasizing their salience over cluttered background products. Second, a\nlong-range spatiotemporal graph network is further designed to achieve both\ninstance-level interaction and frame-level matching, solving the misalignment\ncaused by video-image heterogeneity. Third, we propose a multi-modal hard\nexample mining, assisting the model in distinguishing highly similar products\nwith fine-grained features across the video-image-text domain. Through\nextensive quantitative and qualitative experiments, we demonstrate the superior\nperformance of our proposed SGMN model, surpassing the state-of-the-art methods\nby a substantial margin. The code is available at\nhttps://github.com/Huxiaowan/SGMN.\n","authors":["Xiaowan Hu","Yiyi Chen","Yan Li","Minquan Wang","Haoqian Wang","Quan Chen","Han Li","Peng Jiang"],"pdf_url":"https://arxiv.org/pdf/2407.16248v3.pdf","comment":"16 pages, 12 figures"},{"id":"http://arxiv.org/abs/2311.01090v2","updated":"2024-08-05T08:47:19Z","published":"2023-11-02T08:55:11Z","title":"Infusion: internal diffusion for inpainting of dynamic textures and\n  complex motion","summary":"  Video inpainting is the task of filling a region in a video in a visually\nconvincing manner. It is very challenging due to the high dimensionality of the\ndata and the temporal consistency required for obtaining convincing results.\nRecently, diffusion models have shown impressive results in modeling complex\ndata distributions, including images and videos. Such models remain nonetheless\nvery expensive to train and to perform inference with, which strongly reduce\ntheir applicability to videos, and yields unreasonable computational loads. We\nshow that in the case of video inpainting, thanks to the highly auto-similar\nnature of videos, the training data of a diffusion model can be restricted to\nthe input video and still produce very satisfying results. This leads us to\nadopt an internal learning approach, which also allows us to greatly reduce the\nneural network size by about three orders of magnitude less than current\ndiffusion models used for image inpainting. We also introduce a new method for\nefficient training and inference of diffusion models in the context of internal\nlearning, by splitting the diffusion process into different learning intervals\ncorresponding to different noise levels of the diffusion process. To the best\nof our knowledge, this is the first video inpainting method based purely on\ndiffusion. Other methods require additional components such as optical flow\nestimation, which limits their performance in the case of dynamic textures and\ncomplex motions. We show qualitative and quantitative results, demonstrating\nthat our method reaches state of the art performance in the case of dynamic\ntextures and complex dynamic backgrounds.\n","authors":["Nicolas Cherel","Andrés Almansa","Yann Gousseau","Alasdair Newson"],"pdf_url":"https://arxiv.org/pdf/2311.01090v2.pdf","comment":"11 pages, 10 figures"},{"id":"http://arxiv.org/abs/2408.02307v1","updated":"2024-08-05T08:36:13Z","published":"2024-08-05T08:36:13Z","title":"Low-Cost Self-Ensembles Based on Multi-Branch Transformation and Grouped\n  Convolution","summary":"  Recent advancements in low-cost ensemble learning have demonstrated improved\nefficiency for image classification. However, the existing low-cost ensemble\nmethods show relatively lower accuracy compared to conventional ensemble\nlearning. In this paper, we propose a new low-cost ensemble learning, which can\nsimultaneously achieve high efficiency and classification performance. A CNN is\ntransformed into a multi-branch structure without introduction of additional\ncomponents, which maintains the computational complexity as that of the\noriginal single model and also enhances diversity among the branches' outputs\nvia sufficient separation between different pathways of the branches. In\naddition, we propose a new strategy that applies grouped convolution in the\nbranches with different numbers of groups in different branches, which boosts\nthe diversity of the branches' outputs. For training, we employ knowledge\ndistillation using the ensemble of the outputs as the teacher signal. The high\ndiversity among the outputs enables to form a powerful teacher, enhancing the\nindividual branch's classification performance and consequently the overall\nensemble performance. Experimental results show that our method achieves\nstate-of-the-art classification accuracy and higher uncertainty estimation\nperformance compared to previous low-cost ensemble methods. The code is\navailable at https://github.com/hjdw2/SEMBG.\n","authors":["Hojung Lee","Jong-Seok Lee"],"pdf_url":"https://arxiv.org/pdf/2408.02307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02306v1","updated":"2024-08-05T08:35:59Z","published":"2024-08-05T08:35:59Z","title":"Mixture-of-Noises Enhanced Forgery-Aware Predictor for Multi-Face\n  Manipulation Detection and Localization","summary":"  With the advancement of face manipulation technology, forgery images in\nmulti-face scenarios are gradually becoming a more complex and realistic\nchallenge. Despite this, detection and localization methods for such multi-face\nmanipulations remain underdeveloped. Traditional manipulation localization\nmethods either indirectly derive detection results from localization masks,\nresulting in limited detection performance, or employ a naive two-branch\nstructure to simultaneously obtain detection and localization results, which\ncannot effectively benefit the localization capability due to limited\ninteraction between two tasks. This paper proposes a new framework, namely\nMoNFAP, specifically tailored for multi-face manipulation detection and\nlocalization. The MoNFAP primarily introduces two novel modules: the\nForgery-aware Unified Predictor (FUP) Module and the Mixture-of-Noises Module\n(MNM). The FUP integrates detection and localization tasks using a token\nlearning strategy and multiple forgery-aware transformers, which facilitates\nthe use of classification information to enhance localization capability.\nBesides, motivated by the crucial role of noise information in forgery\ndetection, the MNM leverages multiple noise extractors based on the concept of\nthe mixture of experts to enhance the general RGB features, further boosting\nthe performance of our framework. Finally, we establish a comprehensive\nbenchmark for multi-face detection and localization and the proposed\n\\textit{MoNFAP} achieves significant performance. The codes will be made\navailable.\n","authors":["Changtao Miao","Qi Chu","Tao Gong","Zhentao Tan","Zhenchao Jin","Wanyi Zhuang","Man Luo","Honggang Hu","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2408.02306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15362v2","updated":"2024-08-05T08:26:24Z","published":"2024-07-22T04:09:27Z","title":"A Multimodal Knowledge-enhanced Whole-slide Pathology Foundation Model","summary":"  Remarkable strides in computational pathology have been made in the\ntask-agnostic foundation model that advances the performance of a wide array of\ndownstream clinical tasks. Despite the promising performance, there are still\nseveral challenges. First, prior works have resorted to either vision-only or\nvision-captions data, disregarding invaluable pathology reports and gene\nexpression profiles which respectively offer distinct knowledge for versatile\nclinical applications. Second, the current progress in pathology FMs\npredominantly concentrates on the patch level, where the restricted context of\npatch-level pretraining fails to capture whole-slide patterns. Here we curated\nthe largest multimodal dataset consisting of H\\&E diagnostic whole slide images\nand their associated pathology reports and RNA-Seq data, resulting in 26,169\nslide-level modality pairs from 10,275 patients across 32 cancer types. To\nleverage these data for CPath, we propose a novel whole-slide pretraining\nparadigm which injects multimodal knowledge at the whole-slide context into the\npathology FM, called Multimodal Self-TAught PRetraining (mSTAR). The proposed\nparadigm revolutionizes the workflow of pretraining for CPath, which enables\nthe pathology FM to acquire the whole-slide context. To our knowledge, this is\nthe first attempt to incorporate multimodal knowledge at the slide level for\nenhancing pathology FMs, expanding the modelling context from unimodal to\nmultimodal knowledge and from patch-level to slide-level. To systematically\nevaluate the capabilities of mSTAR, extensive experiments including slide-level\nunimodal and multimodal applications, are conducted across 7 diverse types of\ntasks on 43 subtasks, resulting in the largest spectrum of downstream tasks.\nThe average performance in various slide-level applications consistently\ndemonstrates significant performance enhancements for mSTAR compared to SOTA\nFMs.\n","authors":["Yingxue Xu","Yihui Wang","Fengtao Zhou","Jiabo Ma","Shu Yang","Huangjing Lin","Xin Wang","Jiguang Wang","Li Liang","Anjia Han","Ronald Cheong Kin Chan","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2407.15362v2.pdf","comment":"45 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.02301v1","updated":"2024-08-05T08:23:59Z","published":"2024-08-05T08:23:59Z","title":"Network Fission Ensembles for Low-Cost Self-Ensembles","summary":"  Recent ensemble learning methods for image classification have been shown to\nimprove classification accuracy with low extra cost. However, they still\nrequire multiple trained models for ensemble inference, which eventually\nbecomes a significant burden when the model size increases. In this paper, we\npropose a low-cost ensemble learning and inference, called Network Fission\nEnsembles (NFE), by converting a conventional network itself into a multi-exit\nstructure. Starting from a given initial network, we first prune some of the\nweights to reduce the training burden. We then group the remaining weights into\nseveral sets and create multiple auxiliary paths using each set to construct\nmulti-exits. We call this process Network Fission. Through this, multiple\noutputs can be obtained from a single network, which enables ensemble learning.\nSince this process simply changes the existing network structure to multi-exits\nwithout using additional networks, there is no extra computational burden for\nensemble learning and inference. Moreover, by learning from multiple losses of\nall exits, the multi-exits improve performance via regularization, and high\nperformance can be achieved even with increased network sparsity. With our\nsimple yet effective method, we achieve significant improvement compared to\nexisting ensemble methods. The code is available at\nhttps://github.com/hjdw2/NFE.\n","authors":["Hojung Lee","Jong-Seok Lee"],"pdf_url":"https://arxiv.org/pdf/2408.02301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02297v1","updated":"2024-08-05T08:14:28Z","published":"2024-08-05T08:14:28Z","title":"Perception Matters: Enhancing Embodied AI with Uncertainty-Aware\n  Semantic Segmentation","summary":"  Embodied AI has made significant progress acting in unexplored environments.\nHowever, tasks such as object search have largely focused on efficient policy\nlearning. In this work, we identify several gaps in current search methods:\nThey largely focus on dated perception models, neglect temporal aggregation,\nand transfer from ground truth directly to noisy perception at test time,\nwithout accounting for the resulting overconfidence in the perceived state. We\naddress the identified problems through calibrated perception probabilities and\nuncertainty across aggregation and found decisions, thereby adapting the models\nfor sequential tasks. The resulting methods can be directly integrated with\npretrained models across a wide family of existing search approaches at no\nadditional training cost. We perform extensive evaluations of aggregation\nmethods across both different semantic perception models and policies,\nconfirming the importance of calibrated uncertainties in both the aggregation\nand found decisions. We make the code and trained models available at\nhttp://semantic-search.cs.uni-freiburg.de.\n","authors":["Sai Prasanna","Daniel Honerkamp","Kshitij Sirohi","Tim Welschehold","Wolfram Burgard","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2408.02297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02291v1","updated":"2024-08-05T08:00:30Z","published":"2024-08-05T08:00:30Z","title":"SelfGeo: Self-supervised and Geodesic-consistent Estimation of Keypoints\n  on Deformable Shapes","summary":"  Unsupervised 3D keypoints estimation from Point Cloud Data (PCD) is a complex\ntask, even more challenging when an object shape is deforming. As keypoints\nshould be semantically and geometrically consistent across all the 3D frames -\neach keypoint should be anchored to a specific part of the deforming shape\nirrespective of intrinsic and extrinsic motion. This paper presents, \"SelfGeo\",\na self-supervised method that computes persistent 3D keypoints of non-rigid\nobjects from arbitrary PCDs without the need of human annotations. The gist of\nSelfGeo is to estimate keypoints between frames that respect invariant\nproperties of deforming bodies. Our main contribution is to enforce that\nkeypoints deform along with the shape while keeping constant geodesic distances\namong them. This principle is then propagated to the design of a set of losses\nwhich minimization let emerge repeatable keypoints in specific semantic\nlocations of the non-rigid shape. We show experimentally that the use of\ngeodesic has a clear advantage in challenging dynamic scenes and with different\nclasses of deforming shapes (humans and animals). Code and data are available\nat: https://github.com/IIT-PAVIS/SelfGeo\n","authors":["Mohammad Zohaib","Luca Cosmo","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2408.02291v1.pdf","comment":"This paper has been accepted in ECCV 2024"},{"id":"http://arxiv.org/abs/2407.07720v4","updated":"2024-08-05T07:56:29Z","published":"2024-07-10T14:53:37Z","title":"Exploiting Scale-Variant Attention for Segmenting Small Medical Objects","summary":"  Early detection and accurate diagnosis can predict the risk of malignant\ndisease transformation, thereby increasing the probability of effective\ntreatment. Identifying mild syndrome with small pathological regions serves as\nan ominous warning and is fundamental in the early diagnosis of diseases. While\ndeep learning algorithms, particularly convolutional neural networks (CNNs),\nhave shown promise in segmenting medical objects, analyzing small areas in\nmedical images remains challenging. This difficulty arises due to information\nlosses and compression defects from convolution and pooling operations in CNNs,\nwhich become more pronounced as the network deepens, especially for small\nmedical objects. To address these challenges, we propose a novel scale-variant\nattention-based network (SvANet) for accurately segmenting small-scale objects\nin medical images. The SvANet consists of scale-variant attention, cross-scale\nguidance, Monte Carlo attention, and vision transformer, which incorporates\ncross-scale features and alleviates compression artifacts for enhancing the\ndiscrimination of small medical objects. Quantitative experimental results\ndemonstrate the superior performance of SvANet, achieving 96.12%, 96.11%,\n89.79%, 84.15%, 80.25%, 73.05%, and 72.58% in mean Dice coefficient for\nsegmenting kidney tumors, skin lesions, hepatic tumors, polyps, surgical\nexcision cells, retinal vasculatures, and sperms, which occupy less than 1% of\nthe image areas in KiTS23, ISIC 2018, ATLAS, PolypGen, TissueNet, FIVES, and\nSpermHealth datasets, respectively.\n","authors":["Wei Dai","Rui Liu","Zixuan Wu","Tianyi Wu","Min Wang","Junxian Zhou","Yixuan Yuan","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2407.07720v4.pdf","comment":"14 pages, 9 figures, under review"},{"id":"http://arxiv.org/abs/2408.02285v1","updated":"2024-08-05T07:37:55Z","published":"2024-08-05T07:37:55Z","title":"Joint-Motion Mutual Learning for Pose Estimation in Videos","summary":"  Human pose estimation in videos has long been a compelling yet challenging\ntask within the realm of computer vision. Nevertheless, this task remains\ndifficult because of the complex video scenes, such as video defocus and\nself-occlusion. Recent methods strive to integrate multi-frame visual features\ngenerated by a backbone network for pose estimation. However, they often ignore\nthe useful joint information encoded in the initial heatmap, which is a\nby-product of the backbone generation. Comparatively, methods that attempt to\nrefine the initial heatmap fail to consider any spatio-temporal motion\nfeatures. As a result, the performance of existing methods for pose estimation\nfalls short due to the lack of ability to leverage both local joint (heatmap)\ninformation and global motion (feature) dynamics.\n  To address this problem, we propose a novel joint-motion mutual learning\nframework for pose estimation, which effectively concentrates on both local\njoint dependency and global pixel-level motion dynamics. Specifically, we\nintroduce a context-aware joint learner that adaptively leverages initial\nheatmaps and motion flow to retrieve robust local joint feature. Given that\nlocal joint feature and global motion flow are complementary, we further\npropose a progressive joint-motion mutual learning that synergistically\nexchanges information and interactively learns between joint feature and motion\nflow to improve the capability of the model. More importantly, to capture more\ndiverse joint and motion cues, we theoretically analyze and propose an\ninformation orthogonality objective to avoid learning redundant information\nfrom multi-cues. Empirical experiments show our method outperforms prior arts\non three challenging benchmarks.\n","authors":["Sifan Wu","Haipeng Chen","Yifang Yin","Sihao Hu","Runyang Feng","Yingying Jiao","Ziqi Yang","Zhenguang Liu"],"pdf_url":"https://arxiv.org/pdf/2408.02285v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.18534v2","updated":"2024-08-05T07:37:06Z","published":"2024-07-26T06:29:09Z","title":"Boosting Cross-Domain Point Classification via Distilling Relational\n  Priors from 2D Transformers","summary":"  Semantic pattern of an object point cloud is determined by its topological\nconfiguration of local geometries. Learning discriminative representations can\nbe challenging due to large shape variations of point sets in local regions and\nincomplete surface in a global perspective, which can be made even more severe\nin the context of unsupervised domain adaptation (UDA). In specific,\ntraditional 3D networks mainly focus on local geometric details and ignore the\ntopological structure between local geometries, which greatly limits their\ncross-domain generalization. Recently, the transformer-based models have\nachieved impressive performance gain in a range of image-based tasks,\nbenefiting from its strong generalization capability and scalability stemming\nfrom capturing long range correlation across local patches. Inspired by such\nsuccesses of visual transformers, we propose a novel Relational Priors\nDistillation (RPD) method to extract relational priors from the well-trained\ntransformers on massive images, which can significantly empower cross-domain\nrepresentations with consistent topological priors of objects. To this end, we\nestablish a parameter-frozen pre-trained transformer module shared between 2D\nteacher and 3D student models, complemented by an online knowledge distillation\nstrategy for semantically regularizing the 3D student model. Furthermore, we\nintroduce a novel self-supervised task centered on reconstructing masked point\ncloud patches using corresponding masked multi-view image features, thereby\nempowering the model with incorporating 3D geometric information. Experiments\non the PointDA-10 and the Sim-to-Real datasets verify that the proposed method\nconsistently achieves the state-of-the-art performance of UDA for point cloud\nclassification. The source code of this work is available at\nhttps://github.com/zou-longkun/RPD.git.\n","authors":["Longkun Zou","Wanru Zhu","Ke Chen","Lihua Guo","Kailing Guo","Kui Jia","Yaowei Wang"],"pdf_url":"https://arxiv.org/pdf/2407.18534v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02284v1","updated":"2024-08-05T07:34:44Z","published":"2024-08-05T07:34:44Z","title":"Cascading Refinement Video Denoising with Uncertainty Adaptivity","summary":"  Accurate alignment is crucial for video denoising. However, estimating\nalignment in noisy environments is challenging. This paper introduces a\ncascading refinement video denoising method that can refine alignment and\nrestore images simultaneously. Better alignment enables restoration of more\ndetailed information in each frame. Furthermore, better image quality leads to\nbetter alignment. This method has achieved SOTA performance by a large margin\non the CRVD dataset. Simultaneously, aiming to deal with multi-level noise, an\nuncertainty map was created after each iteration. Because of this, redundant\ncomputation on the easily restored videos was avoided. By applying this method,\nthe entire computation was reduced by 25% on average.\n","authors":["Xinyuan Yu"],"pdf_url":"https://arxiv.org/pdf/2408.02284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04485v2","updated":"2024-08-05T07:18:25Z","published":"2024-06-06T20:15:42Z","title":"GenAI Arena: An Open Evaluation Platform for Generative Models","summary":"  Generative AI has made remarkable strides to revolutionize fields such as\nimage and video generation. These advancements are driven by innovative\nalgorithms, architecture, and data. However, the rapid proliferation of\ngenerative models has highlighted a critical gap: the absence of trustworthy\nevaluation metrics. Current automatic assessments such as FID, CLIP, FVD, etc\noften fail to capture the nuanced quality and user satisfaction associated with\ngenerative outputs. This paper proposes an open platform GenAI-Arena to\nevaluate different image and video generative models, where users can actively\nparticipate in evaluating these models. By leveraging collective user feedback\nand votes, GenAI-Arena aims to provide a more democratic and accurate measure\nof model performance. It covers three arenas for text-to-image generation,\ntext-to-video generation, and image editing respectively. Currently, we cover a\ntotal of 27 open-source generative models. GenAI-Arena has been operating for\nfour months, amassing over 6000 votes from the community. We describe our\nplatform, analyze the data, and explain the statistical methods for ranking the\nmodels. To further promote the research in building model-based evaluation\nmetrics, we release a cleaned version of our preference data for the three\ntasks, namely GenAI-Bench. We prompt the existing multi-modal models like\nGemini, GPT-4o to mimic human voting. We compute the correlation between model\nvoting with human voting to understand their judging abilities. Our results\nshow existing multimodal models are still lagging in assessing the generated\nvisual content, even the best model GPT-4o only achieves a Pearson correlation\nof 0.22 in the quality subscore, and behaves like random guessing in others.\n","authors":["Dongfu Jiang","Max Ku","Tianle Li","Yuansheng Ni","Shizhuo Sun","Rongqi Fan","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2406.04485v2.pdf","comment":"9 pages,7 figures"},{"id":"http://arxiv.org/abs/2408.02275v1","updated":"2024-08-05T07:10:40Z","published":"2024-08-05T07:10:40Z","title":"Geometric Algebra Meets Large Language Models: Instruction-Based\n  Transformations of Separate Meshes in 3D, Interactive and Controllable Scenes","summary":"  This paper introduces a novel integration of Large Language Models (LLMs)\nwith Conformal Geometric Algebra (CGA) to revolutionize controllable 3D scene\nediting, particularly for object repositioning tasks, which traditionally\nrequires intricate manual processes and specialized expertise. These\nconventional methods typically suffer from reliance on large training datasets\nor lack a formalized language for precise edits. Utilizing CGA as a robust\nformal language, our system, shenlong, precisely models spatial transformations\nnecessary for accurate object repositioning. Leveraging the zero-shot learning\ncapabilities of pre-trained LLMs, shenlong translates natural language\ninstructions into CGA operations which are then applied to the scene,\nfacilitating exact spatial transformations within 3D scenes without the need\nfor specialized pre-training. Implemented in a realistic simulation\nenvironment, shenlong ensures compatibility with existing graphics pipelines.\nTo accurately assess the impact of CGA, we benchmark against robust Euclidean\nSpace baselines, evaluating both latency and accuracy. Comparative performance\nevaluations indicate that shenlong significantly reduces LLM response times by\n16% and boosts success rates by 9.6% on average compared to the traditional\nmethods. Notably, shenlong achieves a 100% perfect success rate in common\npractical queries, a benchmark where other systems fall short. These\nadvancements underscore shenlong's potential to democratize 3D scene editing,\nenhancing accessibility and fostering innovation across sectors such as\neducation, digital entertainment, and virtual reality.\n","authors":["Dimitris Angelis","Prodromos Kolyvakis","Manos Kamarianakis","George Papagiannakis"],"pdf_url":"https://arxiv.org/pdf/2408.02275v1.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2408.02272v1","updated":"2024-08-05T07:00:10Z","published":"2024-08-05T07:00:10Z","title":"COM Kitchens: An Unedited Overhead-view Video Dataset as a\n  Vision-Language Benchmark","summary":"  Procedural video understanding is gaining attention in the vision and\nlanguage community. Deep learning-based video analysis requires extensive data.\nConsequently, existing works often use web videos as training resources, making\nit challenging to query instructional contents from raw video observations. To\naddress this issue, we propose a new dataset, COM Kitchens. The dataset\nconsists of unedited overhead-view videos captured by smartphones, in which\nparticipants performed food preparation based on given recipes. Fixed-viewpoint\nvideo datasets often lack environmental diversity due to high camera setup\ncosts. We used modern wide-angle smartphone lenses to cover cooking counters\nfrom sink to cooktop in an overhead view, capturing activity without in-person\nassistance. With this setup, we collected a diverse dataset by distributing\nsmartphones to participants. With this dataset, we propose the novel\nvideo-to-text retrieval task Online Recipe Retrieval (OnRR) and new video\ncaptioning domain Dense Video Captioning on unedited Overhead-View videos\n(DVC-OV). Our experiments verified the capabilities and limitations of current\nweb-video-based SOTA methods in handling these tasks.\n","authors":["Koki Maeda","Tosho Hirasawa","Atsushi Hashimoto","Jun Harashima","Leszek Rybicki","Yusuke Fukasawa","Yoshitaka Ushiku"],"pdf_url":"https://arxiv.org/pdf/2408.02272v1.pdf","comment":"ECCV2024 accepted"},{"id":"http://arxiv.org/abs/2407.18289v2","updated":"2024-08-05T06:53:43Z","published":"2024-07-25T15:18:28Z","title":"MARINE: A Computer Vision Model for Detecting Rare Predator-Prey\n  Interactions in Animal Videos","summary":"  Encounters between predator and prey play an essential role in ecosystems,\nbut their rarity makes them difficult to detect in video recordings. Although\nadvances in action recognition (AR) and temporal action detection (AD),\nespecially transformer-based models and vision foundation models, have achieved\nhigh performance on human action datasets, animal videos remain relatively\nunder-researched. This thesis addresses this gap by proposing the model MARINE,\nwhich utilizes motion-based frame selection designed for fast animal actions\nand DINOv2 feature extraction with a trainable classification head for action\nrecognition. MARINE outperforms VideoMAE in identifying predator attacks in\nvideos of fish, both on a small and specific coral reef dataset (81.53\\%\nagainst 52.64\\% accuracy), and on a subset of the more extensive Animal Kingdom\ndataset (94.86\\% against 83.14\\% accuracy). In a multi-label setting on a\nrepresentative sample of Animal Kingdom, MARINE achieves 23.79\\% mAP,\npositioning it mid-field among existing benchmarks. Furthermore, in an AD task\non the coral reef dataset, MARINE achieves 80.78\\% AP (against VideoMAE's\n34.89\\%) although at a lowered t-IoU threshold of 25\\%. Therefore, despite room\nfor improvement, MARINE offers an effective starter framework to apply to AR\nand AD tasks on animal recordings and thus contribute to the study of natural\necosystems.\n","authors":["Zsófia Katona","Seyed Sahand Mohammadi Ziabari","Fatemeh Karimi Nejadasl"],"pdf_url":"https://arxiv.org/pdf/2407.18289v2.pdf","comment":"This is an MSc thesis by Zsofia Katona, supervised by the two other\n  authors"},{"id":"http://arxiv.org/abs/2407.18288v2","updated":"2024-08-05T06:50:44Z","published":"2024-07-25T14:21:35Z","title":"Leveraging Foundation Models via Knowledge Distillation in Multi-Object\n  Tracking: Distilling DINOv2 Features to FairMOT","summary":"  Multiple Object Tracking (MOT) is a computer vision task that has been\nemployed in a variety of sectors. Some common limitations in MOT are varying\nobject appearances, occlusions, or crowded scenes. To address these challenges,\nmachine learning methods have been extensively deployed, leveraging large\ndatasets, sophisticated models, and substantial computational resources. Due to\npractical limitations, access to the above is not always an option. However,\nwith the recent release of foundation models by prominent AI companies,\npretrained models have been trained on vast datasets and resources using\nstate-of-the-art methods. This work tries to leverage one such foundation\nmodel, called DINOv2, through using knowledge distillation. The proposed method\nuses a teacher-student architecture, where DINOv2 is the teacher and the\nFairMOT backbone HRNetv2 W18 is the student. The results imply that although\nthe proposed method shows improvements in certain scenarios, it does not\nconsistently outperform the original FairMOT model. These findings highlight\nthe potential and limitations of applying foundation models in knowledge\n","authors":["Niels G. Faber","Seyed Sahand Mohammadi Ziabari","Fatemeh Karimi Nejadasl"],"pdf_url":"https://arxiv.org/pdf/2407.18288v2.pdf","comment":"This is an MSc thesis by Niels Faber, supervised by the two other\n  authors"},{"id":"http://arxiv.org/abs/2408.02265v1","updated":"2024-08-05T06:42:00Z","published":"2024-08-05T06:42:00Z","title":"Explain via Any Concept: Concept Bottleneck Model with Open Vocabulary\n  Concepts","summary":"  The concept bottleneck model (CBM) is an interpretable-by-design framework\nthat makes decisions by first predicting a set of interpretable concepts, and\nthen predicting the class label based on the given concepts. Existing CBMs are\ntrained with a fixed set of concepts (concepts are either annotated by the\ndataset or queried from language models). However, this closed-world assumption\nis unrealistic in practice, as users may wonder about the role of any desired\nconcept in decision-making after the model is deployed. Inspired by the large\nsuccess of recent vision-language pre-trained models such as CLIP in zero-shot\nclassification, we propose \"OpenCBM\" to equip the CBM with open vocabulary\nconcepts via: (1) Aligning the feature space of a trainable image feature\nextractor with that of a CLIP's image encoder via a prototype based feature\nalignment; (2) Simultaneously training an image classifier on the downstream\ndataset; (3) Reconstructing the trained classification head via any set of\nuser-desired textual concepts encoded by CLIP's text encoder. To reveal\npotentially missing concepts from users, we further propose to iteratively find\nthe closest concept embedding to the residual parameters during the\nreconstruction until the residual is small enough. To the best of our\nknowledge, our \"OpenCBM\" is the first CBM with concepts of open vocabularies,\nproviding users the unique benefit such as removing, adding, or replacing any\ndesired concept to explain the model's prediction even after a model is\ntrained. Moreover, our model significantly outperforms the previous\nstate-of-the-art CBM by 9% in the classification accuracy on the benchmark\ndataset CUB-200-2011.\n","authors":["Andong Tan","Fengtao Zhou","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2408.02265v1.pdf","comment":"ECCV2024"},{"id":"http://arxiv.org/abs/2305.15213v3","updated":"2024-08-05T06:40:52Z","published":"2023-05-24T14:51:18Z","title":"GTNet: Graph Transformer Network for 3D Point Cloud Classification and\n  Semantic Segmentation","summary":"  Recently, graph-based and Transformer-based deep learning networks have\ndemonstrated excellent performances on various point cloud tasks. Most of the\nexisting graph methods are based on static graph, which take a fixed input to\nestablish graph relations. Moreover, many graph methods apply maximization and\naveraging to aggregate neighboring features, so that only a single neighboring\npoint affects the feature of centroid or different neighboring points have the\nsame influence on the centroid's feature, which ignoring the correlation and\ndifference between points. Most Transformer-based methods extract point cloud\nfeatures based on global attention and lack the feature learning on local\nneighbors. To solve the problems of these two types of models, we propose a new\nfeature extraction block named Graph Transformer and construct a 3D point point\ncloud learning network called GTNet to learn features of point clouds on local\nand global patterns. Graph Transformer integrates the advantages of graph-based\nand Transformer-based methods, and consists of Local Transformer and Global\nTransformer modules. Local Transformer uses a dynamic graph to calculate all\nneighboring point weights by intra-domain cross-attention with dynamically\nupdated graph relations, so that every neighboring point could affect the\nfeatures of centroid with different weights; Global Transformer enlarges the\nreceptive field of Local Transformer by a global self-attention. In addition,\nto avoid the disappearance of the gradient caused by the increasing depth of\nnetwork, we conduct residual connection for centroid features in GTNet; we also\nadopt the features of centroid and neighbors to generate the local geometric\ndescriptors in Local Transformer to strengthen the local information learning\ncapability of the model. Finally, we use GTNet for shape classification, part\nsegmentation and semantic segmentation tasks in this paper.\n","authors":["Wei Zhou","Qian Wang","Weiwei Jin","Xinzhe Shi","Ying He"],"pdf_url":"https://arxiv.org/pdf/2305.15213v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02263v1","updated":"2024-08-05T06:38:43Z","published":"2024-08-05T06:38:43Z","title":"VoxelTrack: Exploring Voxel Representation for 3D Point Cloud Object\n  Tracking","summary":"  Current LiDAR point cloud-based 3D single object tracking (SOT) methods\ntypically rely on point-based representation network. Despite demonstrated\nsuccess, such networks suffer from some fundamental problems: 1) It contains\npooling operation to cope with inherently disordered point clouds, hindering\nthe capture of 3D spatial information that is useful for tracking, a regression\ntask. 2) The adopted set abstraction operation hardly handles\ndensity-inconsistent point clouds, also preventing 3D spatial information from\nbeing modeled. To solve these problems, we introduce a novel tracking\nframework, termed VoxelTrack. By voxelizing inherently disordered point clouds\ninto 3D voxels and extracting their features via sparse convolution blocks,\nVoxelTrack effectively models precise and robust 3D spatial information,\nthereby guiding accurate position prediction for tracked objects. Moreover,\nVoxelTrack incorporates a dual-stream encoder with cross-iterative feature\nfusion module to further explore fine-grained 3D spatial information for\ntracking. Benefiting from accurate 3D spatial information being modeled, our\nVoxelTrack simplifies tracking pipeline with a single regression loss.\nExtensive experiments are conducted on three widely-adopted datasets including\nKITTI, NuScenes and Waymo Open Dataset. The experimental results confirm that\nVoxelTrack achieves state-of-the-art performance (88.3%, 71.4% and 63.6% mean\nprecision on the three datasets, respectively), and outperforms the existing\ntrackers with a real-time speed of 36 Fps on a single TITAN RTX GPU. The source\ncode and model will be released.\n","authors":["Yuxuan Lu","Jiahao Nie","Zhiwei He","Hongjie Gu","Xudong Lv"],"pdf_url":"https://arxiv.org/pdf/2408.02263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08651v4","updated":"2024-08-05T06:37:48Z","published":"2024-03-13T16:06:07Z","title":"HAIFIT: Fashion Image Translation for Human-to-AI Style Learning and\n  Generation","summary":"  In the realm of fashion design, sketches serve as the canvas for expressing\nan artist's distinctive drawing style and creative vision, capturing intricate\ndetails like stroke variations and texture nuances. The advent of\nsketch-to-image cross-modal translation technology has notably aided designers.\nHowever, existing methods often compromise these sketch details during image\ngeneration, resulting in images that deviate from the designer's intended\nconcept. This limitation hampers the ability to offer designers a precise\npreview of the final output. To overcome this challenge, we introduce HAIFIT, a\nnovel approach that transforms sketches into high-fidelity, lifelike clothing\nimages by integrating multi-scale features and capturing extensive feature map\ndependencies from diverse perspectives. Through extensive qualitative and\nquantitative evaluations conducted on our self-collected dataset, our method\ndemonstrates superior performance compared to existing methods in generating\nphotorealistic clothing images. Our method excels in preserving the distinctive\nstyle and intricate details essential for fashion design applications. In\naddition, our method also has obvious advantages in model training and\ninference speed, contributing to reducing designers' time costs and improving\ndesign efficiency.\n","authors":["Jianan Jiang","Xinglin Li","Weiren Yu","Di Wu"],"pdf_url":"https://arxiv.org/pdf/2403.08651v4.pdf","comment":"10 pages,8 figures"},{"id":"http://arxiv.org/abs/2408.02261v1","updated":"2024-08-05T06:32:20Z","published":"2024-08-05T06:32:20Z","title":"Cross-Domain Semantic Segmentation on Inconsistent Taxonomy using VLMs","summary":"  The challenge of semantic segmentation in Unsupervised Domain Adaptation\n(UDA) emerges not only from domain shifts between source and target images but\nalso from discrepancies in class taxonomies across domains. Traditional UDA\nresearch assumes consistent taxonomy between the source and target domains,\nthereby limiting their ability to recognize and adapt to the taxonomy of the\ntarget domain. This paper introduces a novel approach, Cross-Domain Semantic\nSegmentation on Inconsistent Taxonomy using Vision Language Models (CSI), which\neffectively performs domain-adaptive semantic segmentation even in situations\nof source-target class mismatches. CSI leverages the semantic generalization\npotential of Visual Language Models (VLMs) to create synergy with previous UDA\nmethods. It leverages segment reasoning obtained through traditional UDA\nmethods, combined with the rich semantic knowledge embedded in VLMs, to relabel\nnew classes in the target domain. This approach allows for effective adaptation\nto extended taxonomies without requiring any ground truth label for the target\ndomain. Our method has shown to be effective across various benchmarks in\nsituations of inconsistent taxonomy settings (coarse-to-fine taxonomy and open\ntaxonomy) and demonstrates consistent synergy effects when integrated with\nprevious state-of-the-art UDA methods. The implementation is available at\nhttp://github.com/jkee58/CSI.\n","authors":["Jeongkee Lim","Yusung Kim"],"pdf_url":"https://arxiv.org/pdf/2408.02261v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2312.16477v3","updated":"2024-08-05T05:51:21Z","published":"2023-12-27T08:52:41Z","title":"Group Multi-View Transformer for 3D Shape Analysis with Spatial Encoding","summary":"  In recent years, the results of view-based 3D shape recognition methods have\nsaturated, and models with excellent performance cannot be deployed on\nmemory-limited devices due to their huge size of parameters. To address this\nproblem, we introduce a compression method based on knowledge distillation for\nthis field, which largely reduces the number of parameters while preserving\nmodel performance as much as possible. Specifically, to enhance the\ncapabilities of smaller models, we design a high-performing large model called\nGroup Multi-view Vision Transformer (GMViT). In GMViT, the view-level ViT first\nestablishes relationships between view-level features. Additionally, to capture\ndeeper features, we employ the grouping module to enhance view-level features\ninto group-level features. Finally, the group-level ViT aggregates group-level\nfeatures into complete, well-formed 3D shape descriptors. Notably, in both\nViTs, we introduce spatial encoding of camera coordinates as innovative\nposition embeddings. Furthermore, we propose two compressed versions based on\nGMViT, namely GMViT-simple and GMViT-mini. To enhance the training\neffectiveness of the small models, we introduce a knowledge distillation method\nthroughout the GMViT process, where the key outputs of each GMViT component\nserve as distillation targets. Extensive experiments demonstrate the efficacy\nof the proposed method. The large model GMViT achieves excellent 3D\nclassification and retrieval results on the benchmark datasets ModelNet,\nShapeNetCore55, and MCB. The smaller models, GMViT-simple and GMViT-mini,\nreduce the parameter size by 8 and 17.6 times, respectively, and improve shape\nrecognition speed by 1.5 times on average, while preserving at least 90% of the\nclassification and retrieval performance. The code is available at\nhttps://github.com/bigdata-graph/GMViT.\n","authors":["Lixiang Xu","Qingzhe Cui","Richang Hong","Wei Xu","Enhong Chen","Xin Yuan","Chenglong Li","Yuanyan Tang"],"pdf_url":"https://arxiv.org/pdf/2312.16477v3.pdf","comment":"13pages, 8 figuers"},{"id":"http://arxiv.org/abs/2408.02250v1","updated":"2024-08-05T05:48:45Z","published":"2024-08-05T05:48:45Z","title":"Hierarchical Clustering using Reversible Binary Cellular Automata for\n  High-Dimensional Data","summary":"  This work proposes a hierarchical clustering algorithm for high-dimensional\ndatasets using the cyclic space of reversible finite cellular automata. In\ncellular automaton (CA) based clustering, if two objects belong to the same\ncycle, they are closely related and considered as part of the same cluster.\nHowever, if a high-dimensional dataset is clustered using the cycles of one CA,\nclosely related objects may belong to different cycles. This paper identifies\nthe relationship between objects in two different cycles based on the median of\nall elements in each cycle so that they can be grouped in the next stage.\nFurther, to minimize the number of intermediate clusters which in turn reduces\nthe computational cost, a rule selection strategy is taken to find the best\nrules based on information propagation and cycle structure. After encoding the\ndataset using frequency-based encoding such that the consecutive data elements\nmaintain a minimum hamming distance in encoded form, our proposed clustering\nalgorithm iterates over three stages to finally cluster the data elements into\nthe desired number of clusters given by user. This algorithm can be applied to\nvarious fields, including healthcare, sports, chemical research, agriculture,\netc. When verified over standard benchmark datasets with various performance\nmetrics, our algorithm is at par with the existing algorithms with quadratic\ntime complexity.\n","authors":["Baby C. J.","Kamalika Bhattacharjee"],"pdf_url":"https://arxiv.org/pdf/2408.02250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02245v1","updated":"2024-08-05T05:33:59Z","published":"2024-08-05T05:33:59Z","title":"Curriculum learning based pre-training using Multi-Modal Contrastive\n  Masked Autoencoders","summary":"  In this paper, we propose a new pre-training method for image understanding\ntasks under Curriculum Learning (CL) paradigm which leverages RGB-D. The method\nutilizes Multi-Modal Contrastive Masked Autoencoder and Denoising techniques.\nRecent approaches either use masked autoencoding (e.g., MultiMAE) or\ncontrastive learning(e.g., Pri3D, or combine them in a single contrastive\nmasked autoencoder architecture such as CMAE and CAV-MAE. However, none of the\nsingle contrastive masked autoencoder is applicable to RGB-D datasets. To\nimprove the performance and efficacy of such methods, we propose a new\npre-training strategy based on CL. Specifically, in the first stage, we\npre-train the model using contrastive learning to learn cross-modal\nrepresentations. In the second stage, we initialize the modality-specific\nencoders using the weights from the first stage and then pre-train the model\nusing masked autoencoding and denoising/noise prediction used in diffusion\nmodels. Masked autoencoding focuses on reconstructing the missing patches in\nthe input modality using local spatial correlations, while denoising learns\nhigh frequency components of the input data. Our approach is scalable, robust\nand suitable for pre-training with limited RGB-D datasets. Extensive\nexperiments on multiple datasets such as ScanNet, NYUv2 and SUN RGB-D show the\nefficacy and superior performance of our approach. Specifically, we show an\nimprovement of +1.0% mIoU against Mask3D on ScanNet semantic segmentation. We\nfurther demonstrate the effectiveness of our approach in low-data regime by\nevaluating it for semantic segmentation task against the state-of-the-art\nmethods.\n","authors":["Muhammad Abdullah Jamal","Omid Mohareri"],"pdf_url":"https://arxiv.org/pdf/2408.02245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02244v1","updated":"2024-08-05T05:30:36Z","published":"2024-08-05T05:30:36Z","title":"Evaluating Vision-Language Models for Zero-Shot Detection,\n  Classification, and Association of Motorcycles, Passengers, and Helmets","summary":"  Motorcycle accidents pose significant risks, particularly when riders and\npassengers do not wear helmets. This study evaluates the efficacy of an\nadvanced vision-language foundation model, OWLv2, in detecting and classifying\nvarious helmet-wearing statuses of motorcycle occupants using video data. We\nextend the dataset provided by the CVPR AI City Challenge and employ a cascaded\nmodel approach for detection and classification tasks, integrating OWLv2 and\nCNN models. The results highlight the potential of zero-shot learning to\naddress challenges arising from incomplete and biased training datasets,\ndemonstrating the usage of such models in detecting motorcycles, helmet usage,\nand occupant positions under varied conditions. We have achieved an average\nprecision of 0.5324 for helmet detection and provided precision-recall curves\ndetailing the detection and classification performance. Despite limitations\nsuch as low-resolution data and poor visibility, our research shows promising\nadvancements in automated vehicle safety and traffic safety enforcement\nsystems.\n","authors":["Lucas Choi","Ross Greer"],"pdf_url":"https://arxiv.org/pdf/2408.02244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11070v2","updated":"2024-08-05T05:13:06Z","published":"2024-04-17T04:59:36Z","title":"Sky-GVIO: an enhanced GNSS/INS/Vision navigation with FCN-based\n  sky-segmentation in urban canyon","summary":"  Accurate, continuous, and reliable positioning is a critical component of\nachieving autonomous driving. However, in complex urban canyon environments,\nthe vulnerability of a stand-alone sensor and non-line-of-sight (NLOS) caused\nby high buildings, trees, and elevated structures seriously affect positioning\nresults. To address these challenges, a sky-view images segmentation algorithm\nbased on Fully Convolutional Network (FCN) is proposed for GNSS NLOS detection.\nBuilding upon this, a novel NLOS detection and mitigation algorithm (named\nS-NDM) is extended to the tightly coupled Global Navigation Satellite Systems\n(GNSS), Inertial Measurement Units (IMU), and visual feature system which is\ncalled Sky-GVIO, with the aim of achieving continuous and accurate positioning\nin urban canyon environments. Furthermore, the system harmonizes Single Point\nPositioning (SPP) with Real-Time Kinematic (RTK) methodologies to bolster its\noperational versatility and resilience. In urban canyon environments, the\npositioning performance of S-NDM algorithm proposed in this paper is evaluated\nunder different tightly coupled SPP-related and RTK-related models. The results\nexhibit that Sky-GVIO system achieves meter-level accuracy under SPP mode and\nsub-decimeter precision with RTK, surpassing the performance of GNSS/INS/Vision\nframeworks devoid of S-NDM. Additionally, the sky-view image dataset, inclusive\nof training and evaluation subsets, has been made publicly accessible for\nscholarly exploration at https://github.com/whuwangjr/sky-view-images .\n","authors":["Jingrong Wang","Bo Xu","Ronghe Jin","Shoujian Zhang","Kefu Gao","Jingnan Liu"],"pdf_url":"https://arxiv.org/pdf/2404.11070v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02231v1","updated":"2024-08-05T04:51:46Z","published":"2024-08-05T04:51:46Z","title":"REVISION: Rendering Tools Enable Spatial Fidelity in Vision-Language\n  Models","summary":"  Text-to-Image (T2I) and multimodal large language models (MLLMs) have been\nadopted in solutions for several computer vision and multimodal learning tasks.\nHowever, it has been found that such vision-language models lack the ability to\ncorrectly reason over spatial relationships. To tackle this shortcoming, we\ndevelop the REVISION framework which improves spatial fidelity in\nvision-language models. REVISION is a 3D rendering based pipeline that\ngenerates spatially accurate synthetic images, given a textual prompt. REVISION\nis an extendable framework, which currently supports 100+ 3D assets, 11 spatial\nrelationships, all with diverse camera perspectives and backgrounds. Leveraging\nimages from REVISION as additional guidance in a training-free manner\nconsistently improves the spatial consistency of T2I models across all spatial\nrelationships, achieving competitive performance on the VISOR and T2I-CompBench\nbenchmarks. We also design RevQA, a question-answering benchmark to evaluate\nthe spatial reasoning abilities of MLLMs, and find that state-of-the-art models\nare not robust to complex spatial reasoning under adversarial settings. Our\nresults and findings indicate that utilizing rendering-based frameworks is an\neffective approach for developing spatially-aware generative models.\n","authors":["Agneet Chatterjee","Yiran Luo","Tejas Gokhale","Yezhou Yang","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2408.02231v1.pdf","comment":"Accepted to ECCV 2024. Project Page :\n  https://agneetchatterjee.com/revision/"},{"id":"http://arxiv.org/abs/2408.02226v1","updated":"2024-08-05T04:10:52Z","published":"2024-08-05T04:10:52Z","title":"ProCreate, Dont Reproduce! Propulsive Energy Diffusion for Creative\n  Generation","summary":"  In this paper, we propose ProCreate, a simple and easy-to-implement method to\nimprove sample diversity and creativity of diffusion-based image generative\nmodels and to prevent training data reproduction. ProCreate operates on a set\nof reference images and actively propels the generated image embedding away\nfrom the reference embeddings during the generation process. We propose FSCG-8\n(Few-Shot Creative Generation 8), a few-shot creative generation dataset on\neight different categories -- encompassing different concepts, styles, and\nsettings -- in which ProCreate achieves the highest sample diversity and\nfidelity. Furthermore, we show that ProCreate is effective at preventing\nreplicating training data in a large-scale evaluation using training text\nprompts. Code and FSCG-8 are available at\nhttps://github.com/Agentic-Learning-AI-Lab/procreate-diffusion-public. The\nproject page is available at https://procreate-diffusion.github.io.\n","authors":["Jack Lu","Ryan Teehan","Mengye Ren"],"pdf_url":"https://arxiv.org/pdf/2408.02226v1.pdf","comment":"Accepted for ECCV 2024. Project page:\n  https://procreate-diffusion.github.io"},{"id":"http://arxiv.org/abs/2408.02222v1","updated":"2024-08-05T03:54:40Z","published":"2024-08-05T03:54:40Z","title":"Cross-modulated Attention Transformer for RGBT Tracking","summary":"  Existing Transformer-based RGBT trackers achieve remarkable performance\nbenefits by leveraging self-attention to extract uni-modal features and\ncross-attention to enhance multi-modal feature interaction and template-search\ncorrelation computation. Nevertheless, the independent search-template\ncorrelation calculations ignore the consistency between branches, which can\nresult in ambiguous and inappropriate correlation weights. It not only limits\nthe intra-modal feature representation, but also harms the robustness of\ncross-attention for multi-modal feature interaction and search-template\ncorrelation computation. To address these issues, we propose a novel approach\ncalled Cross-modulated Attention Transformer (CAFormer), which performs\nintra-modality self-correlation, inter-modality feature interaction, and\nsearch-template correlation computation in a unified attention model, for RGBT\ntracking. In particular, we first independently generate correlation maps for\neach modality and feed them into the designed Correlation Modulated Enhancement\nmodule, modulating inaccurate correlation weights by seeking the consensus\nbetween modalities. Such kind of design unifies self-attention and\ncross-attention schemes, which not only alleviates inaccurate attention weight\ncomputation in self-attention but also eliminates redundant computation\nintroduced by extra cross-attention scheme. In addition, we propose a\ncollaborative token elimination strategy to further improve tracking inference\nefficiency and accuracy. Extensive experiments on five public RGBT tracking\nbenchmarks show the outstanding performance of the proposed CAFormer against\nstate-of-the-art methods.\n","authors":["Yun Xiao","Jiacong Zhao","Andong Lu","Chenglong Li","Yin Lin","Bing Yin","Cong Liu"],"pdf_url":"https://arxiv.org/pdf/2408.02222v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.02214v1","updated":"2024-08-05T03:33:00Z","published":"2024-08-05T03:33:00Z","title":"More Than Positive and Negative: Communicating Fine Granularity in\n  Medical Diagnosis","summary":"  With the advance of deep learning, much progress has been made in building\npowerful artificial intelligence (AI) systems for automatic Chest X-ray (CXR)\nanalysis. Most existing AI models are trained to be a binary classifier with\nthe aim of distinguishing positive and negative cases. However, a large gap\nexists between the simple binary setting and complicated real-world medical\nscenarios. In this work, we reinvestigate the problem of automatic radiology\ndiagnosis. We first observe that there is considerable diversity among cases\nwithin the positive class, which means simply classifying them as positive\nloses many important details. This motivates us to build AI models that can\ncommunicate fine-grained knowledge from medical images like human experts. To\nthis end, we first propose a new benchmark on fine granularity learning from\nmedical images. Specifically, we devise a division rule based on medical\nknowledge to divide positive cases into two subcategories, namely atypical\npositive and typical positive. Then, we propose a new metric termed\nAUC$^\\text{FG}$ on the two subcategories for evaluation of the ability to\nseparate them apart. With the proposed benchmark, we encourage the community to\ndevelop AI diagnosis systems that could better learn fine granularity from\nmedical images. Last, we propose a simple risk modulation approach to this\nproblem by only using coarse labels in training. Empirical results show that\ndespite its simplicity, the proposed method achieves superior performance and\nthus serves as a strong baseline.\n","authors":["Xiangyu Peng","Kai Wang","Jianfei Yang","Yingying Zhu","Yang You"],"pdf_url":"https://arxiv.org/pdf/2408.02214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02210v1","updated":"2024-08-05T03:22:10Z","published":"2024-08-05T03:22:10Z","title":"ExoViP: Step-by-step Verification and Exploration with Exoskeleton\n  Modules for Compositional Visual Reasoning","summary":"  Compositional visual reasoning methods, which translate a complex query into\na structured composition of feasible visual tasks, have exhibited a strong\npotential in complicated multi-modal tasks. Empowered by recent advances in\nlarge language models (LLMs), this multi-modal challenge has been brought to a\nnew stage by treating LLMs as few-shot/zero-shot planners, i.e.,\nvision-language (VL) programming. Such methods, despite their numerous merits,\nsuffer from challenges due to LLM planning mistakes or inaccuracy of visual\nexecution modules, lagging behind the non-compositional models. In this work,\nwe devise a \"plug-and-play\" method, ExoViP, to correct errors in both the\nplanning and execution stages through introspective verification. We employ\nverification modules as \"exoskeletons\" to enhance current VL programming\nschemes. Specifically, our proposed verification module utilizes a mixture of\nthree sub-verifiers to validate predictions after each reasoning step,\nsubsequently calibrating the visual module predictions and refining the\nreasoning trace planned by LLMs. Experimental results on two representative VL\nprogramming methods showcase consistent improvements on five compositional\nreasoning tasks on standard benchmarks. In light of this, we believe that\nExoViP can foster better performance and generalization on open-domain\nmulti-modal challenges.\n","authors":["Yuxuan Wang","Alan Yuille","Zhuowan Li","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2408.02210v1.pdf","comment":"To Appear at COLM 2024"},{"id":"http://arxiv.org/abs/2408.02209v1","updated":"2024-08-05T03:18:58Z","published":"2024-08-05T03:18:58Z","title":"Source-Free Domain-Invariant Performance Prediction","summary":"  Accurately estimating model performance poses a significant challenge,\nparticularly in scenarios where the source and target domains follow different\ndata distributions. Most existing performance prediction methods heavily rely\non the source data in their estimation process, limiting their applicability in\na more realistic setting where only the trained model is accessible. The few\nmethods that do not require source data exhibit considerably inferior\nperformance. In this work, we propose a source-free approach centred on\nuncertainty-based estimation, using a generative model for calibration in the\nabsence of source data. We establish connections between our approach for\nunsupervised calibration and temperature scaling. We then employ a\ngradient-based strategy to evaluate the correctness of the calibrated\npredictions. Our experiments on benchmark object recognition datasets reveal\nthat existing source-based methods fall short with limited source sample\navailability. Furthermore, our approach significantly outperforms the current\nstate-of-the-art source-free and source-based methods, affirming its\neffectiveness in domain-invariant performance estimation.\n","authors":["Ekaterina Khramtsova","Mahsa Baktashmotlagh","Guido Zuccon","Xi Wang","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2408.02209v1.pdf","comment":"Accepted in ECCV 2024"},{"id":"http://arxiv.org/abs/2402.17521v3","updated":"2024-08-05T03:16:03Z","published":"2024-02-27T14:05:05Z","title":"AVS-Net: Point Sampling with Adaptive Voxel Size for 3D Scene\n  Understanding","summary":"  The recent advancements in point cloud learning have enabled intelligent\nvehicles and robots to comprehend 3D environments better. However, processing\nlarge-scale 3D scenes remains a challenging problem, such that efficient\ndownsampling methods play a crucial role in point cloud learning. Existing\ndownsampling methods either require a huge computational burden or sacrifice\nfine-grained geometric information. For such purpose, this paper presents an\nadvanced sampler that achieves both high accuracy and efficiency. The proposed\nmethod utilizes voxel centroid sampling as a foundation but effectively\naddresses the challenges regarding voxel size determination and the\npreservation of critical geometric cues. Specifically, we propose a Voxel\nAdaptation Module that adaptively adjusts voxel sizes with the reference of\npoint-based downsampling ratio. This ensures that the sampling results exhibit\na favorable distribution for comprehending various 3D objects or scenes.\nMeanwhile, we introduce a network compatible with arbitrary voxel sizes for\nsampling and feature extraction while maintaining high efficiency. The proposed\napproach is demonstrated with 3D object detection and 3D semantic segmentation.\nCompared to existing state-of-the-art methods, our approach achieves better\naccuracy on outdoor and indoor large-scale datasets, e.g. Waymo and ScanNet,\nwith promising efficiency.\n","authors":["Hongcheng Yang","Dingkang Liang","Dingyuan Zhang","Zhe Liu","Zhikang Zou","Xingyu Jiang","Yingying Zhu"],"pdf_url":"https://arxiv.org/pdf/2402.17521v3.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.00380v2","updated":"2024-08-05T02:45:50Z","published":"2024-08-01T08:41:13Z","title":"Enhancing Whole Slide Pathology Foundation Models through Stain\n  Normalization","summary":"  Recent advancements in digital pathology have led to the development of\nnumerous foundational models that utilize self-supervised learning on patches\nextracted from gigapixel whole slide images (WSIs). While this approach\nleverages vast amounts of unlabeled data, we have discovered a significant\nissue: features extracted from these self-supervised models tend to cluster by\nindividual WSIs, a phenomenon we term WSI-specific feature collapse. This\nproblem can potentially limit the model's generalization ability and\nperformance on various downstream tasks. To address this issue, we introduce\nStain Normalized Pathology Foundational Model, a novel foundational model\ntrained on patches that have undergone stain normalization. Stain normalization\nhelps reduce color variability arising from different laboratories and\nscanners, enabling the model to learn more consistent features. Stain\nNormalized Pathology Foundational Model is trained using 285,153,903 patches\nextracted from a total of 34,795 WSIs, combining data from The Cancer Genome\nAtlas (TCGA) and the Genotype-Tissue Expression (GTEx) project. Our experiments\ndemonstrate that Stain Normalized Pathology Foundational Model significantly\nmitigates the feature collapse problem, indicating that the model has learned\nmore generalized features rather than overfitting to individual WSI\ncharacteristics. We compared Stain Normalized Pathology Foundational Model with\nstate-of-the-art models across six downstream task datasets, and our results\nshow that Stain Normalized Pathology Foundational Model achieves excellent\nperformance relative to the number of WSIs used and the model's parameter\ncount. This suggests that the application of stain normalization has\nsubstantially improved the model's efficiency and generalization capabilities.\n","authors":["Juseung Yun","Yi Hu","Jinhyung Kim","Jongseong Jang","Soonyoung Lee"],"pdf_url":"https://arxiv.org/pdf/2408.00380v2.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2404.07950v2","updated":"2024-08-05T02:45:42Z","published":"2024-03-18T16:50:23Z","title":"Reinforcement Learning with Generalizable Gaussian Splatting","summary":"  An excellent representation is crucial for reinforcement learning (RL)\nperformance, especially in vision-based reinforcement learning tasks. The\nquality of the environment representation directly influences the achievement\nof the learning task. Previous vision-based RL typically uses explicit or\nimplicit ways to represent environments, such as images, points, voxels, and\nneural radiance fields. However, these representations contain several\ndrawbacks. They cannot either describe complex local geometries or generalize\nwell to unseen scenes, or require precise foreground masks. Moreover, these\nimplicit neural representations are akin to a ``black box\", significantly\nhindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit\nscene representation and differentiable rendering nature, is considered a\nrevolutionary change for reconstruction and representation methods. In this\npaper, we propose a novel Generalizable Gaussian Splatting framework to be the\nrepresentation of RL tasks, called GSRL. Through validation in the RoboMimic\nenvironment, our method achieves better results than other baselines in\nmultiple tasks, improving the performance by 10%, 44%, and 15% compared with\nbaselines on the hardest task. This work is the first attempt to leverage\ngeneralizable 3DGS as a representation for RL.\n","authors":["Jiaxu Wang","Qiang Zhang","Jingkai Sun","Jiahang Cao","Gang Han","Wen Zhao","Weining Zhang","Yecheng Shao","Yijie Guo","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2404.07950v2.pdf","comment":"7 pages,2 figures"},{"id":"http://arxiv.org/abs/2305.03713v3","updated":"2024-08-05T02:38:33Z","published":"2023-05-05T17:54:34Z","title":"Avatar Fingerprinting for Authorized Use of Synthetic Talking-Head\n  Videos","summary":"  Modern avatar generators allow anyone to synthesize photorealistic real-time\ntalking avatars, ushering in a new era of avatar-based human communication,\nsuch as with immersive AR/VR interactions or videoconferencing with limited\nbandwidths. Their safe adoption, however, requires a mechanism to verify if the\nrendered avatar is trustworthy: does it use the appearance of an individual\nwithout their consent? We term this task avatar fingerprinting. To tackle it,\nwe first introduce a large-scale dataset of real and synthetic videos of people\ninteracting on a video call, where the synthetic videos are generated using the\nfacial appearance of one person and the expressions of another. We verify the\nidentity driving the expressions in a synthetic video, by learning motion\nsignatures that are independent of the facial appearance shown. Our solution,\nthe first in this space, achieves an average AUC of 0.85. Critical to its\npractical use, it also generalizes to new generators never seen in training\n(average AUC of 0.83). The proposed dataset and other resources can be found\nat: https://research.nvidia.com/labs/nxp/avatar-fingerprinting/.\n","authors":["Ekta Prashnani","Koki Nagano","Shalini De Mello","David Luebke","Orazio Gallo"],"pdf_url":"https://arxiv.org/pdf/2305.03713v3.pdf","comment":"26 pages, 8 figures"},{"id":"http://arxiv.org/abs/2408.02192v1","updated":"2024-08-05T02:37:59Z","published":"2024-08-05T02:37:59Z","title":"Unsupervised Domain Adaption Harnessing Vision-Language Pre-training","summary":"  This paper addresses two vital challenges in Unsupervised Domain Adaptation\n(UDA) with a focus on harnessing the power of Vision-Language Pre-training\n(VLP) models. Firstly, UDA has primarily relied on ImageNet pre-trained models.\nHowever, the potential of VLP models in UDA remains largely unexplored. The\nrich representation of VLP models holds significant promise for enhancing UDA\ntasks. To address this, we propose a novel method called Cross-Modal Knowledge\nDistillation (CMKD), leveraging VLP models as teacher models to guide the\nlearning process in the target domain, resulting in state-of-the-art\nperformance. Secondly, current UDA paradigms involve training separate models\nfor each task, leading to significant storage overhead and impractical model\ndeployment as the number of transfer tasks grows. To overcome this challenge,\nwe introduce Residual Sparse Training (RST) exploiting the benefits conferred\nby VLP's extensive pre-training, a technique that requires minimal adjustment\n(approximately 0.1\\%$\\sim$0.5\\%) of VLP model parameters to achieve performance\ncomparable to fine-tuning. Combining CMKD and RST, we present a comprehensive\nsolution that effectively leverages VLP models for UDA tasks while reducing\nstorage overhead for model deployment. Furthermore, CMKD can serve as a\nbaseline in conjunction with other methods like FixMatch, enhancing the\nperformance of UDA. Our proposed method outperforms existing techniques on\nstandard benchmarks. Our code will be available at:\nhttps://github.com/Wenlve-Zhou/VLP-UDA.\n","authors":["Wenlve Zhou","Zhiheng Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.02192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02191v1","updated":"2024-08-05T02:35:13Z","published":"2024-08-05T02:35:13Z","title":"Dense Feature Interaction Network for Image Inpainting Localization","summary":"  Image inpainting, which is the task of filling in missing areas in an image,\nis a common image editing technique. Inpainting can be used to conceal or alter\nimage contents in malicious manipulation of images, driving the need for\nresearch in image inpainting detection. Existing methods mostly rely on a basic\nencoder-decoder structure, which often results in a high number of false\npositives or misses the inpainted regions, especially when dealing with targets\nof varying semantics and scales. Additionally, the absence of an effective\napproach to capture boundary artifacts leads to less accurate edge\nlocalization. In this paper, we describe a new method for inpainting detection\nbased on a Dense Feature Interaction Network (DeFI-Net). DeFI-Net uses a novel\nfeature pyramid architecture to capture and amplify multi-scale representations\nacross various stages, thereby improving the detection of image inpainting by\nbetter revealing feature-level interactions. Additionally, the network can\nadaptively direct the lower-level features, which carry edge and shape\ninformation, to refine the localization of manipulated regions while\nintegrating the higher-level semantic features. Using DeFI-Net, we develop a\nmethod combining complementary representations to accurately identify inpainted\nareas. Evaluation on five image inpainting datasets demonstrate the\neffectiveness of our approach, which achieves state-of-the-art performance in\ndetecting inpainting across diverse models.\n","authors":["Ye Yao","Tingfeng Han","Shan Jia","Siwei Lyu"],"pdf_url":"https://arxiv.org/pdf/2408.02191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01355v2","updated":"2024-08-05T02:14:54Z","published":"2024-08-02T16:07:15Z","title":"Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models\n  within Perturbed Inputs","summary":"  Multi-modal Large Language Models (MLLMs) have demonstrated remarkable\nperformance on various visual-language understanding and generation tasks.\nHowever, MLLMs occasionally generate content inconsistent with the given\nimages, which is known as \"hallucination\". Prior works primarily center on\nevaluating hallucination using standard, unperturbed benchmarks, which overlook\nthe prevalent occurrence of perturbed inputs in real-world scenarios-such as\nimage cropping or blurring-that are critical for a comprehensive assessment of\nMLLMs' hallucination. In this paper, to bridge this gap, we propose Hallu-PI,\nthe first benchmark designed to evaluate Hallucination in MLLMs within\nPerturbed Inputs. Specifically, Hallu-PI consists of seven perturbed scenarios,\ncontaining 1,260 perturbed images from 11 object types. Each image is\naccompanied by detailed annotations, which include fine-grained hallucination\ntypes, such as existence, attribute, and relation. We equip these annotations\nwith a rich set of questions, making Hallu-PI suitable for both discriminative\nand generative tasks. Extensive experiments on 12 mainstream MLLMs, such as\nGPT-4V and Gemini-Pro Vision, demonstrate that these models exhibit significant\nhallucinations on Hallu-PI, which is not observed in unperturbed scenarios.\nFurthermore, our research reveals a severe bias in MLLMs' ability to handle\ndifferent types of hallucinations. We also design two baselines specifically\nfor perturbed scenarios, namely Perturbed-Reminder and Perturbed-ICL. We hope\nthat our study will bring researchers' attention to the limitations of MLLMs\nwhen dealing with perturbed inputs, and spur further investigations to address\nthis issue. Our code and datasets are publicly available at\nhttps://github.com/NJUNLP/Hallu-PI.\n","authors":["Peng Ding","Jingyu Wu","Jun Kuang","Dan Ma","Xuezhi Cao","Xunliang Cai","Shi Chen","Jiajun Chen","Shujian Huang"],"pdf_url":"https://arxiv.org/pdf/2408.01355v2.pdf","comment":"Acccepted by ACM MM 2024, 14 pages, 11 figures, 9 tables"},{"id":"http://arxiv.org/abs/2312.10993v3","updated":"2024-08-05T01:58:58Z","published":"2023-12-18T07:44:40Z","title":"Realistic Human Motion Generation with Cross-Diffusion Models","summary":"  We introduce the Cross Human Motion Diffusion Model (CrossDiff), a novel\napproach for generating high-quality human motion based on textual\ndescriptions. Our method integrates 3D and 2D information using a shared\ntransformer network within the training of the diffusion model, unifying motion\nnoise into a single feature space. This enables cross-decoding of features into\nboth 3D and 2D motion representations, regardless of their original dimension.\nThe primary advantage of CrossDiff is its cross-diffusion mechanism, which\nallows the model to reverse either 2D or 3D noise into clean motion during\ntraining. This capability leverages the complementary information in both\nmotion representations, capturing intricate human movement details often missed\nby models relying solely on 3D information. Consequently, CrossDiff effectively\ncombines the strengths of both representations to generate more realistic\nmotion sequences. In our experiments, our model demonstrates competitive\nstate-of-the-art performance on text-to-motion benchmarks. Moreover, our method\nconsistently provides enhanced motion generation quality, capturing complex\nfull-body movement intricacies. Additionally, with a pretrained model,our\napproach accommodates using in the wild 2D motion data without 3D motion ground\ntruth during training to generate 3D motion, highlighting its potential for\nbroader applications and efficient use of available data resources. Project\npage: https://wonderno.github.io/CrossDiff-webpage/.\n","authors":["Zeping Ren","Shaoli Huang","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2312.10993v3.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2408.02181v1","updated":"2024-08-05T01:50:09Z","published":"2024-08-05T01:50:09Z","title":"AssemAI: Interpretable Image-Based Anomaly Detection for Manufacturing\n  Pipelines","summary":"  Anomaly detection in manufacturing pipelines remains a critical challenge,\nintensified by the complexity and variability of industrial environments. This\npaper introduces AssemAI, an interpretable image-based anomaly detection system\ntailored for smart manufacturing pipelines. Our primary contributions include\nthe creation of a tailored image dataset and the development of a custom object\ndetection model, YOLO-FF, designed explicitly for anomaly detection in\nmanufacturing assembly environments. Utilizing the preprocessed image dataset\nderived from an industry-focused rocket assembly pipeline, we address the\nchallenge of imbalanced image data and demonstrate the importance of\nimage-based methods in anomaly detection. The proposed approach leverages\ndomain knowledge in data preparation, model development and reasoning. We\ncompare our method against several baselines, including simple CNN and custom\nVisual Transformer (ViT) models, showcasing the effectiveness of our custom\ndata preparation and pretrained CNN integration. Additionally, we incorporate\nexplainability techniques at both user and model levels, utilizing ontology for\nuser-friendly explanations and SCORE-CAM for in-depth feature and model\nanalysis. Finally, the model was also deployed in a real-time setting. Our\nresults include ablation studies on the baselines, providing a comprehensive\nevaluation of the proposed system. This work highlights the broader impact of\nadvanced image-based anomaly detection in enhancing the reliability and\nefficiency of smart manufacturing processes.\n","authors":["Renjith Prasad","Chathurangi Shyalika","Ramtin Zand","Fadi El Kalach","Revathy Venkataramanan","Ramy Harik","Amit Sheth"],"pdf_url":"https://arxiv.org/pdf/2408.02181v1.pdf","comment":"8 Pages, 6 Figures, 4 Tables"},{"id":"http://arxiv.org/abs/2407.20937v2","updated":"2024-08-05T01:27:39Z","published":"2024-07-30T16:19:14Z","title":"EAR: Edge-Aware Reconstruction of 3-D vertebrae structures from\n  bi-planar X-ray images","summary":"  X-ray images ease the diagnosis and treatment process due to their rapid\nimaging speed and high resolution. However, due to the projection process of\nX-ray imaging, much spatial information has been lost. To accurately provide\nefficient spinal morphological and structural information, reconstructing the\n3-D structures of the spine from the 2-D X-ray images is essential. It is\nchallenging for current reconstruction methods to preserve the edge information\nand local shapes of the asymmetrical vertebrae structures. In this study, we\npropose a new Edge-Aware Reconstruction network (EAR) to focus on the\nperformance improvement of the edge information and vertebrae shapes. In our\nnetwork, by using the auto-encoder architecture as the backbone, the edge\nattention module and frequency enhancement module are proposed to strengthen\nthe perception of the edge reconstruction. Meanwhile, we also combine four loss\nterms, including reconstruction loss, edge loss, frequency loss and projection\nloss. The proposed method is evaluated using three publicly accessible datasets\nand compared with four state-of-the-art models. The proposed method is superior\nto other methods and achieves 25.32%, 15.32%, 86.44%, 80.13%, 23.7612 and\n0.3014 with regard to MSE, MAE, Dice, SSIM, PSNR and frequency distance. Due to\nthe end-to-end and accurate reconstruction process, EAR can provide sufficient\n3-D spatial information and precise preoperative surgical planning guidance.\n","authors":["Lixing Tan","Shuang Song","Yaofeng He","Kangneng Zhou","Tong Lu","Ruoxiu Xiao"],"pdf_url":"https://arxiv.org/pdf/2407.20937v2.pdf","comment":"13 pages, 11 figures, 3 tables"},{"id":"http://arxiv.org/abs/2311.03572v2","updated":"2024-08-05T01:05:54Z","published":"2023-11-06T22:17:18Z","title":"Unsupervised Region-Growing Network for Object Segmentation in\n  Atmospheric Turbulence","summary":"  Moving object segmentation in the presence of atmospheric turbulence is\nhighly challenging due to turbulence-induced irregular and time-varying\ndistortions. In this paper, we present an unsupervised approach for segmenting\nmoving objects in videos downgraded by atmospheric turbulence. Our key approach\nis a detect-then-grow scheme: we first identify a small set of moving object\npixels with high confidence, then gradually grow a foreground mask from those\nseeds to segment all moving objects. This method leverages rigid geometric\nconsistency among video frames to disentangle different types of motions, and\nthen uses the Sampson distance to initialize the seedling pixels. After growing\nper-frame foreground masks, we use spatial grouping loss and temporal\nconsistency loss to further refine the masks in order to ensure their\nspatio-temporal consistency. Our method is unsupervised and does not require\ntraining on labeled data. For validation, we collect and release the first\nreal-captured long-range turbulent video dataset with ground truth masks for\nmoving objects. Results show that our method achieves good accuracy in\nsegmenting moving objects and is robust for long-range videos with various\nturbulence strengths.\n","authors":["Dehao Qin","Ripon Saha","Suren Jayasuriya","Jinwei Ye","Nianyi Li"],"pdf_url":"https://arxiv.org/pdf/2311.03572v2.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2408.02666v1","updated":"2024-08-05T17:57:02Z","published":"2024-08-05T17:57:02Z","title":"Self-Taught Evaluators","summary":"  Model-based evaluation is at the heart of successful model development -- as\na reward model for training, and as a replacement for human evaluation. To\ntrain such evaluators, the standard approach is to collect a large amount of\nhuman preference judgments over model responses, which is costly and the data\nbecomes stale as models improve. In this work, we present an approach that aims\nto im-prove evaluators without human annotations, using synthetic training data\nonly. Starting from unlabeled instructions, our iterative self-improvement\nscheme generates contrasting model outputs and trains an LLM-as-a-Judge to\nproduce reasoning traces and final judgments, repeating this training at each\nnew iteration using the improved predictions. Without any labeled preference\ndata, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct)\nfrom 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms\ncommonly used LLM judges such as GPT-4 and matches the performance of the\ntop-performing reward models trained with labeled examples.\n","authors":["Tianlu Wang","Ilia Kulikov","Olga Golovneva","Ping Yu","Weizhe Yuan","Jane Dwivedi-Yu","Richard Yuanzhe Pang","Maryam Fazel-Zarandi","Jason Weston","Xian Li"],"pdf_url":"https://arxiv.org/pdf/2408.02666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02651v1","updated":"2024-08-05T17:27:29Z","published":"2024-08-05T17:27:29Z","title":"Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large\n  Language Models?","summary":"  Large Language Models (LLMs) have demonstrated impressive capabilities in\nnatural language tasks, but their safety and morality remain contentious due to\ntheir training on internet text corpora. To address these concerns, alignment\ntechniques have been developed to improve the public usability and safety of\nLLMs. Yet, the potential for generating harmful content through these models\nseems to persist. This paper explores the concept of jailbreaking\nLLMs-reversing their alignment through adversarial triggers. Previous methods,\nsuch as soft embedding prompts, manually crafted prompts, and gradient-based\nautomatic prompts, have had limited success on black-box models due to their\nrequirements for model access and for producing a low variety of manually\ncrafted prompts, making them susceptible to being blocked. This paper\nintroduces a novel approach using reinforcement learning to optimize\nadversarial triggers, requiring only inference API access to the target model\nand a small surrogate model. Our method, which leverages a BERTScore-based\nreward function, enhances the transferability and effectiveness of adversarial\ntriggers on new black-box models. We demonstrate that this approach improves\nthe performance of adversarial triggers on a previously untested language\nmodel.\n","authors":["Mohammad Bahrami Karkevandi","Nishant Vishwamitra","Peyman Najafirad"],"pdf_url":"https://arxiv.org/pdf/2408.02651v1.pdf","comment":"Accepted to AI4CYBER - KDD 2024"},{"id":"http://arxiv.org/abs/2312.06406v2","updated":"2024-08-05T17:00:00Z","published":"2023-12-11T14:27:10Z","title":"Partial End-to-end Reinforcement Learning for Robustness Against\n  Modelling Error in Autonomous Racing","summary":"  In this paper, we address the issue of increasing the performance of\nreinforcement learning (RL) solutions for autonomous racing cars when\nnavigating under conditions where practical vehicle modelling errors (commonly\nknown as \\emph{model mismatches}) are present. To address this challenge, we\npropose a partial end-to-end algorithm that decouples the planning and control\ntasks. Within this framework, an RL agent generates a trajectory comprising a\npath and velocity, which is subsequently tracked using a pure pursuit steering\ncontroller and a proportional velocity controller, respectively. In contrast,\nmany current learning-based (i.e., reinforcement and imitation learning)\nalgorithms utilise an end-to-end approach whereby a deep neural network\ndirectly maps from sensor data to control commands. By leveraging the\nrobustness of a classical controller, our partial end-to-end driving algorithm\nexhibits better robustness towards model mismatches than standard end-to-end\nalgorithms.\n","authors":["Andrew Murdoch","Johannes Cornelius Schoeman","Hendrik Willem Jordaan"],"pdf_url":"https://arxiv.org/pdf/2312.06406v2.pdf","comment":"Submitted to IEEE Transactions on Intelligent Transport Systems"},{"id":"http://arxiv.org/abs/2408.02632v1","updated":"2024-08-05T16:55:06Z","published":"2024-08-05T16:55:06Z","title":"SEAS: Self-Evolving Adversarial Safety Optimization for Large Language\n  Models","summary":"  As large language models (LLMs) continue to advance in capability and\ninfluence, ensuring their security and preventing harmful outputs has become\ncrucial. A promising approach to address these concerns involves training\nmodels to automatically generate adversarial prompts for red teaming. However,\nthe evolving subtlety of vulnerabilities in LLMs challenges the effectiveness\nof current adversarial methods, which struggle to specifically target and\nexplore the weaknesses of these models. To tackle these challenges, we\nintroduce the $\\mathbf{S}\\text{elf-}\\mathbf{E}\\text{volving\n}\\mathbf{A}\\text{dversarial }\\mathbf{S}\\text{afety }\\mathbf{(SEAS)}$\noptimization framework, which enhances security by leveraging data generated by\nthe model itself. SEAS operates through three iterative stages: Initialization,\nAttack, and Adversarial Optimization, refining both the Red Team and Target\nmodels to improve robustness and safety. This framework reduces reliance on\nmanual testing and significantly enhances the security capabilities of LLMs.\nOur contributions include a novel adversarial framework, a comprehensive safety\ndataset, and after three iterations, the Target model achieves a security level\ncomparable to GPT-4, while the Red Team model shows a marked increase in attack\nsuccess rate (ASR) against advanced models.\n","authors":["Muxi Diao","Rumei Li","Shiyang Liu","Guogang Liao","Jingang Wang","Xunliang Cai","Weiran Xu"],"pdf_url":"https://arxiv.org/pdf/2408.02632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02622v1","updated":"2024-08-05T16:47:22Z","published":"2024-08-05T16:47:22Z","title":"Language Model Can Listen While Speaking","summary":"  Dialogue serves as the most natural manner of human-computer interaction\n(HCI). Recent advancements in speech language models (SLM) have significantly\nenhanced speech-based conversational AI. However, these models are limited to\nturn-based conversation, lacking the ability to interact with humans in\nreal-time spoken scenarios, for example, being interrupted when the generated\ncontent is not satisfactory. To address these limitations, we explore full\nduplex modeling (FDM) in interactive speech language models (iSLM), focusing on\nenhancing real-time interaction and, more explicitly, exploring the\nquintessential ability of interruption. We introduce a novel model design,\nnamely listening-while-speaking language model (LSLM), an end-to-end system\nequipped with both listening and speaking channels. Our LSLM employs a\ntoken-based decoder-only TTS for speech generation and a streaming\nself-supervised learning (SSL) encoder for real-time audio input. LSLM fuses\nboth channels for autoregressive generation and detects turn-taking in real\ntime. Three fusion strategies -- early fusion, middle fusion, and late fusion\n-- are explored, with middle fusion achieving an optimal balance between speech\ngeneration and real-time interaction. Two experimental settings, command-based\nFDM and voice-based FDM, demonstrate LSLM's robustness to noise and sensitivity\nto diverse instructions. Our results highlight LSLM's capability to achieve\nduplex communication with minimal impact on existing systems. This study aims\nto advance the development of interactive speech dialogue systems, enhancing\ntheir applicability in real-world contexts.\n","authors":["Ziyang Ma","Yakun Song","Chenpeng Du","Jian Cong","Zhuo Chen","Yuping Wang","Yuxuan Wang","Xie Chen"],"pdf_url":"https://arxiv.org/pdf/2408.02622v1.pdf","comment":"Demo can be found at https://ddlbojack.github.io/LSLM"},{"id":"http://arxiv.org/abs/2408.02606v1","updated":"2024-08-05T16:31:38Z","published":"2024-08-05T16:31:38Z","title":"Backward explanations via redefinition of predicates","summary":"  History eXplanation based on Predicates (HXP), studies the behavior of a\nReinforcement Learning (RL) agent in a sequence of agent's interactions with\nthe environment (a history), through the prism of an arbitrary predicate. To\nthis end, an action importance score is computed for each action in the\nhistory. The explanation consists in displaying the most important actions to\nthe user. As the calculation of an action's importance is #W[1]-hard, it is\nnecessary for long histories to approximate the scores, at the expense of their\nquality. We therefore propose a new HXP method, called Backward-HXP, to provide\nexplanations for these histories without having to approximate scores.\nExperiments show the ability of B-HXP to summarise long histories.\n","authors":["Léo Saulières","Martin C. Cooper","Florence Dupin de Saint Cyr"],"pdf_url":"https://arxiv.org/pdf/2408.02606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02599v1","updated":"2024-08-05T16:21:17Z","published":"2024-08-05T16:21:17Z","title":"Progressively Selective Label Enhancement for Language Model Alignment","summary":"  Large Language Models have demonstrated impressive capabilities in various\nlanguage tasks but may produce content that misaligns with human expectations,\nraising ethical and legal concerns. Therefore, it is important to explore the\nlimitations and implement restrictions on the models to ensure safety and\ncompliance, with Reinforcement Learning from Human Feedback (RLHF) being the\nprimary method. Due to challenges in stability and scalability with the RLHF\nstages, researchers are exploring alternative methods to achieve effects\ncomparable to those of RLHF. However, these methods often depend on large\nhigh-quality datasets and inefficiently utilize generated data. To deal with\nthis problem, we propose PSLE, i.e., Progressively Selective Label Enhancement\nfor Language Model Alignment, a framework that fully utilizes all generated\ndata by guiding the model with principles to align outputs with human\nexpectations. Using a dynamically updated threshold, our approach ensures\nefficient data utilization by incorporating all generated responses and\nweighting them based on their corresponding reward scores. Experimental results\non multiple datasets demonstrate the effectiveness of PSLE compared to existing\nlanguage model alignment methods.\n","authors":["Biao Liu","Ning Xu","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2408.02599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09281v2","updated":"2024-08-05T16:16:27Z","published":"2024-07-12T14:13:06Z","title":"Predicting and Understanding Human Action Decisions: Insights from Large\n  Language Models and Cognitive Instance-Based Learning","summary":"  Large Language Models (LLMs) have demonstrated their capabilities across\nvarious tasks, from language translation to complex reasoning. Understanding\nand predicting human behavior and biases are crucial for artificial\nintelligence (AI) assisted systems to provide useful assistance, yet it remains\nan open question whether these models can achieve this. This paper addresses\nthis gap by leveraging the reasoning and generative capabilities of the LLMs to\npredict human behavior in two sequential decision-making tasks. These tasks\ninvolve balancing between exploitative and exploratory actions and handling\ndelayed feedback, both essential for simulating real-life decision processes.\nWe compare the performance of LLMs with a cognitive instance-based learning\n(IBL) model, which imitates human experiential decision-making. Our findings\nindicate that LLMs excel at rapidly incorporating feedback to enhance\nprediction accuracy. In contrast, the cognitive IBL model better accounts for\nhuman exploratory behaviors and effectively captures loss aversion bias, i.e.,\nthe tendency to choose a sub-optimal goal with fewer step-cost penalties rather\nthan exploring to find the optimal choice, even with limited experience. The\nresults highlight the benefits of integrating LLMs with cognitive\narchitectures, suggesting that this synergy could enhance the modeling and\nunderstanding of complex human decision-making patterns.\n","authors":["Thuy Ngoc Nguyen","Kasturi Jamale","Cleotilde Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2407.09281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02595v1","updated":"2024-08-05T16:07:31Z","published":"2024-08-05T16:07:31Z","title":"Modelling Visual Semantics via Image Captioning to extract Enhanced\n  Multi-Level Cross-Modal Semantic Incongruity Representation with Attention\n  for Multimodal Sarcasm Detection","summary":"  Sarcasm is a type of irony, characterized by an inherent mismatch between the\nliteral interpretation and the intended connotation. Though sarcasm detection\nin text has been extensively studied, there are situations in which textual\ninput alone might be insufficient to perceive sarcasm. The inclusion of\nadditional contextual cues, such as images, is essential to recognize sarcasm\nin social media data effectively. This study presents a novel framework for\nmultimodal sarcasm detection that can process input triplets. Two components of\nthese triplets comprise the input text and its associated image, as provided in\nthe datasets. Additionally, a supplementary modality is introduced in the form\nof descriptive image captions. The motivation behind incorporating this visual\nsemantic representation is to more accurately capture the discrepancies between\nthe textual and visual content, which are fundamental to the sarcasm detection\ntask. The primary contributions of this study are: (1) a robust textual feature\nextraction branch that utilizes a cross-lingual language model; (2) a visual\nfeature extraction branch that incorporates a self-regulated residual ConvNet\nintegrated with a lightweight spatially aware attention module; (3) an\nadditional modality in the form of image captions generated using an\nencoder-decoder architecture capable of reading text embedded in images; (4)\ndistinct attention modules to effectively identify the incongruities between\nthe text and two levels of image representations; (5) multi-level cross-domain\nsemantic incongruity representation achieved through feature fusion. Compared\nwith cutting-edge baselines, the proposed model achieves the best accuracy of\n92.89% and 64.48%, respectively, on the Twitter multimodal sarcasm and\nMultiBully datasets.\n","authors":["Sajal Aggarwal","Ananya Pandey","Dinesh Kumar Vishwakarma"],"pdf_url":"https://arxiv.org/pdf/2408.02595v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02584v1","updated":"2024-08-05T16:00:21Z","published":"2024-08-05T16:00:21Z","title":"Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality\n  Aspect-Based Summarization","summary":"  The ever-increasing volume of digital information necessitates efficient\nmethods for users to extract key insights from lengthy documents. Aspect-based\nsummarization offers a targeted approach, generating summaries focused on\nspecific aspects within a document. Despite advancements in aspect-based\nsummarization research, there is a continuous quest for improved model\nperformance. Given that large language models (LLMs) have demonstrated the\npotential to revolutionize diverse tasks within natural language processing,\nparticularly in the problem of summarization, this paper explores the potential\nof fine-tuning LLMs for the aspect-based summarization task. We evaluate the\nimpact of fine-tuning open-source foundation LLMs, including Llama2, Mistral,\nGemma and Aya, on a publicly available domain-specific aspect based summary\ndataset. We hypothesize that this approach will enable these models to\neffectively identify and extract aspect-related information, leading to\nsuperior quality aspect-based summaries compared to the state-of-the-art. We\nestablish a comprehensive evaluation framework to compare the performance of\nfine-tuned LLMs against competing aspect-based summarization methods and\nvanilla counterparts of the fine-tuned LLMs. Our work contributes to the field\nof aspect-based summarization by demonstrating the efficacy of fine-tuning LLMs\nfor generating high-quality aspect-based summaries. Furthermore, it opens doors\nfor further exploration of using LLMs for targeted information extraction tasks\nacross various NLP domains.\n","authors":["Ankan Mullick","Sombit Bose","Rounak Saha","Ayan Kumar Bhowmick","Aditya Vempaty","Pawan Goyal","Niloy Ganguly","Prasenjit Dey","Ravi Kokku"],"pdf_url":"https://arxiv.org/pdf/2408.02584v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02582v1","updated":"2024-08-05T16:00:07Z","published":"2024-08-05T16:00:07Z","title":"Clustering and Mining Accented Speech for Inclusive and Fair Speech\n  Recognition","summary":"  Modern automatic speech recognition (ASR) systems are typically trained on\nmore than tens of thousands hours of speech data, which is one of the main\nfactors for their great success. However, the distribution of such data is\ntypically biased towards common accents or typical speech patterns. As a\nresult, those systems often poorly perform on atypical accented speech. In this\npaper, we present accent clustering and mining schemes for fair speech\nrecognition systems which can perform equally well on under-represented\naccented speech. For accent recognition, we applied three schemes to overcome\nlimited size of supervised accent data: supervised or unsupervised\npre-training, distributionally robust optimization (DRO) and unsupervised\nclustering. Three schemes can significantly improve the accent recognition\nmodel especially for unbalanced and small accented speech. Fine-tuning ASR on\nthe mined Indian accent speech using the proposed supervised or unsupervised\nclustering schemes showed 10.0% and 5.3% relative improvements compared to\nfine-tuning on the randomly sampled speech, respectively.\n","authors":["Jaeyoung Kim","Han Lu","Soheil Khorram","Anshuman Tripathi","Qian Zhang","Hasim Sak"],"pdf_url":"https://arxiv.org/pdf/2408.02582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18932v2","updated":"2024-08-05T15:59:39Z","published":"2024-07-10T09:11:57Z","title":"Be More Real: Travel Diary Generation Using LLM Agents and Individual\n  Profiles","summary":"  Human mobility is inextricably linked to social issues such as traffic\ncongestion, energy consumption, and public health; however, privacy concerns\nrestrict access to mobility data. Recently, research have utilized Large\nLanguage Models (LLMs) for human mobility generation, in which the challenge is\nhow LLMs can understand individuals' mobility behavioral differences to\ngenerate realistic trajectories conforming to real world contexts. This study\nhandles this problem by presenting an LLM agent-based framework (MobAgent)\ncomposing two phases: understanding-based mobility pattern extraction and\nreasoning-based trajectory generation, which enables generate more real travel\ndiaries at urban scale, considering different individual profiles. MobAgent\nextracts reasons behind specific mobility trendiness and attribute influences\nto provide reliable patterns; infers the relationships between contextual\nfactors and underlying motivations of mobility; and based on the patterns and\nthe recursive reasoning process, MobAgent finally generates more authentic and\npersonalized mobilities that reflect both individual differences and real-world\nconstraints. We validate our framework with 0.2 million travel survey data,\ndemonstrating its effectiveness in producing personalized and accurate travel\ndiaries. This study highlights the capacity of LLMs to provide detailed and\nsophisticated understanding of human mobility through the real-world mobility\ndata.\n","authors":["Xuchuan Li","Fei Huang","Jianrong Lv","Zhixiong Xiao","Guolong Li","Yang Yue"],"pdf_url":"https://arxiv.org/pdf/2407.18932v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01281v2","updated":"2024-08-05T15:50:32Z","published":"2024-07-01T13:35:53Z","title":"Bridging Smoothness and Approximation: Theoretical Insights into\n  Over-Smoothing in Graph Neural Networks","summary":"  In this paper, we explore the approximation theory of functions defined on\ngraphs. Our study builds upon the approximation results derived from the\n$K$-functional. We establish a theoretical framework to assess the lower bounds\nof approximation for target functions using Graph Convolutional Networks (GCNs)\nand examine the over-smoothing phenomenon commonly observed in these networks.\nInitially, we introduce the concept of a $K$-functional on graphs, establishing\nits equivalence to the modulus of smoothness. We then analyze a typical type of\nGCN to demonstrate how the high-frequency energy of the output decays, an\nindicator of over-smoothing. This analysis provides theoretical insights into\nthe nature of over-smoothing within GCNs. Furthermore, we establish a lower\nbound for the approximation of target functions by GCNs, which is governed by\nthe modulus of smoothness of these functions. This finding offers a new\nperspective on the approximation capabilities of GCNs. In our numerical\nexperiments, we analyze several widely applied GCNs and observe the phenomenon\nof energy decay. These observations corroborate our theoretical results on\nexponential decay order.\n","authors":["Guangrui Yang","Jianfei Li","Ming Li","Han Feng","Ding-Xuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.01281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02571v1","updated":"2024-08-05T15:45:59Z","published":"2024-08-05T15:45:59Z","title":"Contrastive Learning-based Multi Modal Architecture for Emoticon\n  Prediction by Employing Image-Text Pairs","summary":"  The emoticons are symbolic representations that generally accompany the\ntextual content to visually enhance or summarize the true intention of a\nwritten message. Although widely utilized in the realm of social media, the\ncore semantics of these emoticons have not been extensively explored based on\nmultiple modalities. Incorporating textual and visual information within a\nsingle message develops an advanced way of conveying information. Hence, this\nresearch aims to analyze the relationship among sentences, visuals, and\nemoticons. For an orderly exposition, this paper initially provides a detailed\nexamination of the various techniques for extracting multimodal features,\nemphasizing the pros and cons of each method. Through conducting a\ncomprehensive examination of several multimodal algorithms, with specific\nemphasis on the fusion approaches, we have proposed a novel contrastive\nlearning based multimodal architecture. The proposed model employs the joint\ntraining of dual-branch encoder along with the contrastive learning to\naccurately map text and images into a common latent space. Our key finding is\nthat by integrating the principle of contrastive learning with that of the\nother two branches yields superior results. The experimental results\ndemonstrate that our suggested methodology surpasses existing multimodal\napproaches in terms of accuracy and robustness. The proposed model attained an\naccuracy of 91% and an MCC-score of 90% while assessing emoticons using the\nMultimodal-Twitter Emoticon dataset acquired from Twitter. We provide evidence\nthat deep features acquired by contrastive learning are more efficient,\nsuggesting that the proposed fusion technique also possesses strong\ngeneralisation capabilities for recognising emoticons across several modes.\n","authors":["Ananya Pandey","Dinesh Kumar Vishwakarma"],"pdf_url":"https://arxiv.org/pdf/2408.02571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.07088v3","updated":"2024-08-05T15:38:05Z","published":"2023-01-17T18:53:24Z","title":"Vision Learners Meet Web Image-Text Pairs","summary":"  Many self-supervised learning methods are pre-trained on the well-curated\nImageNet-1K dataset. In this work, given the excellent scalability of web data,\nwe consider self-supervised pre-training on noisy web sourced image-text paired\ndata. First, we conduct a benchmark study of representative self-supervised\npre-training methods on large-scale web data in a like-for-like setting. We\ncompare a range of methods, including single-modal ones that use masked\ntraining objectives and multi-modal ones that use image-text constrastive\ntraining. We observe that existing multi-modal methods do not outperform their\nsingle-modal counterparts on vision transfer learning tasks. We derive an\ninformation-theoretical view to explain these benchmark results, which provides\ninsight into how to design a novel vision learner. Inspired by this insight, we\npresent a new visual representation pre-training method, MUlti-modal\nGenerator~(MUG), that learns from scalable web sourced image-text data. MUG\nachieves state-of-the-art transfer performance on a variety of tasks and\ndemonstrates promising scaling properties. Pre-trained models and code will be\nmade public upon acceptance.\n","authors":["Bingchen Zhao","Quan Cui","Hao Wu","Osamu Yoshie","Cheng Yang","Oisin Mac Aodha"],"pdf_url":"https://arxiv.org/pdf/2301.07088v3.pdf","comment":"Project page: https://bzhao.me/MUG/"},{"id":"http://arxiv.org/abs/2408.02559v1","updated":"2024-08-05T15:36:46Z","published":"2024-08-05T15:36:46Z","title":"Evaluating and Enhancing LLMs Agent based on Theory of Mind in Guandan:\n  A Multi-Player Cooperative Game under Imperfect Information","summary":"  Large language models (LLMs) have shown success in handling simple games with\nimperfect information and enabling multi-agent coordination, but their ability\nto facilitate practical collaboration against other agents in complex,\nimperfect information environments, especially in a non-English environment,\nstill needs to be explored. This study investigates the applicability of\nknowledge acquired by open-source and API-based LLMs to sophisticated\ntext-based games requiring agent collaboration under imperfect information,\ncomparing their performance to established baselines using other types of\nagents. We propose a Theory of Mind (ToM) planning technique that allows LLM\nagents to adapt their strategy against various adversaries using only game\nrules, current state, and historical context as input. An external tool was\nincorporated to mitigate the challenge of dynamic and extensive action spaces\nin this card game. Our results show that although a performance gap exists\nbetween current LLMs and state-of-the-art reinforcement learning (RL) models,\nLLMs demonstrate ToM capabilities in this game setting. It consistently\nimproves their performance against opposing agents, suggesting their ability to\nunderstand the actions of allies and adversaries and establish collaboration\nwith allies. To encourage further research and understanding, we have made our\ncodebase openly accessible.\n","authors":["Yauwai Yim","Chunkit Chan","Tianyu Shi","Zheye Deng","Wei Fan","Tianshi Zheng","Yangqiu Song"],"pdf_url":"https://arxiv.org/pdf/2408.02559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02555v1","updated":"2024-08-05T15:33:45Z","published":"2024-08-05T15:33:45Z","title":"MeshAnything V2: Artist-Created Mesh Generation With Adjacent Mesh\n  Tokenization","summary":"  We introduce MeshAnything V2, an autoregressive transformer that generates\nArtist-Created Meshes (AM) aligned to given shapes. It can be integrated with\nvarious 3D asset production pipelines to achieve high-quality, highly\ncontrollable AM generation. MeshAnything V2 surpasses previous methods in both\nefficiency and performance using models of the same size. These improvements\nare due to our newly proposed mesh tokenization method: Adjacent Mesh\nTokenization (AMT). Different from previous methods that represent each face\nwith three vertices, AMT uses a single vertex whenever possible. Compared to\nprevious methods, AMT requires about half the token sequence length to\nrepresent the same mesh in average. Furthermore, the token sequences from AMT\nare more compact and well-structured, fundamentally benefiting AM generation.\nOur extensive experiments show that AMT significantly improves the efficiency\nand performance of AM generation. Project Page:\nhttps://buaacyw.github.io/meshanything-v2/\n","authors":["Yiwen Chen","Yikai Wang","Yihao Luo","Zhengyi Wang","Zilong Chen","Jun Zhu","Chi Zhang","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2408.02555v1.pdf","comment":"Project Page: https://buaacyw.github.io/meshanything-v2/ Github:\n  https://github.com/buaacyw/MeshAnythingV2"},{"id":"http://arxiv.org/abs/2408.02547v1","updated":"2024-08-05T15:17:34Z","published":"2024-08-05T15:17:34Z","title":"The Role of Functional Muscle Networks in Improving Hand Gesture\n  Perception for Human-Machine Interfaces","summary":"  Developing accurate hand gesture perception models is critical for various\nrobotic applications, enabling effective communication between humans and\nmachines and directly impacting neurorobotics and interactive robots. Recently,\nsurface electromyography (sEMG) has been explored for its rich informational\ncontext and accessibility when combined with advanced machine learning\napproaches and wearable systems. The literature presents numerous approaches to\nboost performance while ensuring robustness for neurorobots using sEMG, often\nresulting in models requiring high processing power, large datasets, and less\nscalable solutions. This paper addresses this challenge by proposing the\ndecoding of muscle synchronization rather than individual muscle activation. We\nstudy coherence-based functional muscle networks as the core of our perception\nmodel, proposing that functional synchronization between muscles and the\ngraph-based network of muscle connectivity encode contextual information about\nintended hand gestures. This can be decoded using shallow machine learning\napproaches without the need for deep temporal networks. Our technique could\nimpact myoelectric control of neurorobots by reducing computational burdens and\nenhancing efficiency. The approach is benchmarked on the Ninapro database,\nwhich contains 12 EMG signals from 40 subjects performing 17 hand gestures. It\nachieves an accuracy of 85.1%, demonstrating improved performance compared to\nexisting methods while requiring much less computational power. The results\nsupport the hypothesis that a coherence-based functional muscle network encodes\ncritical information related to gesture execution, significantly enhancing hand\ngesture perception with potential applications for neurorobotic systems and\ninteractive machines.\n","authors":["Costanza Armanini","Tuka Alhanai","Farah E. Shamout","S. Farokh Atashzar"],"pdf_url":"https://arxiv.org/pdf/2408.02547v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02545v1","updated":"2024-08-05T15:16:24Z","published":"2024-08-05T15:16:24Z","title":"RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented\n  Generation","summary":"  Implementing Retrieval-Augmented Generation (RAG) systems is inherently\ncomplex, requiring deep understanding of data, use cases, and intricate design\ndecisions. Additionally, evaluating these systems presents significant\nchallenges, necessitating assessment of both retrieval accuracy and generative\nquality through a multi-faceted approach. We introduce RAG Foundry, an\nopen-source framework for augmenting large language models for RAG use cases.\nRAG Foundry integrates data creation, training, inference and evaluation into a\nsingle workflow, facilitating the creation of data-augmented datasets for\ntraining and evaluating large language models in RAG settings. This integration\nenables rapid prototyping and experimentation with various RAG techniques,\nallowing users to easily generate datasets and train RAG models using internal\nor specialized knowledge sources. We demonstrate the framework effectiveness by\naugmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG\nconfigurations, showcasing consistent improvements across three\nknowledge-intensive datasets. Code is released as open-source in\nhttps://github.com/IntelLabs/RAGFoundry.\n","authors":["Daniel Fleischer","Moshe Berchansky","Moshe Wasserblat","Peter Izsak"],"pdf_url":"https://arxiv.org/pdf/2408.02545v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2310.02812v2","updated":"2024-08-05T15:06:24Z","published":"2023-10-04T13:37:34Z","title":"Time-Series Classification in Smart Manufacturing Systems: An\n  Experimental Evaluation of State-of-the-Art Machine Learning Algorithms","summary":"  Manufacturing is gathering extensive amounts of diverse data, thanks to the\ngrowing number of sensors and rapid advances in sensing technologies. Among the\nvarious data types available in SMS settings, time-series data plays a pivotal\nrole. Hence, TSC emerges is crucial in this domain. The objective of this study\nis to fill this gap by providing a rigorous experimental evaluation of the SoTA\nML and DL algorithms for TSC tasks in manufacturing and industrial settings. We\nfirst explored and compiled a comprehensive list of more than 92 SoTA\nalgorithms from both TSC and manufacturing literature. Following, we selected\nthe 36 most representative algorithms from this list. To evaluate their\nperformance across various manufacturing classification tasks, we curated a set\nof 22 manufacturing datasets, representative of different characteristics that\ncover diverse manufacturing problems. Subsequently, we implemented and\nevaluated the algorithms on the manufacturing benchmark datasets, and analyzed\nthe results for each dataset. Based on the results, ResNet, DrCIF,\nInceptionTime, and ARSENAL are the top-performing algorithms, boasting an\naverage accuracy of over 96.6% across all 22 manufacturing TSC datasets. These\nfindings underscore the robustness, efficiency, scalability, and effectiveness\nof convolutional kernels in capturing temporal features in time-series data, as\nthree out of the top four performing algorithms leverage these kernels for\nfeature extraction. Additionally, LSTM, BiLSTM, and TS-LSTM algorithms deserve\nrecognition for their effectiveness in capturing features within time-series\ndata using RNN-based structures.\n","authors":["Mojtaba A. Farahani","M. R. McCormick","Ramy Harik","Thorsten Wuest"],"pdf_url":"https://arxiv.org/pdf/2310.02812v2.pdf","comment":"Published in Robotics and Computer-Integrated Manufacturing journal"},{"id":"http://arxiv.org/abs/2408.02529v1","updated":"2024-08-05T14:49:12Z","published":"2024-08-05T14:49:12Z","title":"Counterfactual Shapley Values for Explaining Reinforcement Learning","summary":"  This paper introduces a novel approach Counterfactual Shapley Values (CSV),\nwhich enhances explainability in reinforcement learning (RL) by integrating\ncounterfactual analysis with Shapley Values. The approach aims to quantify and\ncompare the contributions of different state dimensions to various action\nchoices. To more accurately analyze these impacts, we introduce new\ncharacteristic value functions, the ``Counterfactual Difference Characteristic\nValue\" and the ``Average Counterfactual Difference Characteristic Value.\" These\nfunctions help calculate the Shapley values to evaluate the differences in\ncontributions between optimal and non-optimal actions. Experiments across\nseveral RL domains, such as GridWorld, FrozenLake, and Taxi, demonstrate the\neffectiveness of the CSV method. The results show that this method not only\nimproves transparency in complex RL systems but also quantifies the differences\nacross various decisions.\n","authors":["Yiwei Shi","Qi Zhang","Kevin McAreavey","Weiru Liu"],"pdf_url":"https://arxiv.org/pdf/2408.02529v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14347v2","updated":"2024-08-05T14:46:36Z","published":"2024-05-23T09:19:14Z","title":"Doubly-Dynamic ISAC Precoding for Vehicular Networks: A Constrained Deep\n  Reinforcement Learning (CDRL) Approach","summary":"  Integrated sensing and communication (ISAC) technology is essential for\nenabling the vehicular networks. However, the communication channel in this\nscenario exhibits time-varying characteristics, and the potential targets may\nmove rapidly, creating a doubly-dynamic phenomenon. This nature poses a\nchallenge for real-time precoder design. While optimization-based solutions are\nwidely researched, they are complex and heavily rely on perfect prior\ninformation, which is impractical in double dynamics. To address this\nchallenge, we propose using constrained deep reinforcement learning (CDRL) to\nfacilitate dynamic updates to the ISAC precoder design. Additionally, the\nprimal dual-deep deterministic policy gradient (PD-DDPG) and Wolpertinger\narchitecture are tailored to efficiently train the algorithm under complex\nconstraints and variable numbers of users. The proposed scheme not only adapts\nto the dynamics based on observations but also leverages environmental\ninformation to enhance performance and reduce complexity. Its superiority over\nexisting candidates has been validated through experiments.\n","authors":["Zonghui Yang","Shijian Gao","Xiang Cheng"],"pdf_url":"https://arxiv.org/pdf/2405.14347v2.pdf","comment":"Accepted by 2024 IEEE Global Communications Conference"},{"id":"http://arxiv.org/abs/2408.02525v1","updated":"2024-08-05T14:46:04Z","published":"2024-08-05T14:46:04Z","title":"Single-tap Latency Reduction with Single- or Double- tap Prediction","summary":"  Touch surfaces are widely utilized for smartphones, tablet PCs, and laptops\n(touchpad), and single and double taps are the most basic and common operations\non them. The detection of single or double taps causes the single-tap latency\nproblem, which creates a bottleneck in terms of the sensitivity of touch\ninputs. To reduce the single-tap latency, we propose a novel\nmachine-learning-based tap prediction method called PredicTaps. Our method\npredicts whether a detected tap is a single tap or the first contact of a\ndouble tap without having to wait for the hundreds of milliseconds\nconventionally required. We present three evaluations and one user evaluation\nthat demonstrate its broad applicability and usability for various tap\nsituations on two form factors (touchpad and smartphone). The results showed\nPredicTaps reduces the single-tap latency from 150-500 ms to 12 ms on laptops\nand to 17.6 ms on smartphones without reducing usability.\n","authors":["Naoto Nishida","Kaori Ikematsu","Junichi Sato","Shota Yamanaka","Kota Tsubouchi"],"pdf_url":"https://arxiv.org/pdf/2408.02525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02487v1","updated":"2024-08-05T14:09:30Z","published":"2024-08-05T14:09:30Z","title":"A First Look at License Compliance Capability of LLMs in Code Generation","summary":"  Recent advances in Large Language Models (LLMs) have revolutionized code\ngeneration, leading to widespread adoption of AI coding tools by developers.\nHowever, LLMs can generate license-protected code without providing the\nnecessary license information, leading to potential intellectual property\nviolations during software production. This paper addresses the critical, yet\nunderexplored, issue of license compliance in LLM-generated code by\nestablishing a benchmark to evaluate the ability of LLMs to provide accurate\nlicense information for their generated code. To establish this benchmark, we\nconduct an empirical study to identify a reasonable standard for \"striking\nsimilarity\" that excludes the possibility of independent creation, indicating a\ncopy relationship between the LLM output and certain open-source code. Based on\nthis standard, we propose an evaluation benchmark LiCoEval, to evaluate the\nlicense compliance capabilities of LLMs. Using LiCoEval, we evaluate 14 popular\nLLMs, finding that even top-performing LLMs produce a non-negligible proportion\n(0.88% to 2.01%) of code strikingly similar to existing open-source\nimplementations. Notably, most LLMs fail to provide accurate license\ninformation, particularly for code under copyleft licenses. These findings\nunderscore the urgent need to enhance LLM compliance capabilities in code\ngeneration tasks. Our study provides a foundation for future research and\ndevelopment to improve license compliance in AI-assisted software development,\ncontributing to both the protection of open-source software copyrights and the\nmitigation of legal risks for LLM users.\n","authors":["Weiwei Xu","Kai Gao","Hao He","Minghui Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.02487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02479v1","updated":"2024-08-05T14:01:15Z","published":"2024-08-05T14:01:15Z","title":"From LLMs to LLM-based Agents for Software Engineering: A Survey of\n  Current, Challenges and Future","summary":"  With the rise of large language models (LLMs), researchers are increasingly\nexploring their applications in var ious vertical domains, such as software\nengineering. LLMs have achieved remarkable success in areas including code\ngeneration and vulnerability detection. However, they also exhibit numerous\nlimitations and shortcomings. LLM-based agents, a novel tech nology with the\npotential for Artificial General Intelligence (AGI), combine LLMs as the core\nfor decision-making and action-taking, addressing some of the inherent\nlimitations of LLMs such as lack of autonomy and self-improvement. Despite\nnumerous studies and surveys exploring the possibility of using LLMs in\nsoftware engineering, it lacks a clear distinction between LLMs and LLM based\nagents. It is still in its early stage for a unified standard and benchmarking\nto qualify an LLM solution as an LLM-based agent in its domain. In this survey,\nwe broadly investigate the current practice and solutions for LLMs and\nLLM-based agents for software engineering. In particular we summarise six key\ntopics: requirement engineering, code generation, autonomous decision-making,\nsoftware design, test generation, and software maintenance. We review and\ndifferentiate the work of LLMs and LLM-based agents from these six topics,\nexamining their differences and similarities in tasks, benchmarks, and\nevaluation metrics. Finally, we discuss the models and benchmarks used,\nproviding a comprehensive analysis of their applications and effectiveness in\nsoftware engineering. We anticipate this work will shed some lights on pushing\nthe boundaries of LLM-based agents in software engineering for future research.\n","authors":["Haolin Jin","Linghan Huang","Haipeng Cai","Jun Yan","Bo Li","Huaming Chen"],"pdf_url":"https://arxiv.org/pdf/2408.02479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00860v2","updated":"2024-08-05T14:00:40Z","published":"2024-08-01T18:22:29Z","title":"UlRe-NeRF: 3D Ultrasound Imaging through Neural Rendering with\n  Ultrasound Reflection Direction Parameterization","summary":"  Three-dimensional ultrasound imaging is a critical technology widely used in\nmedical diagnostics. However, traditional 3D ultrasound imaging methods have\nlimitations such as fixed resolution, low storage efficiency, and insufficient\ncontextual connectivity, leading to poor performance in handling complex\nartifacts and reflection characteristics. Recently, techniques based on NeRF\n(Neural Radiance Fields) have made significant progress in view synthesis and\n3D reconstruction, but there remains a research gap in high-quality ultrasound\nimaging. To address these issues, we propose a new model, UlRe-NeRF, which\ncombines implicit neural networks and explicit ultrasound volume rendering into\nan ultrasound neural rendering architecture. This model incorporates reflection\ndirection parameterization and harmonic encoding, using a directional MLP\nmodule to generate view-dependent high-frequency reflection intensity\nestimates, and a spatial MLP module to produce the medium's physical property\nparameters. These parameters are used in the volume rendering process to\naccurately reproduce the propagation and reflection behavior of ultrasound\nwaves in the medium. Experimental results demonstrate that the UlRe-NeRF model\nsignificantly enhances the realism and accuracy of high-fidelity ultrasound\nimage reconstruction, especially in handling complex medium structures.\n","authors":["Ziwen Guo","Zi Fang","Zhuang Fu"],"pdf_url":"https://arxiv.org/pdf/2408.00860v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02462v1","updated":"2024-08-05T13:40:33Z","published":"2024-08-05T13:40:33Z","title":"An investigation into the causes of race bias in AI-based cine CMR\n  segmentation","summary":"  Artificial intelligence (AI) methods are being used increasingly for the\nautomated segmentation of cine cardiac magnetic resonance (CMR) imaging.\nHowever, these methods have been shown to be subject to race bias, i.e. they\nexhibit different levels of performance for different races depending on the\n(im)balance of the data used to train the AI model. In this paper we\ninvestigate the source of this bias, seeking to understand its root cause(s) so\nthat it can be effectively mitigated. We perform a series of classification and\nsegmentation experiments on short-axis cine CMR images acquired from Black and\nWhite subjects from the UK Biobank and apply AI interpretability methods to\nunderstand the results. In the classification experiments, we found that race\ncan be predicted with high accuracy from the images alone, but less accurately\nfrom ground truth segmentations, suggesting that the distributional shift\nbetween races, which is often the cause of AI bias, is mostly image-based\nrather than segmentation-based. The interpretability methods showed that most\nattention in the classification models was focused on non-heart regions, such\nas subcutaneous fat. Cropping the images tightly around the heart reduced\nclassification accuracy to around chance level. Similarly, race can be\npredicted from the latent representations of a biased segmentation model,\nsuggesting that race information is encoded in the model. Cropping images\ntightly around the heart reduced but did not eliminate segmentation bias. We\nalso investigate the influence of possible confounders on the bias observed.\n","authors":["Tiarna Lee","Esther Puyol-Anton","Bram Ruijsink","Sebastien Roujol","Theodore Barfoot","Shaheim Ogbomo-Harmitt","Miaojing Shi","Andrew P. King"],"pdf_url":"https://arxiv.org/pdf/2408.02462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02456v1","updated":"2024-08-05T13:28:51Z","published":"2024-08-05T13:28:51Z","title":"Enhancing Heterogeneous Knowledge Graph Completion with a Novel\n  GAT-based Approach","summary":"  Knowledge graphs (KGs) play a vital role in enhancing search results and\nrecommendation systems. With the rapid increase in the size of the KGs, they\nare becoming inaccuracy and incomplete. This problem can be solved by the\nknowledge graph completion methods, of which graph attention network\n(GAT)-based methods stand out since their superior performance. However,\nexisting GAT-based knowledge graph completion methods often suffer from\noverfitting issues when dealing with heterogeneous knowledge graphs, primarily\ndue to the unbalanced number of samples. Additionally, these methods\ndemonstrate poor performance in predicting the tail (head) entity that shares\nthe same relation and head (tail) entity with others. To solve these problems,\nwe propose GATH, a novel GAT-based method designed for Heterogeneous KGs. GATH\nincorporates two separate attention network modules that work synergistically\nto predict the missing entities. We also introduce novel encoding and feature\ntransformation approaches, enabling the robust performance of GATH in scenarios\nwith imbalanced samples. Comprehensive experiments are conducted to evaluate\nthe GATH's performance. Compared with the existing SOTA GAT-based model on\nHits@10 and MRR metrics, our model improves performance by 5.2% and 5.2% on the\nFB15K-237 dataset, and by 4.5% and 14.6% on the WN18RR dataset, respectively.\n","authors":["Wanxu Wei","Yitong Song","Bin Yao"],"pdf_url":"https://arxiv.org/pdf/2408.02456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02439v1","updated":"2024-08-05T12:59:35Z","published":"2024-08-05T12:59:35Z","title":"Long Input Benchmark for Russian Analysis","summary":"  Recent advancements in Natural Language Processing (NLP) have fostered the\ndevelopment of Large Language Models (LLMs) that can solve an immense variety\nof tasks. One of the key aspects of their application is their ability to work\nwith long text documents and to process long sequences of tokens. This has\ncreated a demand for proper evaluation of long-context understanding. To\naddress this need for the Russian language, we propose LIBRA (Long Input\nBenchmark for Russian Analysis), which comprises 21 adapted datasets to study\nthe LLM's abilities to understand long texts thoroughly. The tests are divided\ninto four complexity groups and allow the evaluation of models across various\ncontext lengths ranging from 4k up to 128k tokens. We provide the open-source\ndatasets, codebase, and public leaderboard for LIBRA to guide forthcoming\nresearch.\n","authors":["Igor Churin","Murat Apishev","Maria Tikhonova","Denis Shevelev","Aydar Bulatov","Yuri Kuratov","Sergej Averkiev","Alena Fenogenova"],"pdf_url":"https://arxiv.org/pdf/2408.02439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14244v2","updated":"2024-08-05T12:59:32Z","published":"2024-05-23T07:23:33Z","title":"Tell me why: Training preferences-based RL with human preferences and\n  step-level explanations","summary":"  Human-in-the-loop reinforcement learning allows the training of agents\nthrough various interfaces, even for non-expert humans. Recently,\npreference-based methods (PbRL), where the human has to give his preference\nover two trajectories, increased in popularity since they allow training in\ndomains where more direct feedback is hard to formulate. However, the current\nPBRL methods have limitations and do not provide humans with an expressive\ninterface for giving feedback. With this work, we propose a new\npreference-based learning method that provides humans with a more expressive\ninterface to provide their preference over trajectories and a factual\nexplanation (or annotation of why they have this preference). These\nexplanations allow the human to explain what parts of the trajectory are most\nrelevant for the preference. We allow the expression of the explanations over\nindividual trajectory steps. We evaluate our method in various simulations\nusing a simulated human oracle (with realistic restrictions), and our results\nshow that our extended feedback can improve the speed of learning.\n","authors":["Jakob Karalus"],"pdf_url":"https://arxiv.org/pdf/2405.14244v2.pdf","comment":"Workshop on Reinforcement Learning Beyond Rewards @ Reinforcement\n  Learning Conference (2024)"},{"id":"http://arxiv.org/abs/2404.07990v2","updated":"2024-08-05T12:55:47Z","published":"2024-04-11T17:59:56Z","title":"OpenBias: Open-set Bias Detection in Text-to-Image Generative Models","summary":"  Text-to-image generative models are becoming increasingly popular and\naccessible to the general public. As these models see large-scale deployments,\nit is necessary to deeply investigate their safety and fairness to not\ndisseminate and perpetuate any kind of biases. However, existing works focus on\ndetecting closed sets of biases defined a priori, limiting the studies to\nwell-known concepts. In this paper, we tackle the challenge of open-set bias\ndetection in text-to-image generative models presenting OpenBias, a new\npipeline that identifies and quantifies the severity of biases agnostically,\nwithout access to any precompiled set. OpenBias has three stages. In the first\nphase, we leverage a Large Language Model (LLM) to propose biases given a set\nof captions. Secondly, the target generative model produces images using the\nsame set of captions. Lastly, a Vision Question Answering model recognizes the\npresence and extent of the previously proposed biases. We study the behavior of\nStable Diffusion 1.5, 2, and XL emphasizing new biases, never investigated\nbefore. Via quantitative experiments, we demonstrate that OpenBias agrees with\ncurrent closed-set bias detection methods and human judgement.\n","authors":["Moreno D'Incà","Elia Peruzzo","Massimiliano Mancini","Dejia Xu","Vidit Goel","Xingqian Xu","Zhangyang Wang","Humphrey Shi","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2404.07990v2.pdf","comment":"CVPR 2024 Highlight - Code:\n  https://github.com/Picsart-AI-Research/OpenBias"},{"id":"http://arxiv.org/abs/2408.02412v1","updated":"2024-08-05T12:11:09Z","published":"2024-08-05T12:11:09Z","title":"PENDRAM: Enabling High-Performance and Energy-Efficient Processing of\n  Deep Neural Networks through a Generalized DRAM Data Mapping Policy","summary":"  Convolutional Neural Networks (CNNs), a prominent type of Deep Neural\nNetworks (DNNs), have emerged as a state-of-the-art solution for solving\nmachine learning tasks. To improve the performance and energy efficiency of CNN\ninference, the employment of specialized hardware accelerators is prevalent.\nHowever, CNN accelerators still face performance- and energy-efficiency\nchallenges due to high off-chip memory (DRAM) access latency and energy, which\nare especially crucial for latency- and energy-constrained embedded\napplications. Moreover, different DRAM architectures have different profiles of\naccess latency and energy, thus making it challenging to optimize them for high\nperformance and energy-efficient CNN accelerators. To address this, we present\nPENDRAM, a novel design space exploration methodology that enables\nhigh-performance and energy-efficient CNN acceleration through a generalized\nDRAM data mapping policy. Specifically, it explores the impact of different\nDRAM data mapping policies and DRAM architectures across different CNN\npartitioning and scheduling schemes on the DRAM access latency and energy, then\nidentifies the pareto-optimal design choices. The experimental results show\nthat our DRAM data mapping policy improves the energy-delay-product of DRAM\naccesses in the CNN accelerator over other mapping policies by up to 96%. In\nthis manner, our PENDRAM methodology offers high-performance and\nenergy-efficient CNN acceleration under any given DRAM architectures for\ndiverse embedded AI applications.\n","authors":["Rachmad Vidya Wicaksana Putra","Muhammad Abdullah Hanif","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2408.02412v1.pdf","comment":"11 pages, 15 figures, 2 tables. arXiv admin note: substantial text\n  overlap with arXiv:2004.10341"},{"id":"http://arxiv.org/abs/2408.02408v1","updated":"2024-08-05T12:09:38Z","published":"2024-08-05T12:09:38Z","title":"Multi-weather Cross-view Geo-localization Using Denoising Diffusion\n  Models","summary":"  Cross-view geo-localization in GNSS-denied environments aims to determine an\nunknown location by matching drone-view images with the correct geo-tagged\nsatellite-view images from a large gallery. Recent research shows that learning\ndiscriminative image representations under specific weather conditions can\nsignificantly enhance performance. However, the frequent occurrence of unseen\nextreme weather conditions hinders progress. This paper introduces MCGF, a\nMulti-weather Cross-view Geo-localization Framework designed to dynamically\nadapt to unseen weather conditions. MCGF establishes a joint optimization\nbetween image restoration and geo-localization using denoising diffusion\nmodels. For image restoration, MCGF incorporates a shared encoder and a\nlightweight restoration module to help the backbone eliminate weather-specific\ninformation. For geo-localization, MCGF uses EVA-02 as a backbone for feature\nextraction, with cross-entropy loss for training and cosine distance for\ntesting. Extensive experiments on University160k-WX demonstrate that MCGF\nachieves competitive results for geo-localization in varying weather\nconditions.\n","authors":["Tongtong Feng","Qing Li","Xin Wang","Mingzi Wang","Guangyao Li","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.02408v1.pdf","comment":"Accepted by ACM MM24 workshop"},{"id":"http://arxiv.org/abs/2403.05996v3","updated":"2024-08-05T11:55:19Z","published":"2024-03-09T19:56:40Z","title":"Dissecting Deep RL with High Update Ratios: Combatting Value Divergence","summary":"  We show that deep reinforcement learning algorithms can retain their ability\nto learn without resetting network parameters in settings where the number of\ngradient updates greatly exceeds the number of environment samples by\ncombatting value function divergence. Under large update-to-data ratios, a\nrecent study by Nikishin et al. (2022) suggested the emergence of a primacy\nbias, in which agents overfit early interactions and downplay later experience,\nimpairing their ability to learn. In this work, we investigate the phenomena\nleading to the primacy bias. We inspect the early stages of training that were\nconjectured to cause the failure to learn and find that one fundamental\nchallenge is a long-standing acquaintance: value function divergence.\nOverinflated Q-values are found not only on out-of-distribution but also\nin-distribution data and can be linked to overestimation on unseen action\nprediction propelled by optimizer momentum. We employ a simple unit-ball\nnormalization that enables learning under large update ratios, show its\nefficacy on the widely used dm_control suite, and obtain strong performance on\nthe challenging dog tasks, competitive with model-based approaches. Our results\nquestion, in parts, the prior explanation for sub-optimal learning due to\noverfitting early data.\n","authors":["Marcel Hussing","Claas Voelcker","Igor Gilitschenski","Amir-massoud Farahmand","Eric Eaton"],"pdf_url":"https://arxiv.org/pdf/2403.05996v3.pdf","comment":"Accepted as a conference paper at the First Reinforcement Learning\n  Conference (RLC)"},{"id":"http://arxiv.org/abs/2408.02402v1","updated":"2024-08-05T11:52:34Z","published":"2024-08-05T11:52:34Z","title":"Enhancing AI-based Generation of Software Exploits with Contextual\n  Information","summary":"  This practical experience report explores Neural Machine Translation (NMT)\nmodels' capability to generate offensive security code from natural language\n(NL) descriptions, highlighting the significance of contextual understanding\nand its impact on model performance. Our study employs a dataset comprising\nreal shellcodes to evaluate the models across various scenarios, including\nmissing information, necessary context, and unnecessary context. The\nexperiments are designed to assess the models' resilience against incomplete\ndescriptions, their proficiency in leveraging context for enhanced accuracy,\nand their ability to discern irrelevant information. The findings reveal that\nthe introduction of contextual data significantly improves performance.\nHowever, the benefits of additional context diminish beyond a certain point,\nindicating an optimal level of contextual information for model training.\nMoreover, the models demonstrate an ability to filter out unnecessary context,\nmaintaining high levels of accuracy in the generation of offensive security\ncode. This study paves the way for future research on optimizing context use in\nAI-driven code generation, particularly for applications requiring a high\ndegree of technical precision such as the generation of offensive code.\n","authors":["Pietro Liguori","Cristina Improta","Roberto Natella","Bojan Cukic","Domenico Cotroneo"],"pdf_url":"https://arxiv.org/pdf/2408.02402v1.pdf","comment":"Accepted for publication at The 35th IEEE International Symposium on\n  Software Reliability Engineering"},{"id":"http://arxiv.org/abs/2408.02380v1","updated":"2024-08-05T11:12:48Z","published":"2024-08-05T11:12:48Z","title":"Perfect Information Monte Carlo with Postponing Reasoning","summary":"  Imperfect information games, such as Bridge and Skat, present challenges due\nto state-space explosion and hidden information, posing formidable obstacles\nfor search algorithms. Determinization-based algorithms offer a resolution by\nsampling hidden information and solving the game in a perfect information\nsetting, facilitating rapid and effective action estimation. However,\ntransitioning to perfect information introduces challenges, notably one called\nstrategy fusion.This research introduces `Extended Perfect Information Monte\nCarlo' (EPIMC), an online algorithm inspired by the state-of-the-art\ndeterminization-based approach Perfect Information Monte Carlo (PIMC). EPIMC\nenhances the capabilities of PIMC by postponing the perfect information\nresolution, reducing alleviating issues related to strategy fusion. However,\nthe decision to postpone the leaf evaluator introduces novel considerations,\nsuch as the interplay between prior levels of reasoning and the newly deferred\nresolution. In our empirical analysis, we investigate the performance of EPIMC\nacross a range of games, with a particular focus on those characterized by\nvarying degrees of strategy fusion. Our results demonstrate notable performance\nenhancements, particularly in games where strategy fusion significantly impacts\ngameplay. Furthermore, our research contributes to the theoretical foundation\nof determinization-based algorithms addressing challenges associated with\nstrategy fusion.%, thereby enhancing our understanding of these algorithms\nwithin the context of imperfect information game scenarios.\n","authors":["Jérôme Arjonilla","Abdallah Saffidine","Tristan Cazenave"],"pdf_url":"https://arxiv.org/pdf/2408.02380v1.pdf","comment":"Accepted in IEEE Conference on Games (CoG) 2024 + Appendix"},{"id":"http://arxiv.org/abs/2408.02377v1","updated":"2024-08-05T11:06:36Z","published":"2024-08-05T11:06:36Z","title":"A Few-Shot Approach for Relation Extraction Domain Adaptation using\n  Large Language Models","summary":"  Knowledge graphs (KGs) have been successfully applied to the analysis of\ncomplex scientific and technological domains, with automatic KG generation\nmethods typically building upon relation extraction models capturing\nfine-grained relations between domain entities in text. While these relations\nare fully applicable across scientific areas, existing models are trained on\nfew domain-specific datasets such as SciERC and do not perform well on new\ntarget domains. In this paper, we experiment with leveraging in-context\nlearning capabilities of Large Language Models to perform schema-constrained\ndata annotation, collecting in-domain training instances for a\nTransformer-based relation extraction model deployed on titles and abstracts of\nresearch papers in the Architecture, Construction, Engineering and Operations\n(AECO) domain. By assessing the performance gain with respect to a baseline\nDeep Learning architecture trained on off-domain data, we show that by using a\nfew-shot learning strategy with structured prompts and only minimal expert\nannotation the presented approach can potentially support domain adaptation of\na science KG generation model.\n","authors":["Vanni Zavarella","Juan Carlos Gamero-Salinas","Sergio Consoli"],"pdf_url":"https://arxiv.org/pdf/2408.02377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02373v1","updated":"2024-08-05T10:53:51Z","published":"2024-08-05T10:53:51Z","title":"Operationalizing Contextual Integrity in Privacy-Conscious Assistants","summary":"  Advanced AI assistants combine frontier LLMs and tool access to autonomously\nperform complex tasks on behalf of users. While the helpfulness of such\nassistants can increase dramatically with access to user information including\nemails and documents, this raises privacy concerns about assistants sharing\ninappropriate information with third parties without user supervision. To steer\ninformation-sharing assistants to behave in accordance with privacy\nexpectations, we propose to operationalize $\\textit{contextual integrity}$\n(CI), a framework that equates privacy with the appropriate flow of information\nin a given context. In particular, we design and evaluate a number of\nstrategies to steer assistants' information-sharing actions to be CI compliant.\nOur evaluation is based on a novel form filling benchmark composed of synthetic\ndata and human annotations, and it reveals that prompting frontier LLMs to\nperform CI-based reasoning yields strong results.\n","authors":["Sahra Ghalebikesabi","Eugene Bagdasaryan","Ren Yi","Itay Yona","Ilia Shumailov","Aneesh Pappu","Chongyang Shi","Laura Weidinger","Robert Stanforth","Leonard Berrada","Pushmeet Kohli","Po-Sen Huang","Borja Balle"],"pdf_url":"https://arxiv.org/pdf/2408.02373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06046v3","updated":"2024-08-05T10:48:32Z","published":"2024-02-08T20:37:51Z","title":"Anatomy of a Robotaxi Crash: Lessons from the Cruise Pedestrian Dragging\n  Mishap","summary":"  An October 2023 crash between a GM Cruise robotaxi and a pedestrian in San\nFrancisco resulted not only in a severe injury, but also dramatic upheaval at\nthat company that will likely have lasting effects throughout the industry.\nIs-sues stem not just from the loss events themselves, but also from how Cruise\nmishandled dealing with their robotaxi dragging a pedestrian under the vehicle\nafter the initial post-crash stop. External investigation reports provide raw\nmaterial describing the incident and critique the company's response from a\nregulatory point of view, but exclude safety engineering recommendations from\nscope. We highlight specific facts and relationships among events by tying\ntogether different pieces of the external report material. We then explore\nsafety lessons that might be learned related to: recognizing and responding to\nnearby mishaps, building an accurate world model of a post-collision scenario,\nthe in-adequacy of a so-called \"minimal risk condition\" strategy in complex\nsituations, poor organizational discipline in responding to a mishap, overly\naggressive post-collision automation choices that made a bad situation worse,\nand a reluctance to admit to a mishap causing much worse organizational harm\ndown-stream.\n","authors":["Philip Koopman"],"pdf_url":"https://arxiv.org/pdf/2402.06046v3.pdf","comment":"15 pages, 2 figures"},{"id":"http://arxiv.org/abs/2407.08583v2","updated":"2024-08-05T10:31:24Z","published":"2024-07-11T15:08:11Z","title":"The Synergy between Data and Multi-Modal Large Language Models: A Survey\n  from Co-Development Perspective","summary":"  The rapid development of large language models (LLMs) has been witnessed in\nrecent years. Based on the powerful LLMs, multi-modal LLMs (MLLMs) extend the\nmodality from text to a broader spectrum of domains, attracting widespread\nattention due to the broader range of application scenarios. As LLMs and MLLMs\nrely on vast amounts of model parameters and data to achieve emergent\ncapabilities, the importance of data is receiving increasingly widespread\nattention and recognition. Tracing and analyzing recent data-oriented works for\nMLLMs, we find that the development of models and data is not two separate\npaths but rather interconnected. On the one hand, vaster and higher-quality\ndata contribute to better performance of MLLMs; on the other hand, MLLMs can\nfacilitate the development of data. The co-development of multi-modal data and\nMLLMs requires a clear view of 1) at which development stages of MLLMs specific\ndata-centric approaches can be employed to enhance certain MLLM capabilities,\nand 2) how MLLMs, utilizing those capabilities, can contribute to multi-modal\ndata in specific roles. To promote the data-model co-development for MLLM\ncommunity, we systematically review existing works related to MLLMs from the\ndata-model co-development perspective. A regularly maintained project\nassociated with this survey is accessible at\nhttps://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md.\n","authors":["Zhen Qin","Daoyuan Chen","Wenhao Zhang","Liuyi Yao","Yilun Huang","Bolin Ding","Yaliang Li","Shuiguang Deng"],"pdf_url":"https://arxiv.org/pdf/2407.08583v2.pdf","comment":"Ongoing work. 21 pages. Related materials are continually maintained\n  and available at\n  https://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md"},{"id":"http://arxiv.org/abs/2408.02361v1","updated":"2024-08-05T10:10:01Z","published":"2024-08-05T10:10:01Z","title":"Dialogue Ontology Relation Extraction via Constrained Chain-of-Thought\n  Decoding","summary":"  State-of-the-art task-oriented dialogue systems typically rely on\ntask-specific ontologies for fulfilling user queries. The majority of\ntask-oriented dialogue data, such as customer service recordings, comes without\nontology and annotation. Such ontologies are normally built manually, limiting\nthe application of specialised systems. Dialogue ontology construction is an\napproach for automating that process and typically consists of two steps: term\nextraction and relation extraction. In this work, we focus on relation\nextraction in a transfer learning set-up. To improve the generalisation, we\npropose an extension to the decoding mechanism of large language models. We\nadapt Chain-of-Thought (CoT) decoding, recently developed for reasoning\nproblems, to generative relation extraction. Here, we generate multiple\nbranches in the decoding space and select the relations based on a confidence\nthreshold. By constraining the decoding to ontology terms and relations, we aim\nto decrease the risk of hallucination. We conduct extensive experimentation on\ntwo widely used datasets and find improvements in performance on target\nontology for source fine-tuned and one-shot prompted large language models.\n","authors":["Renato Vukovic","David Arps","Carel van Niekerk","Benjamin Matthias Ruppik","Hsien-Chin Lin","Michael Heck","Milica Gašić"],"pdf_url":"https://arxiv.org/pdf/2408.02361v1.pdf","comment":"Accepted to appear at SIGDIAL 2024. 9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.02357v1","updated":"2024-08-05T10:06:53Z","published":"2024-08-05T10:06:53Z","title":"On the consistent reasoning paradox of intelligence and optimal trust in\n  AI: The power of 'I don't know'","summary":"  We introduce the Consistent Reasoning Paradox (CRP). Consistent reasoning,\nwhich lies at the core of human intelligence, is the ability to handle tasks\nthat are equivalent, yet described by different sentences ('Tell me the time!'\nand 'What is the time?'). The CRP asserts that consistent reasoning implies\nfallibility -- in particular, human-like intelligence in AI necessarily comes\nwith human-like fallibility. Specifically, it states that there are problems,\ne.g. in basic arithmetic, where any AI that always answers and strives to mimic\nhuman intelligence by reasoning consistently will hallucinate (produce wrong,\nyet plausible answers) infinitely often. The paradox is that there exists a\nnon-consistently reasoning AI (which therefore cannot be on the level of human\nintelligence) that will be correct on the same set of problems. The CRP also\nshows that detecting these hallucinations, even in a probabilistic sense, is\nstrictly harder than solving the original problems, and that there are problems\nthat an AI may answer correctly, but it cannot provide a correct logical\nexplanation for how it arrived at the answer. Therefore, the CRP implies that\nany trustworthy AI (i.e., an AI that never answers incorrectly) that also\nreasons consistently must be able to say 'I don't know'. Moreover, this can\nonly be done by implicitly computing a new concept that we introduce, termed\nthe 'I don't know' function -- something currently lacking in modern AI. In\nview of these insights, the CRP also provides a glimpse into the behaviour of\nArtificial General Intelligence (AGI). An AGI cannot be 'almost sure', nor can\nit always explain itself, and therefore to be trustworthy it must be able to\nsay 'I don't know'.\n","authors":["Alexander Bastounis","Paolo Campodonico","Mihaela van der Schaar","Ben Adcock","Anders C. Hansen"],"pdf_url":"https://arxiv.org/pdf/2408.02357v1.pdf","comment":"12 pages and 50 pages of supplementary material, 7 figures"},{"id":"http://arxiv.org/abs/2408.02349v1","updated":"2024-08-05T09:54:08Z","published":"2024-08-05T09:54:08Z","title":"Active Sensing of Knee Osteoarthritis Progression with Reinforcement\n  Learning","summary":"  Osteoarthritis (OA) is the most common musculoskeletal disease, which has no\ncure. Knee OA (KOA) is one of the highest causes of disability worldwide, and\nit costs billions of United States dollars to the global community. Prediction\nof KOA progression has been of high interest to the community for years, as it\ncan advance treatment development through more efficient clinical trials and\nimprove patient outcomes through more efficient healthcare utilization.\nExisting approaches for predicting KOA, however, are predominantly static, i.e.\nconsider data from a single time point to predict progression many years into\nthe future, and knee level, i.e. consider progression in a single joint only.\nDue to these and related reasons, these methods fail to deliver the level of\npredictive performance, which is sufficient to result in cost savings and\nbetter patient outcomes. Collecting extensive data from all patients on a\nregular basis could address the issue, but it is limited by the high cost at a\npopulation level. In this work, we propose to go beyond static prediction\nmodels in OA, and bring a novel Active Sensing (AS) approach, designed to\ndynamically follow up patients with the objective of maximizing the number of\ninformative data acquisitions, while minimizing their total cost over a period\nof time. Our approach is based on Reinforcement Learning (RL), and it leverages\na novel reward function designed specifically for AS of disease progression in\nmore than one part of a human body. Our method is end-to-end, relies on\nmulti-modal Deep Learning, and requires no human input at inference time.\nThroughout an exhaustive experimental evaluation, we show that using RL can\nprovide a higher monetary benefit when compared to state-of-the-art baselines.\n","authors":["Khanh Nguyen","Huy Hoang Nguyen","Egor Panfilov","Aleksei Tiulpin"],"pdf_url":"https://arxiv.org/pdf/2408.02349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10805v2","updated":"2024-08-05T09:51:15Z","published":"2024-07-15T15:20:40Z","title":"Think-on-Graph 2.0: Deep and Interpretable Large Language Model\n  Reasoning with Knowledge Graph-guided Retrieval","summary":"  Retrieval-augmented generation (RAG) has significantly advanced large\nlanguage models (LLMs) by enabling dynamic information retrieval to mitigate\nknowledge gaps and hallucinations in generated content. However, these systems\noften falter with complex reasoning and consistency across diverse queries. In\nthis work, we present Think-on-Graph 2.0, an enhanced RAG framework that aligns\nquestions with the knowledge graph and uses it as a navigational tool, which\ndeepens and refines the RAG paradigm for information collection and\nintegration. The KG-guided navigation fosters deep and long-range associations\nto uphold logical consistency and optimize the scope of retrieval for precision\nand interoperability. In conjunction, factual consistency can be better ensured\nthrough semantic similarity guided by precise directives. ToG${2.0}$ not only\nimproves the accuracy and reliability of LLMs' responses but also demonstrates\nthe potential of hybrid structured knowledge systems to significantly advance\nLLM reasoning, aligning it closer to human-like performance. We conducted\nextensive experiments on four public datasets to demonstrate the advantages of\nour method compared to the baseline.\n","authors":["Shengjie Ma","Chengjin Xu","Xuhui Jiang","Muzhi Li","Huaren Qu","Jian Guo"],"pdf_url":"https://arxiv.org/pdf/2407.10805v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00938v2","updated":"2024-08-05T09:32:30Z","published":"2024-08-01T22:01:42Z","title":"CIResDiff: A Clinically-Informed Residual Diffusion Model for Predicting\n  Idiopathic Pulmonary Fibrosis Progression","summary":"  The progression of Idiopathic Pulmonary Fibrosis (IPF) significantly\ncorrelates with higher patient mortality rates. Early detection of IPF\nprogression is critical for initiating timely treatment, which can effectively\nslow down the advancement of the disease. However, the current clinical\ncriteria define disease progression requiring two CT scans with a one-year\ninterval, presenting a dilemma: a disease progression is identified only after\nthe disease has already progressed. To this end, in this paper, we develop a\nnovel diffusion model to accurately predict the progression of IPF by\ngenerating patient's follow-up CT scan from the initial CT scan. Specifically,\nfrom the clinical prior knowledge, we tailor improvements to the traditional\ndiffusion model and propose a Clinically-Informed Residual Diffusion model,\ncalled CIResDiff. The key innovations of CIResDiff include 1) performing the\ntarget region pre-registration to align the lung regions of two CT scans at\ndifferent time points for reducing the generation difficulty, 2) adopting the\nresidual diffusion instead of traditional diffusion to enable the model focus\nmore on differences (i.e., lesions) between the two CT scans rather than the\nlargely identical anatomical content, and 3) designing the clinically-informed\nprocess based on CLIP technology to integrate lung function information which\nis highly relevant to diagnosis into the reverse process for assisting\ngeneration. Extensive experiments on clinical data demonstrate that our\napproach can outperform state-of-the-art methods and effectively predict the\nprogression of IPF.\n","authors":["Caiwen Jiang","Xiaodan Xing","Zaixin Ou","Mianxin Liu","Walsh Simon","Guang Yang","Dinggang Shen"],"pdf_url":"https://arxiv.org/pdf/2408.00938v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13244v3","updated":"2024-08-05T09:31:18Z","published":"2024-03-20T02:15:55Z","title":"Instruction Multi-Constraint Molecular Generation Using a\n  Teacher-Student Large Language Model","summary":"  While various models and computational tools have been proposed for structure\nand property analysis of molecules, generating molecules that conform to all\ndesired structures and properties remains a challenge. Here, we introduce a\nmulti-constraint molecular generation large language model, TSMMG, which, akin\nto a student, incorporates knowledge from various small models and tools,\nnamely, the 'teachers'. To train TSMMG, we construct a large set of\ntext-molecule pairs by extracting molecular knowledge from these 'teachers',\nenabling it to generate novel molecules that conform to the descriptions\nthrough various text prompts. We experimentally show that TSMMG remarkably\nperforms in generating molecules meeting complex, natural language-described\nproperty requirements across two-, three-, and four-constraint tasks, with an\naverage molecular validity of over 99% and success ratio of 82.58%, 68.03%, and\n67.48%, respectively. The model also exhibits adaptability through zero-shot\ntesting, creating molecules that satisfy combinations of properties that have\nnot been encountered. It can comprehend text inputs with various language\nstyles, extending beyond the confines of outlined prompts, as confirmed through\nempirical validation. Additionally, the knowledge distillation feature of TSMMG\ncontributes to the continuous enhancement of small models, while the innovative\napproach to dataset construction effectively addresses the issues of data\nscarcity and quality, which positions TSMMG as a promising tool in the domains\nof drug discovery and materials science.\n","authors":["Peng Zhou","Jianmin Wang","Chunyan Li","Zixu Wang","Yiping Liu","Chubo Liu","Siqi Sun","Jianxin Lin","Leyi Wei","Xibao Cai","Houtim Lai","Wei Liu","Longyue Wang","Xiangxiang Zeng","Kenli Li"],"pdf_url":"https://arxiv.org/pdf/2403.13244v3.pdf","comment":"37 pages, 10 figures"},{"id":"http://arxiv.org/abs/2408.02337v1","updated":"2024-08-05T09:23:49Z","published":"2024-08-05T09:23:49Z","title":"Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR\n  Dataset Construction","summary":"  Advancements in AI and natural language processing have revolutionized\nmachine-human language interactions, with question answering (QA) systems\nplaying a pivotal role. The knowledge base question answering (KBQA) task,\nutilizing structured knowledge graphs (KG), allows for handling extensive\nknowledge-intensive questions. However, a significant gap exists in KBQA\ndatasets, especially for low-resource languages. Many existing construction\npipelines for these datasets are outdated and inefficient in human labor, and\nmodern assisting tools like Large Language Models (LLM) are not utilized to\nreduce the workload. To address this, we have designed and implemented a\nmodern, semi-automated approach for creating datasets, encompassing tasks such\nas KBQA, Machine Reading Comprehension (MRC), and Information Retrieval (IR),\ntailored explicitly for low-resource environments. We executed this pipeline\nand introduced the PUGG dataset, the first Polish KBQA dataset, and novel\ndatasets for MRC and IR. Additionally, we provide a comprehensive\nimplementation, insightful findings, detailed statistics, and evaluation of\nbaseline models.\n","authors":["Albert Sawczyn","Katsiaryna Viarenich","Konrad Wojtasik","Aleksandra Domogała","Marcin Oleksy","Maciej Piasecki","Tomasz Kajdanowicz"],"pdf_url":"https://arxiv.org/pdf/2408.02337v1.pdf","comment":"Accepted for ACL 2024 (findings)"},{"id":"http://arxiv.org/abs/2408.00230v2","updated":"2024-08-05T08:36:20Z","published":"2024-08-01T01:54:17Z","title":"Lost in Translation: Latent Concept Misalignment in Text-to-Image\n  Diffusion Models","summary":"  Advancements in text-to-image diffusion models have broadened extensive\ndownstream practical applications, but such models often encounter misalignment\nissues between text and image. Taking the generation of a combination of two\ndisentangled concepts as an example, say given the prompt \"a tea cup of iced\ncoke\", existing models usually generate a glass cup of iced coke because the\niced coke usually co-occurs with the glass cup instead of the tea one during\nmodel training. The root of such misalignment is attributed to the confusion in\nthe latent semantic space of text-to-image diffusion models, and hence we refer\nto the \"a tea cup of iced coke\" phenomenon as Latent Concept Misalignment\n(LC-Mis). We leverage large language models (LLMs) to thoroughly investigate\nthe scope of LC-Mis, and develop an automated pipeline for aligning the latent\nsemantics of diffusion models to text prompts. Empirical assessments confirm\nthe effectiveness of our approach, substantially reducing LC-Mis errors and\nenhancing the robustness and versatility of text-to-image diffusion models. The\ncode and dataset are here: https://github.com/RossoneriZhao/iced_coke.\n","authors":["Juntu Zhao","Junyu Deng","Yixin Ye","Chongxuan Li","Zhijie Deng","Dequan Wang"],"pdf_url":"https://arxiv.org/pdf/2408.00230v2.pdf","comment":"Accepted by the 18th European Conference on Computer Vision ECCV 2024"},{"id":"http://arxiv.org/abs/2403.13682v3","updated":"2024-08-05T08:32:22Z","published":"2024-03-20T15:40:18Z","title":"Threats, Attacks, and Defenses in Machine Unlearning: A Survey","summary":"  Machine Unlearning (MU) has gained considerable attention recently for its\npotential to achieve Safe AI by removing the influence of specific data from\ntrained machine learning models. This process, known as knowledge removal,\naddresses AI governance concerns of training data such as quality, sensitivity,\ncopyright restrictions, and obsolescence. This capability is also crucial for\nensuring compliance with privacy regulations such as the Right To Be Forgotten.\nFurthermore, effective knowledge removal mitigates the risk of harmful\noutcomes, safeguarding against biases, misinformation, and unauthorized data\nexploitation, thereby enhancing the safe and responsible use of AI systems.\nEfforts have been made to design efficient unlearning approaches, with MU\nservices being examined for integration with existing machine learning as a\nservice, allowing users to submit requests to remove specific data from the\ntraining corpus. However, recent research highlights vulnerabilities in machine\nunlearning systems, such as information leakage and malicious unlearning\nrequests, that can lead to significant security and privacy concerns. Moreover,\nextensive research indicates that unlearning methods and prevalent attacks\nfulfill diverse roles within MU systems. For instance, unlearning can act as a\nmechanism to recover models from backdoor attacks, while backdoor attacks\nthemselves can serve as an evaluation metric for unlearning effectiveness. This\nunderscores the intricate relationship and complex interplay among these\nmechanisms in maintaining system functionality and safety. This survey aims to\nfill the gap between the extensive number of studies on threats, attacks, and\ndefenses in machine unlearning and the absence of a comprehensive review that\ncategorizes their taxonomy, methods, and solutions, thus offering valuable\ninsights for future research directions and practical implementations.\n","authors":["Ziyao Liu","Huanyi Ye","Chen Chen","Kwok-Yan Lam"],"pdf_url":"https://arxiv.org/pdf/2403.13682v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.11393v2","updated":"2024-08-05T08:28:59Z","published":"2023-06-20T08:59:51Z","title":"The Cultivated Practices of Text-to-Image Generation","summary":"  Humankind is entering a novel creative era in which anybody can synthesize\ndigital information using generative artificial intelligence (AI).\nText-to-image generation, in particular, has become vastly popular and millions\nof practitioners produce AI-generated images and AI art online. This chapter\nfirst gives an overview of the key developments that enabled a healthy\nco-creative online ecosystem around text-to-image generation to rapidly emerge,\nfollowed by a high-level description of key elements in this ecosystem. A\nparticular focus is placed on prompt engineering, a creative practice that has\nbeen embraced by the AI art community. It is then argued that the emerging\nco-creative ecosystem constitutes an intelligent system on its own - a system\nthat both supports human creativity, but also potentially entraps future\ngenerations and limits future development efforts in AI. The chapter discusses\nthe potential risks and dangers of cultivating this co-creative ecosystem, such\nas the bias inherent in today's training data, potential quality degradation in\nfuture image generation systems due to synthetic data becoming common place,\nand the potential long-term effects of text-to-image generation on people's\nimagination, ambitions, and development.\n","authors":["Jonas Oppenlaender"],"pdf_url":"https://arxiv.org/pdf/2306.11393v2.pdf","comment":"In \"Humane autonomous technology - Re-thinking experience with and in\n  intelligent systems\", Palgrave Macmillan, 2024"},{"id":"http://arxiv.org/abs/2407.15362v2","updated":"2024-08-05T08:26:24Z","published":"2024-07-22T04:09:27Z","title":"A Multimodal Knowledge-enhanced Whole-slide Pathology Foundation Model","summary":"  Remarkable strides in computational pathology have been made in the\ntask-agnostic foundation model that advances the performance of a wide array of\ndownstream clinical tasks. Despite the promising performance, there are still\nseveral challenges. First, prior works have resorted to either vision-only or\nvision-captions data, disregarding invaluable pathology reports and gene\nexpression profiles which respectively offer distinct knowledge for versatile\nclinical applications. Second, the current progress in pathology FMs\npredominantly concentrates on the patch level, where the restricted context of\npatch-level pretraining fails to capture whole-slide patterns. Here we curated\nthe largest multimodal dataset consisting of H\\&E diagnostic whole slide images\nand their associated pathology reports and RNA-Seq data, resulting in 26,169\nslide-level modality pairs from 10,275 patients across 32 cancer types. To\nleverage these data for CPath, we propose a novel whole-slide pretraining\nparadigm which injects multimodal knowledge at the whole-slide context into the\npathology FM, called Multimodal Self-TAught PRetraining (mSTAR). The proposed\nparadigm revolutionizes the workflow of pretraining for CPath, which enables\nthe pathology FM to acquire the whole-slide context. To our knowledge, this is\nthe first attempt to incorporate multimodal knowledge at the slide level for\nenhancing pathology FMs, expanding the modelling context from unimodal to\nmultimodal knowledge and from patch-level to slide-level. To systematically\nevaluate the capabilities of mSTAR, extensive experiments including slide-level\nunimodal and multimodal applications, are conducted across 7 diverse types of\ntasks on 43 subtasks, resulting in the largest spectrum of downstream tasks.\nThe average performance in various slide-level applications consistently\ndemonstrates significant performance enhancements for mSTAR compared to SOTA\nFMs.\n","authors":["Yingxue Xu","Yihui Wang","Fengtao Zhou","Jiabo Ma","Shu Yang","Huangjing Lin","Xin Wang","Jiguang Wang","Li Liang","Anjia Han","Ronald Cheong Kin Chan","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2407.15362v2.pdf","comment":"45 pages, 9 figures"},{"id":"http://arxiv.org/abs/2407.08331v2","updated":"2024-08-05T08:13:37Z","published":"2024-07-11T09:28:27Z","title":"Towards Explainable Evolution Strategies with Large Language Models","summary":"  This paper introduces an approach that integrates self-adaptive Evolution\nStrategies (ES) with Large Language Models (LLMs) to enhance the explainability\nof complex optimization processes. By employing a self-adaptive ES equipped\nwith a restart mechanism, we effectively navigate the challenging landscapes of\nbenchmark functions, capturing detailed logs of the optimization journey. The\nlogs include fitness evolution, step-size adjustments and restart events due to\nstagnation. An LLM is then utilized to process these logs, generating concise,\nuser-friendly summaries that highlight key aspects such as convergence\nbehavior, optimal fitness achievements, and encounters with local optima. Our\ncase study on the Rastrigin function demonstrates how our approach makes the\ncomplexities of ES optimization transparent. Our findings highlight the\npotential of using LLMs to bridge the gap between advanced optimization\nalgorithms and their interpretability.\n","authors":["Jill Baumann","Oliver Kramer"],"pdf_url":"https://arxiv.org/pdf/2407.08331v2.pdf","comment":"Accepted at ESANN 2024"},{"id":"http://arxiv.org/abs/2408.02295v1","updated":"2024-08-05T08:12:25Z","published":"2024-08-05T08:12:25Z","title":"Generalized Gaussian Temporal Difference Error For Uncertainty-aware\n  Reinforcement Learning","summary":"  Conventional uncertainty-aware temporal difference (TD) learning methods\noften rely on simplistic assumptions, typically including a zero-mean Gaussian\ndistribution for TD errors. Such oversimplification can lead to inaccurate\nerror representations and compromised uncertainty estimation. In this paper, we\nintroduce a novel framework for generalized Gaussian error modeling in deep\nreinforcement learning, applicable to both discrete and continuous control\nsettings. Our framework enhances the flexibility of error distribution modeling\nby incorporating higher-order moments, particularly kurtosis, thereby improving\nthe estimation and mitigation of data-dependent noise, i.e., aleatoric\nuncertainty. We examine the influence of the shape parameter of the generalized\nGaussian distribution (GGD) on aleatoric uncertainty and provide a closed-form\nexpression that demonstrates an inverse relationship between uncertainty and\nthe shape parameter. Additionally, we propose a theoretically grounded\nweighting scheme to fully leverage the GGD. To address epistemic uncertainty,\nwe enhance the batch inverse variance weighting by incorporating bias reduction\nand kurtosis considerations, resulting in improved robustness. Extensive\nexperimental evaluations using policy gradient algorithms demonstrate the\nconsistent efficacy of our method, showcasing significant performance\nimprovements.\n","authors":["Seyeon Kim","Joonhun Lee","Namhoon Cho","Sungjun Han","Seungeon Baek"],"pdf_url":"https://arxiv.org/pdf/2408.02295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.16458v2","updated":"2024-08-05T07:59:19Z","published":"2024-01-29T10:11:05Z","title":"Credit Risk Meets Large Language Models: Building a Risk Indicator from\n  Loan Descriptions in P2P Lending","summary":"  Peer-to-peer (P2P) lending has emerged as a distinctive financing mechanism,\nlinking borrowers with lenders through online platforms. However, P2P lending\nfaces the challenge of information asymmetry, as lenders often lack sufficient\ndata to assess the creditworthiness of borrowers. This paper proposes a novel\napproach to address this issue by leveraging the textual descriptions provided\nby borrowers during the loan application process. Our methodology involves\nprocessing these textual descriptions using a Large Language Model (LLM), a\npowerful tool capable of discerning patterns and semantics within the text.\nTransfer learning is applied to adapt the LLM to the specific task at hand.\n  Our results derived from the analysis of the Lending Club dataset show that\nthe risk score generated by BERT, a widely used LLM, significantly improves the\nperformance of credit risk classifiers. However, the inherent opacity of\nLLM-based systems, coupled with uncertainties about potential biases,\nunderscores critical considerations for regulatory frameworks and engenders\ntrust-related concerns among end-users, opening new avenues for future research\nin the dynamic landscape of P2P lending and artificial intelligence.\n","authors":["Mario Sanz-Guerrero","Javier Arroyo"],"pdf_url":"https://arxiv.org/pdf/2401.16458v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03392v2","updated":"2024-08-05T07:55:50Z","published":"2023-10-05T08:57:17Z","title":"Unpacking Human-AI Interaction in Safety-Critical Industries: A\n  Systematic Literature Review","summary":"  Ensuring quality human-AI interaction (HAII) in safety-critical industries is\nessential. Failure to do so can lead to catastrophic and deadly consequences.\nDespite this urgency, existing research on HAII is limited, fragmented, and\ninconsistent. We present here a survey of that literature and recommendations\nfor research best practices that should improve the field. We divided our\ninvestigation into the following areas: 1) terms used to describe HAII, 2)\nprimary roles of AI-enabled systems, 3) factors that influence HAII, and 4) how\nHAII is measured. Additionally, we described the capabilities and maturity of\nthe AI-enabled systems used in safety-critical industries discussed in these\narticles. We found that no single term is used across the literature to\ndescribe HAII and some terms have multiple meanings. According to our\nliterature, seven factors influence HAII: user characteristics (e.g., user\npersonality), user perceptions and attitudes (e.g., user biases), user\nexpectations and experience (e.g., mismatched user expectations and\nexperience), AI interface and features (e.g., interactive design), AI output\n(e.g., perceived accuracy), explainability and interpretability (e.g., level of\ndetail, user understanding), and usage of AI (e.g., heterogeneity of\nenvironments). HAII is most measured with user-related subjective metrics\n(e.g., user perceptions, trust, and attitudes), and AI-assisted decision-making\nis the most common primary role of AI-enabled systems. Based on this review, we\nconclude that there are substantial research gaps in HAII. Researchers and\ndevelopers need to codify HAII terminology, involve users throughout the AI\nlifecycle (especially during development), and tailor HAII in safety-critical\nindustries to the users and environments.\n","authors":["Tita A. Bach","Jenny K. Kristiansen","Aleksandar Babic","Alon Jacovi"],"pdf_url":"https://arxiv.org/pdf/2310.03392v2.pdf","comment":"Accepted for publication in IEEE Access"},{"id":"http://arxiv.org/abs/2206.02963v3","updated":"2024-08-05T07:55:34Z","published":"2022-06-07T01:49:22Z","title":"Confidence-aware Self-Semantic Distillation on Knowledge Graph Embedding","summary":"  Knowledge Graph Embedding (KGE), which projects entities and relations into\ncontinuous vector spaces, has garnered significant attention. Although\nhigh-dimensional KGE methods offer better performance, they come at the expense\nof significant computation and memory overheads. Decreasing embedding\ndimensions significantly deteriorates model performance. While several recent\nefforts utilize knowledge distillation or non-Euclidean representation learning\nto augment the effectiveness of low-dimensional KGE, they either necessitate a\npre-trained high-dimensional teacher model or involve complex non-Euclidean\noperations, thereby incurring considerable additional computational costs. To\naddress this, this work proposes Confidence-aware Self-Knowledge Distillation\n(CSD) that learns from the model itself to enhance KGE in a low-dimensional\nspace. Specifically, CSD extracts knowledge from embeddings in previous\niterations, which would be utilized to supervise the learning of the model in\nthe next iterations. Moreover, a specific semantic module is developed to\nfilter reliable knowledge by estimating the confidence of previously learned\nembeddings. This straightforward strategy bypasses the need for time-consuming\npre-training of teacher models and can be integrated into various KGE methods\nto improve their performance. Our comprehensive experiments on six KGE\nbackbones and four datasets underscore the effectiveness of the proposed CSD.\n","authors":["Yichen Liu","Jiawei Chen","Defang Chen","Zhehui Zhou","Yan Feng","Can Wang"],"pdf_url":"https://arxiv.org/pdf/2206.02963v3.pdf","comment":"CIKM 2024"},{"id":"http://arxiv.org/abs/2408.02288v1","updated":"2024-08-05T07:54:01Z","published":"2024-08-05T07:54:01Z","title":"Spin glass model of in-context learning","summary":"  Large language models show a surprising in-context learning ability -- being\nable to use a prompt to form a prediction for a query, yet without additional\ntraining, in stark contrast to old-fashioned supervised learning. Providing a\nmechanistic interpretation and linking the empirical phenomenon to physics are\nthus challenging and remain unsolved. We study a simple yet expressive\ntransformer with linear attention, and map this structure to a spin glass model\nwith real-valued spins, where the couplings and fields explain the intrinsic\ndisorder in data. The spin glass model explains how the weight parameters\ninteract with each other during pre-training, and most importantly why an\nunseen function can be predicted by providing only a prompt yet without\ntraining. Our theory reveals that for single instance learning, increasing the\ntask diversity leads to the emergence of the in-context learning, by allowing\nthe Boltzmann distribution to converge to a unique correct solution of weight\nparameters. Therefore the pre-trained transformer displays a prediction power\nin a novel prompt setting. The proposed spin glass model thus establishes a\nfoundation to understand the empirical success of large language models.\n","authors":["Yuhao Li","Ruoran Bai","Haiping Huang"],"pdf_url":"https://arxiv.org/pdf/2408.02288v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.02280v1","updated":"2024-08-05T07:30:18Z","published":"2024-08-05T07:30:18Z","title":"Hardware Aware Ensemble Selection for Balancing Predictive Accuracy and\n  Cost","summary":"  Automated Machine Learning (AutoML) significantly simplifies the deployment\nof machine learning models by automating tasks from data preprocessing to model\nselection to ensembling. AutoML systems for tabular data often employ post hoc\nensembling, where multiple models are combined to improve predictive accuracy.\nThis typically results in longer inference times, a major limitation in\npractical deployments. Addressing this, we introduce a hardware-aware ensemble\nselection approach that integrates inference time into post hoc ensembling. By\nleveraging an existing framework for ensemble selection with quality diversity\noptimization, our method evaluates ensemble candidates for their predictive\naccuracy and hardware efficiency. This dual focus allows for a balanced\nconsideration of accuracy and operational efficiency. Thus, our approach\nenables practitioners to choose from a Pareto front of accurate and efficient\nensembles. Our evaluation using 83 classification datasets shows that our\napproach sustains competitive accuracy and can significantly improve ensembles'\noperational efficiency. The results of this study provide a foundation for\nextending these principles to additional hardware constraints, setting the\nstage for the development of more resource-efficient AutoML systems.\n","authors":["Jannis Maier","Felix Möller","Lennart Purucker"],"pdf_url":"https://arxiv.org/pdf/2408.02280v1.pdf","comment":"Accepted at Third International Conference on Automated Machine\n  Learning (AutoML 2024), Workshop Track; for code, see\n  https://github.com/Atraxus/HA-ES"},{"id":"http://arxiv.org/abs/2308.11530v2","updated":"2024-08-05T07:28:32Z","published":"2023-08-22T15:59:06Z","title":"Leveraging Language Model Capabilities for Sound Event Detection","summary":"  Large language models reveal deep comprehension and fluent generation in the\nfield of multi-modality. Although significant advancements have been achieved\nin audio multi-modality, existing methods are rarely leverage language model\nfor sound event detection (SED). In this work, we propose an end-to-end\nframework for understanding audio features while simultaneously generating\nsound event and their temporal location. Specifically, we employ pretrained\nacoustic models to capture discriminative features across different categories\nand language models for autoregressive text generation. Conventional methods\ngenerally struggle to obtain features in pure audio domain for classification.\nIn contrast, our framework utilizes the language model to flexibly understand\nabundant semantic context aligned with the acoustic representation. The\nexperimental results showcase the effectiveness of proposed method in enhancing\ntimestamps precision and event classification.\n","authors":["Hualei Wang","Jianguo Mao","Zhifang Guo","Jiarui Wan","Hong Liu","Xiangdong Wang"],"pdf_url":"https://arxiv.org/pdf/2308.11530v2.pdf","comment":"5 pages, 4 figures, accept by interspeech2024"},{"id":"http://arxiv.org/abs/2408.02279v1","updated":"2024-08-05T07:26:47Z","published":"2024-08-05T07:26:47Z","title":"DRFormer: Multi-Scale Transformer Utilizing Diverse Receptive Fields for\n  Long Time-Series Forecasting","summary":"  Long-term time series forecasting (LTSF) has been widely applied in finance,\ntraffic prediction, and other domains. Recently, patch-based transformers have\nemerged as a promising approach, segmenting data into sub-level patches that\nserve as input tokens. However, existing methods mostly rely on predetermined\npatch lengths, necessitating expert knowledge and posing challenges in\ncapturing diverse characteristics across various scales. Moreover, time series\ndata exhibit diverse variations and fluctuations across different temporal\nscales, which traditional approaches struggle to model effectively. In this\npaper, we propose a dynamic tokenizer with a dynamic sparse learning algorithm\nto capture diverse receptive fields and sparse patterns of time series data. In\norder to build hierarchical receptive fields, we develop a multi-scale\nTransformer model, coupled with multi-scale sequence extraction, capable of\ncapturing multi-resolution features. Additionally, we introduce a group-aware\nrotary position encoding technique to enhance intra- and inter-group position\nawareness among representations across different temporal scales. Our proposed\nmodel, named DRFormer, is evaluated on various real-world datasets, and\nexperimental results demonstrate its superiority compared to existing methods.\nOur code is available at: https://github.com/ruixindingECNU/DRFormer.\n","authors":["Ruixin Ding","Yuqi Chen","Yu-Ting Lan","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.02279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05502v2","updated":"2024-08-05T07:22:58Z","published":"2024-07-07T21:26:36Z","title":"Faux Polyglot: A Study on Information Disparity in Multilingual Large\n  Language Models","summary":"  With Retrieval Augmented Generation (RAG), Large Language Models (LLMs) are\nplaying a pivotal role in information search and are being adopted globally.\nAlthough the multilingual capability of LLMs offers new opportunities to bridge\nthe language barrier, do these capabilities translate into real-life scenarios\nwhere linguistic divide and knowledge conflicts between multilingual sources\nare known occurrences? In this paper, we studied LLM's linguistic preference in\na RAG-based information search setting. We found that LLMs displayed systemic\nbias towards information in the same language as the query language in both\ninformation retrieval and answer generation. Furthermore, in scenarios where\nthere is little information in the language of the query, LLMs prefer documents\nin high-resource languages, reinforcing the dominant views. Such bias exists\nfor both factual and opinion-based queries. Our results highlight the\nlinguistic divide within multilingual LLMs in information search systems. The\nseemingly beneficial multilingual capability of LLMs may backfire on\ninformation parity by reinforcing language-specific information cocoons or\nfilter bubbles further marginalizing low-resource views.\n","authors":["Nikhil Sharma","Kenton Murray","Ziang Xiao"],"pdf_url":"https://arxiv.org/pdf/2407.05502v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04485v2","updated":"2024-08-05T07:18:25Z","published":"2024-06-06T20:15:42Z","title":"GenAI Arena: An Open Evaluation Platform for Generative Models","summary":"  Generative AI has made remarkable strides to revolutionize fields such as\nimage and video generation. These advancements are driven by innovative\nalgorithms, architecture, and data. However, the rapid proliferation of\ngenerative models has highlighted a critical gap: the absence of trustworthy\nevaluation metrics. Current automatic assessments such as FID, CLIP, FVD, etc\noften fail to capture the nuanced quality and user satisfaction associated with\ngenerative outputs. This paper proposes an open platform GenAI-Arena to\nevaluate different image and video generative models, where users can actively\nparticipate in evaluating these models. By leveraging collective user feedback\nand votes, GenAI-Arena aims to provide a more democratic and accurate measure\nof model performance. It covers three arenas for text-to-image generation,\ntext-to-video generation, and image editing respectively. Currently, we cover a\ntotal of 27 open-source generative models. GenAI-Arena has been operating for\nfour months, amassing over 6000 votes from the community. We describe our\nplatform, analyze the data, and explain the statistical methods for ranking the\nmodels. To further promote the research in building model-based evaluation\nmetrics, we release a cleaned version of our preference data for the three\ntasks, namely GenAI-Bench. We prompt the existing multi-modal models like\nGemini, GPT-4o to mimic human voting. We compute the correlation between model\nvoting with human voting to understand their judging abilities. Our results\nshow existing multimodal models are still lagging in assessing the generated\nvisual content, even the best model GPT-4o only achieves a Pearson correlation\nof 0.22 in the quality subscore, and behaves like random guessing in others.\n","authors":["Dongfu Jiang","Max Ku","Tianle Li","Yuansheng Ni","Shizhuo Sun","Rongqi Fan","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2406.04485v2.pdf","comment":"9 pages,7 figures"},{"id":"http://arxiv.org/abs/2408.02275v1","updated":"2024-08-05T07:10:40Z","published":"2024-08-05T07:10:40Z","title":"Geometric Algebra Meets Large Language Models: Instruction-Based\n  Transformations of Separate Meshes in 3D, Interactive and Controllable Scenes","summary":"  This paper introduces a novel integration of Large Language Models (LLMs)\nwith Conformal Geometric Algebra (CGA) to revolutionize controllable 3D scene\nediting, particularly for object repositioning tasks, which traditionally\nrequires intricate manual processes and specialized expertise. These\nconventional methods typically suffer from reliance on large training datasets\nor lack a formalized language for precise edits. Utilizing CGA as a robust\nformal language, our system, shenlong, precisely models spatial transformations\nnecessary for accurate object repositioning. Leveraging the zero-shot learning\ncapabilities of pre-trained LLMs, shenlong translates natural language\ninstructions into CGA operations which are then applied to the scene,\nfacilitating exact spatial transformations within 3D scenes without the need\nfor specialized pre-training. Implemented in a realistic simulation\nenvironment, shenlong ensures compatibility with existing graphics pipelines.\nTo accurately assess the impact of CGA, we benchmark against robust Euclidean\nSpace baselines, evaluating both latency and accuracy. Comparative performance\nevaluations indicate that shenlong significantly reduces LLM response times by\n16% and boosts success rates by 9.6% on average compared to the traditional\nmethods. Notably, shenlong achieves a 100% perfect success rate in common\npractical queries, a benchmark where other systems fall short. These\nadvancements underscore shenlong's potential to democratize 3D scene editing,\nenhancing accessibility and fostering innovation across sectors such as\neducation, digital entertainment, and virtual reality.\n","authors":["Dimitris Angelis","Prodromos Kolyvakis","Manos Kamarianakis","George Papagiannakis"],"pdf_url":"https://arxiv.org/pdf/2408.02275v1.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2408.01091v2","updated":"2024-08-05T06:56:44Z","published":"2024-08-02T08:11:11Z","title":"Dissecting Dissonance: Benchmarking Large Multimodal Models Against\n  Self-Contradictory Instructions","summary":"  Large multimodal models (LMMs) excel in adhering to human instructions.\nHowever, self-contradictory instructions may arise due to the increasing trend\nof multimodal interaction and context length, which is challenging for language\nbeginners and vulnerable populations. We introduce the Self-Contradictory\nInstructions benchmark to evaluate the capability of LMMs in recognizing\nconflicting commands. It comprises 20,000 conflicts, evenly distributed between\nlanguage and vision paradigms. It is constructed by a novel automatic dataset\ncreation framework, which expedites the process and enables us to encompass a\nwide range of instruction forms. Our comprehensive evaluation reveals current\nLMMs consistently struggle to identify multimodal instruction discordance due\nto a lack of self-awareness. Hence, we propose the Cognitive Awakening\nPrompting to inject cognition from external, largely enhancing dissonance\ndetection. The dataset and code are here: https://selfcontradiction.github.io/.\n","authors":["Jin Gao","Lei Gan","Yuankai Li","Yixin Ye","Dequan Wang"],"pdf_url":"https://arxiv.org/pdf/2408.01091v2.pdf","comment":"Accepted by the 18th European Conference on Computer Vision ECCV 2024"},{"id":"http://arxiv.org/abs/2407.18271v2","updated":"2024-08-05T06:12:46Z","published":"2024-07-21T11:25:21Z","title":"Large Language Model for Verilog Generation with Golden Code Feedback","summary":"  Recent advancements in large language models (LLMs) have catalyzed\nsignificant interest in the automatic generation of Register-Transfer Level\n(RTL) code, particularly Verilog, from natural language instructions. While\ncommercial LLMs like ChatGPT have dominated this domain, open-source\nalternatives have lagged considerably in performance, limiting the flexibility\nand data privacy of this emerging technology. This study introduces a novel\napproach utilizing reinforcement learning with golden code feedback to enhance\nthe performance of pre-trained models. Leveraging open-source data and base\nmodels, we have achieved state-of-the-art (SOTA) results with a substantial\nmargin. Notably, our 6.7B parameter model \\ours{} demonstrates superior\nperformance compared to current best-in-class 13B and 16B models. Furthermore,\nthrough a comprehensive analysis of the limitations in direct fine-tuning and\nthe training dynamics of reinforcement learning, we posit that the development\nof comprehensive supervisory signals, which are align with the inherent\nparallel semantics of Verilog code, is critical to effective generation. The\ncode and data associated with this research are publicly available at\n\\url{https://github.com/CatIIIIIIII/veriseek}. The model weights can be\naccessed at \\url{https://huggingface.co/WANGNingroci/VeriSeek}.\n","authors":["Ning Wang","Bingkun Yao","Jie Zhou","Xi Wang","Zhe Jiang","Nan Guan"],"pdf_url":"https://arxiv.org/pdf/2407.18271v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20708v3","updated":"2024-08-05T05:53:55Z","published":"2024-07-30T10:04:16Z","title":"Integer-Valued Training and Spike-Driven Inference Spiking Neural\n  Network for High-performance and Energy-efficient Object Detection","summary":"  Brain-inspired Spiking Neural Networks (SNNs) have bio-plausibility and\nlow-power advantages over Artificial Neural Networks (ANNs). Applications of\nSNNs are currently limited to simple classification tasks because of their poor\nperformance. In this work, we focus on bridging the performance gap between\nANNs and SNNs on object detection. Our design revolves around network\narchitecture and spiking neuron. First, the overly complex module design causes\nspike degradation when the YOLO series is converted to the corresponding\nspiking version. We design a SpikeYOLO architecture to solve this problem by\nsimplifying the vanilla YOLO and incorporating meta SNN blocks. Second, object\ndetection is more sensitive to quantization errors in the conversion of\nmembrane potentials into binary spikes by spiking neurons. To address this\nchallenge, we design a new spiking neuron that activates Integer values during\ntraining while maintaining spike-driven by extending virtual timesteps during\ninference. The proposed method is validated on both static and neuromorphic\nobject detection datasets. On the static COCO dataset, we obtain 66.2% mAP@50\nand 48.9% mAP@50:95, which is +15.0% and +18.7% higher than the prior\nstate-of-the-art SNN, respectively. On the neuromorphic Gen1 dataset, we\nachieve 67.2% mAP@50, which is +2.5% greater than the ANN with equivalent\narchitecture, and the energy efficiency is improved by 5.7*. Code:\nhttps://github.com/BICLab/SpikeYOLO\n","authors":["Xinhao Luo","Man Yao","Yuhong Chou","Bo Xu","Guoqi Li"],"pdf_url":"https://arxiv.org/pdf/2407.20708v3.pdf","comment":"Accepted by ECCV2024; 19 pages, 4 figures"},{"id":"http://arxiv.org/abs/2312.16477v3","updated":"2024-08-05T05:51:21Z","published":"2023-12-27T08:52:41Z","title":"Group Multi-View Transformer for 3D Shape Analysis with Spatial Encoding","summary":"  In recent years, the results of view-based 3D shape recognition methods have\nsaturated, and models with excellent performance cannot be deployed on\nmemory-limited devices due to their huge size of parameters. To address this\nproblem, we introduce a compression method based on knowledge distillation for\nthis field, which largely reduces the number of parameters while preserving\nmodel performance as much as possible. Specifically, to enhance the\ncapabilities of smaller models, we design a high-performing large model called\nGroup Multi-view Vision Transformer (GMViT). In GMViT, the view-level ViT first\nestablishes relationships between view-level features. Additionally, to capture\ndeeper features, we employ the grouping module to enhance view-level features\ninto group-level features. Finally, the group-level ViT aggregates group-level\nfeatures into complete, well-formed 3D shape descriptors. Notably, in both\nViTs, we introduce spatial encoding of camera coordinates as innovative\nposition embeddings. Furthermore, we propose two compressed versions based on\nGMViT, namely GMViT-simple and GMViT-mini. To enhance the training\neffectiveness of the small models, we introduce a knowledge distillation method\nthroughout the GMViT process, where the key outputs of each GMViT component\nserve as distillation targets. Extensive experiments demonstrate the efficacy\nof the proposed method. The large model GMViT achieves excellent 3D\nclassification and retrieval results on the benchmark datasets ModelNet,\nShapeNetCore55, and MCB. The smaller models, GMViT-simple and GMViT-mini,\nreduce the parameter size by 8 and 17.6 times, respectively, and improve shape\nrecognition speed by 1.5 times on average, while preserving at least 90% of the\nclassification and retrieval performance. The code is available at\nhttps://github.com/bigdata-graph/GMViT.\n","authors":["Lixiang Xu","Qingzhe Cui","Richang Hong","Wei Xu","Enhong Chen","Xin Yuan","Chenglong Li","Yuanyan Tang"],"pdf_url":"https://arxiv.org/pdf/2312.16477v3.pdf","comment":"13pages, 8 figuers"},{"id":"http://arxiv.org/abs/2408.02247v1","updated":"2024-08-05T05:41:16Z","published":"2024-08-05T05:41:16Z","title":"Contrastive Learning and Abstract Concepts: The Case of Natural Numbers","summary":"  Contrastive Learning (CL) has been successfully applied to classification and\nother downstream tasks related to concrete concepts, such as objects contained\nin the ImageNet dataset. No attempts seem to have been made so far in applying\nthis promising scheme to more abstract entities. A prominent example of these\ncould be the concept of (discrete) Quantity. CL can be frequently interpreted\nas a self-supervised scheme guided by some profound and ubiquitous conservation\nprinciple (e.g. conservation of identity in object classification tasks). In\nthis introductory work we apply a suitable conservation principle to the\nsemi-abstract concept of natural numbers by which discrete quantities can be\nestimated or predicted. We experimentally show, by means of a toy problem, that\ncontrastive learning can be trained to count at a glance with high accuracy\nboth at human as well as at super-human ranges.. We compare this with the\nresults of a trained-to-count at a glance supervised learning (SL) neural\nnetwork scheme of similar architecture. We show that both schemes exhibit\nsimilar good performance on baseline experiments, where the distributions of\nthe training and testing stages are equal. Importantly, we demonstrate that in\nsome generalization scenarios, where training and testing distributions differ,\nCL boasts more robust and much better error performance.\n","authors":["Daniel N. Nissani"],"pdf_url":"https://arxiv.org/pdf/2408.02247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02244v1","updated":"2024-08-05T05:30:36Z","published":"2024-08-05T05:30:36Z","title":"Evaluating Vision-Language Models for Zero-Shot Detection,\n  Classification, and Association of Motorcycles, Passengers, and Helmets","summary":"  Motorcycle accidents pose significant risks, particularly when riders and\npassengers do not wear helmets. This study evaluates the efficacy of an\nadvanced vision-language foundation model, OWLv2, in detecting and classifying\nvarious helmet-wearing statuses of motorcycle occupants using video data. We\nextend the dataset provided by the CVPR AI City Challenge and employ a cascaded\nmodel approach for detection and classification tasks, integrating OWLv2 and\nCNN models. The results highlight the potential of zero-shot learning to\naddress challenges arising from incomplete and biased training datasets,\ndemonstrating the usage of such models in detecting motorcycles, helmet usage,\nand occupant positions under varied conditions. We have achieved an average\nprecision of 0.5324 for helmet detection and provided precision-recall curves\ndetailing the detection and classification performance. Despite limitations\nsuch as low-resolution data and poor visibility, our research shows promising\nadvancements in automated vehicle safety and traffic safety enforcement\nsystems.\n","authors":["Lucas Choi","Ross Greer"],"pdf_url":"https://arxiv.org/pdf/2408.02244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02233v1","updated":"2024-08-05T04:53:17Z","published":"2024-08-05T04:53:17Z","title":"A Multi-Source Heterogeneous Knowledge Injected Prompt Learning Method\n  for Legal Charge Prediction","summary":"  Legal charge prediction, an essential task in legal AI, seeks to assign\naccurate charge labels to case descriptions, attracting significant recent\ninterest. Existing methods primarily employ diverse neural network structures\nfor modeling case descriptions directly, failing to effectively leverage\nmulti-source external knowledge. We propose a prompt learning framework-based\nmethod that simultaneously leverages multi-source heterogeneous external\nknowledge from a legal knowledge base, a conversational LLM, and related legal\narticles. Specifically, we match knowledge snippets in case descriptions via\nthe legal knowledge base and encapsulate them into the input through a hard\nprompt template. Additionally, we retrieve legal articles related to a given\ncase description through contrastive learning, and then obtain factual elements\nwithin the case description through a conversational LLM. We fuse the embedding\nvectors of soft prompt tokens with the encoding vector of factual elements to\nachieve knowledge-enhanced model forward inference. Experimental results show\nthat our method achieved state-of-the-art results on CAIL-2018, the largest\nlegal charge prediction dataset, and our method has lower data dependency. Case\nstudies also demonstrate our method's strong interpretability.\n","authors":["Jingyun Sun","Chi Wei","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2408.02233v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2408.02232v1","updated":"2024-08-05T04:53:01Z","published":"2024-08-05T04:53:01Z","title":"SpecRover: Code Intent Extraction via LLMs","summary":"  Autonomous program improvement typically involves automatically producing bug\nfixes and feature additions. Such program improvement can be accomplished by a\ncombination of large language model (LLM) and program analysis capabilities, in\nthe form of an LLM agent. Since program repair or program improvement typically\nrequires a specification of intended behavior - specification inference can be\nuseful for producing high quality program patches. In this work, we examine\nefficient and low-cost workflows for iterative specification inference within\nan LLM agent. Given a GitHub issue to be resolved in a software project, our\ngoal is to conduct iterative code search accompanied by specification inference\n- thereby inferring intent from both the project structure and behavior. The\nintent thus captured is examined by a reviewer agent with the goal of vetting\nthe patches as well as providing a measure of confidence in the vetted patches.\nOur approach SpecRover (AutoCodeRover-v2) is built on the open-source LLM agent\nAutoCodeRover. In an evaluation on the full SWE-Bench consisting of 2294 GitHub\nissues, it shows more than 50% improvement in efficacy over AutoCodeRover.\nCompared to the open-source agents available, our work shows modest cost ($0.65\nper issue) in resolving an average GitHub issue in SWE-Bench lite. The\nproduction of explanation by SpecRover allows for a better \"signal\" to be given\nto the developer, on when the suggested patches can be accepted with\nconfidence. SpecRover also seeks to demonstrate the continued importance of\nspecification inference in automated program repair, even as program repair\ntechnologies enter the LLM era.\n","authors":["Haifeng Ruan","Yuntong Zhang","Abhik Roychoudhury"],"pdf_url":"https://arxiv.org/pdf/2408.02232v1.pdf","comment":"Haifeng Ruan and Yuntong Zhang contributed equally to this work"},{"id":"http://arxiv.org/abs/2310.13028v2","updated":"2024-08-05T03:47:59Z","published":"2023-10-19T07:39:07Z","title":"Reliable Academic Conference Question Answering: A Study Based on Large\n  Language Model","summary":"  As the development of academic conferences fosters global scholarly\ncommunication, researchers consistently need to obtain accurate and up-to-date\ninformation about academic conferences. Since the information is scattered,\nusing an intelligent question-answering system to efficiently handle\nresearchers' queries and ensure awareness of the latest advancements is\nnecessary. Recently, Large Language Models (LLMs) have demonstrated impressive\ncapabilities in question answering, and have been enhanced by retrieving\nexternal knowledge to deal with outdated knowledge. However, these methods fail\nto work due to the lack of the latest conference knowledge. To address this\nchallenge, we develop the ConferenceQA dataset, consisting of seven diverse\nacademic conferences. Specifically, for each conference, we first organize\nacademic conference data in a tree-structured format through a semi-automated\nmethod. Then we annotate question-answer pairs and classify the pairs into four\ndifferent types to better distinguish their difficulty. With the constructed\ndataset, we further propose a novel method STAR (STructure-Aware Retrieval) to\nimprove the question-answering abilities of LLMs, leveraging inherent\nstructural information during the retrieval process. Experimental results on\nthe ConferenceQA dataset show the effectiveness of our retrieval method. The\ndataset and code are available at https://github.com/zjukg/ConferenceQA.\n","authors":["Zhiwei Huang","Juan Li","Long Jin","Junjie Wang","Mingchen Tu","Yin Hua","Zhiqiang Liu","Jiawei Meng","Wen Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.13028v2.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2401.05631v4","updated":"2024-08-05T03:46:34Z","published":"2024-01-11T03:02:17Z","title":"DrawTalking: Building Interactive Worlds by Sketching and Speaking","summary":"  We introduce DrawTalking, an approach to building and controlling interactive\nworlds by sketching and speaking while telling stories. It emphasizes user\ncontrol and flexibility, and gives programming-like capability without\nrequiring code. An early open-ended study with our prototype shows that the\nmechanics resonate and are applicable to many creative-exploratory use cases,\nwith the potential to inspire and inform research in future natural interfaces\nfor creative exploration and authoring.\n","authors":["Karl Toby Rosenberg","Rubaiat Habib Kazi","Li-Yi Wei","Haijun Xia","Ken Perlin"],"pdf_url":"https://arxiv.org/pdf/2401.05631v4.pdf","comment":"25 pages, 27 figures; Matching version accepted at UIST 2024"},{"id":"http://arxiv.org/abs/2408.02213v1","updated":"2024-08-05T03:26:01Z","published":"2024-08-05T03:26:01Z","title":"Is Large Language Model Good at Database Knob Tuning? A Comprehensive\n  Experimental Evaluation","summary":"  Knob tuning plays a crucial role in optimizing databases by adjusting knobs\nto enhance database performance. However, traditional tuning methods often\nfollow a Try-Collect-Adjust approach, proving inefficient and\ndatabase-specific. Moreover, these methods are often opaque, making it\nchallenging for DBAs to grasp the underlying decision-making process.\n  The emergence of large language models (LLMs) like GPT-4 and Claude-3 has\nexcelled in complex natural language tasks, yet their potential in database\nknob tuning remains largely unexplored. This study harnesses LLMs as\nexperienced DBAs for knob-tuning tasks with carefully designed prompts. We\nidentify three key subtasks in the tuning system: knob pruning, model\ninitialization, and knob recommendation, proposing LLM-driven solutions to\nreplace conventional methods for each subtask.\n  We conduct extensive experiments to compare LLM-driven approaches against\ntraditional methods across the subtasks to evaluate LLMs' efficacy in the knob\ntuning domain. Furthermore, we explore the adaptability of LLM-based solutions\nin diverse evaluation settings, encompassing new benchmarks, database engines,\nand hardware environments. Our findings reveal that LLMs not only match or\nsurpass traditional methods but also exhibit notable interpretability by\ngenerating responses in a coherent ``chain-of-thought'' manner. We further\nobserve that LLMs exhibit remarkable generalizability through simple\nadjustments in prompts, eliminating the necessity for additional training or\nextensive code modifications.\n  Drawing insights from our experimental findings, we identify several\nopportunities for future research aimed at advancing the utilization of LLMs in\nthe realm of database management.\n","authors":["Yiyan Li","Haoyang Li","Zhao Pu","Jing Zhang","Xinyi Zhang","Tao Ji","Luming Sun","Cuiping Li","Hong Chen"],"pdf_url":"https://arxiv.org/pdf/2408.02213v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02207v1","updated":"2024-08-05T03:15:21Z","published":"2024-08-05T03:15:21Z","title":"MARCO: A Memory-Augmented Reinforcement Framework for Combinatorial\n  Optimization","summary":"  Neural Combinatorial Optimization (NCO) is an emerging domain where deep\nlearning techniques are employed to address combinatorial optimization problems\nas a standalone solver. Despite their potential, existing NCO methods often\nsuffer from inefficient search space exploration, frequently leading to local\noptima entrapment or redundant exploration of previously visited states. This\npaper introduces a versatile framework, referred to as Memory-Augmented\nReinforcement for Combinatorial Optimization (MARCO), that can be used to\nenhance both constructive and improvement methods in NCO through an innovative\nmemory module. MARCO stores data collected throughout the optimization\ntrajectory and retrieves contextually relevant information at each state. This\nway, the search is guided by two competing criteria: making the best decision\nin terms of the quality of the solution and avoiding revisiting already\nexplored solutions. This approach promotes a more efficient use of the\navailable optimization budget. Moreover, thanks to the parallel nature of NCO\nmodels, several search threads can run simultaneously, all sharing the same\nmemory module, enabling an efficient collaborative exploration. Empirical\nevaluations, carried out on the maximum cut, maximum independent set and\ntravelling salesman problems, reveal that the memory module effectively\nincreases the exploration, enabling the model to discover diverse,\nhigher-quality solutions. MARCO achieves good performance in a low\ncomputational cost, establishing a promising new direction in the field of NCO.\n","authors":["Andoni I. Garmendia","Quentin Cappart","Josu Ceberio","Alexander Mendiburu"],"pdf_url":"https://arxiv.org/pdf/2408.02207v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02205v1","updated":"2024-08-05T03:08:51Z","published":"2024-08-05T03:08:51Z","title":"Towards AI-Safety-by-Design: A Taxonomy of Runtime Guardrails in\n  Foundation Model based Systems","summary":"  The rapid advancement and widespread deployment of foundation model (FM)\nbased systems have revolutionized numerous applications across various domains.\nHowever, the fast-growing capabilities and autonomy have also raised\nsignificant concerns about responsible AI and AI safety. Recently, there have\nbeen increasing attention toward implementing guardrails to ensure the runtime\nbehavior of FM-based systems is safe and responsible. Given the early stage of\nFMs and their applications (such as agents), the design of guardrails have not\nyet been systematically studied. It remains underexplored which software\nqualities should be considered when designing guardrails and how these\nqualities can be ensured from a software architecture perspective. Therefore,\nin this paper, we present a taxonomy for guardrails to classify and compare the\ncharacteristics and design options of guardrails. Our taxonomy is organized\ninto three main categories: the motivation behind adopting runtime guardrails,\nthe quality attributes to consider, and the design options available. This\ntaxonomy provides structured and concrete guidance for making architectural\ndesign decisions when designing guardrails and highlights trade-offs arising\nfrom the design decisions.\n","authors":["Md Shamsujjoha","Qinghua Lu","Dehai Zhao","Liming Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.02205v1.pdf","comment":"15 Pages"},{"id":"http://arxiv.org/abs/2405.19220v4","updated":"2024-08-05T03:02:06Z","published":"2024-05-29T16:00:46Z","title":"WRDScore: New Metric for Evaluation of Natural Language Generation\n  Models","summary":"  Evaluating natural language generation models, particularly for method name\nprediction, poses significant challenges. A robust metric must account for the\nversatility of method naming, considering both semantic and syntactic\nvariations. Traditional overlap-based metrics fail to capture these nuances.\nExisting embedding-based metrics often suffer from imbalanced precision and\nrecall, lack normalized scores, or make unrealistic assumptions about\nsequences. To address these limitations, we propose WRDScore, a novel metric\nthat strikes a balance between simplicity and effectiveness. Our metric is\nlightweight, normalized, and precision-recall-oriented, avoiding unrealistic\nassumptions while aligning well with human judgments.\n","authors":["Ravil Mussabayev"],"pdf_url":"https://arxiv.org/pdf/2405.19220v4.pdf","comment":"Accepted to IEEE Xplore"},{"id":"http://arxiv.org/abs/2403.18344v2","updated":"2024-08-05T02:47:09Z","published":"2024-03-27T08:34:55Z","title":"LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions\n  with Large Language Models","summary":"  To ensure safe driving in dynamic environments, autonomous vehicles should\npossess the capability to accurately predict lane change intentions of\nsurrounding vehicles in advance and forecast their future trajectories.\nExisting motion prediction approaches have ample room for improvement,\nparticularly in terms of long-term prediction accuracy and interpretability. In\nthis paper, we address these challenges by proposing LC-LLM, an explainable\nlane change prediction model that leverages the strong reasoning capabilities\nand self-explanation abilities of Large Language Models (LLMs). Essentially, we\nreformulate the lane change prediction task as a language modeling problem,\nprocessing heterogeneous driving scenario information as natural language\nprompts for LLMs and employing supervised fine-tuning to tailor LLMs\nspecifically for lane change prediction task. Additionally, we finetune the\nChain-of-Thought (CoT) reasoning to improve prediction transparency and\nreliability, and include explanatory requirements in the prompts during\ninference stage. Therefore, our LC-LLM model not only predicts lane change\nintentions and trajectories but also provides CoT reasoning and explanations\nfor its predictions, enhancing its interpretability. Extensive experiments\nbased on the large-scale highD dataset demonstrate the superior performance and\ninterpretability of our LC-LLM in lane change prediction task. To the best of\nour knowledge, this is the first attempt to utilize LLMs for predicting lane\nchange behavior. Our study shows that LLMs can effectively encode comprehensive\ninteraction information for driving behavior understanding.\n","authors":["Mingxing Peng","Xusen Guo","Xianda Chen","Meixin Zhu","Kehua Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18344v2.pdf","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.00380v2","updated":"2024-08-05T02:45:50Z","published":"2024-08-01T08:41:13Z","title":"Enhancing Whole Slide Pathology Foundation Models through Stain\n  Normalization","summary":"  Recent advancements in digital pathology have led to the development of\nnumerous foundational models that utilize self-supervised learning on patches\nextracted from gigapixel whole slide images (WSIs). While this approach\nleverages vast amounts of unlabeled data, we have discovered a significant\nissue: features extracted from these self-supervised models tend to cluster by\nindividual WSIs, a phenomenon we term WSI-specific feature collapse. This\nproblem can potentially limit the model's generalization ability and\nperformance on various downstream tasks. To address this issue, we introduce\nStain Normalized Pathology Foundational Model, a novel foundational model\ntrained on patches that have undergone stain normalization. Stain normalization\nhelps reduce color variability arising from different laboratories and\nscanners, enabling the model to learn more consistent features. Stain\nNormalized Pathology Foundational Model is trained using 285,153,903 patches\nextracted from a total of 34,795 WSIs, combining data from The Cancer Genome\nAtlas (TCGA) and the Genotype-Tissue Expression (GTEx) project. Our experiments\ndemonstrate that Stain Normalized Pathology Foundational Model significantly\nmitigates the feature collapse problem, indicating that the model has learned\nmore generalized features rather than overfitting to individual WSI\ncharacteristics. We compared Stain Normalized Pathology Foundational Model with\nstate-of-the-art models across six downstream task datasets, and our results\nshow that Stain Normalized Pathology Foundational Model achieves excellent\nperformance relative to the number of WSIs used and the model's parameter\ncount. This suggests that the application of stain normalization has\nsubstantially improved the model's efficiency and generalization capabilities.\n","authors":["Juseung Yun","Yi Hu","Jinhyung Kim","Jongseong Jang","Soonyoung Lee"],"pdf_url":"https://arxiv.org/pdf/2408.00380v2.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2404.07950v2","updated":"2024-08-05T02:45:42Z","published":"2024-03-18T16:50:23Z","title":"Reinforcement Learning with Generalizable Gaussian Splatting","summary":"  An excellent representation is crucial for reinforcement learning (RL)\nperformance, especially in vision-based reinforcement learning tasks. The\nquality of the environment representation directly influences the achievement\nof the learning task. Previous vision-based RL typically uses explicit or\nimplicit ways to represent environments, such as images, points, voxels, and\nneural radiance fields. However, these representations contain several\ndrawbacks. They cannot either describe complex local geometries or generalize\nwell to unseen scenes, or require precise foreground masks. Moreover, these\nimplicit neural representations are akin to a ``black box\", significantly\nhindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit\nscene representation and differentiable rendering nature, is considered a\nrevolutionary change for reconstruction and representation methods. In this\npaper, we propose a novel Generalizable Gaussian Splatting framework to be the\nrepresentation of RL tasks, called GSRL. Through validation in the RoboMimic\nenvironment, our method achieves better results than other baselines in\nmultiple tasks, improving the performance by 10%, 44%, and 15% compared with\nbaselines on the hardest task. This work is the first attempt to leverage\ngeneralizable 3DGS as a representation for RL.\n","authors":["Jiaxu Wang","Qiang Zhang","Jingkai Sun","Jiahang Cao","Gang Han","Wen Zhao","Weining Zhang","Yecheng Shao","Yijie Guo","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2404.07950v2.pdf","comment":"7 pages,2 figures"},{"id":"http://arxiv.org/abs/2402.02977v4","updated":"2024-08-05T01:24:52Z","published":"2024-02-05T12:58:29Z","title":"Variational Flow Models: Flowing in Your Style","summary":"  We propose a systematic training-free method to transform the probability\nflow of a \"linear\" stochastic process characterized by the equation\nX_{t}=a_{t}X_{0}+\\sigma_{t}X_{1} into a straight constant-speed (SC) flow,\nreminiscent of Rectified Flow. This transformation facilitates fast sampling\nalong the original probability flow via the Euler method without training a new\nmodel of the SC flow. The flexibility of our approach allows us to extend our\ntransformation to inter-convert two posterior flows of two distinct linear\nstochastic processes. Moreover, we can easily integrate high-order numerical\nsolvers into the transformed SC flow, further enhancing the sampling accuracy\nand efficiency. Rigorous theoretical analysis and extensive experimental\nresults substantiate the advantages of our framework. Our code is available at\nthis [https://github.com/clarken92/VFM||link].\n","authors":["Kien Do","Duc Kieu","Toan Nguyen","Dang Nguyen","Hung Le","Dung Nguyen","Thin Nguyen"],"pdf_url":"https://arxiv.org/pdf/2402.02977v4.pdf","comment":"Our code is available at: https://github.com/clarken92/VFM"},{"id":"http://arxiv.org/abs/2305.05821v2","updated":"2024-08-05T00:51:59Z","published":"2023-05-10T00:33:08Z","title":"Context-dependent communication under environmental constraints","summary":"  There is significant evidence that real-world communication cannot be reduced\nto sending signals with context-independent meaning. In this work, based on a\nvariant of the classical Lewis (1969) signaling model, we explore the\nconditions for the emergence of context-dependent communication in a situated\nscenario. In particular, we demonstrate that pressure to minimise the\nvocabulary size is sufficient for such emergence. At the same time, we study\nthe environmental conditions and cognitive capabilities that enable contextual\ndisambiguation of symbol meanings. We show that environmental constraints on\nthe receiver's referent choice can be unilaterally exploited by the sender,\nwithout disambiguation capabilities on the receiver's end. Consistent with\ncommon assumptions, the sender's awareness of the context appears to be\nrequired for contextual communication. We suggest that context-dependent\ncommunication is a situated multilayered phenomenon, crucially influenced by\nenvironment properties such as distribution of contexts. The model developed in\nthis work is a demonstration of how signals may be ambiguous out of context,\nbut still allow for near-perfect communication accuracy.\n","authors":["Krzysztof Główka","Julian Zubek","Joanna Rączaszek-Leonardi"],"pdf_url":"https://arxiv.org/pdf/2305.05821v2.pdf","comment":"14 pages, submitted to Cognitive Systems Research"},{"id":"http://arxiv.org/abs/2408.02865v1","updated":"2024-08-05T23:31:07Z","published":"2024-08-05T23:31:07Z","title":"VisionUnite: A Vision-Language Foundation Model for Ophthalmology\n  Enhanced with Clinical Knowledge","summary":"  The need for improved diagnostic methods in ophthalmology is acute,\nespecially in the less developed regions with limited access to specialists and\nadvanced equipment. Therefore, we introduce VisionUnite, a novel\nvision-language foundation model for ophthalmology enhanced with clinical\nknowledge. VisionUnite has been pretrained on an extensive dataset comprising\n1.24 million image-text pairs, and further refined using our proposed MMFundus\ndataset, which includes 296,379 high-quality fundus image-text pairs and\n889,137 simulated doctor-patient dialogue instances. Our experiments indicate\nthat VisionUnite outperforms existing generative foundation models such as\nGPT-4V and Gemini Pro. It also demonstrates diagnostic capabilities comparable\nto junior ophthalmologists. VisionUnite performs well in various clinical\nscenarios including open-ended multi-disease diagnosis, clinical explanation,\nand patient interaction, making it a highly versatile tool for initial\nophthalmic disease screening. VisionUnite can also serve as an educational aid\nfor junior ophthalmologists, accelerating their acquisition of knowledge\nregarding both common and rare ophthalmic conditions. VisionUnite represents a\nsignificant advancement in ophthalmology, with broad implications for\ndiagnostics, medical education, and understanding of disease mechanisms.\n","authors":["Zihan Li","Diping Song","Zefeng Yang","Deming Wang","Fei Li","Xiulan Zhang","Paul E. Kinahan","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2408.02865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.09797v2","updated":"2024-08-05T23:23:14Z","published":"2023-07-19T07:31:37Z","title":"Probabilistic Forecasting with Coherent Aggregation","summary":"  Obtaining accurate probabilistic forecasts is an important operational\nchallenge in many applications, perhaps most obviously in energy management,\nclimate forecasting, supply chain planning, and resource allocation. In many of\nthese applications, there is a natural hierarchical structure over the\nforecasted quantities; and forecasting systems that adhere to this hierarchical\nstructure are said to be coherent. Furthermore, operational planning benefits\nfrom accuracy at all levels of the aggregation hierarchy. Building accurate and\ncoherent forecasting systems, however, is challenging: classic multivariate\ntime series tools and neural network methods are still being adapted for this\npurpose. In this paper, we augment an MQForecaster neural network architecture\nwith a novel deep Gaussian factor forecasting model that achieves coherence by\nconstruction, yielding a method we call the Deep Coherent Factor Model Neural\nNetwork (DeepCoFactor) model. DeepCoFactor generates samples that can be\ndifferentiated with respect to model parameters, allowing optimization on\nvarious sample-based learning objectives that align with the forecasting\nsystem's goals, including quantile loss and the scaled Continuous Ranked\nProbability Score (CRPS). In a comparison to state-of-the-art coherent\nforecasting methods, DeepCoFactor achieves significant improvements in scaled\nCRPS forecast accuracy, with gains between 4.16 and 54.40%, as measured on\nthree publicly available hierarchical forecasting datasets.\n","authors":["Kin G. Olivares","Geoffrey Négiar","Ruijun Ma","O. Nangba Meetei","Mengfei Cao","Michael W. Mahoney"],"pdf_url":"https://arxiv.org/pdf/2307.09797v2.pdf","comment":"10 pages of main text. Updated method and results"},{"id":"http://arxiv.org/abs/2408.02862v1","updated":"2024-08-05T23:20:47Z","published":"2024-08-05T23:20:47Z","title":"On The Stability of Moral Preferences: A Problem with Computational\n  Elicitation Methods","summary":"  Preference elicitation frameworks feature heavily in the research on\nparticipatory ethical AI tools and provide a viable mechanism to enquire and\nincorporate the moral values of various stakeholders. As part of the\nelicitation process, surveys about moral preferences, opinions, and judgments\nare typically administered only once to each participant. This methodological\npractice is reasonable if participants' responses are stable over time such\nthat, all other relevant factors being held constant, their responses today\nwill be the same as their responses to the same questions at a later time.\nHowever, we do not know how often that is the case. It is possible that\nparticipants' true moral preferences change, are subject to temporary moods or\nwhims, or are influenced by environmental factors we don't track. If\nparticipants' moral responses are unstable in such ways, it would raise\nimportant methodological and theoretical issues for how participants' true\nmoral preferences, opinions, and judgments can be ascertained. We address this\npossibility here by asking the same survey participants the same moral\nquestions about which patient should receive a kidney when only one is\navailable ten times in ten different sessions over two weeks, varying only\npresentation order across sessions. We measured how often participants gave\ndifferent responses to simple (Study One) and more complicated (Study Two)\nrepeated scenarios. On average, the fraction of times participants changed\ntheir responses to controversial scenarios was around 10-18% across studies,\nand this instability is observed to have positive associations with response\ntime and decision-making difficulty. We discuss the implications of these\nresults for the efficacy of moral preference elicitation, highlighting the role\nof response instability in causing value misalignment between stakeholders and\nAI tools trained on their moral judgments.\n","authors":["Kyle Boerstler","Vijay Keswani","Lok Chan","Jana Schaich Borg","Vincent Conitzer","Hoda Heidari","Walter Sinnott-Armstrong"],"pdf_url":"https://arxiv.org/pdf/2408.02862v1.pdf","comment":"To appear in AIES 2024"},{"id":"http://arxiv.org/abs/2408.02859v1","updated":"2024-08-05T22:59:50Z","published":"2024-08-05T22:59:50Z","title":"Multistain Pretraining for Slide Representation Learning in Pathology","summary":"  Developing self-supervised learning (SSL) models that can learn universal and\ntransferable representations of H&E gigapixel whole-slide images (WSIs) is\nbecoming increasingly valuable in computational pathology. These models hold\nthe potential to advance critical tasks such as few-shot classification, slide\nretrieval, and patient stratification. Existing approaches for slide\nrepresentation learning extend the principles of SSL from small images (e.g.,\n224 x 224 patches) to entire slides, usually by aligning two different\naugmentations (or views) of the slide. Yet the resulting representation remains\nconstrained by the limited clinical and biological diversity of the views.\nInstead, we postulate that slides stained with multiple markers, such as\nimmunohistochemistry, can be used as different views to form a rich\ntask-agnostic training signal. To this end, we introduce Madeleine, a\nmultimodal pretraining strategy for slide representation learning. Madeleine is\ntrained with a dual global-local cross-stain alignment objective on large\ncohorts of breast cancer samples (N=4,211 WSIs across five stains) and kidney\ntransplant samples (N=12,070 WSIs across four stains). We demonstrate the\nquality of slide representations learned by Madeleine on various downstream\nevaluations, ranging from morphological and molecular classification to\nprognostic prediction, comprising 21 tasks using 7,299 WSIs from multiple\nmedical centers. Code is available at https://github.com/mahmoodlab/MADELEINE.\n","authors":["Guillaume Jaume","Anurag Vaidya","Andrew Zhang","Andrew H. Song","Richard J. Chen","Sharifa Sahai","Dandan Mo","Emilio Madrigal","Long Phi Le","Faisal Mahmood"],"pdf_url":"https://arxiv.org/pdf/2408.02859v1.pdf","comment":"ECCV'24"},{"id":"http://arxiv.org/abs/2402.15821v2","updated":"2024-08-05T22:54:36Z","published":"2024-02-24T14:17:41Z","title":"Cooperation and Control in Delegation Games","summary":"  Many settings of interest involving humans and machines -- from virtual\npersonal assistants to autonomous vehicles -- can naturally be modelled as\nprincipals (humans) delegating to agents (machines), which then interact with\neach other on their principals' behalf. We refer to these multi-principal,\nmulti-agent scenarios as delegation games. In such games, there are two\nimportant failure modes: problems of control (where an agent fails to act in\nline their principal's preferences) and problems of cooperation (where the\nagents fail to work well together). In this paper we formalise and analyse\nthese problems, further breaking them down into issues of alignment (do the\nplayers have similar preferences?) and capabilities (how competent are the\nplayers at satisfying those preferences?). We show -- theoretically and\nempirically -- how these measures determine the principals' welfare, how they\ncan be estimated using limited observations, and thus how they might be used to\nhelp us design more aligned and cooperative AI systems.\n","authors":["Oliver Sourbut","Lewis Hammond","Harriet Wood"],"pdf_url":"https://arxiv.org/pdf/2402.15821v2.pdf","comment":"Published at IJCAI 2024"},{"id":"http://arxiv.org/abs/2406.12038v2","updated":"2024-08-05T21:48:22Z","published":"2024-06-17T19:11:40Z","title":"Soft Prompting for Unlearning in Large Language Models","summary":"  The widespread popularity of Large Language Models (LLMs), partly due to\ntheir unique ability to perform in-context learning, has also brought to light\nthe importance of ethical and safety considerations when deploying these\npre-trained models. In this work, we focus on investigating machine unlearning\nfor LLMs motivated by data protection regulations. In contrast to the growing\nliterature on fine-tuning methods to achieve unlearning, we focus on a\ncomparatively lightweight alternative called soft prompting to realize the\nunlearning of a subset of training data. With losses designed to enforce\nforgetting as well as utility preservation, our framework \\textbf{S}oft\n\\textbf{P}rompting for \\textbf{U}n\\textbf{l}earning (SPUL) learns prompt tokens\nthat can be appended to an arbitrary query to induce unlearning of specific\nexamples at inference time without updating LLM parameters. We conduct a\nrigorous evaluation of the proposed method and our results indicate that SPUL\ncan significantly improve the trade-off between utility and forgetting in the\ncontext of text classification and question answering with LLMs. We further\nvalidate our method using multiple LLMs to highlight the scalability of our\nframework and provide detailed insights into the choice of hyperparameters and\nthe influence of the size of unlearning data. Our implementation is available\nat \\url{https://github.com/karuna-bhaila/llm_unlearning}.\n","authors":["Karuna Bhaila","Minh-Hao Van","Xintao Wu"],"pdf_url":"https://arxiv.org/pdf/2406.12038v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02835v1","updated":"2024-08-05T21:12:12Z","published":"2024-08-05T21:12:12Z","title":"Training a multilayer dynamical spintronic network with standard machine\n  learning tools to perform time series classification","summary":"  The ability to process time-series at low energy cost is critical for many\napplications. Recurrent neural network, which can perform such tasks, are\ncomputationally expensive when implementing in software on conventional\ncomputers. Here we propose to implement a recurrent neural network in hardware\nusing spintronic oscillators as dynamical neurons. Using numerical simulations,\nwe build a multi-layer network and demonstrate that we can use backpropagation\nthrough time (BPTT) and standard machine learning tools to train this network.\nLeveraging the transient dynamics of the spintronic oscillators, we solve the\nsequential digits classification task with $89.83\\pm2.91~\\%$ accuracy, as good\nas the equivalent software network. We devise guidelines on how to choose the\ntime constant of the oscillators as well as hyper-parameters of the network to\nadapt to different input time scales.\n","authors":["Erwan Plouet","Dédalo Sanz-Hernández","Aymeric Vecchiola","Julie Grollier","Frank Mizrahi"],"pdf_url":"https://arxiv.org/pdf/2408.02835v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.19082v2","updated":"2024-08-05T21:09:50Z","published":"2024-07-26T21:02:11Z","title":"Regularized Multi-Decoder Ensemble for an Error-Aware Scene\n  Representation Network","summary":"  Feature grid Scene Representation Networks (SRNs) have been applied to\nscientific data as compact functional surrogates for analysis and\nvisualization. As SRNs are black-box lossy data representations, assessing the\nprediction quality is critical for scientific visualization applications to\nensure that scientists can trust the information being visualized. Currently,\nexisting architectures do not support inference time reconstruction quality\nassessment, as coordinate-level errors cannot be evaluated in the absence of\nground truth data. We propose a parameter-efficient multi-decoder SRN (MDSRN)\nensemble architecture consisting of a shared feature grid with multiple\nlightweight multi-layer perceptron decoders. MDSRN can generate a set of\nplausible predictions for a given input coordinate to compute the mean as the\nprediction of the multi-decoder ensemble and the variance as a confidence\nscore. The coordinate-level variance can be rendered along with the data to\ninform the reconstruction quality, or be integrated into uncertainty-aware\nvolume visualization algorithms. To prevent the misalignment between the\nquantified variance and the prediction quality, we propose a novel variance\nregularization loss for ensemble learning that promotes the Regularized\nmulti-decoder SRN (RMDSRN) to obtain a more reliable variance that correlates\nclosely to the true model error. We comprehensively evaluate the quality of\nvariance quantification and data reconstruction of Monte Carlo Dropout, Mean\nField Variational Inference, Deep Ensemble, and Predicting Variance compared to\nthe proposed MDSRN and RMDSRN across diverse scalar field datasets. We\ndemonstrate that RMDSRN attains the most accurate data reconstruction and\ncompetitive variance-error correlation among uncertain SRNs under the same\nneural network parameter budgets.\n","authors":["Tianyu Xiong","Skylar W. Wurster","Hanqi Guo","Tom Peterka","Han-Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2407.19082v2.pdf","comment":"To be published in Proc. IEEE VIS 2024"},{"id":"http://arxiv.org/abs/2404.03745v2","updated":"2024-08-05T21:05:08Z","published":"2024-04-04T18:34:32Z","title":"Fakes of Varying Shades: How Warning Affects Human Perception and\n  Engagement Regarding LLM Hallucinations","summary":"  The widespread adoption and transformative effects of large language models\n(LLMs) have sparked concerns regarding their capacity to produce inaccurate and\nfictitious content, referred to as `hallucinations'. Given the potential risks\nassociated with hallucinations, humans should be able to identify them. This\nresearch aims to understand the human perception of LLM hallucinations by\nsystematically varying the degree of hallucination (genuine, minor\nhallucination, major hallucination) and examining its interaction with warning\n(i.e., a warning of potential inaccuracies: absent vs. present). Participants\n(N=419) from Prolific rated the perceived accuracy and engaged with content\n(e.g., like, dislike, share) in a Q/A format. Participants ranked content as\ntruthful in the order of genuine, minor hallucination, and major hallucination,\nand user engagement behaviors mirrored this pattern. More importantly, we\nobserved that warning improved the detection of hallucination without\nsignificantly affecting the perceived truthfulness of genuine content. We\nconclude by offering insights for future tools to aid human detection of\nhallucinations. All survey materials, demographic questions, and post-session\nquestions are available at:\nhttps://github.com/MahjabinNahar/fakes-of-varying-shades-survey-materials\n","authors":["Mahjabin Nahar","Haeseung Seo","Eun-Ju Lee","Aiping Xiong","Dongwon Lee"],"pdf_url":"https://arxiv.org/pdf/2404.03745v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.04428v2","updated":"2024-08-05T20:24:09Z","published":"2022-11-08T18:14:09Z","title":"Review of coreference resolution in English and Persian","summary":"  Coreference resolution (CR), identifying expressions referring to the same\nreal-world entity, is a fundamental challenge in natural language processing\n(NLP). This paper explores the latest advancements in CR, spanning coreference\nand anaphora resolution. We critically analyze the diverse corpora that have\nfueled CR research, highlighting their strengths, limitations, and suitability\nfor various tasks. We examine the spectrum of evaluation metrics used to assess\nCR systems, emphasizing their advantages, disadvantages, and the need for more\nnuanced, task-specific metrics. Tracing the evolution of CR algorithms, we\nprovide a detailed overview of methodologies, from rule-based approaches to\ncutting-edge deep learning architectures. We delve into mention-pair,\nentity-based, cluster-ranking, sequence-to-sequence, and graph neural network\nmodels, elucidating their theoretical foundations and performance on benchmark\ndatasets. Recognizing the unique challenges of Persian CR, we dedicate a\nfocused analysis to this under-resourced language. We examine existing Persian\nCR systems and highlight the emergence of end-to-end neural models leveraging\npre-trained language models like ParsBERT. This review is an essential resource\nfor researchers and practitioners, offering a comprehensive overview of the\ncurrent state-of-the-art in CR, identifying key challenges, and charting a\ncourse for future research in this rapidly evolving field.\n","authors":["Hassan Haji Mohammadi","Alireza Talebpour","Ahmad Mahmoudi Aznaveh","Samaneh Yazdani"],"pdf_url":"https://arxiv.org/pdf/2211.04428v2.pdf","comment":"44 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2408.02811v1","updated":"2024-08-05T20:21:54Z","published":"2024-08-05T20:21:54Z","title":"Development of REGAI: Rubric Enabled Generative Artificial Intelligence","summary":"  This paper presents and evaluates a new retrieval augmented generation (RAG)\nand large language model (LLM)-based artificial intelligence (AI) technique:\nrubric enabled generative artificial intelligence (REGAI). REGAI uses rubrics,\nwhich can be created manually or automatically by the system, to enhance the\nperformance of LLMs for evaluation purposes. REGAI improves on the performance\nof both classical LLMs and RAG-based LLM techniques. This paper describes\nREGAI, presents data regarding its performance and discusses several possible\napplication areas for the technology.\n","authors":["Zach Johnson","Jeremy Straub"],"pdf_url":"https://arxiv.org/pdf/2408.02811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02798v1","updated":"2024-08-05T19:28:58Z","published":"2024-08-05T19:28:58Z","title":"Examining Gender and Power on Wikipedia Through Face and Politeness","summary":"  We propose a framework for analyzing discourse by combining two\ninterdependent concepts from sociolinguistic theory: face acts and politeness.\nWhile politeness has robust existing tools and data, face acts are less\nresourced. We introduce a new corpus created by annotating Wikipedia talk pages\nwith face acts and we use this to train a face act tagger. We then employ our\nframework to study how face and politeness interact with gender and power in\ndiscussions between Wikipedia editors. Among other findings, we observe that\nfemale Wikipedians are not only more polite, which is consistent with prior\nstudies, but that this difference corresponds with significantly more language\ndirected at humbling aspects of their own face. Interestingly, the distinction\nnearly vanishes once limiting to editors with administrative power.\n","authors":["Adil Soubki","Shyne Choi","Owen Rambow"],"pdf_url":"https://arxiv.org/pdf/2408.02798v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2405.04634v3","updated":"2024-08-05T17:53:28Z","published":"2024-05-07T19:37:22Z","title":"FRACTAL: An Ultra-Large-Scale Aerial Lidar Dataset for 3D Semantic\n  Segmentation of Diverse Landscapes","summary":"  Mapping agencies are increasingly adopting Aerial Lidar Scanning (ALS) as a\nnew tool to monitor territory and support public policies. Processing ALS data\nat scale requires efficient point classification methods that perform well over\nhighly diverse territories. To evaluate them, researchers need large annotated\nLidar datasets, however, current Lidar benchmark datasets have restricted scope\nand often cover a single urban area. To bridge this data gap, we present the\nFRench ALS Clouds from TArgeted Landscapes (FRACTAL) dataset: an\nultra-large-scale aerial Lidar dataset made of 100,000 dense point clouds with\nhigh-quality labels for 7 semantic classes and spanning 250 km$^2$. FRACTAL is\nbuilt upon France's nationwide open Lidar data. It achieves spatial and\nsemantic diversity via a sampling scheme that explicitly concentrates rare\nclasses and challenging landscapes from five French regions. It should support\nthe development of 3D deep learning approaches for large-scale land monitoring.\nWe describe the nature of the source data, the sampling workflow, the content\nof the resulting dataset, and provide an initial evaluation of segmentation\nperformance using a performant 3D neural architecture.\n","authors":["Charles Gaydon","Michel Daab","Floryne Roche"],"pdf_url":"https://arxiv.org/pdf/2405.04634v3.pdf","comment":"15 pages | 9 figures | 8 tables | Dataset is available at\n  https://huggingface.co/datasets/IGNF/FRACTAL | Trained model is available at\n  https://huggingface.co/IGNF/FRACTAL-LidarHD_7cl_randlanet | Deep learning\n  code repository is on Gihtub at https://github.com/IGNF/myria3d | Data\n  engineering code repository is on Github at https://github.com/IGNF/pacasam"},{"id":"http://arxiv.org/abs/2407.11913v2","updated":"2024-08-05T17:50:03Z","published":"2024-07-16T17:05:20Z","title":"Quantised Global Autoencoder: A Holistic Approach to Representing Visual\n  Data","summary":"  In quantised autoencoders, images are usually split into local patches, each\nencoded by one token. This representation is redundant in the sense that the\nsame number of tokens is spend per region, regardless of the visual information\ncontent in that region. Adaptive discretisation schemes like quadtrees are\napplied to allocate tokens for patches with varying sizes, but this just varies\nthe region of influence for a token which nevertheless remains a local\ndescriptor. Modern architectures add an attention mechanism to the autoencoder\nwhich infuses some degree of global information into the local tokens. Despite\nthe global context, tokens are still associated with a local image region. In\ncontrast, our method is inspired by spectral decompositions which transform an\ninput signal into a superposition of global frequencies. Taking the data-driven\nperspective, we learn custom basis functions corresponding to the codebook\nentries in our VQ-VAE setup. Furthermore, a decoder combines these basis\nfunctions in a non-linear fashion, going beyond the simple linear superposition\nof spectral decompositions. We can achieve this global description with an\nefficient transpose operation between features and channels and demonstrate our\nperformance on compression.\n","authors":["Tim Elsner","Paula Usinger","Victor Czech","Gregor Kobsik","Yanjiang He","Isaak Lim","Leif Kobbelt"],"pdf_url":"https://arxiv.org/pdf/2407.11913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02654v1","updated":"2024-08-05T17:33:09Z","published":"2024-08-05T17:33:09Z","title":"On Using Quasirandom Sequences in Machine Learning for Model Weight\n  Initialization","summary":"  The effectiveness of training neural networks directly impacts computational\ncosts, resource allocation, and model development timelines in machine learning\napplications. An optimizer's ability to train the model adequately (in terms of\ntrained model performance) depends on the model's initial weights. Model weight\ninitialization schemes use pseudorandom number generators (PRNGs) as a source\nof randomness.\n  We investigate whether substituting PRNGs for low-discrepancy quasirandom\nnumber generators (QRNGs) -- namely Sobol' sequences -- as a source of\nrandomness for initializers can improve model performance. We examine\nMulti-Layer Perceptrons (MLP), Convolutional Neural Networks (CNN), Long\nShort-Term Memory (LSTM), and Transformer architectures trained on MNIST,\nCIFAR-10, and IMDB datasets using SGD and Adam optimizers. Our analysis uses\nten initialization schemes: Glorot, He, Lecun (both Uniform and Normal);\nOrthogonal, Random Normal, Truncated Normal, and Random Uniform. Models with\nweights set using PRNG- and QRNG-based initializers are compared pairwise for\neach combination of dataset, architecture, optimizer, and initialization\nscheme.\n  Our findings indicate that QRNG-based neural network initializers either\nreach a higher accuracy or achieve the same accuracy more quickly than\nPRNG-based initializers in 60% of the 120 experiments conducted. Thus, using\nQRNG-based initializers instead of PRNG-based initializers can speed up and\nimprove model training.\n","authors":["Andriy Miranskyy","Adam Sorrenti","Viral Thakar"],"pdf_url":"https://arxiv.org/pdf/2408.02654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02641v1","updated":"2024-08-05T17:14:35Z","published":"2024-08-05T17:14:35Z","title":"Detection of Compromised Functions in a Serverless Cloud Environment","summary":"  Serverless computing is an emerging cloud paradigm with serverless functions\nat its core. While serverless environments enable software developers to focus\non developing applications without the need to actively manage the underlying\nruntime infrastructure, they open the door to a wide variety of security\nthreats that can be challenging to mitigate with existing methods. Existing\nsecurity solutions do not apply to all serverless architectures, since they\nrequire significant modifications to the serverless infrastructure or rely on\nthird-party services for the collection of more detailed data. In this paper,\nwe present an extendable serverless security threat detection model that\nleverages cloud providers' native monitoring tools to detect anomalous behavior\nin serverless applications. Our model aims to detect compromised serverless\nfunctions by identifying post-exploitation abnormal behavior related to\ndifferent types of attacks on serverless functions, and therefore, it is a last\nline of defense. Our approach is not tied to any specific serverless\napplication, is agnostic to the type of threats, and is adaptable through model\nadjustments. To evaluate our model's performance, we developed a serverless\ncybersecurity testbed in an AWS cloud environment, which includes two different\nserverless applications and simulates a variety of attack scenarios that cover\nthe main security threats faced by serverless functions. Our evaluation\ndemonstrates our model's ability to detect all implemented attacks while\nmaintaining a negligible false alarm rate.\n","authors":["Danielle Lavi","Oleg Brodt","Dudu Mimran","Yuval Elovici","Asaf Shabtai"],"pdf_url":"https://arxiv.org/pdf/2408.02641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02637v1","updated":"2024-08-05T17:01:33Z","published":"2024-08-05T17:01:33Z","title":"Command-line Obfuscation Detection using Small Language Models","summary":"  To avoid detection, adversaries often use command-line obfuscation. There are\nnumerous techniques of the command-line obfuscation, all designed to alter the\ncommand-line syntax without affecting its original functionality. This\nvariability forces most security solutions to create an exhaustive enumeration\nof signatures for even a single pattern. In contrast to using signatures, we\nhave implemented a scalable NLP-based detection method that leverages a\ncustom-trained, small transformer language model that can be applied to any\nsource of execution logs. The evaluation on top of real-world telemetry\ndemonstrates that our approach yields high-precision detections even on\nhigh-volume telemetry from a diverse set of environments spanning from\nuniversities and businesses to healthcare or finance. The practical value is\ndemonstrated in a case study of real-world samples detected by our model. We\nshow the model's superiority to signatures on established malware known to\nemploy obfuscation and showcase previously unseen obfuscated samples detected\nby our model.\n","authors":["Vojtech Outrata","Michael Adam Polak","Martin Kopp"],"pdf_url":"https://arxiv.org/pdf/2408.02637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02396v3","updated":"2024-08-05T16:49:51Z","published":"2023-12-04T23:26:12Z","title":"Unsupervised Change Detection for Space Habitats Using 3D Point Clouds","summary":"  This work presents an algorithm for scene change detection from point clouds\nto enable autonomous robotic caretaking in future space habitats. Autonomous\nrobotic systems will help maintain future deep-space habitats, such as the\nGateway space station, which will be uncrewed for extended periods. Existing\nscene analysis software used on the International Space Station (ISS) relies on\nmanually-labeled images for detecting changes. In contrast, the algorithm\npresented in this work uses raw, unlabeled point clouds as inputs. The\nalgorithm first applies modified Expectation-Maximization Gaussian Mixture\nModel (GMM) clustering to two input point clouds. It then performs change\ndetection by comparing the GMMs using the Earth Mover's Distance. The algorithm\nis validated quantitatively and qualitatively using a test dataset collected by\nan Astrobee robot in the NASA Ames Granite Lab comprising single frame depth\nimages taken directly by Astrobee and full-scene reconstructed maps built with\nRGB-D and pose data from Astrobee. The runtimes of the approach are also\nanalyzed in depth. The source code is publicly released to promote further\ndevelopment.\n","authors":["Jamie Santos","Holly Dinkel","Julia Di","Paulo V. K. Borges","Marina Moreira","Oleg Alexandrov","Brian Coltin","Trey Smith"],"pdf_url":"https://arxiv.org/pdf/2312.02396v3.pdf","comment":"15 pages, 7 figures, Manuscript was presented at the AIAA SciTech\n  Forum in Orlando, FL, USA, 8 - 12 January 2024. Video presentation:\n  [https://www.youtube.com/watch?v=7WHp0dQYG4Y]. Code:\n  [https://github.com/nasa/isaac/tree/master/anomaly/gmm-change-detection]"},{"id":"http://arxiv.org/abs/2303.07338v2","updated":"2024-08-05T16:34:43Z","published":"2023-03-13T17:59:02Z","title":"Revisiting Class-Incremental Learning with Pre-Trained Models:\n  Generalizability and Adaptivity are All You Need","summary":"  Class-incremental learning (CIL) aims to adapt to emerging new classes\nwithout forgetting old ones. Traditional CIL models are trained from scratch to\ncontinually acquire knowledge as data evolves. Recently, pre-training has\nachieved substantial progress, making vast pre-trained models (PTMs) accessible\nfor CIL. Contrary to traditional methods, PTMs possess generalizable\nembeddings, which can be easily transferred for CIL. In this work, we revisit\nCIL with PTMs and argue that the core factors in CIL are adaptivity for model\nupdating and generalizability for knowledge transferring. 1) We first reveal\nthat frozen PTM can already provide generalizable embeddings for CIL.\nSurprisingly, a simple baseline (SimpleCIL) which continually sets the\nclassifiers of PTM to prototype features can beat state-of-the-art even without\ntraining on the downstream task. 2) Due to the distribution gap between\npre-trained and downstream datasets, PTM can be further cultivated with\nadaptivity via model adaptation. We propose AdaPt and mERge (APER), which\naggregates the embeddings of PTM and adapted models for classifier\nconstruction. APER is a general framework that can be orthogonally combined\nwith any parameter-efficient tuning method, which holds the advantages of PTM's\ngeneralizability and adapted model's adaptivity. 3) Additionally, considering\nprevious ImageNet-based benchmarks are unsuitable in the era of PTM due to data\noverlapping, we propose four new benchmarks for assessment, namely ImageNet-A,\nObjectNet, OmniBenchmark, and VTAB. Extensive experiments validate the\neffectiveness of APER with a unified and concise framework. Code is available\nat https://github.com/zhoudw-zdw/RevisitingCIL\n","authors":["Da-Wei Zhou","Zi-Wen Cai","Han-Jia Ye","De-Chuan Zhan","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2303.07338v2.pdf","comment":"Accepted to IJCV. Code is available at:\n  https://github.com/zhoudw-zdw/RevisitingCIL"},{"id":"http://arxiv.org/abs/2408.02604v1","updated":"2024-08-05T16:27:38Z","published":"2024-08-05T16:27:38Z","title":"Learning rheological parameters of non-Newtonian fluids from velocimetry\n  data","summary":"  We solve a Bayesian inverse Navier-Stokes (N-S) problem that assimilates\nvelocimetry data in order to jointly reconstruct the flow field and learn the\nunknown N-S parameters. By incorporating a Carreau shear-thinning viscosity\nmodel into the N-S problem, we devise an algorithm that learns the most likely\nCarreau parameters of a shear-thinning fluid, and estimates their\nuncertainties, from velocimetry data alone. We then conduct a flow-MRI\nexperiment to obtain velocimetry data of an axisymmetric laminar jet through an\nidealised medical device (FDA nozzle) for a blood analogue fluid. We show that\nthe algorithm can successfully reconstruct the flow field by learning the most\nlikely Carreau parameters, and that the learned parameters are in very good\nagreement with rheometry measurements. The algorithm accepts any algebraic\neffective viscosity model, as long as the model is differentiable, and it can\nbe extended to more complicated non-Newtonian fluids (e.g. Oldroyd-B fluid) if\na viscoelastic model is incorporated into the N-S problem.\n","authors":["Alexandros Kontogiannis","Richard Hodgkinson","Emily L. Manchester"],"pdf_url":"https://arxiv.org/pdf/2408.02604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02598v1","updated":"2024-08-05T16:15:31Z","published":"2024-08-05T16:15:31Z","title":"AI-Driven Strategies for Reducing Student Withdrawal -- A Study of EMU\n  Student Stopout","summary":"  Not everyone who enrolls in college will leave with a certificate or degree,\nbut the number of people who drop out or take a break is much higher than\nexperts previously believed. In December 2013, there were 29 million people\nwith some college education but no degree. That number jumped to 36 million by\nDecember of 2018, according to a new report from the National Student\nClearinghouse Research Center[1]. It is imperative to understand the underlying\nfactors contributing to student withdrawal and to assist decision-makers to\nidentify effective strategies to prevent it. By analyzing the characteristics\nand educational pathways of the stopout student population, our aim is to\nprovide actionable insights that can benefit institutions facing similar\nchallenges. Eastern Michigan University (EMU) faces significant challenges in\nstudent retention, with approximately 55% of its undergraduate students not\ncompleting their degrees within six years. As an institution committed to\nstudent success, EMU conducted a comprehensive study of student withdrawals to\nunderstand the influencing factors. And the paper revealed a high correlation\nbetween certain factors and withdrawals, even in the early stages of university\nattendance. Based on these findings, we developed a predictive model that\nemploys artificial intelligence techniques to assess the potential risk that\nstudents abandon their studies. These models enable universities to implement\nearly intervention strategies, support at-risk students, and improve overall\nhigher education success.\n","authors":["Yan Zhao","Amy Otteson"],"pdf_url":"https://arxiv.org/pdf/2408.02598v1.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2402.16517v2","updated":"2024-08-05T16:02:51Z","published":"2024-02-26T11:58:02Z","title":"Discovering Artificial Viscosity Models for Discontinuous Galerkin\n  Approximation of Conservation Laws using Physics-Informed Machine Learning","summary":"  Finite element-based high-order solvers of conservation laws offer large\naccuracy but face challenges near discontinuities due to the Gibbs phenomenon.\nArtificial viscosity is a popular and effective solution to this problem based\non physical insight. In this work, we present a physics-informed machine\nlearning algorithm to automate the discovery of artificial viscosity models in\na non-supervised paradigm. The algorithm is inspired by reinforcement learning\nand trains a neural network acting cell-by-cell (the viscosity model) by\nminimizing a loss defined as the difference with respect to a reference\nsolution thanks to automatic differentiation. This enables a dataset-free\ntraining procedure. We prove that the algorithm is effective by integrating it\ninto a state-of-the-art Runge-Kutta discontinuous Galerkin solver. We showcase\nseveral numerical tests on scalar and vectorial problems, such as Burgers' and\nEuler's equations in one and two dimensions. Results demonstrate that the\nproposed approach trains a model that is able to outperform classical viscosity\nmodels. Moreover, we show that the learnt artificial viscosity model is able to\ngeneralize across different problems and parameters.\n","authors":["Matteo Caldana","Paola F. Antonietti","Luca Dede'"],"pdf_url":"https://arxiv.org/pdf/2402.16517v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02581v1","updated":"2024-08-05T15:59:36Z","published":"2024-08-05T15:59:36Z","title":"Operational range bounding of spectroscopy models with anomaly detection","summary":"  Safe operation of machine learning models requires architectures that\nexplicitly delimit their operational ranges. We evaluate the ability of anomaly\ndetection algorithms to provide indicators correlated with degraded model\nperformance. By placing acceptance thresholds over such indicators, hard\nboundaries are formed that define the model's coverage. As a use case, we\nconsider the extraction of exoplanetary spectra from transit light curves,\nspecifically within the context of ESA's upcoming Ariel mission. Isolation\nForests are shown to effectively identify contexts where prediction models are\nlikely to fail. Coverage/error trade-offs are evaluated under conditions of\ndata and concept drift. The best performance is seen when Isolation Forests\nmodel projections of the prediction model's explainability SHAP values.\n","authors":["Luís F. Simões","Pierluigi Casale","Marília Felismino","Kai Hou Yip","Ingo P. Waldmann","Giovanna Tinetti","Theresa Lueftinger"],"pdf_url":"https://arxiv.org/pdf/2408.02581v1.pdf","comment":"To appear in \"Proceedings of SPAICE 2024: 1st ESA/IAA conference on\n  AI in and for Space\". Conference page at https://spaice.esa.int/"},{"id":"http://arxiv.org/abs/2405.06093v2","updated":"2024-08-05T15:51:50Z","published":"2024-05-09T20:45:58Z","title":"Selective Fine-tuning on LLM-labeled Data May Reduce Reliance on Human\n  Annotation: A Case Study Using Schedule-of-Event Table Detection","summary":"  Large Language Models (LLMs) have demonstrated their efficacy across a broad\nspectrum of tasks in healthcare applications. However, often LLMs need to be\nfine-tuned on task-specific expert annotated data to achieve optimal\nperformance, which can be expensive and time consuming. In this study, we\nfine-tune PaLM-2 with parameter efficient fine-tuning (PEFT) using noisy labels\nobtained from gemini-pro 1.0 for the detection of Schedule-of-Event (SoE)\ntables, which specify care plan in clinical trial protocols. We introduce a\nfiltering mechanism to select high-confidence labels for this table\nclassification task, thereby reducing the noise in the auto-generated labels.\nWe show that fine-tuned PaLM-2 with those labels achieves performance that\nexceeds the gemini-pro 1.0 and other LLMs. Furthermore, its performance is\nclose to a PaLM-2 fine-tuned on labels obtained from non-expert annotators. Our\nresults show that leveraging LLM-generated labels through powerful models like\ngemini-pro can potentially serve as a viable strategy for improving LLM\nperformance through fine-tuning in specialized tasks, particularly in domains\nwhere expert annotations are scarce, expensive, or time-consuming to obtain.\n","authors":["Bhawesh Kumar","Jonathan Amar","Eric Yang","Nan Li","Yugang Jia"],"pdf_url":"https://arxiv.org/pdf/2405.06093v2.pdf","comment":"23 pages. Published in MLHC 2024"},{"id":"http://arxiv.org/abs/2407.01281v2","updated":"2024-08-05T15:50:32Z","published":"2024-07-01T13:35:53Z","title":"Bridging Smoothness and Approximation: Theoretical Insights into\n  Over-Smoothing in Graph Neural Networks","summary":"  In this paper, we explore the approximation theory of functions defined on\ngraphs. Our study builds upon the approximation results derived from the\n$K$-functional. We establish a theoretical framework to assess the lower bounds\nof approximation for target functions using Graph Convolutional Networks (GCNs)\nand examine the over-smoothing phenomenon commonly observed in these networks.\nInitially, we introduce the concept of a $K$-functional on graphs, establishing\nits equivalence to the modulus of smoothness. We then analyze a typical type of\nGCN to demonstrate how the high-frequency energy of the output decays, an\nindicator of over-smoothing. This analysis provides theoretical insights into\nthe nature of over-smoothing within GCNs. Furthermore, we establish a lower\nbound for the approximation of target functions by GCNs, which is governed by\nthe modulus of smoothness of these functions. This finding offers a new\nperspective on the approximation capabilities of GCNs. In our numerical\nexperiments, we analyze several widely applied GCNs and observe the phenomenon\nof energy decay. These observations corroborate our theoretical results on\nexponential decay order.\n","authors":["Guangrui Yang","Jianfei Li","Ming Li","Han Feng","Ding-Xuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.01281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02575v1","updated":"2024-08-05T15:48:51Z","published":"2024-08-05T15:48:51Z","title":"Artificial Intelligence for Public Health Surveillance in Africa:\n  Applications and Opportunities","summary":"  Artificial Intelligence (AI) is revolutionizing various fields, including\npublic health surveillance. In Africa, where health systems frequently\nencounter challenges such as limited resources, inadequate infrastructure,\nfailed health information systems and a shortage of skilled health\nprofessionals, AI offers a transformative opportunity. This paper investigates\nthe applications of AI in public health surveillance across the continent,\npresenting successful case studies and examining the benefits, opportunities,\nand challenges of implementing AI technologies in African healthcare settings.\nOur paper highlights AI's potential to enhance disease monitoring and health\noutcomes, and support effective public health interventions. The findings\npresented in the paper demonstrate that AI can significantly improve the\naccuracy and timeliness of disease detection and prediction, optimize resource\nallocation, and facilitate targeted public health strategies. Additionally, our\npaper identified key barriers to the widespread adoption of AI in African\npublic health systems and proposed actionable recommendations to overcome these\nchallenges.\n","authors":["Jean Marie Tshimula","Mitterrand Kalengayi","Dieumerci Makenga","Dorcas Lilonge","Marius Asumani","Déborah Madiya","Élie Nkuba Kalonji","Hugues Kanda","René Manassé Galekwa","Josias Kumbu","Hardy Mikese","Grace Tshimula","Jean Tshibangu Muabila","Christian N. Mayemba","D'Jeff K. Nkashama","Kalonji Kalala","Steve Ataky","Tighana Wenge Basele","Mbuyi Mukendi Didier","Selain K. Kasereka","Maximilien V. Dialufuma","Godwill Ilunga Wa Kumwita","Lionel Muyuku","Jean-Paul Kimpesa","Dominique Muteba","Aaron Aruna Abedi","Lambert Mukendi Ntobo","Gloria M. Bundutidi","Désiré Kulimba Mashinda","Emmanuel Kabengele Mpinga","Nathanaël M. Kasoro"],"pdf_url":"https://arxiv.org/pdf/2408.02575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02568v1","updated":"2024-08-05T15:43:56Z","published":"2024-08-05T15:43:56Z","title":"Cross-Modality Clustering-based Self-Labeling for Multimodal Data\n  Classification","summary":"  Technological advances facilitate the ability to acquire multimodal data,\nposing a challenge for recognition systems while also providing an opportunity\nto use the heterogeneous nature of the information to increase the\ngeneralization capability of models. An often overlooked issue is the cost of\nthe labeling process, which is typically high due to the need for a significant\ninvestment in time and money associated with human experts. Existing\nsemi-supervised learning methods often focus on operating in the feature space\ncreated by the fusion of available modalities, neglecting the potential for\ncross-utilizing complementary information available in each modality. To\naddress this problem, we propose Cross-Modality Clustering-based Self-Labeling\n(CMCSL). Based on a small set of pre-labeled data, CMCSL groups instances\nbelonging to each modality in the deep feature space and then propagates known\nlabels within the resulting clusters. Next, information about the instances'\nclass membership in each modality is exchanged based on the Euclidean distance\nto ensure more accurate labeling. Experimental evaluation conducted on 20\ndatasets derived from the MM-IMDb dataset indicates that cross-propagation of\nlabels between modalities -- especially when the number of pre-labeled\ninstances is small -- can allow for more reliable labeling and thus increase\nthe classification performance in each modality.\n","authors":["Paweł Zyblewski","Leandro L. Minku"],"pdf_url":"https://arxiv.org/pdf/2408.02568v1.pdf","comment":"10 pages, 5 figures, 9 tables"},{"id":"http://arxiv.org/abs/2301.07088v3","updated":"2024-08-05T15:38:05Z","published":"2023-01-17T18:53:24Z","title":"Vision Learners Meet Web Image-Text Pairs","summary":"  Many self-supervised learning methods are pre-trained on the well-curated\nImageNet-1K dataset. In this work, given the excellent scalability of web data,\nwe consider self-supervised pre-training on noisy web sourced image-text paired\ndata. First, we conduct a benchmark study of representative self-supervised\npre-training methods on large-scale web data in a like-for-like setting. We\ncompare a range of methods, including single-modal ones that use masked\ntraining objectives and multi-modal ones that use image-text constrastive\ntraining. We observe that existing multi-modal methods do not outperform their\nsingle-modal counterparts on vision transfer learning tasks. We derive an\ninformation-theoretical view to explain these benchmark results, which provides\ninsight into how to design a novel vision learner. Inspired by this insight, we\npresent a new visual representation pre-training method, MUlti-modal\nGenerator~(MUG), that learns from scalable web sourced image-text data. MUG\nachieves state-of-the-art transfer performance on a variety of tasks and\ndemonstrates promising scaling properties. Pre-trained models and code will be\nmade public upon acceptance.\n","authors":["Bingchen Zhao","Quan Cui","Hao Wu","Osamu Yoshie","Cheng Yang","Oisin Mac Aodha"],"pdf_url":"https://arxiv.org/pdf/2301.07088v3.pdf","comment":"Project page: https://bzhao.me/MUG/"},{"id":"http://arxiv.org/abs/2408.02551v1","updated":"2024-08-05T15:26:39Z","published":"2024-08-05T15:26:39Z","title":"Process-constrained batch Bayesian approaches for yield optimization in\n  multi-reactor systems","summary":"  The optimization of yields in multi-reactor systems, which are advanced tools\nin heterogeneous catalysis research, presents a significant challenge due to\nhierarchical technical constraints. To this respect, this work introduces a\nnovel approach called process-constrained batch Bayesian optimization via\nThompson sampling (pc-BO-TS) and its generalized hierarchical extension\n(hpc-BO-TS). This method, tailored for the efficiency demands in multi-reactor\nsystems, integrates experimental constraints and balances between exploration\nand exploitation in a sequential batch optimization strategy. It offers an\nimprovement over other Bayesian optimization methods. The performance of\npc-BO-TS and hpc-BO-TS is validated in synthetic cases as well as in a\nrealistic scenario based on data obtained from high-throughput experiments done\non a multi-reactor system available in the REALCAT platform. The proposed\nmethods often outperform other sequential Bayesian optimizations and existing\nprocess-constrained batch Bayesian optimization methods. This work proposes a\nnovel approach to optimize the yield of a reaction in a multi-reactor system,\nmarking a significant step forward in digital catalysis and generally in\noptimization methods for chemical engineering.\n","authors":["Markus Grimm","Sébastien Paul","Pierre Chainais"],"pdf_url":"https://arxiv.org/pdf/2408.02551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02547v1","updated":"2024-08-05T15:17:34Z","published":"2024-08-05T15:17:34Z","title":"The Role of Functional Muscle Networks in Improving Hand Gesture\n  Perception for Human-Machine Interfaces","summary":"  Developing accurate hand gesture perception models is critical for various\nrobotic applications, enabling effective communication between humans and\nmachines and directly impacting neurorobotics and interactive robots. Recently,\nsurface electromyography (sEMG) has been explored for its rich informational\ncontext and accessibility when combined with advanced machine learning\napproaches and wearable systems. The literature presents numerous approaches to\nboost performance while ensuring robustness for neurorobots using sEMG, often\nresulting in models requiring high processing power, large datasets, and less\nscalable solutions. This paper addresses this challenge by proposing the\ndecoding of muscle synchronization rather than individual muscle activation. We\nstudy coherence-based functional muscle networks as the core of our perception\nmodel, proposing that functional synchronization between muscles and the\ngraph-based network of muscle connectivity encode contextual information about\nintended hand gestures. This can be decoded using shallow machine learning\napproaches without the need for deep temporal networks. Our technique could\nimpact myoelectric control of neurorobots by reducing computational burdens and\nenhancing efficiency. The approach is benchmarked on the Ninapro database,\nwhich contains 12 EMG signals from 40 subjects performing 17 hand gestures. It\nachieves an accuracy of 85.1%, demonstrating improved performance compared to\nexisting methods while requiring much less computational power. The results\nsupport the hypothesis that a coherence-based functional muscle network encodes\ncritical information related to gesture execution, significantly enhancing hand\ngesture perception with potential applications for neurorobotic systems and\ninteractive machines.\n","authors":["Costanza Armanini","Tuka Alhanai","Farah E. Shamout","S. Farokh Atashzar"],"pdf_url":"https://arxiv.org/pdf/2408.02547v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02545v1","updated":"2024-08-05T15:16:24Z","published":"2024-08-05T15:16:24Z","title":"RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented\n  Generation","summary":"  Implementing Retrieval-Augmented Generation (RAG) systems is inherently\ncomplex, requiring deep understanding of data, use cases, and intricate design\ndecisions. Additionally, evaluating these systems presents significant\nchallenges, necessitating assessment of both retrieval accuracy and generative\nquality through a multi-faceted approach. We introduce RAG Foundry, an\nopen-source framework for augmenting large language models for RAG use cases.\nRAG Foundry integrates data creation, training, inference and evaluation into a\nsingle workflow, facilitating the creation of data-augmented datasets for\ntraining and evaluating large language models in RAG settings. This integration\nenables rapid prototyping and experimentation with various RAG techniques,\nallowing users to easily generate datasets and train RAG models using internal\nor specialized knowledge sources. We demonstrate the framework effectiveness by\naugmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG\nconfigurations, showcasing consistent improvements across three\nknowledge-intensive datasets. Code is released as open-source in\nhttps://github.com/IntelLabs/RAGFoundry.\n","authors":["Daniel Fleischer","Moshe Berchansky","Moshe Wasserblat","Peter Izsak"],"pdf_url":"https://arxiv.org/pdf/2408.02545v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2406.04216v3","updated":"2024-08-05T15:08:02Z","published":"2024-06-06T16:15:34Z","title":"What Do Language Models Learn in Context? The Structured Task Hypothesis","summary":"  Large language models (LLMs) exhibit an intriguing ability to learn a novel\ntask from in-context examples presented in a demonstration, termed in-context\nlearning (ICL). Understandably, a swath of research has been dedicated to\nuncovering the theories underpinning ICL. One popular hypothesis explains ICL\nby task selection. LLMs identify the task based on the demonstration and\ngeneralize it to the prompt. Another popular hypothesis is that ICL is a form\nof meta-learning, i.e., the models learn a learning algorithm at pre-training\ntime and apply it to the demonstration. Finally, a third hypothesis argues that\nLLMs use the demonstration to select a composition of tasks learned during\npre-training to perform ICL. In this paper, we empirically explore these three\nhypotheses that explain LLMs' ability to learn in context with a suite of\nexperiments derived from common text classification tasks. We invalidate the\nfirst two hypotheses with counterexamples and provide evidence in support of\nthe last hypothesis. Our results suggest an LLM could learn a novel task in\ncontext via composing tasks learned during pre-training.\n","authors":["Jiaoda Li","Yifan Hou","Mrinmaya Sachan","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2406.04216v3.pdf","comment":"This work is published in ACL 2024"},{"id":"http://arxiv.org/abs/2310.02812v2","updated":"2024-08-05T15:06:24Z","published":"2023-10-04T13:37:34Z","title":"Time-Series Classification in Smart Manufacturing Systems: An\n  Experimental Evaluation of State-of-the-Art Machine Learning Algorithms","summary":"  Manufacturing is gathering extensive amounts of diverse data, thanks to the\ngrowing number of sensors and rapid advances in sensing technologies. Among the\nvarious data types available in SMS settings, time-series data plays a pivotal\nrole. Hence, TSC emerges is crucial in this domain. The objective of this study\nis to fill this gap by providing a rigorous experimental evaluation of the SoTA\nML and DL algorithms for TSC tasks in manufacturing and industrial settings. We\nfirst explored and compiled a comprehensive list of more than 92 SoTA\nalgorithms from both TSC and manufacturing literature. Following, we selected\nthe 36 most representative algorithms from this list. To evaluate their\nperformance across various manufacturing classification tasks, we curated a set\nof 22 manufacturing datasets, representative of different characteristics that\ncover diverse manufacturing problems. Subsequently, we implemented and\nevaluated the algorithms on the manufacturing benchmark datasets, and analyzed\nthe results for each dataset. Based on the results, ResNet, DrCIF,\nInceptionTime, and ARSENAL are the top-performing algorithms, boasting an\naverage accuracy of over 96.6% across all 22 manufacturing TSC datasets. These\nfindings underscore the robustness, efficiency, scalability, and effectiveness\nof convolutional kernels in capturing temporal features in time-series data, as\nthree out of the top four performing algorithms leverage these kernels for\nfeature extraction. Additionally, LSTM, BiLSTM, and TS-LSTM algorithms deserve\nrecognition for their effectiveness in capturing features within time-series\ndata using RNN-based structures.\n","authors":["Mojtaba A. Farahani","M. R. McCormick","Ramy Harik","Thorsten Wuest"],"pdf_url":"https://arxiv.org/pdf/2310.02812v2.pdf","comment":"Published in Robotics and Computer-Integrated Manufacturing journal"},{"id":"http://arxiv.org/abs/2408.02533v1","updated":"2024-08-05T15:03:19Z","published":"2024-08-05T15:03:19Z","title":"LMEMs for post-hoc analysis of HPO Benchmarking","summary":"  The importance of tuning hyperparameters in Machine Learning (ML) and Deep\nLearning (DL) is established through empirical research and applications,\nevident from the increase in new hyperparameter optimization (HPO) algorithms\nand benchmarks steadily added by the community. However, current benchmarking\npractices using averaged performance across many datasets may obscure key\ndifferences between HPO methods, especially for pairwise comparisons. In this\nwork, we apply Linear Mixed-Effect Models-based (LMEMs) significance testing\nfor post-hoc analysis of HPO benchmarking runs. LMEMs allow flexible and\nexpressive modeling on the entire experiment data, including information such\nas benchmark meta-features, offering deeper insights than current analysis\npractices. We demonstrate this through a case study on the PriorBand paper's\nexperiment data to find insights not reported in the original work.\n","authors":["Anton Geburek","Neeratyoy Mallik","Danny Stoll","Xavier Bouthillier","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2408.02533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14294v2","updated":"2024-08-05T14:59:27Z","published":"2024-02-22T05:16:04Z","title":"High-arity PAC learning via exchangeability","summary":"  We develop a theory of high-arity PAC learning, which is statistical learning\nin the presence of \"structured correlation\". In this theory, hypotheses are\neither graphs, hypergraphs or, more generally, structures in finite relational\nlanguages, and i.i.d. sampling is replaced by sampling an induced substructure,\nproducing an exchangeable distribution. Our main theorems establish a\nhigh-arity (agnostic) version of the fundamental theorem of statistical\nlearning.\n","authors":["Leonardo N. Coregliano","Maryanthe Malliaris"],"pdf_url":"https://arxiv.org/pdf/2402.14294v2.pdf","comment":"150 pages, 1 figure. (This version makes expository changes to\n  Sections 1 and 2 and adds Appendix B on Bayes predictors.)"},{"id":"http://arxiv.org/abs/2408.02525v1","updated":"2024-08-05T14:46:04Z","published":"2024-08-05T14:46:04Z","title":"Single-tap Latency Reduction with Single- or Double- tap Prediction","summary":"  Touch surfaces are widely utilized for smartphones, tablet PCs, and laptops\n(touchpad), and single and double taps are the most basic and common operations\non them. The detection of single or double taps causes the single-tap latency\nproblem, which creates a bottleneck in terms of the sensitivity of touch\ninputs. To reduce the single-tap latency, we propose a novel\nmachine-learning-based tap prediction method called PredicTaps. Our method\npredicts whether a detected tap is a single tap or the first contact of a\ndouble tap without having to wait for the hundreds of milliseconds\nconventionally required. We present three evaluations and one user evaluation\nthat demonstrate its broad applicability and usability for various tap\nsituations on two form factors (touchpad and smartphone). The results showed\nPredicTaps reduces the single-tap latency from 150-500 ms to 12 ms on laptops\nand to 17.6 ms on smartphones without reducing usability.\n","authors":["Naoto Nishida","Kaori Ikematsu","Junichi Sato","Shota Yamanaka","Kota Tsubouchi"],"pdf_url":"https://arxiv.org/pdf/2408.02525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02514v1","updated":"2024-08-05T14:34:40Z","published":"2024-08-05T14:34:40Z","title":"Stem-JEPA: A Joint-Embedding Predictive Architecture for Musical Stem\n  Compatibility Estimation","summary":"  This paper explores the automated process of determining stem compatibility\nby identifying audio recordings of single instruments that blend well with a\ngiven musical context. To tackle this challenge, we present Stem-JEPA, a novel\nJoint-Embedding Predictive Architecture (JEPA) trained on a multi-track dataset\nusing a self-supervised learning approach.\n  Our model comprises two networks: an encoder and a predictor, which are\njointly trained to predict the embeddings of compatible stems from the\nembeddings of a given context, typically a mix of several instruments. Training\na model in this manner allows its use in estimating stem compatibility -\nretrieving, aligning, or generating a stem to match a given mix - or for\ndownstream tasks such as genre or key estimation, as the training paradigm\nrequires the model to learn information related to timbre, harmony, and rhythm.\n  We evaluate our model's performance on a retrieval task on the MUSDB18\ndataset, testing its ability to find the missing stem from a mix and through a\nsubjective user study. We also show that the learned embeddings capture\ntemporal alignment information and, finally, evaluate the representations\nlearned by our model on several downstream tasks, highlighting that they\neffectively capture meaningful musical features.\n","authors":["Alain Riou","Stefan Lattner","Gaëtan Hadjeres","Michael Anslow","Geoffroy Peeters"],"pdf_url":"https://arxiv.org/pdf/2408.02514v1.pdf","comment":"Proceedings of the 25th International Society for Music Information\n  Retrieval Conference, ISMIR 2024"},{"id":"http://arxiv.org/abs/2408.02509v1","updated":"2024-08-05T14:31:26Z","published":"2024-08-05T14:31:26Z","title":"Practical Attacks against Black-box Code Completion Engines","summary":"  Modern code completion engines, powered by large language models, have\ndemonstrated impressive capabilities to generate functionally correct code\nbased on surrounding context. As these tools are extensively used by millions\nof developers, it is crucial to investigate their security implications. In\nthis work, we present INSEC, a novel attack that directs code completion\nengines towards generating vulnerable code. In line with most commercial\ncompletion engines, such as GitHub Copilot, INSEC assumes only black-box query\naccess to the targeted engine, without requiring any knowledge of the engine's\ninternals. Our attack works by inserting a malicious attack string as a short\ncomment in the completion input. To derive the attack string, we design a\nseries of specialized initialization schemes and an optimization procedure for\nfurther refinement. We demonstrate the strength of INSEC not only on\nstate-of-the-art open-source models but also on black-box commercial services\nsuch as the OpenAI API and GitHub Copilot. On a comprehensive set of\nsecurity-critical test cases covering 16 CWEs across 5 programming languages,\nINSEC significantly increases the likelihood of the considered completion\nengines in generating unsafe code by >50% in absolute, while maintaining the\nability in producing functionally correct code. At the same time, our attack\nhas low resource requirements, and can be developed for a cost of well under\nten USD on commodity hardware.\n","authors":["Slobodan Jenko","Jingxuan He","Niels Mündler","Mark Vero","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2408.02509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02496v1","updated":"2024-08-05T14:19:03Z","published":"2024-08-05T14:19:03Z","title":"Automatic rating of incomplete hippocampal inversions evaluated across\n  multiple cohorts","summary":"  Incomplete Hippocampal Inversion (IHI), sometimes called hippocampal\nmalrotation, is an atypical anatomical pattern of the hippocampus found in\nabout 20% of the general population. IHI can be visually assessed on coronal\nslices of T1 weighted MR images, using a composite score that combines four\nanatomical criteria. IHI has been associated with several brain disorders\n(epilepsy, schizophrenia). However, these studies were based on small samples.\nFurthermore, the factors (genetic or environmental) that contribute to the\ngenesis of IHI are largely unknown. Large-scale studies are thus needed to\nfurther understand IHI and their potential relationships to neurological and\npsychiatric disorders. However, visual evaluation is long and tedious,\njustifying the need for an automatic method. In this paper, we propose, for the\nfirst time, to automatically rate IHI. We proceed by predicting four anatomical\ncriteria, which are then summed up to form the IHI score, providing the\nadvantage of an interpretable score. We provided an extensive experimental\ninvestigation of different machine learning methods and training strategies. We\nperformed automatic rating using a variety of deep learning models (conv5-FC3,\nResNet and SECNN) as well as a ridge regression. We studied the generalization\nof our models using different cohorts and performed multi-cohort learning. We\nrelied on a large population of 2,008 participants from the IMAGEN study, 993\nand 403 participants from the QTIM/QTAB studies as well as 985 subjects from\nthe UKBiobank. We showed that deep learning models outperformed a ridge\nregression. We demonstrated that the performances of the conv5-FC3 network were\nat least as good as more complex networks while maintaining a low complexity\nand computation time. We showed that training on a single cohort may lack in\nvariability while training on several cohorts improves generalization.\n","authors":["Lisa Hemforth","Baptiste Couvy-Duchesne","Kevin De Matos","Camille Brianceau","Matthieu Joulot","Tobias Banaschewski","Arun L. W. Bokde","Sylvane Desrivières","Herta Flor","Antoine Grigis","Hugh Garavan","Penny Gowland","Andreas Heinz","Rüdiger Brühl","Jean-Luc Martinot","Marie-Laure Paillère Martinot","Eric Artiges","Dimitri Papadopoulos","Herve Lemaitre","Tomas Paus","Luise Poustka","Sarah Hohmann","Nathalie Holz","Juliane H. Fröhner","Michael N. Smolka","Nilakshi Vaidya","Henrik Walter","Robert Whelan","Gunter Schumann","Christian Büchel","JB Poline","Bernd Itterman","Vincent Frouin","Alexandre Martin","IMAGEN study group","Claire Cury","Olivier Colliot"],"pdf_url":"https://arxiv.org/pdf/2408.02496v1.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2024:016"},{"id":"http://arxiv.org/abs/2408.02487v1","updated":"2024-08-05T14:09:30Z","published":"2024-08-05T14:09:30Z","title":"A First Look at License Compliance Capability of LLMs in Code Generation","summary":"  Recent advances in Large Language Models (LLMs) have revolutionized code\ngeneration, leading to widespread adoption of AI coding tools by developers.\nHowever, LLMs can generate license-protected code without providing the\nnecessary license information, leading to potential intellectual property\nviolations during software production. This paper addresses the critical, yet\nunderexplored, issue of license compliance in LLM-generated code by\nestablishing a benchmark to evaluate the ability of LLMs to provide accurate\nlicense information for their generated code. To establish this benchmark, we\nconduct an empirical study to identify a reasonable standard for \"striking\nsimilarity\" that excludes the possibility of independent creation, indicating a\ncopy relationship between the LLM output and certain open-source code. Based on\nthis standard, we propose an evaluation benchmark LiCoEval, to evaluate the\nlicense compliance capabilities of LLMs. Using LiCoEval, we evaluate 14 popular\nLLMs, finding that even top-performing LLMs produce a non-negligible proportion\n(0.88% to 2.01%) of code strikingly similar to existing open-source\nimplementations. Notably, most LLMs fail to provide accurate license\ninformation, particularly for code under copyleft licenses. These findings\nunderscore the urgent need to enhance LLM compliance capabilities in code\ngeneration tasks. Our study provides a foundation for future research and\ndevelopment to improve license compliance in AI-assisted software development,\ncontributing to both the protection of open-source software copyrights and the\nmitigation of legal risks for LLM users.\n","authors":["Weiwei Xu","Kai Gao","Hao He","Minghui Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.02487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02473v1","updated":"2024-08-05T13:57:32Z","published":"2024-08-05T13:57:32Z","title":"Toward Attention-based TinyML: A Heterogeneous Accelerated Architecture\n  and Automated Deployment Flow","summary":"  One of the challenges for Tiny Machine Learning (tinyML) is keeping up with\nthe evolution of Machine Learning models from Convolutional Neural Networks to\nTransformers. We address this by leveraging a heterogeneous architectural\ntemplate coupling RISC-V processors with hardwired accelerators supported by an\nautomated deployment flow. We demonstrate an Attention-based model in a tinyML\npower envelope with an octa-core cluster coupled with an accelerator for\nquantized Attention. Our deployment flow enables an end-to-end 8-bit\nMobileBERT, achieving leading-edge energy efficiency and throughput of 2960\nGOp/J and 154 GOp/s at 32.5 Inf/s consuming 52.0 mW (0.65 V, 22 nm FD-SOI\ntechnology).\n","authors":["Philip Wiese","Gamze İslamoğlu","Moritz Scherer","Luka Macan","Victor J. B. Jung","Alessio Burrello","Francesco Conti","Luca Benini"],"pdf_url":"https://arxiv.org/pdf/2408.02473v1.pdf","comment":"Pre-print manuscript submitted for review to the IEEE Design and Test\n  Special Issue on tinyML"},{"id":"http://arxiv.org/abs/2408.02456v1","updated":"2024-08-05T13:28:51Z","published":"2024-08-05T13:28:51Z","title":"Enhancing Heterogeneous Knowledge Graph Completion with a Novel\n  GAT-based Approach","summary":"  Knowledge graphs (KGs) play a vital role in enhancing search results and\nrecommendation systems. With the rapid increase in the size of the KGs, they\nare becoming inaccuracy and incomplete. This problem can be solved by the\nknowledge graph completion methods, of which graph attention network\n(GAT)-based methods stand out since their superior performance. However,\nexisting GAT-based knowledge graph completion methods often suffer from\noverfitting issues when dealing with heterogeneous knowledge graphs, primarily\ndue to the unbalanced number of samples. Additionally, these methods\ndemonstrate poor performance in predicting the tail (head) entity that shares\nthe same relation and head (tail) entity with others. To solve these problems,\nwe propose GATH, a novel GAT-based method designed for Heterogeneous KGs. GATH\nincorporates two separate attention network modules that work synergistically\nto predict the missing entities. We also introduce novel encoding and feature\ntransformation approaches, enabling the robust performance of GATH in scenarios\nwith imbalanced samples. Comprehensive experiments are conducted to evaluate\nthe GATH's performance. Compared with the existing SOTA GAT-based model on\nHits@10 and MRR metrics, our model improves performance by 5.2% and 5.2% on the\nFB15K-237 dataset, and by 4.5% and 14.6% on the WN18RR dataset, respectively.\n","authors":["Wanxu Wei","Yitong Song","Bin Yao"],"pdf_url":"https://arxiv.org/pdf/2408.02456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03453v2","updated":"2024-08-05T13:01:47Z","published":"2024-04-04T13:57:44Z","title":"Conditioning of Banach Space Valued Gaussian Random Variables: An\n  Approximation Approach Based on Martingales","summary":"  In this paper we investigate the conditional distributions of two Banach\nspace valued, jointly Gaussian random variables. We show that these conditional\ndistributions are again Gaussian and that their means and covariances are\ndetermined by a general finite dimensional approximation scheme based upon a\nmartingale approach. In particular, it turns out that the covariance operators\noccurring in this scheme converge with respect to the nuclear norm and that the\nconditional probabilities converge weakly. Moreover, we discuss in detail, how\nour approximation scheme can be implemented in several classes of important\nBanach spaces such as RKHSs and $C(T)$. As an example, we then apply our\ngeneral results to the case of Gaussian processes with continuous paths\nconditioned to partial but infinite observations of their paths. Here we show\nthat conditioning on sufficiently rich, increasing sets of finitely many\nobservations leads to consistent approximations, in the sense that both the\nmean and covariance functions converge uniformly. Moreover, we discuss how\nthese results improve our understanding of the popular Gaussian processes for\nmachine learning.\n","authors":["Ingo Steinwart"],"pdf_url":"https://arxiv.org/pdf/2404.03453v2.pdf","comment":"52 pages plus 22 pages of supplemental material"},{"id":"http://arxiv.org/abs/2405.14244v2","updated":"2024-08-05T12:59:32Z","published":"2024-05-23T07:23:33Z","title":"Tell me why: Training preferences-based RL with human preferences and\n  step-level explanations","summary":"  Human-in-the-loop reinforcement learning allows the training of agents\nthrough various interfaces, even for non-expert humans. Recently,\npreference-based methods (PbRL), where the human has to give his preference\nover two trajectories, increased in popularity since they allow training in\ndomains where more direct feedback is hard to formulate. However, the current\nPBRL methods have limitations and do not provide humans with an expressive\ninterface for giving feedback. With this work, we propose a new\npreference-based learning method that provides humans with a more expressive\ninterface to provide their preference over trajectories and a factual\nexplanation (or annotation of why they have this preference). These\nexplanations allow the human to explain what parts of the trajectory are most\nrelevant for the preference. We allow the expression of the explanations over\nindividual trajectory steps. We evaluate our method in various simulations\nusing a simulated human oracle (with realistic restrictions), and our results\nshow that our extended feedback can improve the speed of learning.\n","authors":["Jakob Karalus"],"pdf_url":"https://arxiv.org/pdf/2405.14244v2.pdf","comment":"Workshop on Reinforcement Learning Beyond Rewards @ Reinforcement\n  Learning Conference (2024)"},{"id":"http://arxiv.org/abs/2202.04309v2","updated":"2024-08-05T12:58:37Z","published":"2022-02-09T06:56:41Z","title":"Vertical Federated Learning: Challenges, Methodologies and Experiments","summary":"  Recently, federated learning (FL) has emerged as a promising distributed\nmachine learning (ML) technology, owing to the advancing computational and\nsensing capacities of end-user devices, however with the increasing concerns on\nusers' privacy. As a special architecture in FL, vertical FL (VFL) is capable\nof constructing a hyper ML model by embracing sub-models from different\nclients. These sub-models are trained locally by vertically partitioned data\nwith distinct attributes. Therefore, the design of VFL is fundamentally\ndifferent from that of conventional FL, raising new and unique research issues.\nIn this paper, we aim to discuss key challenges in VFL with effective\nsolutions, and conduct experiments on real-life datasets to shed light on these\nissues. Specifically, we first propose a general framework on VFL, and\nhighlight the key differences between VFL and conventional FL. Then, we discuss\nresearch challenges rooted in VFL systems under four aspects, i.e., security\nand privacy risks, expensive computation and communication costs, possible\nstructural damage caused by model splitting, and system heterogeneity.\nAfterwards, we develop solutions to addressing the aforementioned challenges,\nand conduct extensive experiments to showcase the effectiveness of our proposed\nsolutions.\n","authors":["Kang Wei","Jun Li","Chuan Ma","Ming Ding","Sha Wei","Fan Wu","Guihai Chen","Thilina Ranbaduge"],"pdf_url":"https://arxiv.org/pdf/2202.04309v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02433v1","updated":"2024-08-05T12:46:21Z","published":"2024-08-05T12:46:21Z","title":"On Probabilistic Embeddings in Optimal Dimension Reduction","summary":"  Dimension reduction algorithms are a crucial part of many data science\npipelines, including data exploration, feature creation and selection, and\ndenoising. Despite their wide utilization, many non-linear dimension reduction\nalgorithms are poorly understood from a theoretical perspective. In this work\nwe consider a generalized version of multidimensional scaling, which is posed\nas an optimization problem in which a mapping from a high-dimensional feature\nspace to a lower-dimensional embedding space seeks to preserve either inner\nproducts or norms of the distribution in feature space, and which encompasses\nmany commonly used dimension reduction algorithms. We analytically investigate\nthe variational properties of this problem, leading to the following insights:\n1) Solutions found using standard particle descent methods may lead to\nnon-deterministic embeddings, 2) A relaxed or probabilistic formulation of the\nproblem admits solutions with easily interpretable necessary conditions, 3) The\nglobally optimal solutions to the relaxed problem actually must give a\ndeterministic embedding. This progression of results mirrors the classical\ndevelopment of optimal transportation, and in a case relating to the\nGromov-Wasserstein distance actually gives explicit insight into the structure\nof the optimal embeddings, which are parametrically determined and\ndiscontinuous. Finally, we illustrate that a standard computational\nimplementation of this task does not learn deterministic embeddings, which\nmeans that it learns sub-optimal mappings, and that the embeddings learned in\nthat context have highly misleading clustering structure, underscoring the\ndelicate nature of solving this problem computationally.\n","authors":["Ryan Murray","Adam Pickarski"],"pdf_url":"https://arxiv.org/pdf/2408.02433v1.pdf","comment":"26 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2303.10571v2","updated":"2024-08-05T12:44:04Z","published":"2023-03-19T05:20:52Z","title":"Reinforcement Learning Friendly Vision-Language Model for Minecraft","summary":"  One of the essential missions in the AI research community is to build an\nautonomous embodied agent that can achieve high-level performance across a wide\nspectrum of tasks. However, acquiring or manually designing rewards for all\nopen-ended tasks is unrealistic. In this paper, we propose a novel cross-modal\ncontrastive learning framework architecture, CLIP4MC, aiming to learn a\nreinforcement learning (RL) friendly vision-language model (VLM) that serves as\nan intrinsic reward function for open-ended tasks. Simply utilizing the\nsimilarity between the video snippet and the language prompt is not RL-friendly\nsince standard VLMs may only capture the similarity at a coarse level. To\nachieve RL-friendliness, we incorporate the task completion degree into the VLM\ntraining objective, as this information can assist agents in distinguishing the\nimportance between different states. Moreover, we provide neat YouTube datasets\nbased on the large-scale YouTube database provided by MineDojo. Specifically,\ntwo rounds of filtering operations guarantee that the dataset covers enough\nessential information and that the video-text pair is highly correlated.\nEmpirically, we demonstrate that the proposed method achieves better\nperformance on RL tasks compared with baselines. The code and datasets are\navailable at https://github.com/PKU-RL/CLIP4MC.\n","authors":["Haobin Jiang","Junpeng Yue","Hao Luo","Ziluo Ding","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2303.10571v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.19858v2","updated":"2024-08-05T12:42:38Z","published":"2024-07-29T10:26:52Z","title":"AI-Powered Energy Algorithmic Trading: Integrating Hidden Markov Models\n  with Neural Networks","summary":"  In quantitative finance, machine learning methods are essential for alpha\ngeneration. This study introduces a new approach that combines Hidden Markov\nModels (HMM) and neural networks, integrated with Black-Litterman portfolio\noptimization. During the COVID period (2019-2022), this dual-model approach\nachieved a 97% return with a Sharpe ratio of 0.992. It incorporates two risk\nmodels to enhance risk management, showing efficiency during volatile periods.\nThe methodology was implemented on the QuantConnect platform, which was chosen\nfor its robust framework and experimental reproducibility. The system, which\npredicts future price movements, includes a three-year warm-up to ensure proper\nalgorithm function. It targets highly liquid, large-cap energy stocks to ensure\nstable and predictable performance while also considering broker payments. The\ndual-model alpha system utilizes log returns to select the optimal state based\non the historical performance. It combines state predictions with neural\nnetwork outputs, which are based on historical data, to generate trading\nsignals. This study examined the architecture of the trading system, data\npre-processing, training, and performance. The full code and backtesting data\nare available under the MIT license.\n","authors":["Tiago Monteiro"],"pdf_url":"https://arxiv.org/pdf/2407.19858v2.pdf","comment":"14 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2408.02427v1","updated":"2024-08-05T12:34:49Z","published":"2024-08-05T12:34:49Z","title":"Attenuation-adjusted deep learning of pore defects in 2D radiographs of\n  additive manufacturing powders","summary":"  The presence of gas pores in metal feedstock powder for additive\nmanufacturing greatly affects the final AM product. Since current porosity\nanalysis often involves lengthy X-ray computed tomography (XCT) scans with a\nfull rotation around the sample, motivation exists to explore methods that\nallow for high throughput -- possibly enabling in-line porosity analysis during\nmanufacturing. Through labelling pore pixels on single 2D radiographs of\npowders, this work seeks to simulate such future efficient setups. High\nsegmentation accuracy is achieved by combining a model of X-ray attenuation\nthrough particles with a variant of the widely applied UNet architecture;\nnotably, F1-score increases by $11.4\\%$ compared to the baseline UNet. The\nproposed pore segmentation is enabled by: 1) pretraining on synthetic data, 2)\nmaking tight particle cutouts, and 3) subtracting an ideal particle without\npores generated from a distance map inspired by Lambert-Beers law. This paper\nexplores four image processing methods, where the fastest (yet still\nunoptimized) segments a particle in mean $0.014s$ time with F1-score $0.78$,\nand the most accurate in $0.291s$ with F1-score $0.87$. Due to their scalable\nnature, these strategies can be involved in making high throughput porosity\nanalysis of metal feedstock powder for additive manufacturing.\n","authors":["Andreas Bjerregaard","David Schumacher","Jon Sporring"],"pdf_url":"https://arxiv.org/pdf/2408.02427v1.pdf","comment":"Implementation on https://github.com/yhsure/porosity"},{"id":"http://arxiv.org/abs/2402.12198v2","updated":"2024-08-05T12:20:49Z","published":"2024-02-19T15:03:04Z","title":"Zero shot VLMs for hate meme detection: Are we there yet?","summary":"  Multimedia content on social media is rapidly evolving, with memes gaining\nprominence as a distinctive form. Unfortunately, some malicious users exploit\nmemes to target individuals or vulnerable communities, making it imperative to\nidentify and address such instances of hateful memes. Extensive research has\nbeen conducted to address this issue by developing hate meme detection models.\nHowever, a notable limitation of traditional machine/deep learning models is\nthe requirement for labeled datasets for accurate classification. Recently, the\nresearch community has witnessed the emergence of several visual language\nmodels that have exhibited outstanding performance across various tasks. In\nthis study, we aim to investigate the efficacy of these visual language models\nin handling intricate tasks such as hate meme detection. We use various prompt\nsettings to focus on zero-shot classification of hateful/harmful memes. Through\nour analysis, we observe that large VLMs are still vulnerable for zero-shot\nhate meme detection.\n","authors":["Naquee Rizwan","Paramananda Bhaskar","Mithun Das","Swadhin Satyaprakash Majhi","Punyajoy Saha","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2402.12198v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02412v1","updated":"2024-08-05T12:11:09Z","published":"2024-08-05T12:11:09Z","title":"PENDRAM: Enabling High-Performance and Energy-Efficient Processing of\n  Deep Neural Networks through a Generalized DRAM Data Mapping Policy","summary":"  Convolutional Neural Networks (CNNs), a prominent type of Deep Neural\nNetworks (DNNs), have emerged as a state-of-the-art solution for solving\nmachine learning tasks. To improve the performance and energy efficiency of CNN\ninference, the employment of specialized hardware accelerators is prevalent.\nHowever, CNN accelerators still face performance- and energy-efficiency\nchallenges due to high off-chip memory (DRAM) access latency and energy, which\nare especially crucial for latency- and energy-constrained embedded\napplications. Moreover, different DRAM architectures have different profiles of\naccess latency and energy, thus making it challenging to optimize them for high\nperformance and energy-efficient CNN accelerators. To address this, we present\nPENDRAM, a novel design space exploration methodology that enables\nhigh-performance and energy-efficient CNN acceleration through a generalized\nDRAM data mapping policy. Specifically, it explores the impact of different\nDRAM data mapping policies and DRAM architectures across different CNN\npartitioning and scheduling schemes on the DRAM access latency and energy, then\nidentifies the pareto-optimal design choices. The experimental results show\nthat our DRAM data mapping policy improves the energy-delay-product of DRAM\naccesses in the CNN accelerator over other mapping policies by up to 96%. In\nthis manner, our PENDRAM methodology offers high-performance and\nenergy-efficient CNN acceleration under any given DRAM architectures for\ndiverse embedded AI applications.\n","authors":["Rachmad Vidya Wicaksana Putra","Muhammad Abdullah Hanif","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2408.02412v1.pdf","comment":"11 pages, 15 figures, 2 tables. arXiv admin note: substantial text\n  overlap with arXiv:2004.10341"},{"id":"http://arxiv.org/abs/2408.02407v1","updated":"2024-08-05T12:01:42Z","published":"2024-08-05T12:01:42Z","title":"Terracorder: Sense Long and Prosper","summary":"  In-situ sensing devices need to be deployed in remote environments for long\nperiods of time; minimizing their power consumption is vital for maximising\nboth their operational lifetime and coverage. We introduce Terracorder -- a\nversatile multi-sensor device -- and showcase its exceptionally low power\nconsumption using an on-device reinforcement learning scheduler. We prototype a\nunique device setup for biodiversity monitoring and compare its battery life\nusing our scheduler against a number of fixed schedules; the scheduler captures\nmore than 80% of events at less than 50% of the number of activations of the\nbest-performing fixed schedule. We then explore how a collaborative scheduler\ncan maximise the useful operation of a network of devices, improving overall\nnetwork power consumption and robustness.\n","authors":["Josh Millar","Sarab Sethi","Hamed Haddadi","Anil Madhavapeddy"],"pdf_url":"https://arxiv.org/pdf/2408.02407v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2403.05996v3","updated":"2024-08-05T11:55:19Z","published":"2024-03-09T19:56:40Z","title":"Dissecting Deep RL with High Update Ratios: Combatting Value Divergence","summary":"  We show that deep reinforcement learning algorithms can retain their ability\nto learn without resetting network parameters in settings where the number of\ngradient updates greatly exceeds the number of environment samples by\ncombatting value function divergence. Under large update-to-data ratios, a\nrecent study by Nikishin et al. (2022) suggested the emergence of a primacy\nbias, in which agents overfit early interactions and downplay later experience,\nimpairing their ability to learn. In this work, we investigate the phenomena\nleading to the primacy bias. We inspect the early stages of training that were\nconjectured to cause the failure to learn and find that one fundamental\nchallenge is a long-standing acquaintance: value function divergence.\nOverinflated Q-values are found not only on out-of-distribution but also\nin-distribution data and can be linked to overestimation on unseen action\nprediction propelled by optimizer momentum. We employ a simple unit-ball\nnormalization that enables learning under large update ratios, show its\nefficacy on the widely used dm_control suite, and obtain strong performance on\nthe challenging dog tasks, competitive with model-based approaches. Our results\nquestion, in parts, the prior explanation for sub-optimal learning due to\noverfitting early data.\n","authors":["Marcel Hussing","Claas Voelcker","Igor Gilitschenski","Amir-massoud Farahmand","Eric Eaton"],"pdf_url":"https://arxiv.org/pdf/2403.05996v3.pdf","comment":"Accepted as a conference paper at the First Reinforcement Learning\n  Conference (RLC)"},{"id":"http://arxiv.org/abs/2407.19707v3","updated":"2024-08-05T11:22:34Z","published":"2024-07-29T05:05:13Z","title":"Neural networks for bifurcation and linear stability analysis of steady\n  states in partial differential equations","summary":"  This research introduces an extended application of neural networks for\nsolving nonlinear partial differential equations (PDEs). A neural network,\ncombined with a pseudo-arclength continuation, is proposed to construct\nbifurcation diagrams from parameterized nonlinear PDEs. Additionally, a neural\nnetwork approach is also presented for solving eigenvalue problems to analyze\nsolution linear stability, focusing on identifying the largest eigenvalue. The\neffectiveness of the proposed neural network is examined through experiments on\nthe Bratu equation and the Burgers equation. Results from a finite difference\nmethod are also presented as comparison. Varying numbers of grid points are\nemployed in each case to assess the behavior and accuracy of both the neural\nnetwork and the finite difference method. The experimental results demonstrate\nthat the proposed neural network produces better solutions, generates more\naccurate bifurcation diagrams, has reasonable computational times, and proves\neffective for linear stability analysis.\n","authors":["Muhammad Luthfi Shahab","Hadi Susanto"],"pdf_url":"https://arxiv.org/pdf/2407.19707v3.pdf","comment":"Accepted for publication in Applied Mathematics and Computation"},{"id":"http://arxiv.org/abs/2309.14857v2","updated":"2024-08-05T11:20:33Z","published":"2023-09-26T11:35:25Z","title":"Cluster Exploration using Informative Manifold Projections","summary":"  Dimensionality reduction (DR) is one of the key tools for the visual\nexploration of high-dimensional data and uncovering its cluster structure in\ntwo- or three-dimensional spaces. The vast majority of DR methods in the\nliterature do not take into account any prior knowledge a practitioner may have\nregarding the dataset under consideration. We propose a novel method to\ngenerate informative embeddings which not only factor out the structure\nassociated with different kinds of prior knowledge but also aim to reveal any\nremaining underlying structure. To achieve this, we employ a linear combination\nof two objectives: firstly, contrastive PCA that discounts the structure\nassociated with the prior information, and secondly, kurtosis projection\npursuit which ensures meaningful data separation in the obtained embeddings. We\nformulate this task as a manifold optimization problem and validate it\nempirically across a variety of datasets considering three distinct types of\nprior knowledge. Lastly, we provide an automated framework to perform iterative\nvisual exploration of high-dimensional data.\n","authors":["Stavros Gerolymatos","Xenophon Evangelopoulos","Vladimir Gusev","John Y. Goulermas"],"pdf_url":"https://arxiv.org/pdf/2309.14857v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02384v1","updated":"2024-08-05T11:16:26Z","published":"2024-08-05T11:16:26Z","title":"Strategic Federated Learning: Application to Smart Meter Data Clustering","summary":"  Federated learning (FL) involves several clients that share with a fusion\ncenter (FC), the model each client has trained with its own data. Conventional\nFL, which can be interpreted as an estimation or distortion-based approach,\nignores the final use of model information (MI) by the FC and the other\nclients. In this paper, we introduce a novel FL framework in which the FC uses\nan aggregate version of the MI to make decisions that affect the client's\nutility functions. Clients cannot choose the decisions and can only use the MI\nreported to the FC to maximize their utility. Depending on the alignment\nbetween the client and FC utilities, the client may have an individual interest\nin adding strategic noise to the model. This general framework is stated and\nspecialized to the case of clustering, in which noisy cluster representative\ninformation is reported. This is applied to the problem of power consumption\nscheduling. In this context, utility non-alignment occurs, for instance, when\nthe client wants to consume when the price of electricity is low, whereas the\nFC wants the consumption to occur when the total power is the lowest. This is\nillustrated with aggregated real data from Ausgrid \\cite{ausgrid}. Our\nnumerical analysis clearly shows that the client can increase his utility by\nadding noise to the model reported to the FC. Corresponding results and source\ncodes can be downloaded from \\cite{source-code}.\n","authors":["Hassan Mohamad","Chao Zhang","Samson Lasaulce","Vineeth S Varma","Mérouane Debbah","Mounir Ghogho"],"pdf_url":"https://arxiv.org/pdf/2408.02384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18103v2","updated":"2024-08-05T11:13:57Z","published":"2024-07-25T15:07:35Z","title":"Fine-Tuning Large Language Models for Stock Return Prediction Using\n  Newsflow","summary":"  Large language models (LLMs) and their fine-tuning techniques have\ndemonstrated superior performance in various language understanding and\ngeneration tasks. This paper explores fine-tuning LLMs for stock return\nforecasting with financial newsflow. In quantitative investing, return\nforecasting is fundamental for subsequent tasks like stock picking, portfolio\noptimization, etc. We formulate the model to include text representation and\nforecasting modules. We propose to compare the encoder-only and decoder-only\nLLMs, considering they generate text representations in distinct ways. The\nimpact of these different representations on forecasting performance remains an\nopen question. Meanwhile, we compare two simple methods of integrating LLMs'\ntoken-level representations into the forecasting module. The experiments on\nreal news and investment universes reveal that: (1) aggregated representations\nfrom LLMs' token-level embeddings generally produce return predictions that\nenhance the performance of long-only and long-short portfolios; (2) in the\nrelatively large investment universe, the decoder LLMs-based prediction model\nleads to stronger portfolios, whereas in the small universes, there are no\nconsistent winners. Among the three LLMs studied (DeBERTa, Mistral, Llama),\nMistral performs more robustly across different universes; (3) return\npredictions derived from LLMs' text representations are a strong signal for\nportfolio construction, outperforming conventional sentiment scores.\n","authors":["Tian Guo","Emmanuel Hauptmann"],"pdf_url":"https://arxiv.org/pdf/2407.18103v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02367v1","updated":"2024-08-05T10:32:06Z","published":"2024-08-05T10:32:06Z","title":"StoDIP: Efficient 3D MRF image reconstruction with deep image priors and\n  stochastic iterations","summary":"  Magnetic Resonance Fingerprinting (MRF) is a time-efficient approach to\nquantitative MRI for multiparametric tissue mapping. The reconstruction of\nquantitative maps requires tailored algorithms for removing aliasing artefacts\nfrom the compressed sampled MRF acquisitions. Within approaches found in the\nliterature, many focus solely on two-dimensional (2D) image reconstruction,\nneglecting the extension to volumetric (3D) scans despite their higher\nrelevance and clinical value. A reason for this is that transitioning to 3D\nimaging without appropriate mitigations presents significant challenges,\nincluding increased computational cost and storage requirements, and the need\nfor large amount of ground-truth (artefact-free) data for training. To address\nthese issues, we introduce StoDIP, a new algorithm that extends the\nground-truth-free Deep Image Prior (DIP) reconstruction to 3D MRF imaging.\nStoDIP employs memory-efficient stochastic updates across the multicoil MRF\ndata, a carefully selected neural network architecture, as well as faster\nnonuniform FFT (NUFFT) transformations. This enables a faster convergence\ncompared against a conventional DIP implementation without these features.\nTested on a dataset of whole-brain scans from healthy volunteers, StoDIP\ndemonstrated superior performance over the ground-truth-free reconstruction\nbaselines, both quantitatively and qualitatively.\n","authors":["Perla Mayo","Matteo Cencini","Carolin M. Pirkl","Marion I. Menzel","Michela Tosetti","Bjoern H. Menze","Mohammad Golbabaee"],"pdf_url":"https://arxiv.org/pdf/2408.02367v1.pdf","comment":"10 pages, 2 figures, 1 table, 1 algorithm"},{"id":"http://arxiv.org/abs/2407.08583v2","updated":"2024-08-05T10:31:24Z","published":"2024-07-11T15:08:11Z","title":"The Synergy between Data and Multi-Modal Large Language Models: A Survey\n  from Co-Development Perspective","summary":"  The rapid development of large language models (LLMs) has been witnessed in\nrecent years. Based on the powerful LLMs, multi-modal LLMs (MLLMs) extend the\nmodality from text to a broader spectrum of domains, attracting widespread\nattention due to the broader range of application scenarios. As LLMs and MLLMs\nrely on vast amounts of model parameters and data to achieve emergent\ncapabilities, the importance of data is receiving increasingly widespread\nattention and recognition. Tracing and analyzing recent data-oriented works for\nMLLMs, we find that the development of models and data is not two separate\npaths but rather interconnected. On the one hand, vaster and higher-quality\ndata contribute to better performance of MLLMs; on the other hand, MLLMs can\nfacilitate the development of data. The co-development of multi-modal data and\nMLLMs requires a clear view of 1) at which development stages of MLLMs specific\ndata-centric approaches can be employed to enhance certain MLLM capabilities,\nand 2) how MLLMs, utilizing those capabilities, can contribute to multi-modal\ndata in specific roles. To promote the data-model co-development for MLLM\ncommunity, we systematically review existing works related to MLLMs from the\ndata-model co-development perspective. A regularly maintained project\nassociated with this survey is accessible at\nhttps://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md.\n","authors":["Zhen Qin","Daoyuan Chen","Wenhao Zhang","Liuyi Yao","Yilun Huang","Bolin Ding","Yaliang Li","Shuiguang Deng"],"pdf_url":"https://arxiv.org/pdf/2407.08583v2.pdf","comment":"Ongoing work. 21 pages. Related materials are continually maintained\n  and available at\n  https://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md"},{"id":"http://arxiv.org/abs/2305.09958v2","updated":"2024-08-05T10:24:09Z","published":"2023-05-17T05:35:49Z","title":"SIGMA: Similarity-based Efficient Global Aggregation for Heterophilous\n  Graph Neural Networks","summary":"  Graph neural networks (GNNs) realize great success in graph learning but\nsuffer from performance loss when meeting heterophily, i.e. neighboring nodes\nare dissimilar, due to their local and uniform aggregation. Existing attempts\nof heterophilous GNNs incorporate long-range or global aggregations to\ndistinguish nodes in the graph. However, these aggregations usually require\niteratively maintaining and updating full-graph information, which limits their\nefficiency when applying to large-scale graphs. In this paper, we propose\n\\aggname{}, an efficient global heterophilous GNN aggregation integrating the\nstructural similarity measurement SimRank. Our theoretical analysis illustrates\nthat \\aggname{} inherently captures distant global similarity even under\nheterophily, that conventional approaches can only achieve after iterative\naggregations. Furthermore, it enjoys efficient one-time computation with a\ncomplexity only linear to the node set size $\\mathcal{O}(n)$. Comprehensive\nevaluation demonstrates that \\aggname{} achieves state-of-the-art performance\nwith superior aggregation and overall efficiency. Notably, it obtains 5$\\times$\nacceleration on the large-scale heterophily dataset \\emph{pokec} with over 30\nmillion edges compared to the best baseline aggregation.\n","authors":["Haoyu Liu","Ningyi Liao","Siqiang Luo"],"pdf_url":"https://arxiv.org/pdf/2305.09958v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2405.10802v2","updated":"2024-08-05T10:20:11Z","published":"2024-05-17T14:16:40Z","title":"Reduced storage direct tensor ring decomposition for convolutional\n  neural networks compression","summary":"  Convolutional neural networks (CNNs) are among the most widely used machine\nlearning models for computer vision tasks, such as image classification. To\nimprove the efficiency of CNNs, many CNNs compressing approaches have been\ndeveloped. Low-rank methods approximate the original convolutional kernel with\na sequence of smaller convolutional kernels, which leads to reduced storage and\ntime complexities. In this study, we propose a novel low-rank CNNs compression\nmethod that is based on reduced storage direct tensor ring decomposition\n(RSDTR). The proposed method offers a higher circular mode permutation\nflexibility, and it is characterized by large parameter and FLOPS compression\nrates, while preserving a good classification accuracy of the compressed\nnetwork. The experiments, performed on the CIFAR-10 and ImageNet datasets,\nclearly demonstrate the efficiency of RSDTR in comparison to other\nstate-of-the-art CNNs compression approaches.\n","authors":["Mateusz Gabor","Rafał Zdunek"],"pdf_url":"https://arxiv.org/pdf/2405.10802v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02361v1","updated":"2024-08-05T10:10:01Z","published":"2024-08-05T10:10:01Z","title":"Dialogue Ontology Relation Extraction via Constrained Chain-of-Thought\n  Decoding","summary":"  State-of-the-art task-oriented dialogue systems typically rely on\ntask-specific ontologies for fulfilling user queries. The majority of\ntask-oriented dialogue data, such as customer service recordings, comes without\nontology and annotation. Such ontologies are normally built manually, limiting\nthe application of specialised systems. Dialogue ontology construction is an\napproach for automating that process and typically consists of two steps: term\nextraction and relation extraction. In this work, we focus on relation\nextraction in a transfer learning set-up. To improve the generalisation, we\npropose an extension to the decoding mechanism of large language models. We\nadapt Chain-of-Thought (CoT) decoding, recently developed for reasoning\nproblems, to generative relation extraction. Here, we generate multiple\nbranches in the decoding space and select the relations based on a confidence\nthreshold. By constraining the decoding to ontology terms and relations, we aim\nto decrease the risk of hallucination. We conduct extensive experimentation on\ntwo widely used datasets and find improvements in performance on target\nontology for source fine-tuned and one-shot prompted large language models.\n","authors":["Renato Vukovic","David Arps","Carel van Niekerk","Benjamin Matthias Ruppik","Hsien-Chin Lin","Michael Heck","Milica Gašić"],"pdf_url":"https://arxiv.org/pdf/2408.02361v1.pdf","comment":"Accepted to appear at SIGDIAL 2024. 9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.02357v1","updated":"2024-08-05T10:06:53Z","published":"2024-08-05T10:06:53Z","title":"On the consistent reasoning paradox of intelligence and optimal trust in\n  AI: The power of 'I don't know'","summary":"  We introduce the Consistent Reasoning Paradox (CRP). Consistent reasoning,\nwhich lies at the core of human intelligence, is the ability to handle tasks\nthat are equivalent, yet described by different sentences ('Tell me the time!'\nand 'What is the time?'). The CRP asserts that consistent reasoning implies\nfallibility -- in particular, human-like intelligence in AI necessarily comes\nwith human-like fallibility. Specifically, it states that there are problems,\ne.g. in basic arithmetic, where any AI that always answers and strives to mimic\nhuman intelligence by reasoning consistently will hallucinate (produce wrong,\nyet plausible answers) infinitely often. The paradox is that there exists a\nnon-consistently reasoning AI (which therefore cannot be on the level of human\nintelligence) that will be correct on the same set of problems. The CRP also\nshows that detecting these hallucinations, even in a probabilistic sense, is\nstrictly harder than solving the original problems, and that there are problems\nthat an AI may answer correctly, but it cannot provide a correct logical\nexplanation for how it arrived at the answer. Therefore, the CRP implies that\nany trustworthy AI (i.e., an AI that never answers incorrectly) that also\nreasons consistently must be able to say 'I don't know'. Moreover, this can\nonly be done by implicitly computing a new concept that we introduce, termed\nthe 'I don't know' function -- something currently lacking in modern AI. In\nview of these insights, the CRP also provides a glimpse into the behaviour of\nArtificial General Intelligence (AGI). An AGI cannot be 'almost sure', nor can\nit always explain itself, and therefore to be trustworthy it must be able to\nsay 'I don't know'.\n","authors":["Alexander Bastounis","Paolo Campodonico","Mihaela van der Schaar","Ben Adcock","Anders C. Hansen"],"pdf_url":"https://arxiv.org/pdf/2408.02357v1.pdf","comment":"12 pages and 50 pages of supplementary material, 7 figures"},{"id":"http://arxiv.org/abs/2408.02355v1","updated":"2024-08-05T10:02:33Z","published":"2024-08-05T10:02:33Z","title":"Quantile Regression using Random Forest Proximities","summary":"  Due to the dynamic nature of financial markets, maintaining models that\nproduce precise predictions over time is difficult. Often the goal isn't just\npoint prediction but determining uncertainty. Quantifying uncertainty,\nespecially the aleatoric uncertainty due to the unpredictable nature of market\ndrivers, helps investors understand varying risk levels. Recently, quantile\nregression forests (QRF) have emerged as a promising solution: Unlike most\nbasic quantile regression methods that need separate models for each quantile,\nquantile regression forests estimate the entire conditional distribution of the\ntarget variable with a single model, while retaining all the salient features\nof a typical random forest. We introduce a novel approach to compute quantile\nregressions from random forests that leverages the proximity (i.e., distance\nmetric) learned by the model and infers the conditional distribution of the\ntarget variable. We evaluate the proposed methodology using publicly available\ndatasets and then apply it towards the problem of forecasting the average daily\nvolume of corporate bonds. We show that using quantile regression using Random\nForest proximities demonstrates superior performance in approximating\nconditional target distributions and prediction intervals to the original\nversion of QRF. We also demonstrate that the proposed framework is\nsignificantly more computationally efficient than traditional approaches to\nquantile regressions.\n","authors":["Mingshu Li","Bhaskarjit Sarmah","Dhruv Desai","Joshua Rosaler","Snigdha Bhagat","Philip Sommer","Dhagash Mehta"],"pdf_url":"https://arxiv.org/pdf/2408.02355v1.pdf","comment":"9 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2408.02354v1","updated":"2024-08-05T10:02:29Z","published":"2024-08-05T10:02:29Z","title":"RECE: Reduced Cross-Entropy Loss for Large-Catalogue Sequential\n  Recommenders","summary":"  Scalability is a major challenge in modern recommender systems. In sequential\nrecommendations, full Cross-Entropy (CE) loss achieves state-of-the-art\nrecommendation quality but consumes excessive GPU memory with large item\ncatalogs, limiting its practicality. Using a GPU-efficient locality-sensitive\nhashing-like algorithm for approximating large tensor of logits, this paper\nintroduces a novel RECE (REduced Cross-Entropy) loss. RECE significantly\nreduces memory consumption while allowing one to enjoy the state-of-the-art\nperformance of full CE loss. Experimental results on various datasets show that\nRECE cuts training peak memory usage by up to 12 times compared to existing\nmethods while retaining or exceeding performance metrics of CE loss. The\napproach also opens up new possibilities for large-scale applications in other\ndomains.\n","authors":["Danil Gusak","Gleb Mezentsev","Ivan Oseledets","Evgeny Frolov"],"pdf_url":"https://arxiv.org/pdf/2408.02354v1.pdf","comment":"5 pages, 4 figures, submitted to CIKM'24"},{"id":"http://arxiv.org/abs/2401.13185v2","updated":"2024-08-05T10:01:48Z","published":"2024-01-24T02:16:03Z","title":"Fast Partition-Based Cross-Validation With Centering and Scaling for\n  $\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$","summary":"  We present algorithms that substantially accelerate partition-based\ncross-validation for machine learning models that require matrix products\n$\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$. Our\nalgorithms have applications in model selection for, e.g., principal component\nanalysis (PCA), principal component regression (PCR), ridge regression (RR),\nordinary least squares (OLS), and partial least squares (PLS). Our algorithms\nsupport all combinations of column-wise centering and scaling of $\\mathbf{X}$\nand $\\mathbf{Y}$, and we demonstrate in our accompanying implementation that\nthis adds only a manageable, practical constant over efficient variants without\npreprocessing. We prove the correctness of our algorithms under a fold-based\npartitioning scheme and show that the running time is independent of the number\nof folds; that is, they have the same time complexity as that of computing\n$\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$ and\nspace complexity equivalent to storing $\\mathbf{X}$, $\\mathbf{Y}$,\n$\\mathbf{X}^\\mathbf{T}\\mathbf{X}$, and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$.\nImportantly, unlike alternatives found in the literature, we avoid data leakage\ndue to preprocessing. We achieve these results by eliminating redundant\ncomputations in the overlap between training partitions. Concretely, we show\nhow to manipulate $\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and\n$\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$ using only samples from the validation\npartition to obtain the preprocessed training partition-wise\n$\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$. To our\nknowledge, we are the first to derive correct and efficient cross-validation\nalgorithms for any of the $16$ combinations of column-wise centering and\nscaling, for which we also prove only $12$ give distinct matrix products.\n","authors":["Ole-Christian Galbo Engstrøm","Martin Holm Jensen"],"pdf_url":"https://arxiv.org/pdf/2401.13185v2.pdf","comment":"31 pages, 2 tables, 1 figure, 7 algorithms"},{"id":"http://arxiv.org/abs/2408.02349v1","updated":"2024-08-05T09:54:08Z","published":"2024-08-05T09:54:08Z","title":"Active Sensing of Knee Osteoarthritis Progression with Reinforcement\n  Learning","summary":"  Osteoarthritis (OA) is the most common musculoskeletal disease, which has no\ncure. Knee OA (KOA) is one of the highest causes of disability worldwide, and\nit costs billions of United States dollars to the global community. Prediction\nof KOA progression has been of high interest to the community for years, as it\ncan advance treatment development through more efficient clinical trials and\nimprove patient outcomes through more efficient healthcare utilization.\nExisting approaches for predicting KOA, however, are predominantly static, i.e.\nconsider data from a single time point to predict progression many years into\nthe future, and knee level, i.e. consider progression in a single joint only.\nDue to these and related reasons, these methods fail to deliver the level of\npredictive performance, which is sufficient to result in cost savings and\nbetter patient outcomes. Collecting extensive data from all patients on a\nregular basis could address the issue, but it is limited by the high cost at a\npopulation level. In this work, we propose to go beyond static prediction\nmodels in OA, and bring a novel Active Sensing (AS) approach, designed to\ndynamically follow up patients with the objective of maximizing the number of\ninformative data acquisitions, while minimizing their total cost over a period\nof time. Our approach is based on Reinforcement Learning (RL), and it leverages\na novel reward function designed specifically for AS of disease progression in\nmore than one part of a human body. Our method is end-to-end, relies on\nmulti-modal Deep Learning, and requires no human input at inference time.\nThroughout an exhaustive experimental evaluation, we show that using RL can\nprovide a higher monetary benefit when compared to state-of-the-art baselines.\n","authors":["Khanh Nguyen","Huy Hoang Nguyen","Egor Panfilov","Aleksei Tiulpin"],"pdf_url":"https://arxiv.org/pdf/2408.02349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02346v1","updated":"2024-08-05T09:45:31Z","published":"2024-08-05T09:45:31Z","title":"Exploiting Hankel-Toeplitz Structures for Fast Computation of Kernel\n  Precision Matrices","summary":"  The Hilbert-space Gaussian Process (HGP) approach offers a\nhyperparameter-independent basis function approximation for speeding up\nGaussian Process (GP) inference by projecting the GP onto M basis functions.\nThese properties result in a favorable data-independent $\\mathcal{O}(M^3)$\ncomputational complexity during hyperparameter optimization but require a\ndominating one-time precomputation of the precision matrix costing\n$\\mathcal{O}(NM^2)$ operations. In this paper, we lower this dominating\ncomputational complexity to $\\mathcal{O}(NM)$ with no additional\napproximations. We can do this because we realize that the precision matrix can\nbe split into a sum of Hankel-Toeplitz matrices, each having $\\mathcal{O}(M)$\nunique entries. Based on this realization we propose computing only these\nunique entries at $\\mathcal{O}(NM)$ costs. Further, we develop two theorems\nthat prescribe sufficient conditions for the complexity reduction to hold\ngenerally for a wide range of other approximate GP models, such as the\nVariational Fourier Feature (VFF) approach. The two theorems do this with no\nassumptions on the data and no additional approximations of the GP models\nthemselves. Thus, our contribution provides a pure speed-up of several\nexisting, widely used, GP approximations, without further approximations.\n","authors":["Frida Viset","Anton Kullberg","Frederiek Wesel","Arno Solin"],"pdf_url":"https://arxiv.org/pdf/2408.02346v1.pdf","comment":"Published in Transactions on Machine Learning (TMLR) July 2024"},{"id":"http://arxiv.org/abs/2408.02344v1","updated":"2024-08-05T09:41:34Z","published":"2024-08-05T09:41:34Z","title":"Machine Learning Applications in Medical Prognostics: A Comprehensive\n  Review","summary":"  Machine learning (ML) has revolutionized medical prognostics by integrating\nadvanced algorithms with clinical data to enhance disease prediction, risk\nassessment, and patient outcome forecasting. This comprehensive review\ncritically examines the application of various ML techniques in medical\nprognostics, focusing on their efficacy, challenges, and future directions. The\nmethodologies discussed include Random Forest (RF) for sepsis prediction,\nlogistic regression for cardiovascular risk assessment, Convolutional Neural\nNetworks (CNNs) for cancer detection, and Long Short-Term Memory (LSTM)\nnetworks for predicting clinical deterioration. RF models demonstrate robust\nperformance in handling high-dimensional data and capturing non-linear\nrelationships, making them particularly effective for sepsis prediction.\nLogistic regression remains valuable for its interpretability and ease of use\nin cardiovascular risk assessment. CNNs have shown exceptional accuracy in\ncancer detection, leveraging their ability to learn complex visual patterns\nfrom medical imaging. LSTM networks excel in analyzing temporal data, providing\naccurate predictions of clinical deterioration. The review highlights the\nstrengths and limitations of each technique, the importance of model\ninterpretability, and the challenges of data quality and privacy. Future\nresearch directions include the integration of multi-modal data sources, the\napplication of transfer learning, and the development of continuous learning\nsystems. These advancements aim to enhance the predictive power and clinical\napplicability of ML models, ultimately improving patient outcomes in healthcare\nsettings.\n","authors":["Michael Fascia"],"pdf_url":"https://arxiv.org/pdf/2408.02344v1.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2408.02337v1","updated":"2024-08-05T09:23:49Z","published":"2024-08-05T09:23:49Z","title":"Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR\n  Dataset Construction","summary":"  Advancements in AI and natural language processing have revolutionized\nmachine-human language interactions, with question answering (QA) systems\nplaying a pivotal role. The knowledge base question answering (KBQA) task,\nutilizing structured knowledge graphs (KG), allows for handling extensive\nknowledge-intensive questions. However, a significant gap exists in KBQA\ndatasets, especially for low-resource languages. Many existing construction\npipelines for these datasets are outdated and inefficient in human labor, and\nmodern assisting tools like Large Language Models (LLM) are not utilized to\nreduce the workload. To address this, we have designed and implemented a\nmodern, semi-automated approach for creating datasets, encompassing tasks such\nas KBQA, Machine Reading Comprehension (MRC), and Information Retrieval (IR),\ntailored explicitly for low-resource environments. We executed this pipeline\nand introduced the PUGG dataset, the first Polish KBQA dataset, and novel\ndatasets for MRC and IR. Additionally, we provide a comprehensive\nimplementation, insightful findings, detailed statistics, and evaluation of\nbaseline models.\n","authors":["Albert Sawczyn","Katsiaryna Viarenich","Konrad Wojtasik","Aleksandra Domogała","Marcin Oleksy","Maciej Piasecki","Tomasz Kajdanowicz"],"pdf_url":"https://arxiv.org/pdf/2408.02337v1.pdf","comment":"Accepted for ACL 2024 (findings)"},{"id":"http://arxiv.org/abs/2408.02336v1","updated":"2024-08-05T09:19:52Z","published":"2024-08-05T09:19:52Z","title":"Infusing Environmental Captions for Long-Form Video Language Grounding","summary":"  In this work, we tackle the problem of long-form video-language grounding\n(VLG). Given a long-form video and a natural language query, a model should\ntemporally localize the precise moment that answers the query. Humans can\neasily solve VLG tasks, even with arbitrarily long videos, by discarding\nirrelevant moments using extensive and robust knowledge gained from experience.\nUnlike humans, existing VLG methods are prone to fall into superficial cues\nlearned from small-scale datasets, even when they are within irrelevant frames.\nTo overcome this challenge, we propose EI-VLG, a VLG method that leverages\nricher textual information provided by a Multi-modal Large Language Model\n(MLLM) as a proxy for human experiences, helping to effectively exclude\nirrelevant frames. We validate the effectiveness of the proposed method via\nextensive experiments on a challenging EgoNLQ benchmark.\n","authors":["Hyogun Lee","Soyeon Hong","Mujeen Sung","Jinwoo Choi"],"pdf_url":"https://arxiv.org/pdf/2408.02336v1.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2408.02320v1","updated":"2024-08-05T09:02:24Z","published":"2024-08-05T09:02:24Z","title":"A Sharp Convergence Theory for The Probability Flow ODEs of Diffusion\n  Models","summary":"  Diffusion models, which convert noise into new data instances by learning to\nreverse a diffusion process, have become a cornerstone in contemporary\ngenerative modeling. In this work, we develop non-asymptotic convergence theory\nfor a popular diffusion-based sampler (i.e., the probability flow ODE sampler)\nin discrete time, assuming access to $\\ell_2$-accurate estimates of the (Stein)\nscore functions. For distributions in $\\mathbb{R}^d$, we prove that\n$d/\\varepsilon$ iterations -- modulo some logarithmic and lower-order terms --\nare sufficient to approximate the target distribution to within $\\varepsilon$\ntotal-variation distance. This is the first result establishing nearly linear\ndimension-dependency (in $d$) for the probability flow ODE sampler. Imposing\nonly minimal assumptions on the target data distribution (e.g., no smoothness\nassumption is imposed), our results also characterize how $\\ell_2$ score\nestimation errors affect the quality of the data generation processes. In\ncontrast to prior works, our theory is developed based on an elementary yet\nversatile non-asymptotic approach without the need of resorting to SDE and ODE\ntoolboxes.\n","authors":["Gen Li","Yuting Wei","Yuejie Chi","Yuxin Chen"],"pdf_url":"https://arxiv.org/pdf/2408.02320v1.pdf","comment":"This manuscript presents improved theory for probability flow ODEs\n  compared to its earlier version arXiv:2306.09251"},{"id":"http://arxiv.org/abs/2308.09605v2","updated":"2024-08-05T08:53:12Z","published":"2023-08-18T14:58:23Z","title":"Solving PDEs on Spheres with Physics-Informed Convolutional Neural\n  Networks","summary":"  Physics-informed neural networks (PINNs) have been demonstrated to be\nefficient in solving partial differential equations (PDEs) from a variety of\nexperimental perspectives. Some recent studies have also proposed PINN\nalgorithms for PDEs on surfaces, including spheres. However, theoretical\nunderstanding of the numerical performance of PINNs, especially PINNs on\nsurfaces or manifolds, is still lacking. In this paper, we establish rigorous\nanalysis of the physics-informed convolutional neural network (PICNN) for\nsolving PDEs on the sphere. By using and improving the latest approximation\nresults of deep convolutional neural networks and spherical harmonic analysis,\nwe prove an upper bound for the approximation error with respect to the Sobolev\nnorm. Subsequently, we integrate this with innovative localization complexity\nanalysis to establish fast convergence rates for PICNN. Our theoretical results\nare also confirmed and supplemented by our experiments. In light of these\nfindings, we explore potential strategies for circumventing the curse of\ndimensionality that arises when solving high-dimensional PDEs.\n","authors":["Guanhang Lei","Zhen Lei","Lei Shi","Chenyu Zeng","Ding-Xuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2308.09605v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02313v1","updated":"2024-08-05T08:46:46Z","published":"2024-08-05T08:46:46Z","title":"A Lean Transformer Model for Dynamic Malware Analysis and Detection","summary":"  Malware is a fast-growing threat to the modern computing world and existing\nlines of defense are not efficient enough to address this issue. This is mainly\ndue to the fact that many prevention solutions rely on signature-based\ndetection methods that can easily be circumvented by hackers. Therefore, there\nis a recurrent need for behavior-based analysis where a suspicious file is ran\nin a secured environment and its traces are collected to reports for analysis.\nPrevious works have shown some success leveraging Neural Networks and API calls\nsequences extracted from these execution reports.\n  Recently, Large Language Models and Generative AI have demonstrated\nimpressive capabilities mainly in Natural Language Processing tasks and\npromising applications in the cybersecurity field for both attackers and\ndefenders.\n  In this paper, we design an Encoder-Only model, based on the Transformers\narchitecture, to detect malicious files, digesting their API call sequences\ncollected by an execution emulation solution. We are also limiting the size of\nthe model architecture and the number of its parameters since it is often\nconsidered that Large Language Models may be overkill for specific tasks such\nas the one we are dealing with hereafter. In addition to achieving decent\ndetection results, this approach has the advantage of reducing our carbon\nfootprint by limiting training and inference times and facilitating technical\noperations with less hardware requirements.\n  We also carry out some analysis of our results and highlight the limits and\npossible improvements when using Transformers to analyze malicious files.\n","authors":["Tony Quertier","Benjamin Marais","Grégoire Barrué","Stéphane Morucci","Sévan Azé","Sébastien Salladin"],"pdf_url":"https://arxiv.org/pdf/2408.02313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02312v1","updated":"2024-08-05T08:45:50Z","published":"2024-08-05T08:45:50Z","title":"Optimization of Iterative Blind Detection based on Expectation\n  Maximization and Belief Propagation","summary":"  We study iterative blind symbol detection for block-fading linear\ninter-symbol interference channels. Based on the factor graph framework, we\ndesign a joint channel estimation and detection scheme that combines the\nexpectation maximization (EM) algorithm and the ubiquitous belief propagation\n(BP) algorithm. Interweaving the iterations of both schemes significantly\nreduces the EM algorithm's computational burden while retaining its excellent\nperformance. To this end, we apply simple yet effective model-based learning\nmethods to find a suitable parameter update schedule by introducing momentum in\nboth the EM parameter updates as well as in the BP message passing. Numerical\nsimulations verify that the proposed method can learn efficient schedules that\ngeneralize well and even outperform coherent BP detection in high\nsignal-to-noise scenarios.\n","authors":["Luca Schmid","Tomer Raviv","Nir Shlezinger","Laurent Schmalen"],"pdf_url":"https://arxiv.org/pdf/2408.02312v1.pdf","comment":"Accepted for presentation at Asilomar Conference on Signals, Systems,\n  and Computers 2024"},{"id":"http://arxiv.org/abs/2408.02310v1","updated":"2024-08-05T08:41:07Z","published":"2024-08-05T08:41:07Z","title":"On the Robustness of Malware Detectors to Adversarial Samples","summary":"  Adversarial examples add imperceptible alterations to inputs with the\nobjective to induce misclassification in machine learning models. They have\nbeen demonstrated to pose significant challenges in domains like image\nclassification, with results showing that an adversarially perturbed image to\nevade detection against one classifier is most likely transferable to other\nclassifiers. Adversarial examples have also been studied in malware analysis.\nUnlike images, program binaries cannot be arbitrarily perturbed without\nrendering them non-functional. Due to the difficulty of crafting adversarial\nprogram binaries, there is no consensus on the transferability of adversarially\nperturbed programs to different detectors. In this work, we explore the\nrobustness of malware detectors against adversarially perturbed malware. We\ninvestigate the transferability of adversarial attacks developed against one\ndetector, against other machine learning-based malware detectors, and code\nsimilarity techniques, specifically, locality sensitive hashing-based\ndetectors. Our analysis reveals that adversarial program binaries crafted for\none detector are generally less effective against others. We also evaluate an\nensemble of detectors and show that they can potentially mitigate the impact of\nadversarial program binaries. Finally, we demonstrate that substantial program\nchanges made to evade detection may result in the transformation technique\nbeing identified, implying that the adversary must make minimal changes to the\nprogram binary.\n","authors":["Muhammad Salman","Benjamin Zi Hao Zhao","Hassan Jameel Asghar","Muhammad Ikram","Sidharth Kaushik","Mohamed Ali Kaafar"],"pdf_url":"https://arxiv.org/pdf/2408.02310v1.pdf","comment":"This is the full version of the paper with the same title to appear\n  in the proceedings of the 2024 Workshop on Security and Artificial\n  Intelligence (SECAI 2024)"},{"id":"http://arxiv.org/abs/2408.02301v1","updated":"2024-08-05T08:23:59Z","published":"2024-08-05T08:23:59Z","title":"Network Fission Ensembles for Low-Cost Self-Ensembles","summary":"  Recent ensemble learning methods for image classification have been shown to\nimprove classification accuracy with low extra cost. However, they still\nrequire multiple trained models for ensemble inference, which eventually\nbecomes a significant burden when the model size increases. In this paper, we\npropose a low-cost ensemble learning and inference, called Network Fission\nEnsembles (NFE), by converting a conventional network itself into a multi-exit\nstructure. Starting from a given initial network, we first prune some of the\nweights to reduce the training burden. We then group the remaining weights into\nseveral sets and create multiple auxiliary paths using each set to construct\nmulti-exits. We call this process Network Fission. Through this, multiple\noutputs can be obtained from a single network, which enables ensemble learning.\nSince this process simply changes the existing network structure to multi-exits\nwithout using additional networks, there is no extra computational burden for\nensemble learning and inference. Moreover, by learning from multiple losses of\nall exits, the multi-exits improve performance via regularization, and high\nperformance can be achieved even with increased network sparsity. With our\nsimple yet effective method, we achieve significant improvement compared to\nexisting ensemble methods. The code is available at\nhttps://github.com/hjdw2/NFE.\n","authors":["Hojung Lee","Jong-Seok Lee"],"pdf_url":"https://arxiv.org/pdf/2408.02301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10504v2","updated":"2024-08-05T08:22:56Z","published":"2024-02-16T08:27:55Z","title":"On the resilience of the quadratic Littlewood-Offord problem","summary":"  We study the statistical resilience of the anti-concentration properties of\nRademacher polynomials in face of adversarial deterministic noise taking the\nform of sign-flips. Given a multilinear polynomial $f:\\mathbb{R}^n \\to\n\\mathbb{R}$ and a Rademacher vector $\\boldsymbol{\\xi} \\in \\{\\pm 1\\}^n$ (with\nindependent entries), our results provide probabilistic lower bound estimations\non the number of sign-flips that $\\boldsymbol{\\xi}$ can sustain without\n``inflating\" the atom probability $\\sup_{x \\in \\mathbb{R} }\n\\mathbb{P}\\{f(\\boldsymbol{\\xi}) = x\\}$ otherwise resulting in an adversarially\nbiased distribution. Special emphasis is put on bilinear and quadratic forms,\nfor which strengthened estimates are attained. From a computational\nperspective, our results in this venue are instance-bound in such a way that\nallows for an efficient computation of the statistical resilience guarantees\nfrom the quadratic polynomial itself directly. All of our probabilistic lower\nbound resilience guarantees are asymptotically tight.\n  On route, we provide a short proof for a new small-ball probability estimate\nfitting Rademacher multilinear polynomials $f: \\mathbb{R}^n \\to \\mathbb{R}$\nremoveing a polylog-factor from the classical Meka-Nguyen-Vu bound provided the\ncoefficients are independent of $n$ (dimension-free, hereafter). This removal\nwas conjectured to be possible by Meka-Nguyen-Vu regardless of our assumption.\nBilinear Rademacher forms with dimension-free coefficients arise naturally in\nCombinatorics and specifically in the dense case of the edge-statistics\nconjecture posed by Alon, Hefetz, Krivelevich, and Tyomkyn. This case of the\nconjecture was resolved by Kwan and Sauermann. Replacing the appeal to the\nMeka-Nguyen-Vu classical bound in the work of Kwan, Sudakov, and Tran with our\nshortly proved result attains an additional proof of the dense case of the\nedge-statistics conjecture.\n","authors":["Elad Aigner-Horev","Daniel Rosenberg","Roi Weiss"],"pdf_url":"https://arxiv.org/pdf/2402.10504v2.pdf","comment":"Numerous changes from the last version: 1. An oversight in the proof\n  fixed. 2. Added treatment of high degree polynomials 3. New results added"},{"id":"http://arxiv.org/abs/2408.02298v1","updated":"2024-08-05T08:14:32Z","published":"2024-08-05T08:14:32Z","title":"Backward Compatibility in Attributive Explanation and Enhanced Model\n  Training Method","summary":"  Model update is a crucial process in the operation of ML/AI systems. While\nupdating a model generally enhances the average prediction performance, it also\nsignificantly impacts the explanations of predictions. In real-world\napplications, even minor changes in explanations can have detrimental\nconsequences. To tackle this issue, this paper introduces BCX, a quantitative\nmetric that evaluates the backward compatibility of feature attribution\nexplanations between pre- and post-update models. BCX utilizes practical\nagreement metrics to calculate the average agreement between the explanations\nof pre- and post-update models, specifically among samples on which both models\naccurately predict. In addition, we propose BCXR, a BCX-aware model training\nmethod by designing surrogate losses which theoretically lower bounds agreement\nscores. Furthermore, we present a universal variant of BCXR that improves all\nagreement metrics, utilizing L2 distance among the explanations of the models.\nTo validate our approach, we conducted experiments on eight real-world\ndatasets, demonstrating that BCXR achieves superior trade-offs between\npredictive performances and BCX scores, showcasing the effectiveness of our\nBCXR methods.\n","authors":["Ryuta Matsuno"],"pdf_url":"https://arxiv.org/pdf/2408.02298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02296v1","updated":"2024-08-05T08:14:04Z","published":"2024-08-05T08:14:04Z","title":"Heart Rate and its Variability from Short-term ECG Recordings as\n  Biomarkers for Detecting Mild Cognitive Impairment in Indian Population","summary":"  Alterations in Heart Rate (HR) and Heart Rate Variability (HRV) can reflect\nautonomic dysfunction associated with neurodegeneration. We investigate the\ninfluence of Mild Cognitive Impairment (MCI) on HR and its variability measures\nin the Indian population by designing a complete signal processing pipeline to\ndetect the R-wave peaks and compute HR and HRV features from ECG recordings of\n10 seconds, for point-of-care applications. The study cohort involves 297 urban\nparticipants, among which 48.48% are male and 51.51% are female. From the\nAddenbrooke's Cognitive Examination-III (ACE-III), MCI is detected in 19.19% of\nparticipants and the rest, 80.8% of them are cognitively healthy. Statistical\nfeatures like central tendency (mean and root mean square (RMS) of the\nNormal-to-Normal (NN) intervals) and dispersion (standard deviation (SD) of all\nNN intervals (SDNN) and root mean square of successive differences of NN\nintervals (RMSSD)) of beat-to-beat intervals are computed. The Wilcoxon rank\nsum test reveals that mean of NN intervals (p = 0.0021), the RMS of NN\nintervals (p = 0.0014), the SDNN (p = 0.0192) and the RMSSD (p = 0.0206) values\ndiffer significantly between MCI and non-MCI classes, for a level of\nsignificance, 0.05. Machine learning classifiers like, Support Vector Machine\n(SVM), Discriminant Analysis (DA) and Naive Bayes (NB) driven by mean NN\nintervals, RMS, SDNN and RMSSD, show a high accuracy of 80.80% on each\nindividual feature input. Individuals with MCI are observed to have\ncomparatively higher HR than healthy subjects. HR and its variability can be\nconsidered as potential biomarkers for detecting MCI.\n","authors":["Anjo Xavier","Sneha Noble","Justin Joseph","Thomas Gregor Issac"],"pdf_url":"https://arxiv.org/pdf/2408.02296v1.pdf","comment":"Nil"},{"id":"http://arxiv.org/abs/2408.02295v1","updated":"2024-08-05T08:12:25Z","published":"2024-08-05T08:12:25Z","title":"Generalized Gaussian Temporal Difference Error For Uncertainty-aware\n  Reinforcement Learning","summary":"  Conventional uncertainty-aware temporal difference (TD) learning methods\noften rely on simplistic assumptions, typically including a zero-mean Gaussian\ndistribution for TD errors. Such oversimplification can lead to inaccurate\nerror representations and compromised uncertainty estimation. In this paper, we\nintroduce a novel framework for generalized Gaussian error modeling in deep\nreinforcement learning, applicable to both discrete and continuous control\nsettings. Our framework enhances the flexibility of error distribution modeling\nby incorporating higher-order moments, particularly kurtosis, thereby improving\nthe estimation and mitigation of data-dependent noise, i.e., aleatoric\nuncertainty. We examine the influence of the shape parameter of the generalized\nGaussian distribution (GGD) on aleatoric uncertainty and provide a closed-form\nexpression that demonstrates an inverse relationship between uncertainty and\nthe shape parameter. Additionally, we propose a theoretically grounded\nweighting scheme to fully leverage the GGD. To address epistemic uncertainty,\nwe enhance the batch inverse variance weighting by incorporating bias reduction\nand kurtosis considerations, resulting in improved robustness. Extensive\nexperimental evaluations using policy gradient algorithms demonstrate the\nconsistent efficacy of our method, showcasing significant performance\nimprovements.\n","authors":["Seyeon Kim","Joonhun Lee","Namhoon Cho","Sungjun Han","Seungeon Baek"],"pdf_url":"https://arxiv.org/pdf/2408.02295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.16458v2","updated":"2024-08-05T07:59:19Z","published":"2024-01-29T10:11:05Z","title":"Credit Risk Meets Large Language Models: Building a Risk Indicator from\n  Loan Descriptions in P2P Lending","summary":"  Peer-to-peer (P2P) lending has emerged as a distinctive financing mechanism,\nlinking borrowers with lenders through online platforms. However, P2P lending\nfaces the challenge of information asymmetry, as lenders often lack sufficient\ndata to assess the creditworthiness of borrowers. This paper proposes a novel\napproach to address this issue by leveraging the textual descriptions provided\nby borrowers during the loan application process. Our methodology involves\nprocessing these textual descriptions using a Large Language Model (LLM), a\npowerful tool capable of discerning patterns and semantics within the text.\nTransfer learning is applied to adapt the LLM to the specific task at hand.\n  Our results derived from the analysis of the Lending Club dataset show that\nthe risk score generated by BERT, a widely used LLM, significantly improves the\nperformance of credit risk classifiers. However, the inherent opacity of\nLLM-based systems, coupled with uncertainties about potential biases,\nunderscores critical considerations for regulatory frameworks and engenders\ntrust-related concerns among end-users, opening new avenues for future research\nin the dynamic landscape of P2P lending and artificial intelligence.\n","authors":["Mario Sanz-Guerrero","Javier Arroyo"],"pdf_url":"https://arxiv.org/pdf/2401.16458v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09780v2","updated":"2024-08-05T07:56:33Z","published":"2024-02-15T08:09:17Z","title":"TinyCL: An Efficient Hardware Architecture for Continual Learning on\n  Autonomous Systems","summary":"  The Continuous Learning (CL) paradigm consists of continuously evolving the\nparameters of the Deep Neural Network (DNN) model to progressively learn to\nperform new tasks without reducing the performance on previous tasks, i.e.,\navoiding the so-called catastrophic forgetting. However, the DNN parameter\nupdate in CL-based autonomous systems is extremely resource-hungry. The\nexisting DNN accelerators cannot be directly employed in CL because they only\nsupport the execution of the forward propagation. Only a few prior\narchitectures execute the backpropagation and weight update, but they lack the\ncontrol and management for CL. Towards this, we design a hardware architecture,\nTinyCL, to perform CL on resource-constrained autonomous systems. It consists\nof a processing unit that executes both forward and backward propagation, and a\ncontrol unit that manages memory-based CL workload. To minimize the memory\naccesses, the sliding window of the convolutional layer moves in a snake-like\nfashion. Moreover, the Multiply-and-Accumulate units can be reconfigured at\nruntime to execute different operations. As per our knowledge, our proposed\nTinyCL represents the first hardware accelerator that executes CL on autonomous\nsystems. We synthesize the complete TinyCL architecture in a 65 nm CMOS\ntechnology node with the conventional ASIC design flow. It executes 1 epoch of\ntraining on a Conv + ReLU + Dense model on the CIFAR10 dataset in 1.76 s, while\n1 training epoch of the same model using an Nvidia Tesla P100 GPU takes 103 s,\nthus achieving a 58x speedup, consuming 86 mW in a 4.74 mm2 die.\n","authors":["Eugenio Ressa","Alberto Marchisio","Maurizio Martina","Guido Masera","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2402.09780v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.02963v3","updated":"2024-08-05T07:55:34Z","published":"2022-06-07T01:49:22Z","title":"Confidence-aware Self-Semantic Distillation on Knowledge Graph Embedding","summary":"  Knowledge Graph Embedding (KGE), which projects entities and relations into\ncontinuous vector spaces, has garnered significant attention. Although\nhigh-dimensional KGE methods offer better performance, they come at the expense\nof significant computation and memory overheads. Decreasing embedding\ndimensions significantly deteriorates model performance. While several recent\nefforts utilize knowledge distillation or non-Euclidean representation learning\nto augment the effectiveness of low-dimensional KGE, they either necessitate a\npre-trained high-dimensional teacher model or involve complex non-Euclidean\noperations, thereby incurring considerable additional computational costs. To\naddress this, this work proposes Confidence-aware Self-Knowledge Distillation\n(CSD) that learns from the model itself to enhance KGE in a low-dimensional\nspace. Specifically, CSD extracts knowledge from embeddings in previous\niterations, which would be utilized to supervise the learning of the model in\nthe next iterations. Moreover, a specific semantic module is developed to\nfilter reliable knowledge by estimating the confidence of previously learned\nembeddings. This straightforward strategy bypasses the need for time-consuming\npre-training of teacher models and can be integrated into various KGE methods\nto improve their performance. Our comprehensive experiments on six KGE\nbackbones and four datasets underscore the effectiveness of the proposed CSD.\n","authors":["Yichen Liu","Jiawei Chen","Defang Chen","Zhehui Zhou","Yan Feng","Can Wang"],"pdf_url":"https://arxiv.org/pdf/2206.02963v3.pdf","comment":"CIKM 2024"},{"id":"http://arxiv.org/abs/2408.02280v1","updated":"2024-08-05T07:30:18Z","published":"2024-08-05T07:30:18Z","title":"Hardware Aware Ensemble Selection for Balancing Predictive Accuracy and\n  Cost","summary":"  Automated Machine Learning (AutoML) significantly simplifies the deployment\nof machine learning models by automating tasks from data preprocessing to model\nselection to ensembling. AutoML systems for tabular data often employ post hoc\nensembling, where multiple models are combined to improve predictive accuracy.\nThis typically results in longer inference times, a major limitation in\npractical deployments. Addressing this, we introduce a hardware-aware ensemble\nselection approach that integrates inference time into post hoc ensembling. By\nleveraging an existing framework for ensemble selection with quality diversity\noptimization, our method evaluates ensemble candidates for their predictive\naccuracy and hardware efficiency. This dual focus allows for a balanced\nconsideration of accuracy and operational efficiency. Thus, our approach\nenables practitioners to choose from a Pareto front of accurate and efficient\nensembles. Our evaluation using 83 classification datasets shows that our\napproach sustains competitive accuracy and can significantly improve ensembles'\noperational efficiency. The results of this study provide a foundation for\nextending these principles to additional hardware constraints, setting the\nstage for the development of more resource-efficient AutoML systems.\n","authors":["Jannis Maier","Felix Möller","Lennart Purucker"],"pdf_url":"https://arxiv.org/pdf/2408.02280v1.pdf","comment":"Accepted at Third International Conference on Automated Machine\n  Learning (AutoML 2024), Workshop Track; for code, see\n  https://github.com/Atraxus/HA-ES"},{"id":"http://arxiv.org/abs/2408.02279v1","updated":"2024-08-05T07:26:47Z","published":"2024-08-05T07:26:47Z","title":"DRFormer: Multi-Scale Transformer Utilizing Diverse Receptive Fields for\n  Long Time-Series Forecasting","summary":"  Long-term time series forecasting (LTSF) has been widely applied in finance,\ntraffic prediction, and other domains. Recently, patch-based transformers have\nemerged as a promising approach, segmenting data into sub-level patches that\nserve as input tokens. However, existing methods mostly rely on predetermined\npatch lengths, necessitating expert knowledge and posing challenges in\ncapturing diverse characteristics across various scales. Moreover, time series\ndata exhibit diverse variations and fluctuations across different temporal\nscales, which traditional approaches struggle to model effectively. In this\npaper, we propose a dynamic tokenizer with a dynamic sparse learning algorithm\nto capture diverse receptive fields and sparse patterns of time series data. In\norder to build hierarchical receptive fields, we develop a multi-scale\nTransformer model, coupled with multi-scale sequence extraction, capable of\ncapturing multi-resolution features. Additionally, we introduce a group-aware\nrotary position encoding technique to enhance intra- and inter-group position\nawareness among representations across different temporal scales. Our proposed\nmodel, named DRFormer, is evaluated on various real-world datasets, and\nexperimental results demonstrate its superiority compared to existing methods.\nOur code is available at: https://github.com/ruixindingECNU/DRFormer.\n","authors":["Ruixin Ding","Yuqi Chen","Yu-Ting Lan","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.02279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17900v4","updated":"2024-08-05T07:06:14Z","published":"2024-07-25T09:42:24Z","title":"The Power of Combining Data and Knowledge: GPT-4o is an Effective\n  Interpreter of Machine Learning Models in Predicting Lymph Node Metastasis of\n  Lung Cancer","summary":"  Lymph node metastasis (LNM) is a crucial factor in determining the initial\ntreatment for patients with lung cancer, yet accurate preoperative diagnosis of\nLNM remains challenging. Recently, large language models (LLMs) have garnered\nsignificant attention due to their remarkable text generation capabilities.\nLeveraging the extensive medical knowledge learned from vast corpora, LLMs can\nestimate probabilities for clinical problems, though their performance has\nhistorically been inferior to data-driven machine learning models. In this\npaper, we propose a novel ensemble method that combines the medical knowledge\nacquired by LLMs with the latent patterns identified by machine learning models\nto enhance LNM prediction performance. Initially, we developed machine learning\nmodels using patient data. We then designed a prompt template to integrate the\npatient data with the predicted probability from the machine learning model.\nSubsequently, we instructed GPT-4o, the most advanced LLM developed by OpenAI,\nto estimate the likelihood of LNM based on patient data and then adjust the\nestimate using the machine learning output. Finally, we collected three outputs\nfrom the GPT-4o using the same prompt and ensembled these results as the final\nprediction. Using the proposed method, our models achieved an AUC value of\n0.778 and an AP value of 0.426 for LNM prediction, significantly improving\npredictive performance compared to baseline machine learning models. The\nexperimental results indicate that GPT-4o can effectively leverage its medical\nknowledge and the probabilities predicted by machine learning models to achieve\nmore accurate LNM predictions. These findings demonstrate that LLMs can perform\nwell in clinical risk prediction tasks, offering a new paradigm for integrating\nmedical knowledge and patient data in clinical predictions.\n","authors":["Danqing Hu","Bing Liu","Xiaofeng Zhu","Nan Wu"],"pdf_url":"https://arxiv.org/pdf/2407.17900v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02266v1","updated":"2024-08-05T06:47:32Z","published":"2024-08-05T06:47:32Z","title":"One-Shot Collaborative Data Distillation","summary":"  Large machine-learning training datasets can be distilled into small\ncollections of informative synthetic data samples. These synthetic sets support\nefficient model learning and reduce the communication cost of data sharing.\nThus, high-fidelity distilled data can support the efficient deployment of\nmachine learning applications in distributed network environments. A naive way\nto construct a synthetic set in a distributed environment is to allow each\nclient to perform local data distillation and to merge local distillations at a\ncentral server. However, the quality of the resulting set is impaired by\nheterogeneity in the distributions of the local data held by clients. To\novercome this challenge, we introduce the first collaborative data distillation\ntechnique, called CollabDM, which captures the global distribution of the data\nand requires only a single round of communication between client and server.\nOur method outperforms the state-of-the-art one-shot learning method on skewed\ndata in distributed learning environments. We also show the promising practical\nbenefits of our method when applied to attack detection in 5G networks.\n","authors":["Rayne Holland","Chandra Thapa","Sarah Ali Siddiqui","Wei Shao","Seyit Camtepe"],"pdf_url":"https://arxiv.org/pdf/2408.02266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02247v1","updated":"2024-08-05T05:41:16Z","published":"2024-08-05T05:41:16Z","title":"Contrastive Learning and Abstract Concepts: The Case of Natural Numbers","summary":"  Contrastive Learning (CL) has been successfully applied to classification and\nother downstream tasks related to concrete concepts, such as objects contained\nin the ImageNet dataset. No attempts seem to have been made so far in applying\nthis promising scheme to more abstract entities. A prominent example of these\ncould be the concept of (discrete) Quantity. CL can be frequently interpreted\nas a self-supervised scheme guided by some profound and ubiquitous conservation\nprinciple (e.g. conservation of identity in object classification tasks). In\nthis introductory work we apply a suitable conservation principle to the\nsemi-abstract concept of natural numbers by which discrete quantities can be\nestimated or predicted. We experimentally show, by means of a toy problem, that\ncontrastive learning can be trained to count at a glance with high accuracy\nboth at human as well as at super-human ranges.. We compare this with the\nresults of a trained-to-count at a glance supervised learning (SL) neural\nnetwork scheme of similar architecture. We show that both schemes exhibit\nsimilar good performance on baseline experiments, where the distributions of\nthe training and testing stages are equal. Importantly, we demonstrate that in\nsome generalization scenarios, where training and testing distributions differ,\nCL boasts more robust and much better error performance.\n","authors":["Daniel N. Nissani"],"pdf_url":"https://arxiv.org/pdf/2408.02247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02242v1","updated":"2024-08-05T05:27:19Z","published":"2024-08-05T05:27:19Z","title":"Methods to improve run time of hydrologic models: opportunities and\n  challenges in the machine learning era","summary":"  The application of Machine Learning (ML) to hydrologic modeling is fledgling.\nIts applicability to capture the dependencies on watersheds to forecast better\nwithin a short period is fascinating. One of the key reasons to adopt ML\nalgorithms over physics-based models is its computational efficiency advantage\nand flexibility to work with various data sets. The diverse applications,\nparticularly in emergency response and expanding over a large scale, demand the\nhydrological model in a short time and make researchers adopt data-driven\nmodeling approaches unhesitatingly. In this work, in the era of ML and deep\nlearning (DL), how it can help to improve the overall run time of physics-based\nmodel and potential constraints that should be addressed while modeling. This\npaper covers the opportunities and challenges of adopting ML for hydrological\nmodeling and subsequently how it can help to improve the simulation time of\nphysics-based models and future works that should be addressed.\n","authors":["Supath Dhital"],"pdf_url":"https://arxiv.org/pdf/2408.02242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.03560v3","updated":"2024-08-05T04:35:48Z","published":"2022-12-07T10:25:59Z","title":"SeqLink: A Robust Neural-ODE Architecture for Modelling Partially\n  Observed Time Series","summary":"  Ordinary Differential Equations (ODE) based models have become popular as\nfoundation models for solving many time series problems. Combining neural ODEs\nwith traditional RNN models has provided the best representation for irregular\ntime series. However, ODE-based models typically require the trajectory of\nhidden states to be defined based on either the initial observed value or the\nmost recent observation, raising questions about their effectiveness when\ndealing with longer sequences and extended time intervals. In this article, we\nexplore the behaviour of the ODE models in the context of time series data with\nvarying degrees of sparsity. We introduce SeqLink, an innovative neural\narchitecture designed to enhance the robustness of sequence representation.\nUnlike traditional approaches that solely rely on the hidden state generated\nfrom the last observed value, SeqLink leverages ODE latent representations\nderived from multiple data samples, enabling it to generate robust data\nrepresentations regardless of sequence length or data sparsity level. The core\nconcept behind our model is the definition of hidden states for the unobserved\nvalues based on the relationships between samples (links between sequences).\nThrough extensive experiments on partially observed synthetic and real-world\ndatasets, we demonstrate that SeqLink improves the modelling of intermittent\ntime series, consistently outperforming state-of-the-art approaches.\n","authors":["Futoon M. Abushaqra","Hao Xue","Yongli Ren","Flora D. Salim"],"pdf_url":"https://arxiv.org/pdf/2212.03560v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02223v1","updated":"2024-08-05T03:54:52Z","published":"2024-08-05T03:54:52Z","title":"Large Language Model Aided QoS Prediction for Service Recommendation","summary":"  Large language models (LLMs) have seen rapid improvement in the recent years,\nand are used in a wider range of applications. After being trained on large\ntext corpus, LLMs obtain the capability of extracting rich features from\ntextual data. Such capability is potentially useful for the web service\nrecommendation task, where the web users and services have intrinsic attributes\nthat can be described using natural language sentences and are useful for\nrecommendation. In this paper, we explore the possibility and practicality of\nusing LLMs for web service recommendation. We propose the large language model\naided QoS prediction (llmQoS) model, which use LLMs to extract useful\ninformation from attributes of web users and services via descriptive\nsentences. This information is then used in combination with the QoS values of\nhistorical interactions of users and services, to predict QoS values for any\ngiven user-service pair. Our proposed model is shown to overcome the data\nsparsity issue for QoS prediction. We show that on the WSDream dataset, llmQoS\noutperforms comparable baseline models consistently.\n","authors":["Huiying Liu","Zekun Zhang","Qilin Wu","Yiwen Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.02223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.00861v2","updated":"2024-08-05T03:47:54Z","published":"2022-06-02T03:48:29Z","title":"Dynamic Structure Estimation from Bandit Feedback using Nonvanishing\n  Exponential Sums","summary":"  This work tackles the dynamic structure estimation problems for periodically\nbehaved discrete dynamical system in the Euclidean space. We assume the\nobservations become sequentially available in a form of bandit feedback\ncontaminated by a sub-Gaussian noise. Under such fairly general assumptions on\nthe noise distribution, we carefully identify a set of recoverable information\nof periodic structures. Our main results are the (computation and sample)\nefficient algorithms that exploit asymptotic behaviors of exponential sums to\neffectively average out the noise effect while preventing the information to be\nestimated from vanishing. In particular, the novel use of the Weyl sum, a\nvariant of exponential sums, allows us to extract spectrum information for\nlinear systems. We provide sample complexity bounds for our algorithms, and we\nexperimentally validate our theoretical claims on simulations of toy examples,\nincluding Cellular Automata.\n","authors":["Motoya Ohnishi","Isao Ishikawa","Yuko Kuroki","Masahiro Ikeda"],"pdf_url":"https://arxiv.org/pdf/2206.00861v2.pdf","comment":"35 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.02217v1","updated":"2024-08-05T03:38:38Z","published":"2024-08-05T03:38:38Z","title":"Climate-Driven Doubling of Maize Loss Probability in U.S. Crop\n  Insurance: Spatiotemporal Prediction and Possible Policy Responses","summary":"  Climate change not only threatens agricultural producers but also strains\nfinancial institutions. These important food system actors include government\nentities tasked with both insuring grower livelihoods and supporting response\nto continued global warming. We use an artificial neural network to predict\nfuture maize yields in the U.S. Corn Belt, finding alarming changes to\ninstitutional risk exposure within the Federal Crop Insurance Program.\nSpecifically, our machine learning method anticipates more frequent and more\nsevere yield losses that would result in the annual probability of Yield\nProtection (YP) claims to more than double at mid-century relative to\nsimulations without continued climate change. Furthermore, our dual finding of\nrelatively unchanged average yields paired with decreasing yield stability\nreveals targeted opportunities to adjust coverage formulas to include\nvariability. This important structural shift may help regulators support grower\nadaptation to continued climate change by recognizing the value of\nrisk-reducing strategies such as regenerative agriculture. Altogether, paired\nwith open source interactive tools for deeper investigation, our risk profile\nsimulations fill an actionable gap in current understanding, bridging granular\nhistoric yield estimation and climate-informed prediction of future\ninsurer-relevant loss.\n","authors":["A Samuel Pottinger","Lawson Connor","Brookie Guzder-Williams","Maya Weltman-Fahs","Timothy Bowles"],"pdf_url":"https://arxiv.org/pdf/2408.02217v1.pdf","comment":"24 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.00657v2","updated":"2024-08-05T03:25:01Z","published":"2024-08-01T15:46:22Z","title":"Disentangling Dense Embeddings with Sparse Autoencoders","summary":"  Sparse autoencoders (SAEs) have shown promise in extracting interpretable\nfeatures from complex neural networks. We present one of the first applications\nof SAEs to dense text embeddings from large language models, demonstrating\ntheir effectiveness in disentangling semantic concepts. By training SAEs on\nembeddings of over 420,000 scientific paper abstracts from computer science and\nastronomy, we show that the resulting sparse representations maintain semantic\nfidelity while offering interpretability. We analyse these learned features,\nexploring their behaviour across different model capacities and introducing a\nnovel method for identifying ``feature families'' that represent related\nconcepts at varying levels of abstraction. To demonstrate the practical utility\nof our approach, we show how these interpretable features can be used to\nprecisely steer semantic search, allowing for fine-grained control over query\nsemantics. This work bridges the gap between the semantic richness of dense\nembeddings and the interpretability of sparse representations. We open source\nour embeddings, trained sparse autoencoders, and interpreted features, as well\nas a web app for exploring them.\n","authors":["Charles O'Neill","Christine Ye","Kartheik Iyer","John F. Wu"],"pdf_url":"https://arxiv.org/pdf/2408.00657v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02903v4","updated":"2024-08-05T03:24:09Z","published":"2023-10-04T15:42:23Z","title":"FroSSL: Frobenius Norm Minimization for Efficient Multiview\n  Self-Supervised Learning","summary":"  Self-supervised learning (SSL) is a popular paradigm for representation\nlearning. Recent multiview methods can be classified as sample-contrastive,\ndimension-contrastive, or asymmetric network-based, with each family having its\nown approach to avoiding informational collapse. While these families converge\nto solutions of similar quality, it can be empirically shown that some methods\nare epoch-inefficient and require longer training to reach a target\nperformance. Two main approaches to improving efficiency are covariance\neigenvalue regularization and using more views. However, these two approaches\nare difficult to combine due to the computational complexity of computing\neigenvalues. We present the objective function FroSSL which reconciles both\napproaches while avoiding eigendecomposition entirely. FroSSL works by\nminimizing covariance Frobenius norms to avoid collapse and minimizing\nmean-squared error to achieve augmentation invariance. We show that FroSSL\nreaches competitive accuracies more quickly than any other SSL method and\nprovide theoretical and empirical support that this faster convergence is due\nto how FroSSL affects the eigenvalues of the embedding covariance matrices. We\nalso show that FroSSL learns competitive representations on linear probe\nevaluation when used to train a ResNet-18 on several datasets, including\nSTL-10, Tiny ImageNet, and ImageNet-100.\n","authors":["Oscar Skean","Aayush Dhakal","Nathan Jacobs","Luis Gonzalo Sanchez Giraldo"],"pdf_url":"https://arxiv.org/pdf/2310.02903v4.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2408.02208v1","updated":"2024-08-05T03:17:44Z","published":"2024-08-05T03:17:44Z","title":"Multi-level Traffic-Responsive Tilt Camera Surveillance through\n  Predictive Correlated Online Learning","summary":"  In urban traffic management, the primary challenge of dynamically and\nefficiently monitoring traffic conditions is compounded by the insufficient\nutilization of thousands of surveillance cameras along the intelligent\ntransportation system. This paper introduces the multi-level Traffic-responsive\nTilt Camera surveillance system (TTC-X), a novel framework designed for dynamic\nand efficient monitoring and management of traffic in urban networks. By\nleveraging widely deployed pan-tilt-cameras (PTCs), TTC-X overcomes the\nlimitations of a fixed field of view in traditional surveillance systems by\nproviding mobilized and 360-degree coverage. The innovation of TTC-X lies in\nthe integration of advanced machine learning modules, including a\ndetector-predictor-controller structure, with a novel Predictive Correlated\nOnline Learning (PiCOL) methodology and the Spatial-Temporal Graph Predictor\n(STGP) for real-time traffic estimation and PTC control. The TTC-X is tested\nand evaluated under three experimental scenarios (e.g., maximum traffic flow\ncapture, dynamic route planning, traffic state estimation) based on a\nsimulation environment calibrated using real-world traffic data in Brooklyn,\nNew York. The experimental results showed that TTC-X captured over 60\\% total\nnumber of vehicles at the network level, dynamically adjusted its route\nrecommendation in reaction to unexpected full-lane closure events, and\nreconstructed link-level traffic states with best MAE less than 1.25\nvehicle/hour. Demonstrating scalability, cost-efficiency, and adaptability,\nTTC-X emerges as a powerful solution for urban traffic management in both\ncyber-physical and real-world environments.\n","authors":["Tao Li","Zilin Bian","Haozhe Lei","Fan Zuo","Ya-Ting Yang","Quanyan Zhu","Zhenning Li","Kaan Ozbay"],"pdf_url":"https://arxiv.org/pdf/2408.02208v1.pdf","comment":"Accepted to Transportation Research Part C special issue: Modelling,\n  Learning, and Control of Conventional, Cooperative and Automated Motorway and\n  Urban Traffic Systems"},{"id":"http://arxiv.org/abs/2408.02201v1","updated":"2024-08-05T03:05:02Z","published":"2024-08-05T03:05:02Z","title":"Evaluating the Performance of Large Language Models for SDG Mapping\n  (Technical Report)","summary":"  The use of large language models (LLMs) is expanding rapidly, and open-source\nversions are becoming available, offering users safer and more adaptable\noptions. These models enable users to protect data privacy by eliminating the\nneed to provide data to third parties and can be customized for specific tasks.\nIn this study, we compare the performance of various language models on the\nSustainable Development Goal (SDG) mapping task, using the output of GPT-4o as\nthe baseline. The selected open-source models for comparison include Mixtral,\nLLaMA 2, LLaMA 3, Gemma, and Qwen2. Additionally, GPT-4o-mini, a more\nspecialized version of GPT-4o, was included to extend the comparison. Given the\nmulti-label nature of the SDG mapping task, we employed metrics such as F1\nscore, precision, and recall with micro-averaging to evaluate different aspects\nof the models' performance. These metrics are derived from the confusion matrix\nto ensure a comprehensive evaluation. We provide a clear observation and\nanalysis of each model's performance by plotting curves based on F1 score,\nprecision, and recall at different thresholds. According to the results of this\nexperiment, LLaMA 2 and Gemma still have significant room for improvement. The\nother four models do not exhibit particularly large differences in performance.\nThe outputs from all seven models are available on Zenodo:\nhttps://doi.org/10.5281/zenodo.12789375.\n","authors":["Hui Yin","Amir Aryani","Nakul Nambiar"],"pdf_url":"https://arxiv.org/pdf/2408.02201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02198v1","updated":"2024-08-05T02:50:58Z","published":"2024-08-05T02:50:58Z","title":"Synergistic Learning with Multi-Task DeepONet for Efficient PDE Problem\n  Solving","summary":"  Multi-task learning (MTL) is an inductive transfer mechanism designed to\nleverage useful information from multiple tasks to improve generalization\nperformance compared to single-task learning. It has been extensively explored\nin traditional machine learning to address issues such as data sparsity and\noverfitting in neural networks. In this work, we apply MTL to problems in\nscience and engineering governed by partial differential equations (PDEs).\nHowever, implementing MTL in this context is complex, as it requires\ntask-specific modifications to accommodate various scenarios representing\ndifferent physical processes. To this end, we present a multi-task deep\noperator network (MT-DeepONet) to learn solutions across various functional\nforms of source terms in a PDE and multiple geometries in a single concurrent\ntraining session. We introduce modifications in the branch network of the\nvanilla DeepONet to account for various functional forms of a parameterized\ncoefficient in a PDE. Additionally, we handle parameterized geometries by\nintroducing a binary mask in the branch network and incorporating it into the\nloss term to improve convergence and generalization to new geometry tasks. Our\napproach is demonstrated on three benchmark problems: (1) learning different\nfunctional forms of the source term in the Fisher equation; (2) learning\nmultiple geometries in a 2D Darcy Flow problem and showcasing better transfer\nlearning capabilities to new geometries; and (3) learning 3D parameterized\ngeometries for a heat transfer problem and demonstrate the ability to predict\non new but similar geometries. Our MT-DeepONet framework offers a novel\napproach to solving PDE problems in engineering and science under a unified\numbrella based on synergistic learning that reduces the overall training cost\nfor neural operators.\n","authors":["Varun Kumar","Somdatta Goswami","Katiana Kontolati","Michael D. Shields","George Em Karniadakis"],"pdf_url":"https://arxiv.org/pdf/2408.02198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00380v2","updated":"2024-08-05T02:45:50Z","published":"2024-08-01T08:41:13Z","title":"Enhancing Whole Slide Pathology Foundation Models through Stain\n  Normalization","summary":"  Recent advancements in digital pathology have led to the development of\nnumerous foundational models that utilize self-supervised learning on patches\nextracted from gigapixel whole slide images (WSIs). While this approach\nleverages vast amounts of unlabeled data, we have discovered a significant\nissue: features extracted from these self-supervised models tend to cluster by\nindividual WSIs, a phenomenon we term WSI-specific feature collapse. This\nproblem can potentially limit the model's generalization ability and\nperformance on various downstream tasks. To address this issue, we introduce\nStain Normalized Pathology Foundational Model, a novel foundational model\ntrained on patches that have undergone stain normalization. Stain normalization\nhelps reduce color variability arising from different laboratories and\nscanners, enabling the model to learn more consistent features. Stain\nNormalized Pathology Foundational Model is trained using 285,153,903 patches\nextracted from a total of 34,795 WSIs, combining data from The Cancer Genome\nAtlas (TCGA) and the Genotype-Tissue Expression (GTEx) project. Our experiments\ndemonstrate that Stain Normalized Pathology Foundational Model significantly\nmitigates the feature collapse problem, indicating that the model has learned\nmore generalized features rather than overfitting to individual WSI\ncharacteristics. We compared Stain Normalized Pathology Foundational Model with\nstate-of-the-art models across six downstream task datasets, and our results\nshow that Stain Normalized Pathology Foundational Model achieves excellent\nperformance relative to the number of WSIs used and the model's parameter\ncount. This suggests that the application of stain normalization has\nsubstantially improved the model's efficiency and generalization capabilities.\n","authors":["Juseung Yun","Yi Hu","Jinhyung Kim","Jongseong Jang","Soonyoung Lee"],"pdf_url":"https://arxiv.org/pdf/2408.00380v2.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2404.07950v2","updated":"2024-08-05T02:45:42Z","published":"2024-03-18T16:50:23Z","title":"Reinforcement Learning with Generalizable Gaussian Splatting","summary":"  An excellent representation is crucial for reinforcement learning (RL)\nperformance, especially in vision-based reinforcement learning tasks. The\nquality of the environment representation directly influences the achievement\nof the learning task. Previous vision-based RL typically uses explicit or\nimplicit ways to represent environments, such as images, points, voxels, and\nneural radiance fields. However, these representations contain several\ndrawbacks. They cannot either describe complex local geometries or generalize\nwell to unseen scenes, or require precise foreground masks. Moreover, these\nimplicit neural representations are akin to a ``black box\", significantly\nhindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit\nscene representation and differentiable rendering nature, is considered a\nrevolutionary change for reconstruction and representation methods. In this\npaper, we propose a novel Generalizable Gaussian Splatting framework to be the\nrepresentation of RL tasks, called GSRL. Through validation in the RoboMimic\nenvironment, our method achieves better results than other baselines in\nmultiple tasks, improving the performance by 10%, 44%, and 15% compared with\nbaselines on the hardest task. This work is the first attempt to leverage\ngeneralizable 3DGS as a representation for RL.\n","authors":["Jiaxu Wang","Qiang Zhang","Jingkai Sun","Jiahang Cao","Gang Han","Wen Zhao","Weining Zhang","Yecheng Shao","Yijie Guo","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2404.07950v2.pdf","comment":"7 pages,2 figures"},{"id":"http://arxiv.org/abs/2408.02193v1","updated":"2024-08-05T02:38:48Z","published":"2024-08-05T02:38:48Z","title":"CodeACT: Code Adaptive Compute-efficient Tuning Framework for Code LLMs","summary":"  Large language models (LLMs) have shown great potential in code-related\ntasks, yet open-source models lag behind their closed-source counterparts. To\nbridge this performance gap, existing methods generate vast amounts of\nsynthetic data for fine-tuning, leading to inefficiencies in training.\nMotivated by the need for more effective and efficient training, we propose the\nCode Adaptive Compute-efficient Tuning (CodeACT) framework. CodeACT introduces\nthe Complexity and Diversity Aware Sampling (CDAS) method to select\nhigh-quality training data based on complexity and diversity, and the Dynamic\nPack padding strategy to reduce computational resource usage by minimizing\npadding tokens during training. Experimental results demonstrate that\nCodeACT-DeepSeek-Coder-6.7B, fine-tuned on only 40% of the EVOL-Instruct data,\nachieves an 8.6% performance increase on HumanEval, reduces training time by\n78%, and decreases peak GPU memory usage by 27%. These findings underscore\nCodeACT's ability to enhance the performance and efficiency of open-source\nmodels. By optimizing both the data selection and training processes, CodeACT\noffers a comprehensive approach to improving the capabilities of open-source\nLLMs while significantly reducing computational requirements, addressing the\ndual challenges of data quality and training efficiency, and paving the way for\nmore resource-efficient and performant models.\n","authors":["Weijie Lv","Xuan Xia","Sheng-Jun Huang"],"pdf_url":"https://arxiv.org/pdf/2408.02193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00992v2","updated":"2024-08-05T02:09:58Z","published":"2024-08-02T03:44:14Z","title":"Fairness in Large Language Models in Three Hour","summary":"  Large Language Models (LLMs) have demonstrated remarkable success across\nvarious domains but often lack fairness considerations, potentially leading to\ndiscriminatory outcomes against marginalized populations. Unlike fairness in\ntraditional machine learning, fairness in LLMs involves unique backgrounds,\ntaxonomies, and fulfillment techniques. This tutorial provides a systematic\noverview of recent advances in the literature concerning fair LLMs, beginning\nwith real-world case studies to introduce LLMs, followed by an analysis of bias\ncauses therein. The concept of fairness in LLMs is then explored, summarizing\nthe strategies for evaluating bias and the algorithms designed to promote\nfairness. Additionally, resources for assessing bias in LLMs, including\ntoolkits and datasets, are compiled, and current research challenges and open\nquestions in the field are discussed. The repository is available at\n\\url{https://github.com/LavinWong/Fairness-in-Large-Language-Models}.\n","authors":["Thang Doan Viet","Zichong Wang","Minh Nhat Nguyen","Wenbin Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00992v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16020v4","updated":"2024-08-05T02:01:12Z","published":"2024-07-22T19:55:44Z","title":"Sparks of Quantum Advantage and Rapid Retraining in Machine Learning","summary":"  The advent of quantum computing holds the potential to revolutionize various\nfields by solving complex problems more efficiently than classical computers.\nDespite this promise, practical quantum advantage is hindered by current\nhardware limitations, notably the small number of qubits and high noise levels.\nIn this study, we leverage adiabatic quantum computers to optimize\nKolmogorov-Arnold Networks, a powerful neural network architecture for\nrepresenting complex functions with minimal parameters. By modifying the\nnetwork to use Bezier curves as the basis functions and formulating the\noptimization problem into a Quadratic Unconstrained Binary Optimization\nproblem, we create a fixed-sized solution space, independent of the number of\ntraining samples. This strategy allows for the optimization of an entire neural\nnetwork in a single training iteration in which, due to order of operations, a\nmajority of the processing is done using a collapsed version of the training\ndataset. This inherently creates extremely fast training speeds, which are\nvalidated experimentally, compared to classical optimizers including Adam,\nStochastic Gradient Descent, Adaptive Gradient, and simulated annealing.\nAdditionally, we introduce a novel rapid retraining capability, enabling the\nnetwork to be retrained with new data without reprocessing old samples, thus\nenhancing learning efficiency in dynamic environments. Experiments on\nretraining demonstrate a hundred times speed up using adiabatic quantum\ncomputing based optimization compared to that of the gradient descent based\noptimizers, with theoretical models allowing this speed up to be much larger!\nOur findings suggest that with further advancements in quantum hardware and\nalgorithm optimization, quantum-optimized machine learning models could have\nbroad applications across various domains, with initial focus on rapid\nretraining.\n","authors":["William Troy"],"pdf_url":"https://arxiv.org/pdf/2407.16020v4.pdf","comment":"Major updates to the paper for timings and explanations of\n  optimization strategies used. Further optimized the code and updated the\n  figures to reflect the faster timings for v3"},{"id":"http://arxiv.org/abs/2402.02977v4","updated":"2024-08-05T01:24:52Z","published":"2024-02-05T12:58:29Z","title":"Variational Flow Models: Flowing in Your Style","summary":"  We propose a systematic training-free method to transform the probability\nflow of a \"linear\" stochastic process characterized by the equation\nX_{t}=a_{t}X_{0}+\\sigma_{t}X_{1} into a straight constant-speed (SC) flow,\nreminiscent of Rectified Flow. This transformation facilitates fast sampling\nalong the original probability flow via the Euler method without training a new\nmodel of the SC flow. The flexibility of our approach allows us to extend our\ntransformation to inter-convert two posterior flows of two distinct linear\nstochastic processes. Moreover, we can easily integrate high-order numerical\nsolvers into the transformed SC flow, further enhancing the sampling accuracy\nand efficiency. Rigorous theoretical analysis and extensive experimental\nresults substantiate the advantages of our framework. Our code is available at\nthis [https://github.com/clarken92/VFM||link].\n","authors":["Kien Do","Duc Kieu","Toan Nguyen","Dang Nguyen","Hung Le","Dung Nguyen","Thin Nguyen"],"pdf_url":"https://arxiv.org/pdf/2402.02977v4.pdf","comment":"Our code is available at: https://github.com/clarken92/VFM"},{"id":"http://arxiv.org/abs/2302.00857v2","updated":"2024-08-05T01:10:35Z","published":"2023-02-02T04:02:49Z","title":"Algorithm Design for Online Meta-Learning with Task Boundary Detection","summary":"  Online meta-learning has recently emerged as a marriage between batch\nmeta-learning and online learning, for achieving the capability of quick\nadaptation on new tasks in a lifelong manner. However, most existing approaches\nfocus on the restrictive setting where the distribution of the online tasks\nremains fixed with known task boundaries. In this work, we relax these\nassumptions and propose a novel algorithm for task-agnostic online\nmeta-learning in non-stationary environments. More specifically, we first\npropose two simple but effective detection mechanisms of task switches and\ndistribution shift based on empirical observations, which serve as a key\nbuilding block for more elegant online model updates in our algorithm: the task\nswitch detection mechanism allows reusing of the best model available for the\ncurrent task at hand, and the distribution shift detection mechanism\ndifferentiates the meta model update in order to preserve the knowledge for\nin-distribution tasks and quickly learn the new knowledge for\nout-of-distribution tasks. In particular, our online meta model updates are\nbased only on the current data, which eliminates the need of storing previous\ndata as required in most existing methods. We further show that a sublinear\ntask-averaged regret can be achieved for our algorithm under mild conditions.\nEmpirical studies on three different benchmarks clearly demonstrate the\nsignificant advantage of our algorithm over related baseline approaches.\n","authors":["Daouda Sow","Sen Lin","Yingbin Liang","Junshan Zhang"],"pdf_url":"https://arxiv.org/pdf/2302.00857v2.pdf","comment":"CPAL 2024"},{"id":"http://arxiv.org/abs/2402.08225v3","updated":"2024-08-05T00:54:02Z","published":"2024-02-13T05:33:35Z","title":"Improving Black-box Robustness with In-Context Rewriting","summary":"  Machine learning models for text classification often excel on\nin-distribution (ID) data but struggle with unseen out-of-distribution (OOD)\ninputs. Most techniques for improving OOD robustness are not applicable to\nsettings where the model is effectively a black box, such as when the weights\nare frozen, retraining is costly, or the model is leveraged via an API.\nTest-time augmentation (TTA) is a simple post-hoc technique for improving\nrobustness that sidesteps black-box constraints by aggregating predictions\nacross multiple augmentations of the test input. TTA has seen limited use in\nNLP due to the challenge of generating effective natural language\naugmentations. In this work, we propose LLM-TTA, which uses LLM-generated\naugmentations as TTA's augmentation function. LLM-TTA outperforms conventional\naugmentation functions across sentiment, toxicity, and news classification\ntasks for BERT and T5 models, with BERT's OOD robustness improving by an\naverage of 4.48 percentage points without regressing average ID performance. We\nexplore selectively augmenting inputs based on prediction entropy to reduce the\nrate of expensive LLM augmentations, allowing us to maintain performance gains\nwhile reducing the average number of generated augmentations by 57.74\\%.\nLLM-TTA is agnostic to the task model architecture, does not require OOD\nlabels, and is effective across low and high-resource settings. We share our\ndata, models, and code for reproducibility.\n","authors":["Kyle O'Brien","Nathan Ng","Isha Puri","Jorge Mendez","Hamid Palangi","Yoon Kim","Marzyeh Ghassemi","Thomas Hartvigsen"],"pdf_url":"https://arxiv.org/pdf/2402.08225v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02866v1","updated":"2024-08-05T23:33:24Z","published":"2024-08-05T23:33:24Z","title":"Back-Projection Diffusion: Solving the Wideband Inverse Scattering\n  Problem with Diffusion Models","summary":"  We present \\textit{Wideband back-projection diffusion}, an end-to-end\nprobabilistic framework for approximating the posterior distribution induced by\nthe inverse scattering map from wideband scattering data. This framework\nleverages conditional diffusion models coupled with the underlying physics of\nwave-propagation and symmetries in the problem, to produce highly accurate\nreconstructions. The framework introduces a factorization of the score function\ninto a physics-based latent representation inspired by the filtered\nback-propagation formula and a conditional score function conditioned on this\nlatent representation. These two steps are also constrained to obey symmetries\nin the formulation while being amenable to compression by imposing the rank\nstructure found in the filtered back-projection formula. As a result,\nempirically, our framework is able to provide sharp reconstructions\neffortlessly, even recovering sub-Nyquist features in the multiple-scattering\nregime. It has low-sample and computational complexity, its number of\nparameters scales sub-linearly with the target resolution, and it has stable\ntraining dynamics.\n","authors":["Borong Zhang","Martín Guerra","Qin Li","Leonardo Zepeda-Núñez"],"pdf_url":"https://arxiv.org/pdf/2408.02866v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.09797v2","updated":"2024-08-05T23:23:14Z","published":"2023-07-19T07:31:37Z","title":"Probabilistic Forecasting with Coherent Aggregation","summary":"  Obtaining accurate probabilistic forecasts is an important operational\nchallenge in many applications, perhaps most obviously in energy management,\nclimate forecasting, supply chain planning, and resource allocation. In many of\nthese applications, there is a natural hierarchical structure over the\nforecasted quantities; and forecasting systems that adhere to this hierarchical\nstructure are said to be coherent. Furthermore, operational planning benefits\nfrom accuracy at all levels of the aggregation hierarchy. Building accurate and\ncoherent forecasting systems, however, is challenging: classic multivariate\ntime series tools and neural network methods are still being adapted for this\npurpose. In this paper, we augment an MQForecaster neural network architecture\nwith a novel deep Gaussian factor forecasting model that achieves coherence by\nconstruction, yielding a method we call the Deep Coherent Factor Model Neural\nNetwork (DeepCoFactor) model. DeepCoFactor generates samples that can be\ndifferentiated with respect to model parameters, allowing optimization on\nvarious sample-based learning objectives that align with the forecasting\nsystem's goals, including quantile loss and the scaled Continuous Ranked\nProbability Score (CRPS). In a comparison to state-of-the-art coherent\nforecasting methods, DeepCoFactor achieves significant improvements in scaled\nCRPS forecast accuracy, with gains between 4.16 and 54.40%, as measured on\nthree publicly available hierarchical forecasting datasets.\n","authors":["Kin G. Olivares","Geoffrey Négiar","Ruijun Ma","O. Nangba Meetei","Mengfei Cao","Michael W. Mahoney"],"pdf_url":"https://arxiv.org/pdf/2307.09797v2.pdf","comment":"10 pages of main text. Updated method and results"}]},"2024-08-04T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2408.02162v1","updated":"2024-08-04T23:12:31Z","published":"2024-08-04T23:12:31Z","title":"Improvement and Empirical Testing of a Novel Autonomous\n  Microplastics-Collecting Semisubmersible","summary":"  Since their invention, plastics have become ubiquitous in modern societies\nall around the world, and their impact on the environment has, in recent years,\nbecome nearly as well-known. Plastics produced by humans have reached nearly\nevery corner of the world, and throughout their centuries-long lifetimes,\nplastics continually break down into smaller and smaller particles due to the\nphysical stresses which they are subjected to. These stresses eventually,\ninevitably, break these plastics down into microplastics -pieces of plastic\nsmall enough to be consumed by organisms in bodies of water throughout the\nglobe. These microplastics can very easily bioaccumulate, and have been found\neverywhere from the Great Lakes to the bloodstreams of humans. The effects of\nthese plastics are poorly understood, however, they have been linked to\ninfertility, halted growth, and a host of other maladies in aquatic organisms.\nCurrently, removal of these plastics has been neglected, with no governmental\naction to remove them from marine environments, and this project aims to begin\nprototyping a solution to this issue. A significant percentage of microplastics\nare found at the surface of waterways, thus trawling in surface waters using an\nautonomously propelled net is proposed as a way to solve this seemingly\nintractable issue. By attaching motors and a guidance system to a manta trawl,\na device currently used for collecting microorganisms, the process of\ncollecting microplastics in open water can be automated, and thus the work of\nremoving plastics from the environment on a large scale can begin.\n","authors":["Ziddane Isahaku"],"pdf_url":"https://arxiv.org/pdf/2408.02162v1.pdf","comment":"45 pages, 48 figures, presented at ISEF 2024, winning 4th place in\n  Environmental Engineering, associated data regarding microplastic collection\n  here:\n  https://docs.google.com/spreadsheets/d/16K9OQgByD8XZWK6_KAz4vc348iu1EmvRCOol1QKNjt8/edit?usp=sharing"},{"id":"http://arxiv.org/abs/2408.02141v1","updated":"2024-08-04T20:53:03Z","published":"2024-08-04T20:53:03Z","title":"An efficient strategy for path planning with a tethered marsupial\n  robotics system","summary":"  A marsupial robotics system comprises three components: an Unmanned Ground\nVehicle (UGV), an Unmanned Aerial Vehicle (UAV), and a tether connecting both\nrobots. Marsupial systems are highly beneficial in industry as they extend the\nUAV's battery life during flight. This paper introduces a novel strategy for a\nspecific path planning problem in marsupial systems, where each of the\ncomponents must avoid collisions with ground and aerial obstacles modeled as 3D\ncuboids. Given an initial configuration in which the UAV is positioned atop the\nUGV, the goal is to reach an aerial target with the UAV. We assume that the UGV\nfirst moves to a position from which the UAV can take off and fly through a\nvertical plane to reach an aerial target. We propose an approach that\ndiscretizes the space to approximate an optimal solution, minimizing the sum of\nthe lengths of the ground and air paths. First, we assume a taut tether and use\na novel algorithm that leverages the convexity of the tether and the geometry\nof obstacles to efficiently determine the locus of feasible take-off points for\nthe UAV. We then apply this result to scenarios that involve loose tethers. The\nsimulation test results show that our approach can solve complex situations in\nseconds, outperforming a baseline planning algorithm based on RRT* (Rapidly\nexploring Random Trees).\n","authors":["Jesús Capitán","José M. Díaz-Báñez","Miguel A. Pérez-Cutiño","Fabio Rodríguez","Inmaculada Ventura"],"pdf_url":"https://arxiv.org/pdf/2408.02141v1.pdf","comment":"15 pages, 9 figures, 4 tables"},{"id":"http://arxiv.org/abs/2408.02061v1","updated":"2024-08-04T15:20:39Z","published":"2024-08-04T15:20:39Z","title":"ParkingE2E: Camera-based End-to-end Parking Network, from Images to\n  Planning","summary":"  Autonomous parking is a crucial task in the intelligent driving field.\nTraditional parking algorithms are usually implemented using rule-based\nschemes. However, these methods are less effective in complex parking scenarios\ndue to the intricate design of the algorithms. In contrast,\nneural-network-based methods tend to be more intuitive and versatile than the\nrule-based methods. By collecting a large number of expert parking trajectory\ndata and emulating human strategy via learning-based methods, the parking task\ncan be effectively addressed. In this paper, we employ imitation learning to\nperform end-to-end planning from RGB images to path planning by imitating human\ndriving trajectories. The proposed end-to-end approach utilizes a target query\nencoder to fuse images and target features, and a transformer-based decoder to\nautoregressively predict future waypoints. We conducted extensive experiments\nin real-world scenarios, and the results demonstrate that the proposed method\nachieved an average parking success rate of 87.8% across four different\nreal-world garages. Real-vehicle experiments further validate the feasibility\nand effectiveness of the method proposed in this paper.\n","authors":["Changze Li","Ziheng Ji","Zhe Chen","Tong Qin","Ming Yang"],"pdf_url":"https://arxiv.org/pdf/2408.02061v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18569v3","updated":"2024-08-04T09:01:00Z","published":"2024-07-26T07:51:11Z","title":"PP-TIL: Personalized Planning for Autonomous Driving with Instance-based\n  Transfer Imitation Learning","summary":"  Personalized motion planning holds significant importance within urban\nautomated driving, catering to the unique requirements of individual users.\nNevertheless, prior endeavors have frequently encountered difficulties in\nsimultaneously addressing two crucial aspects: personalized planning within\nintricate urban settings and enhancing planning performance through data\nutilization. The challenge arises from the expensive and limited nature of user\ndata, coupled with the scene state space tending towards infinity. These\nfactors contribute to overfitting and poor generalization problems during model\ntraining. Henceforth, we propose an instance-based transfer imitation learning\napproach. This method facilitates knowledge transfer from extensive expert\ndomain data to the user domain, presenting a fundamental resolution to these\nissues. We initially train a pre-trained model using large-scale expert data.\nSubsequently, during the fine-tuning phase, we feed the batch data, which\ncomprises expert and user data. Employing the inverse reinforcement learning\ntechnique, we extract the style feature distribution from user demonstrations,\nconstructing the regularization term for the approximation of user style. In\nour experiments, we conducted extensive evaluations of the proposed method.\nCompared to the baseline methods, our approach mitigates the overfitting issue\ncaused by sparse user data. Furthermore, we discovered that integrating the\ndriving model with a differentiable nonlinear optimizer as a safety protection\nlayer for end-to-end personalized fine-tuning results in superior planning\nperformance.\n","authors":["Fangze Lin","Ying He","Fei Yu"],"pdf_url":"https://arxiv.org/pdf/2407.18569v3.pdf","comment":"IROS 2024 Accepted"},{"id":"http://arxiv.org/abs/2408.01953v1","updated":"2024-08-04T07:59:17Z","published":"2024-08-04T07:59:17Z","title":"EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning","summary":"  Humans perceive and interact with the world with the awareness of\nequivariance, facilitating us in manipulating different objects in diverse\nposes. For robotic manipulation, such equivariance also exists in many\nscenarios. For example, no matter what the pose of a drawer is (translation,\nrotation and tilt), the manipulation strategy is consistent (grasp the handle\nand pull in a line). While traditional models usually do not have the awareness\nof equivariance for robotic manipulation, which might result in more data for\ntraining and poor performance in novel object poses, we propose our EqvAfford\nframework, with novel designs to guarantee the equivariance in point-level\naffordance learning for downstream robotic manipulation, with great performance\nand generalization ability on representative tasks on objects in diverse poses.\n","authors":["Yue Chen","Chenrui Tie","Ruihai Wu","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2408.01953v1.pdf","comment":"Accept to CVPRWorkshop on Equivariant Vision: From Theory to Practice\n  2024"},{"id":"http://arxiv.org/abs/2408.01945v1","updated":"2024-08-04T07:06:04Z","published":"2024-08-04T07:06:04Z","title":"Generalized Maximum Likelihood Estimation for Perspective-n-Point\n  Problem","summary":"  The Perspective-n-Point (PnP) problem has been widely studied in the\nliterature and applied in various vision-based pose estimation scenarios.\nHowever, existing methods ignore the anisotropy uncertainty of observations, as\ndemonstrated in several real-world datasets in this paper. This oversight may\nlead to suboptimal and inaccurate estimation, particularly in the presence of\nnoisy observations. To this end, we propose a generalized maximum likelihood\nPnP solver, named GMLPnP, that minimizes the determinant criterion by iterating\nthe GLS procedure to estimate the pose and uncertainty simultaneously. Further,\nthe proposed method is decoupled from the camera model. Results of synthetic\nand real experiments show that our method achieves better accuracy in common\npose estimation scenarios, GMLPnP improves rotation/translation accuracy by\n4.7%/2.0% on TUM-RGBD and 18.6%/18.4% on KITTI-360 dataset compared to the best\nbaseline. It is more accurate under very noisy observations in a vision-based\nUAV localization task, outperforming the best baseline by 34.4% in translation\nestimation accuracy.\n","authors":["Tian Zhan","Chunfeng Xu","Cheng Zhang","Ke Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.01945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01941v1","updated":"2024-08-04T06:29:57Z","published":"2024-08-04T06:29:57Z","title":"A Jellyfish Cyborg: Exploiting Natural Embodied Intelligence as Soft\n  Robots","summary":"  In the advanced field of bio-inspired robotics, the emergence of cyborgs\nrepresents the successful integration of engineering and biological systems.\nBuilding on previous research that showed how electrical stimuli could initiate\nand speed up a jellyfish's movement, this study presents a groundbreaking\napproach that explores how the natural embodied intelligence of the animal can\nbe harnessed to address pivotal challenges such as spontaneous exploration,\nnavigation in various environments, control of whole-body motion, and real-time\npredictions of behavior. We have developed a comprehensive data acquisition\nsystem and a unique setup for stimulating jellyfish, allowing for a detailed\nstudy of their movements. Through careful analysis of both spontaneous\nbehaviors and behaviors induced by targeted stimulation, we have identified\nsubtle differences between natural and induced motion patterns. By using a\nmachine learning method called physical reservoir computing, we have\nsuccessfully shown that future behaviors can be accurately predicted by\ndirectly measuring the jellyfish's body shape when the stimuli align with the\nanimal's natural dynamics. Our findings also reveal significant advancements in\nmotion control and real-time prediction capabilities of jellyfish cyborgs. In\nsummary, this research provides a comprehensive roadmap for optimizing the\ncapabilities of jellyfish cyborgs, with potential implications in marine\nreconnaissance and sustainable ecological interventions.\n","authors":["Dai Owaki","Max Austin","Shuhei Ikeda","Kazuya Okuizumi","Kohei Nakajima"],"pdf_url":"https://arxiv.org/pdf/2408.01941v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01923v1","updated":"2024-08-04T04:34:29Z","published":"2024-08-04T04:34:29Z","title":"Scalable Signal Temporal Logic Guided Reinforcement Learning via Value\n  Function Space Optimization","summary":"  The integration of reinforcement learning (RL) and formal methods has emerged\nas a promising framework for solving long-horizon planning problems.\nConventional approaches typically involve abstraction of the state and action\nspaces and manually created labeling functions or predicates. However, the\nefficiency of these approaches deteriorates as the tasks become increasingly\ncomplex, which results in exponential growth in the size of labeling functions\nor predicates. To address these issues, we propose a scalable model-based RL\nframework, called VFSTL, which schedules pre-trained skills to follow unseen\nSTL specifications without using hand-crafted predicates. Given a set of value\nfunctions obtained by goal-conditioned RL, we formulate an optimization problem\nto maximize the robustness value of Signal Temporal Logic (STL) defined\nspecifications, which is computed using value functions as predicates. To\nfurther reduce the computation burden, we abstract the environment state space\ninto the value function space (VFS). Then the optimization problem is solved by\nModel-Based Reinforcement Learning. Simulation results show that STL with value\nfunctions as predicates approximates the ground truth robustness and the\nplanning in VFS directly achieves unseen specifications using data from\nsensors.\n","authors":["Yiting He","Peiran Liu","Yiding Ji"],"pdf_url":"https://arxiv.org/pdf/2408.01923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04666v2","updated":"2024-08-04T02:50:29Z","published":"2024-06-07T06:25:38Z","title":"Underactuated Control of Multiple Soft Pneumatic Actuators via Stable\n  Inversion","summary":"  Soft grippers, with their inherent compliance and adaptability, show\nadvantages for delicate and versatile manipulation tasks in robotics. This\npaper presents a novel approach to underactuated control of multiple soft\nactuators, explicitly focusing on the coordination of soft fingers within a\nsoft gripper. Utilizing a single syringe pump as the actuation mechanism, we\naddress the challenge of coordinating multiple degrees of freedom of a\ncompliant system. The theoretical framework applies concepts from stable\ninversion theory, adapting them to the unique dynamics of the underactuated\nsoft gripper. Through meticulous mechatronic system design and controller\nsynthesis, we demonstrate the efficacy and applicability of our approach in\nachieving precise and coordinated manipulation tasks in simulation and\nexperimentation. Our findings not only contribute to the advancement of soft\nrobot control but also offer practical insights into the design and control of\nunderactuated systems for real-world applications.\n","authors":["Wu-Te Yang","Burak Kurkcu","Masayoshi Tomizuka"],"pdf_url":"https://arxiv.org/pdf/2406.04666v2.pdf","comment":"11 pages, 7 figures"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2408.02164v1","updated":"2024-08-04T23:21:46Z","published":"2024-08-04T23:21:46Z","title":"Rethinking Affect Analysis: A Protocol for Ensuring Fairness and\n  Consistency","summary":"  Evaluating affect analysis methods presents challenges due to inconsistencies\nin database partitioning and evaluation protocols, leading to unfair and biased\nresults. Previous studies claim continuous performance improvements, but our\nfindings challenge such assertions. Using these insights, we propose a unified\nprotocol for database partitioning that ensures fairness and comparability. We\nprovide detailed demographic annotations (in terms of race, gender and age),\nevaluation metrics, and a common framework for expression recognition, action\nunit detection and valence-arousal estimation. We also rerun the methods with\nthe new protocol and introduce a new leaderboards to encourage future research\nin affect recognition with a fairer comparison. Our annotations, code, and\npre-trained models are available on\n\\hyperlink{https://github.com/dkollias/Fair-Consistent-Affect-Analysis}{Github}.\n","authors":["Guanyu Hu","Dimitrios Kollias","Eleni Papadopoulou","Paraskevi Tzouveli","Jie Wei","Xinyu Yang"],"pdf_url":"https://arxiv.org/pdf/2408.02164v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2405.06841"},{"id":"http://arxiv.org/abs/2408.02157v1","updated":"2024-08-04T22:23:10Z","published":"2024-08-04T22:23:10Z","title":"PanoFree: Tuning-Free Holistic Multi-view Image Generation with\n  Cross-view Self-Guidance","summary":"  Immersive scene generation, notably panorama creation, benefits significantly\nfrom the adaptation of large pre-trained text-to-image (T2I) models for\nmulti-view image generation. Due to the high cost of acquiring multi-view\nimages, tuning-free generation is preferred. However, existing methods are\neither limited to simple correspondences or require extensive fine-tuning to\ncapture complex ones. We present PanoFree, a novel method for tuning-free\nmulti-view image generation that supports an extensive array of\ncorrespondences. PanoFree sequentially generates multi-view images using\niterative warping and inpainting, addressing the key issues of inconsistency\nand artifacts from error accumulation without the need for fine-tuning. It\nimproves error accumulation by enhancing cross-view awareness and refines the\nwarping and inpainting processes via cross-view guidance, risky area estimation\nand erasing, and symmetric bidirectional guided generation for loop closure,\nalongside guidance-based semantic and density control for scene structure\npreservation. In experiments on Planar, 360{\\deg}, and Full Spherical\nPanoramas, PanoFree demonstrates significant error reduction, improves global\nconsistency, and boosts image quality without extra fine-tuning. Compared to\nexisting methods, PanoFree is up to 5x more efficient in time and 3x more\nefficient in GPU memory usage, and maintains superior diversity of results (2x\nbetter in our user study). PanoFree offers a viable alternative to costly\nfine-tuning or the use of additional pre-trained models. Project website at\nhttps://panofree.github.io/.\n","authors":["Aoming Liu","Zhong Li","Zhang Chen","Nannan Li","Yi Xu","Bryan A. Plummer"],"pdf_url":"https://arxiv.org/pdf/2408.02157v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.01996v3","updated":"2024-08-04T21:56:57Z","published":"2024-07-02T07:10:10Z","title":"ViG-Bias: Visually Grounded Bias Discovery and Mitigation","summary":"  The proliferation of machine learning models in critical decision making\nprocesses has underscored the need for bias discovery and mitigation\nstrategies. Identifying the reasons behind a biased system is not\nstraightforward, since in many occasions they are associated with hidden\nspurious correlations which are not easy to spot. Standard approaches rely on\nbias audits performed by analyzing model performance in pre-defined subgroups\nof data samples, usually characterized by common attributes like gender or\nethnicity when it comes to people, or other specific attributes defining\nsemantically coherent groups of images. However, it is not always possible to\nknow a-priori the specific attributes defining the failure modes of visual\nrecognition systems. Recent approaches propose to discover these groups by\nleveraging large vision language models, which enable the extraction of\ncross-modal embeddings and the generation of textual descriptions to\ncharacterize the subgroups where a certain model is underperforming. In this\nwork, we argue that incorporating visual explanations (e.g. heatmaps generated\nvia GradCAM or other approaches) can boost the performance of such bias\ndiscovery and mitigation frameworks. To this end, we introduce Visually\nGrounded Bias Discovery and Mitigation (ViG-Bias), a simple yet effective\ntechnique which can be integrated to a variety of existing frameworks to\nimprove both, discovery and mitigation performance. Our comprehensive\nevaluation shows that incorporating visual explanations enhances existing\ntechniques like DOMINO, FACTS and Bias-to-Text, across several challenging\ndatasets, including CelebA, Waterbirds, and NICO++.\n","authors":["Badr-Eddine Marani","Mohamed Hanini","Nihitha Malayarukil","Stergios Christodoulidis","Maria Vakalopoulou","Enzo Ferrante"],"pdf_url":"https://arxiv.org/pdf/2407.01996v3.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2408.02146v1","updated":"2024-08-04T21:09:09Z","published":"2024-08-04T21:09:09Z","title":"Video-based Pedestrian and Vehicle Traffic Analysis During Football\n  Games","summary":"  This paper utilizes video analytics to study pedestrian and vehicle traffic\nbehavior, focusing on analyzing traffic patterns during football gamedays. The\nUniversity of Florida (UF) hosts six to seven home football games on Saturdays\nduring the college football season, attracting significant pedestrian activity.\nThrough video analytics, this study provides valuable insights into the impact\nof these events on traffic volumes and safety at intersections. Comparing\npedestrian and vehicle activities on gamedays versus non-gamedays reveals\ndiffering patterns. For example, pedestrian volume substantially increases\nduring gamedays, which is positively correlated with the probability of the\naway team winning. This correlation is likely because fans of the home team\nenjoy watching difficult games. Win probabilities as an early predictor of\npedestrian volumes at intersections can be a tool to help traffic professionals\nanticipate traffic management needs. Pedestrian-to-vehicle (P2V) conflicts\nnotably increase on gamedays, particularly a few hours before games start.\nAddressing this, a \"Barnes Dance\" movement phase within the intersection is\nrecommended. Law enforcement presence during high-activity gamedays can help\nensure pedestrian compliance and enhance safety. In contrast, we identified\nthat vehicle-to-vehicle (V2V) conflicts generally do not increase on gamedays\nand may even decrease due to heightened driver caution.\n","authors":["Jacques P. Fleischer","Ryan Pallack","Ahan Mishra","Gustavo Riente de Andrade","Subhadipto Poddar","Emmanuel Posadas","Robert Schenck","Tania Banerjee","Anand Rangarajan","Sanjay Ranka"],"pdf_url":"https://arxiv.org/pdf/2408.02146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02140v1","updated":"2024-08-04T20:38:45Z","published":"2024-08-04T20:38:45Z","title":"VidModEx: Interpretable and Efficient Black Box Model Extraction for\n  High-Dimensional Spaces","summary":"  In the domain of black-box model extraction, conventional methods reliant on\nsoft labels or surrogate datasets struggle with scaling to high-dimensional\ninput spaces and managing the complexity of an extensive array of interrelated\nclasses. In this work, we present a novel approach that utilizes SHAP (SHapley\nAdditive exPlanations) to enhance synthetic data generation. SHAP quantifies\nthe individual contributions of each input feature towards the victim model's\noutput, facilitating the optimization of an energy-based GAN towards a\ndesirable output. This method significantly boosts performance, achieving a\n16.45% increase in the accuracy of image classification models and extending to\nvideo classification models with an average improvement of 26.11% and a maximum\nof 33.36% on challenging datasets such as UCF11, UCF101, Kinetics 400, Kinetics\n600, and Something-Something V2. We further demonstrate the effectiveness and\npractical utility of our method under various scenarios, including the\navailability of top-k prediction probabilities, top-k prediction labels, and\ntop-1 labels.\n","authors":["Somnath Sendhil Kumar","Yuvaraj Govindarajulu","Pavan Kulkarni","Manojkumar Parmar"],"pdf_url":"https://arxiv.org/pdf/2408.02140v1.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2408.02165v1","updated":"2024-08-04T23:23:48Z","published":"2024-08-04T23:23:48Z","title":"SelfBC: Self Behavior Cloning for Offline Reinforcement Learning","summary":"  Policy constraint methods in offline reinforcement learning employ additional\nregularization techniques to constrain the discrepancy between the learned\npolicy and the offline dataset. However, these methods tend to result in overly\nconservative policies that resemble the behavior policy, thus limiting their\nperformance. We investigate this limitation and attribute it to the static\nnature of traditional constraints. In this paper, we propose a novel dynamic\npolicy constraint that restricts the learned policy on the samples generated by\nthe exponential moving average of previously learned policies. By integrating\nthis self-constraint mechanism into off-policy methods, our method facilitates\nthe learning of non-conservative policies while avoiding policy collapse in the\noffline setting. Theoretical results show that our approach results in a nearly\nmonotonically improved reference policy. Extensive experiments on the D4RL\nMuJoCo domain demonstrate that our proposed method achieves state-of-the-art\nperformance among the policy constraint methods.\n","authors":["Shirong Liu","Chenjia Bai","Zixian Guo","Hao Zhang","Gaurav Sharma","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2408.02165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02156v1","updated":"2024-08-04T22:23:09Z","published":"2024-08-04T22:23:09Z","title":"Calibration-Disentangled Learning and Relevance-Prioritized Reranking\n  for Calibrated Sequential Recommendation","summary":"  Calibrated recommendation, which aims to maintain personalized proportions of\ncategories within recommendations, is crucial in practical scenarios since it\nenhances user satisfaction by reflecting diverse interests. However, achieving\ncalibration in a sequential setting (i.e., calibrated sequential\nrecommendation) is challenging due to the need to adapt to users' evolving\npreferences. Previous methods typically leverage reranking algorithms to\ncalibrate recommendations after training a model without considering the effect\nof calibration and do not effectively tackle the conflict between relevance and\ncalibration during the reranking process. In this work, we propose LeapRec\n(Calibration-Disentangled Learning and Relevance-Prioritized Reranking), a\nnovel approach for the calibrated sequential recommendation that addresses\nthese challenges. LeapRec consists of two phases, model training phase and\nreranking phase. In the training phase, a backbone model is trained using our\nproposed calibration-disentangled learning-to-rank loss, which optimizes\npersonalized rankings while integrating calibration considerations. In the\nreranking phase, relevant items are prioritized at the top of the list, with\nitems needed for calibration following later to address potential conflicts\nbetween relevance and calibration. Through extensive experiments on four\nreal-world datasets, we show that LeapRec consistently outperforms previous\nmethods in the calibrated sequential recommendation. Our code is available at\nhttps://github.com/jeon185/LeapRec.\n","authors":["Hyunsik Jeon","Se-eun Yoon","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2408.02156v1.pdf","comment":"Published at CIKM '24 as a full research paper"},{"id":"http://arxiv.org/abs/2408.02153v1","updated":"2024-08-04T22:13:14Z","published":"2024-08-04T22:13:14Z","title":"ARVO: Atlas of Reproducible Vulnerabilities for Open Source Software","summary":"  High-quality datasets of real-world vulnerabilities are enormously valuable\nfor downstream research in software security, but existing datasets are\ntypically small, require extensive manual effort to update, and are missing\ncrucial features that such research needs. In this paper, we introduce ARVO: an\nAtlas of Reproducible Vulnerabilities in Open-source software. By sourcing\nvulnerabilities from C/C++ projects that Google's OSS-Fuzz discovered and\nimplementing a reliable re-compilation system, we successfully reproduce more\nthan 5,000 memory vulnerabilities across over 250 projects, each with a\ntriggering input, the canonical developer-written patch for fixing the\nvulnerability, and the ability to automatically rebuild the project from source\nand run it at its vulnerable and patched revisions. Moreover, our dataset can\nbe automatically updated as OSS-Fuzz finds new vulnerabilities, allowing it to\ngrow over time. We provide a thorough characterization of the ARVO dataset,\nshow that it can locate fixes more accurately than Google's own OSV\nreproduction effort, and demonstrate its value for future research through two\ncase studies: firstly evaluating real-world LLM-based vulnerability repair, and\nsecondly identifying over 300 falsely patched (still-active) zero-day\nvulnerabilities from projects improperly labeled by OSS-Fuzz.\n","authors":["Xiang Mei","Pulkit Singh Singaria","Jordi Del Castillo","Haoran Xi"," Abdelouahab"," Benchikh","Tiffany Bao","Ruoyu Wang","Yan Shoshitaishvili","Adam Doupé","Hammond Pearce","Brendan Dolan-Gavitt"],"pdf_url":"https://arxiv.org/pdf/2408.02153v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.02152v1","updated":"2024-08-04T22:00:34Z","published":"2024-08-04T22:00:34Z","title":"Generative Retrieval with Few-shot Indexing","summary":"  Existing generative retrieval (GR) approaches rely on training-based\nindexing, i.e., fine-tuning a model to memorise the associations between a\nquery and the document identifier (docid) of a relevant document.\nTraining-based indexing has three limitations: high training overhead,\nunder-utilization of the pre-trained knowledge of large language models (LLMs),\nand challenges in adapting to a dynamic document corpus. To address the above\nissues, we propose a novel few-shot indexing-based GR framework (Few-Shot GR).\nIt has a novel few-shot indexing process, where we prompt an LLM to generate\ndocids for all documents in a corpus, ultimately creating a docid bank for the\nentire corpus. During retrieval, we feed a query to the same LLM and constrain\nit to generate a docid within the docid bank created during indexing, and then\nmap the generated docid back to its corresponding document. Few-Shot GR relies\nsolely on prompting an LLM without requiring any training, making it more\nefficient. Moreover, we devise few-shot indexing with one-to-many mapping to\nfurther enhance Few-Shot GR. Experiments show that Few-Shot GR achieves\nsuperior performance to state-of-the-art GR methods that require heavy\ntraining.\n","authors":["Arian Askari","Chuan Meng","Mohammad Aliannejadi","Zhaochun Ren","Evangelos Kanoulas","Suzan Verberne"],"pdf_url":"https://arxiv.org/pdf/2408.02152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02148v1","updated":"2024-08-04T21:27:36Z","published":"2024-08-04T21:27:36Z","title":"Environment Complexity and Nash Equilibria in a Sequential Social\n  Dilemma","summary":"  Multi-agent reinforcement learning (MARL) methods, while effective in\nzero-sum or positive-sum games, often yield suboptimal outcomes in general-sum\ngames where cooperation is essential for achieving globally optimal outcomes.\nMatrix game social dilemmas, which abstract key aspects of general-sum\ninteractions, such as cooperation, risk, and trust, fail to model the temporal\nand spatial dynamics characteristic of real-world scenarios. In response, our\nstudy extends matrix game social dilemmas into more complex, higher-dimensional\nMARL environments. We adapt a gridworld implementation of the Stag Hunt dilemma\nto more closely match the decision-space of a one-shot matrix game while also\nintroducing variable environment complexity. Our findings indicate that as\ncomplexity increases, MARL agents trained in these environments converge to\nsuboptimal strategies, consistent with the risk-dominant Nash equilibria\nstrategies found in matrix games. Our work highlights the impact of environment\ncomplexity on achieving optimal outcomes in higher-dimensional game-theoretic\nMARL environments.\n","authors":["Mustafa Yasir","Andrew Howes","Vasilios Mavroudis","Chris Hicks"],"pdf_url":"https://arxiv.org/pdf/2408.02148v1.pdf","comment":"Accepted to the 17th European Workshop on Reinforcement Learning\n  (EWRL)"},{"id":"http://arxiv.org/abs/2408.02143v1","updated":"2024-08-04T20:56:05Z","published":"2024-08-04T20:56:05Z","title":"Analyzing Cultural Representations of Emotions in LLMs through Mixed\n  Emotion Survey","summary":"  Large Language Models (LLMs) have gained widespread global adoption,\nshowcasing advanced linguistic capabilities across multiple of languages. There\nis a growing interest in academia to use these models to simulate and study\nhuman behaviors. However, it is crucial to acknowledge that an LLM's\nproficiency in a specific language might not fully encapsulate the norms and\nvalues associated with its culture. Concerns have emerged regarding potential\nbiases towards Anglo-centric cultures and values due to the predominance of\nWestern and US-based training data. This study focuses on analyzing the\ncultural representations of emotions in LLMs, in the specific case of\nmixed-emotion situations. Our methodology is based on the studies of Miyamoto\net al. (2010), which identified distinctive emotional indicators in Japanese\nand American human responses. We first administer their mixed emotion survey to\nfive different LLMs and analyze their outputs. Second, we experiment with\ncontextual variables to explore variations in responses considering both\nlanguage and speaker origin. Thirdly, we expand our investigation to encompass\nadditional East Asian and Western European origin languages to gauge their\nalignment with their respective cultures, anticipating a closer fit. We find\nthat (1) models have limited alignment with the evidence in the literature; (2)\nwritten language has greater effect on LLMs' response than information on\nparticipants origin; and (3) LLMs responses were found more similar for East\nAsian languages than Western European languages.\n","authors":["Shiran Dudy","Ibrahim Said Ahmad","Ryoko Kitajima","Agata Lapedriza"],"pdf_url":"https://arxiv.org/pdf/2408.02143v1.pdf","comment":"Was accepted to ACII 2024"},{"id":"http://arxiv.org/abs/2305.16056v2","updated":"2024-08-04T20:45:16Z","published":"2023-05-25T13:38:53Z","title":"Markov Decision Processes under External Temporal Processes","summary":"  Most reinforcement learning algorithms treat the context under which they\noperate as a stationary, isolated, and undisturbed environment. However, in\nreal world applications, environments constantly change due to a variety of\nexternal events. To address this problem, we study Markov Decision Processes\n(MDP) under the influence of an external temporal process. We formalize this\nnotion and discuss conditions under which the problem becomes tractable with\nsuitable solutions. We propose a policy iteration algorithm to solve this\nproblem and theoretically analyze its performance. We derive results on the\nsample complexity of the algorithm and study its dependency on the extent of\nnon-stationarity of the environment. We then conduct experiments to illustrate\nour results in a classic control environment.\n","authors":["Ranga Shaarad Ayyagari","Ambedkar Dukkipati"],"pdf_url":"https://arxiv.org/pdf/2305.16056v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02140v1","updated":"2024-08-04T20:38:45Z","published":"2024-08-04T20:38:45Z","title":"VidModEx: Interpretable and Efficient Black Box Model Extraction for\n  High-Dimensional Spaces","summary":"  In the domain of black-box model extraction, conventional methods reliant on\nsoft labels or surrogate datasets struggle with scaling to high-dimensional\ninput spaces and managing the complexity of an extensive array of interrelated\nclasses. In this work, we present a novel approach that utilizes SHAP (SHapley\nAdditive exPlanations) to enhance synthetic data generation. SHAP quantifies\nthe individual contributions of each input feature towards the victim model's\noutput, facilitating the optimization of an energy-based GAN towards a\ndesirable output. This method significantly boosts performance, achieving a\n16.45% increase in the accuracy of image classification models and extending to\nvideo classification models with an average improvement of 26.11% and a maximum\nof 33.36% on challenging datasets such as UCF11, UCF101, Kinetics 400, Kinetics\n600, and Something-Something V2. We further demonstrate the effectiveness and\npractical utility of our method under various scenarios, including the\navailability of top-k prediction probabilities, top-k prediction labels, and\ntop-1 labels.\n","authors":["Somnath Sendhil Kumar","Yuvaraj Govindarajulu","Pavan Kulkarni","Manojkumar Parmar"],"pdf_url":"https://arxiv.org/pdf/2408.02140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02117v1","updated":"2024-08-04T19:14:36Z","published":"2024-08-04T19:14:36Z","title":"Value-Based Rationales Improve Social Experience: A Multiagent\n  Simulation Study","summary":"  We propose Exanna, a framework to realize agents that incorporate values in\ndecision making. An Exannaagent considers the values of itself and others when\nproviding rationales for its actions and evaluating the rationales provided by\nothers. Via multiagent simulation, we demonstrate that considering values in\ndecision making and producing rationales, especially for norm-deviating\nactions, leads to (1) higher conflict resolution, (2) better social experience,\n(3) higher privacy, and (4) higher flexibility.\n","authors":["Sz-Ting Tzeng","Nirav Ajmeri","Munindar P. Singh"],"pdf_url":"https://arxiv.org/pdf/2408.02117v1.pdf","comment":"13 pages, 13 figures, 13 tables (and supplementary material with\n  reproducibility and additional results), accepted at ECAI 2024"},{"id":"http://arxiv.org/abs/2408.02113v1","updated":"2024-08-04T18:54:59Z","published":"2024-08-04T18:54:59Z","title":"Diseño de sonido para producciones audiovisuales e historias sonoras\n  en el aula. Hacia una docencia creativa mediante el uso de herramientas\n  inteligentes","summary":"  This study aims to share a teaching experience teaching sound design for\naudiovisual productions and compares different projects tackled by students. It\nis not intended to be a comparative analysis of different types of teaching but\nrather an analysis of different problems observed in different profiles of\nstudents of the subject who study it in different grades. The world of audio\ncan be very interesting for a large part of the students, both those with\ncreative and technical inclinations. Musical creation and production,\nsynchronization with images, dubbing, etc. They are disciplines that are\ngenerally interesting but can have a very high barrier to entry due to their\ngreat technical complexity. Sometimes it can take weeks or even months for the\nuninitiated to begin to use audio editing programs with the necessary ease,\nwhich are not always particularly intuitive for students. Learning through the\nuse of PBL methodologies generates, in our experience, results much superior to\nthose that can be observed through the use of other teaching methods such as\nmaster classes. Students acquire technical skills while developing creative\nprojects in which they get personally involved. Despite everything mentioned\nabove, most interactions between teachers and students focus on aspects of\ntechnical correction. From different parameters in reverbs (such as pre-delay,\ndecay, modulation...) to how to correctly adjust compressors, noise gates,\netc.; The number of tools with which to work with audio is incredibly\nextensive, as well as many of its features that can present serious differences\ndepending on their manufacturers.\n","authors":["Miguel Civit","Francisco Cuadrado"],"pdf_url":"https://arxiv.org/pdf/2408.02113v1.pdf","comment":"11 pages, in Spanish language. 1 figure. In La nueva era del\n  p\\'odcast"},{"id":"http://arxiv.org/abs/2408.02111v1","updated":"2024-08-04T18:47:55Z","published":"2024-08-04T18:47:55Z","title":"Understanding Deep Learning via Notions of Rank","summary":"  Despite the extreme popularity of deep learning in science and industry, its\nformal understanding is limited. This thesis puts forth notions of rank as key\nfor developing a theory of deep learning, focusing on the fundamental aspects\nof generalization and expressiveness. In particular, we establish that\ngradient-based training can induce an implicit regularization towards low rank\nfor several neural network architectures, and demonstrate empirically that this\nphenomenon may facilitate an explanation of generalization over natural data\n(e.g., audio, images, and text). Then, we characterize the ability of graph\nneural networks to model interactions via a notion of rank, which is commonly\nused for quantifying entanglement in quantum physics. A central tool underlying\nthese results is a connection between neural networks and tensor\nfactorizations. Practical implications of our theory for designing explicit\nregularization schemes and data preprocessing algorithms are presented.\n","authors":["Noam Razin"],"pdf_url":"https://arxiv.org/pdf/2408.02111v1.pdf","comment":"PhD thesis"},{"id":"http://arxiv.org/abs/2405.01013v2","updated":"2024-08-04T18:09:39Z","published":"2024-05-02T05:29:22Z","title":"Non-clairvoyant Scheduling with Partial Predictions","summary":"  The non-clairvoyant scheduling problem has gained new interest within\nlearning-augmented algorithms, where the decision-maker is equipped with\npredictions without any quality guarantees. In practical settings, access to\npredictions may be reduced to specific instances, due to cost or data\nlimitations. Our investigation focuses on scenarios where predictions for only\n$B$ job sizes out of $n$ are available to the algorithm. We first establish\nnear-optimal lower bounds and algorithms in the case of perfect predictions.\nSubsequently, we present a learning-augmented algorithm satisfying the\nrobustness, consistency, and smoothness criteria, and revealing a novel\ntradeoff between consistency and smoothness inherent in the scenario with a\nrestricted number of predictions.\n","authors":["Ziyad Benomar","Vianney Perchet"],"pdf_url":"https://arxiv.org/pdf/2405.01013v2.pdf","comment":"Accepted as a conference paper at ICML 2024"},{"id":"http://arxiv.org/abs/2306.07209v6","updated":"2024-08-04T17:54:15Z","published":"2023-06-12T16:12:56Z","title":"Data-Copilot: Bridging Billions of Data and Humans with Autonomous\n  Workflow","summary":"  Industries such as finance, meteorology, and energy generate vast amounts of\ndata daily. Efficiently managing, processing, and displaying this data requires\nspecialized expertise and is often tedious and repetitive. Leveraging large\nlanguage models (LLMs) to develop an automated workflow presents a highly\npromising solution. However, LLMs are not adept at handling complex numerical\ncomputations and table manipulations and are also constrained by a limited\ncontext budget. Based on this, we propose Data-Copilot, a data analysis agent\nthat autonomously performs querying, processing, and visualization of massive\ndata tailored to diverse human requests. The advancements are twofold: First,\nit is a code-centric agent that receives human requests and generates code as\nan intermediary to handle massive data, which is quite flexible for large-scale\ndata processing tasks. Second, Data-Copilot involves a data exploration phase\nin advance, which explores how to design more universal and error-free\ninterfaces for real-time response. Specifically, it actively explores data\nsources, discovers numerous common requests, and abstracts them into many\nuniversal interfaces for daily invocation. When deployed in real-time requests,\nData-Copilot only needs to invoke these pre-designed interfaces, transforming\nraw data into visualized outputs (e.g., charts, tables) that best match the\nuser's intent. Compared to generating code from scratch, invoking these\npre-designed and compiler-validated interfaces can significantly reduce errors\nduring real-time requests. Additionally, interface workflows are more efficient\nand offer greater interpretability than code. We open-sourced Data-Copilot with\nmassive Chinese financial data, such as stocks, funds, and news, demonstrating\npromising application prospects.\n","authors":["Wenqi Zhang","Yongliang Shen","Weiming Lu","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2306.07209v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14302v2","updated":"2024-08-04T17:48:33Z","published":"2023-12-21T21:22:41Z","title":"Exploiting Novel GPT-4 APIs","summary":"  Language model attacks typically assume one of two extreme threat models:\nfull white-box access to model weights, or black-box access limited to a text\ngeneration API. However, real-world APIs are often more flexible than just text\ngeneration: these APIs expose \"gray-box\" access leading to new threat vectors.\nTo explore this, we red-team three new functionalities exposed in the GPT-4\nAPIs: fine-tuning, function calling and knowledge retrieval. We find that\nfine-tuning a model on as few as 15 harmful examples or 100 benign examples can\nremove core safeguards from GPT-4, enabling a range of harmful outputs.\nFurthermore, we find that GPT-4 Assistants readily divulge the function call\nschema and can be made to execute arbitrary function calls. Finally, we find\nthat knowledge retrieval can be hijacked by injecting instructions into\nretrieval documents. These vulnerabilities highlight that any additions to the\nfunctionality exposed by an API can create new vulnerabilities.\n","authors":["Kellin Pelrine","Mohammad Taufeeque","Michał Zając","Euan McLean","Adam Gleave"],"pdf_url":"https://arxiv.org/pdf/2312.14302v2.pdf","comment":"10 pages, 1 figure, 4 tables"},{"id":"http://arxiv.org/abs/2311.13852v3","updated":"2024-08-04T17:07:23Z","published":"2023-11-23T08:42:18Z","title":"A Cross Attention Approach to Diagnostic Explainability using Clinical\n  Practice Guidelines for Depression","summary":"  The lack of explainability using relevant clinical knowledge hinders the\nadoption of Artificial Intelligence-powered analysis of unstructured clinical\ndialogue. A wealth of relevant, untapped Mental Health (MH) data is available\nin online communities, providing the opportunity to address the explainability\nproblem with substantial potential impact as a screening tool for both online\nand offline applications. We develop a method to enhance attention in popular\ntransformer models and generate clinician-understandable explanations for\nclassification by incorporating external clinical knowledge. Inspired by how\nclinicians rely on their expertise when interacting with patients, we leverage\nrelevant clinical knowledge to model patient inputs, providing meaningful\nexplanations for classification. This will save manual review time and engender\ntrust. We develop such a system in the context of MH using clinical practice\nguidelines (CPG) for diagnosing depression, a mental health disorder of global\nconcern. We propose an application-specific language model called ProcesS\nknowledge-infused cross ATtention (PSAT), which incorporates CPGs when\ncomputing attention. Through rigorous evaluation on three expert-curated\ndatasets related to depression, we demonstrate application-relevant\nexplainability of PSAT. PSAT also surpasses the performance of nine baseline\nmodels and can provide explanations where other baselines fall short. We\ntransform a CPG resource focused on depression, such as the Patient Health\nQuestionnaire (e.g. PHQ-9) and related questions, into a machine-readable\nontology using SNOMED-CT. With this resource, PSAT enhances the ability of\nmodels like GPT-3.5 to generate application-relevant explanations.\n","authors":["Sumit Dalal","Deepa Tilwani","Kaushik Roy","Manas Gaur","Sarika Jain","Valerie Shalin","Amit Sheth"],"pdf_url":"https://arxiv.org/pdf/2311.13852v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02088v1","updated":"2024-08-04T16:54:49Z","published":"2024-08-04T16:54:49Z","title":"KAN-RCBEVDepth: A multi-modal fusion algorithm in object detection for\n  autonomous driving","summary":"  Accurate 3D object detection in autonomous driving is critical yet\nchallenging due to occlusions, varying object scales, and complex urban\nenvironments. This paper introduces the RCBEV-KAN algorithm, a pioneering\nmethod designed to enhance 3D object detection by fusing multimodal sensor data\nfrom cameras, LiDAR, and millimeter-wave radar. Our innovative Bird's Eye View\n(BEV)-based approach, utilizing a Transformer architecture, significantly\nboosts detection precision and efficiency by seamlessly integrating diverse\ndata sources, improving spatial relationship handling, and optimizing\ncomputational processes. Experimental results show that the RCBEV-KAN model\ndemonstrates superior performance across most detection categories, achieving\nhigher Mean Distance AP (0.389 vs. 0.316, a 23% improvement), better ND Score\n(0.484 vs. 0.415, a 17% improvement), and faster Evaluation Time (71.28s, 8%\nfaster). These results indicate that RCBEV-KAN is more accurate, reliable, and\nefficient, making it ideal for dynamic and challenging autonomous driving\nenvironments.\n","authors":["Zhihao Lai","Chuanhao Liu","Shihui Sheng","Zhiqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.02088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02085v1","updated":"2024-08-04T16:50:07Z","published":"2024-08-04T16:50:07Z","title":"Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data\n  Assessment and Selection for Instruction Tuning of Language Models","summary":"  Instruction tuning plays a critical role in aligning large language models\n(LLMs) with human preference. Despite the vast amount of open instruction\ndatasets, naively training a LLM on all existing instructions may not be\noptimal and practical. To pinpoint the most beneficial datapoints, data\nassessment and selection methods have been proposed in the fields of natural\nlanguage processing (NLP) and deep learning. However, under the context of\ninstruction tuning, there still exists a gap in knowledge on what kind of data\nevaluation metrics can be employed and how they can be integrated into the\nselection mechanism. To bridge this gap, we present a comprehensive review on\nexisting literature of data assessment and selection especially for instruction\ntuning of LLMs. We systematically categorize all applicable methods into\nquality-based, diversity-based, and importance-based ones where a unified,\nfine-grained taxonomy is structured. For each category, representative methods\nare elaborated to describe the landscape of relevant research. In addition,\ncomparison between latest methods is conducted on their officially reported\nresults to provide in-depth discussions on their limitations. Finally, we\nsummarize the open challenges and propose the promosing avenues for future\nstudies. All related contents are available at\nhttps://github.com/yuleiqin/fantastic-data-engineering.\n","authors":["Yulei Qin","Yuncheng Yang","Pengcheng Guo","Gang Li","Hang Shao","Yuchen Shi","Zihan Xu","Yun Gu","Ke Li","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2408.02085v1.pdf","comment":"review, survey, 28 pages, 2 figures, 4 tables"},{"id":"http://arxiv.org/abs/2407.17454v2","updated":"2024-08-04T16:46:00Z","published":"2024-07-24T17:41:32Z","title":"Automated Explanation Selection for Scientific Discovery","summary":"  Automated reasoning is a key technology in the young but rapidly growing\nfield of Explainable Artificial Intelligence (XAI). Explanability helps build\ntrust in artificial intelligence systems beyond their mere predictive accuracy\nand robustness. In this paper, we propose a cycle of scientific discovery that\ncombines machine learning with automated reasoning for the generation and the\nselection of explanations. We present a taxonomy of explanation selection\nproblems that draws on insights from sociology and cognitive science. These\nselection criteria subsume existing notions and extend them with new\nproperties.\n","authors":["Markus Iser"],"pdf_url":"https://arxiv.org/pdf/2407.17454v2.pdf","comment":"Composite AI Workshop at ECAI 2024 (accepted for publication)"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2209.03885v4","updated":"2024-08-04T23:45:40Z","published":"2022-09-08T15:41:31Z","title":"A Framework for Evaluating Privacy-Utility Trade-off in Vertical\n  Federated Learning","summary":"  Federated learning (FL) has emerged as a practical solution to tackle data\nsilo issues without compromising user privacy. One of its variants, vertical\nfederated learning (VFL), has recently gained increasing attention as the VFL\nmatches the enterprises' demands of leveraging more valuable features to build\nbetter machine learning models while preserving user privacy. Current works in\nVFL concentrate on developing a specific protection or attack mechanism for a\nparticular VFL algorithm. In this work, we propose an evaluation framework that\nformulates the privacy-utility evaluation problem. We then use this framework\nas a guide to comprehensively evaluate a broad range of protection mechanisms\nagainst most of the state-of-the-art privacy attacks for three widely deployed\nVFL algorithms. These evaluations may help FL practitioners select appropriate\nprotection mechanisms given specific requirements. Our evaluation results\ndemonstrate that: the model inversion and most of the label inference attacks\ncan be thwarted by existing protection mechanisms; the model completion (MC)\nattack is difficult to be prevented, which calls for more advanced MC-targeted\nprotection mechanisms. Based on our evaluation results, we offer concrete\nadvice on improving the privacy-preserving capability of VFL systems. The code\nis available at https://github.com/yankang18/Attack-Defense-VFL\n","authors":["Yan Kang","Jiahuan Luo","Yuanqin He","Xiaojin Zhang","Lixin Fan","Qiang Yang"],"pdf_url":"https://arxiv.org/pdf/2209.03885v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02165v1","updated":"2024-08-04T23:23:48Z","published":"2024-08-04T23:23:48Z","title":"SelfBC: Self Behavior Cloning for Offline Reinforcement Learning","summary":"  Policy constraint methods in offline reinforcement learning employ additional\nregularization techniques to constrain the discrepancy between the learned\npolicy and the offline dataset. However, these methods tend to result in overly\nconservative policies that resemble the behavior policy, thus limiting their\nperformance. We investigate this limitation and attribute it to the static\nnature of traditional constraints. In this paper, we propose a novel dynamic\npolicy constraint that restricts the learned policy on the samples generated by\nthe exponential moving average of previously learned policies. By integrating\nthis self-constraint mechanism into off-policy methods, our method facilitates\nthe learning of non-conservative policies while avoiding policy collapse in the\noffline setting. Theoretical results show that our approach results in a nearly\nmonotonically improved reference policy. Extensive experiments on the D4RL\nMuJoCo domain demonstrate that our proposed method achieves state-of-the-art\nperformance among the policy constraint methods.\n","authors":["Shirong Liu","Chenjia Bai","Zixian Guo","Hao Zhang","Gaurav Sharma","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2408.02165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02161v1","updated":"2024-08-04T23:05:42Z","published":"2024-08-04T23:05:42Z","title":"Distilling Machine Learning's Added Value: Pareto Fronts in Atmospheric\n  Applications","summary":"  While the added value of machine learning (ML) for weather and climate\napplications is measurable, explaining it remains challenging, especially for\nlarge deep learning models. Inspired by climate model hierarchies, we propose\nthat a full hierarchy of Pareto-optimal models, defined within an appropriately\ndetermined error-complexity plane, can guide model development and help\nunderstand the models' added value. We demonstrate the use of Pareto fronts in\natmospheric physics through three sample applications, with hierarchies ranging\nfrom semi-empirical models with minimal tunable parameters (simplest) to deep\nlearning algorithms (most complex). First, in cloud cover parameterization, we\nfind that neural networks identify nonlinear relationships between cloud cover\nand its thermodynamic environment, and assimilate previously neglected features\nsuch as vertical gradients in relative humidity that improve the representation\nof low cloud cover. This added value is condensed into a ten-parameter equation\nthat rivals the performance of deep learning models. Second, we establish a ML\nmodel hierarchy for emulating shortwave radiative transfer, distilling the\nimportance of bidirectional vertical connectivity for accurately representing\nabsorption and scattering, especially for multiple cloud layers. Third, we\nemphasize the importance of convective organization information when modeling\nthe relationship between tropical precipitation and its surrounding\nenvironment. We discuss the added value of temporal memory when high-resolution\nspatial information is unavailable, with implications for precipitation\nparameterization. Therefore, by comparing data-driven models directly with\nexisting schemes using Pareto optimality, we promote process understanding by\nhierarchically unveiling system complexity, with the hope of improving the\ntrustworthiness of ML models in atmospheric applications.\n","authors":["Tom Beucler","Arthur Grundner","Sara Shamekh","Peter Ukkonen","Matthew Chantry","Ryan Lagerquist"],"pdf_url":"https://arxiv.org/pdf/2408.02161v1.pdf","comment":"18 pages, 4 figures, submitted to AMS Artificial Intelligence for the\n  Earth Systems (AIES)"},{"id":"http://arxiv.org/abs/2307.01357v3","updated":"2024-08-04T22:31:59Z","published":"2023-07-03T21:13:40Z","title":"Adaptive Principal Component Regression with Applications to Panel Data","summary":"  Principal component regression (PCR) is a popular technique for fixed-design\nerror-in-variables regression, a generalization of the linear regression\nsetting in which the observed covariates are corrupted with random noise. We\nprovide the first time-uniform finite sample guarantees for (regularized) PCR\nwhenever data is collected adaptively. Since the proof techniques for analyzing\nPCR in the fixed design setting do not readily extend to the online setting,\nour results rely on adapting tools from modern martingale concentration to the\nerror-in-variables setting. We demonstrate the usefulness of our bounds by\napplying them to the domain of panel data, a ubiquitous setting in econometrics\nand statistics. As our first application, we provide a framework for experiment\ndesign in panel data settings when interventions are assigned adaptively. Our\nframework may be thought of as a generalization of the synthetic control and\nsynthetic interventions frameworks, where data is collected via an adaptive\nintervention assignment policy. Our second application is a procedure for\nlearning such an intervention assignment policy in a setting where units arrive\nsequentially to be treated. In addition to providing theoretical performance\nguarantees (as measured by regret), we show that our method empirically\noutperforms a baseline which does not leverage error-in-variables regression.\n","authors":["Anish Agarwal","Keegan Harris","Justin Whitehouse","Zhiwei Steven Wu"],"pdf_url":"https://arxiv.org/pdf/2307.01357v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02159v1","updated":"2024-08-04T22:26:34Z","published":"2024-08-04T22:26:34Z","title":"SPINEX-TimeSeries: Similarity-based Predictions with Explainable\n  Neighbors Exploration for Time Series and Forecasting Problems","summary":"  This paper introduces a new addition to the SPINEX (Similarity-based\nPredictions with Explainable Neighbors Exploration) family, tailored\nspecifically for time series and forecasting analysis. This new algorithm\nleverages the concept of similarity and higher-order temporal interactions\nacross multiple time scales to enhance predictive accuracy and interpretability\nin forecasting. To evaluate the effectiveness of SPINEX, we present\ncomprehensive benchmarking experiments comparing it against 18 algorithms and\nacross 49 synthetic and real datasets characterized by varying trends,\nseasonality, and noise levels. Our performance assessment focused on\nforecasting accuracy and computational efficiency. Our findings reveal that\nSPINEX consistently ranks among the top 5 performers in forecasting precision\nand has a superior ability to handle complex temporal dynamics compared to\ncommonly adopted algorithms. Moreover, the algorithm's explainability features,\nPareto efficiency, and medium complexity (on the order of O(log n)) are\ndemonstrated through detailed visualizations to enhance the prediction and\ndecision-making process. We note that integrating similarity-based concepts\nopens new avenues for research in predictive analytics, promising more accurate\nand transparent decision making.\n","authors":["Ahmed Z Naser","MZ Naser"],"pdf_url":"https://arxiv.org/pdf/2408.02159v1.pdf","comment":null}]},"2024-08-03T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2408.01877v1","updated":"2024-08-03T22:55:26Z","published":"2024-08-03T22:55:26Z","title":"Is Generative Communication between Embodied Agents Good for Zero-Shot\n  ObjectNav?","summary":"  In Zero-Shot ObjectNav, an embodied ground agent is expected to navigate to a\ntarget object specified by a natural language label without any\nenvironment-specific fine-tuning. This is challenging, given the limited view\nof a ground agent and its independent exploratory behavior. To address these\nissues, we consider an assistive overhead agent with a bounded global view\nalongside the ground agent and present two coordinated navigation schemes for\njudicious exploration. We establish the influence of the Generative\nCommunication (GC) between the embodied agents equipped with Vision-Language\nModels (VLMs) in improving zero-shot ObjectNav, achieving a 10% improvement in\nthe ground agent's ability to find the target object in comparison with an\nunassisted setup in simulation. We further analyze the GC for unique traits\nquantifying the presence of hallucination and cooperation. In particular, we\nidentify a unique trait of \"preemptive hallucination\" specific to our embodied\nsetting, where the overhead agent assumes that the ground agent has executed an\naction in the dialogue when it is yet to move. Finally, we conduct real-world\ninferences with GC and showcase qualitative examples where countering\npre-emptive hallucination via prompt finetuning improves real-world ObjectNav\nperformance.\n","authors":["Vishnu Sashank Dorbala","Vishnu Dutt Sharma","Pratap Tokekar","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2408.01877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01867v1","updated":"2024-08-03T21:32:43Z","published":"2024-08-03T21:32:43Z","title":"TrustNavGPT: Modeling Uncertainty to Improve Trustworthiness of\n  Audio-Guided LLM-Based Robot Navigation","summary":"  While LLMs are proficient at processing text in human conversations, they\noften encounter difficulties with the nuances of verbal instructions and, thus,\nremain prone to hallucinate trust in human command. In this work, we present\nTrustNavGPT, an LLM based audio guided navigation agent that uses affective\ncues in spoken communication elements such as tone and inflection that convey\nmeaning beyond words, allowing it to assess the trustworthiness of human\ncommands and make effective, safe decisions. Our approach provides a\nlightweight yet effective approach that extends existing LLMs to model audio\nvocal features embedded in the voice command and model uncertainty for safe\nrobotic navigation.\n","authors":["Xingpeng Sun","Yiran Zhang","Xindi Tang","Amrit Singh Bedi","Aniket Bera"],"pdf_url":"https://arxiv.org/pdf/2408.01867v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01841v1","updated":"2024-08-03T18:48:41Z","published":"2024-08-03T18:48:41Z","title":"BEVPlace++: Fast, Robust, and Lightweight LiDAR Global Localization for\n  Unmanned Ground Vehicles","summary":"  This article introduces BEVPlace++, a novel, fast, and robust LiDAR global\nlocalization method for unmanned ground vehicles. It uses lightweight\nconvolutional neural networks (CNNs) on Bird's Eye View (BEV) image-like\nrepresentations of LiDAR data to achieve accurate global localization through\nplace recognition followed by 3-DoF pose estimation. Our detailed analyses\nreveal an interesting fact that CNNs are inherently effective at extracting\ndistinctive features from LiDAR BEV images. Remarkably, keypoints of two BEV\nimages with large translations can be effectively matched using CNN-extracted\nfeatures. Building on this insight, we design a rotation equivariant module\n(REM) to obtain distinctive features while enhancing robustness to rotational\nchanges. A Rotation Equivariant and Invariant Network (REIN) is then developed\nby cascading REM and a descriptor generator, NetVLAD, to sequentially generate\nrotation equivariant local features and rotation invariant global descriptors.\nThe global descriptors are used first to achieve robust place recognition, and\nthe local features are used for accurate pose estimation. Experimental results\non multiple public datasets demonstrate that BEVPlace++, even when trained on a\nsmall dataset (3000 frames of KITTI) only with place labels, generalizes well\nto unseen environments, performs consistently across different days and years,\nand adapts to various types of LiDAR scanners. BEVPlace++ achieves\nstate-of-the-art performance in subtasks of global localization including place\nrecognition, loop closure detection, and global localization. Additionally,\nBEVPlace++ is lightweight, runs in real-time, and does not require accurate\npose supervision, making it highly convenient for deployment. The source codes\nare publicly available at\n\\href{https://github.com/zjuluolun/BEVPlace}{https://github.com/zjuluolun/BEVPlace}.\n","authors":["Lun Luo","Siyuan Cao","Xiaorui Li","Jintao Xu","Rui Ai","Zhu Yu","Xieyuanli Chen"],"pdf_url":"https://arxiv.org/pdf/2408.01841v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2404.03017v2","updated":"2024-08-03T18:43:14Z","published":"2024-04-03T18:57:54Z","title":"Distributionally Robust Policy and Lyapunov-Certificate Learning","summary":"  This article presents novel methods for synthesizing distributionally robust\nstabilizing neural controllers and certificates for control systems under model\nuncertainty. A key challenge in designing controllers with stability guarantees\nfor uncertain systems is the accurate determination of and adaptation to shifts\nin model parametric uncertainty during online deployment. We tackle this with a\nnovel distributionally robust formulation of the Lyapunov derivative chance\nconstraint ensuring a monotonic decrease of the Lyapunov certificate. To avoid\nthe computational complexity involved in dealing with the space of probability\nmeasures, we identify a sufficient condition in the form of deterministic\nconvex constraints that ensures the Lyapunov derivative constraint is\nsatisfied. We integrate this condition into a loss function for training a\nneural network-based controller and show that, for the resulting closed-loop\nsystem, the global asymptotic stability of its equilibrium can be certified\nwith high confidence, even with Out-of-Distribution (OoD) model uncertainties.\nTo demonstrate the efficacy and efficiency of the proposed methodology, we\ncompare it with an uncertainty-agnostic baseline approach and several\nreinforcement learning approaches in two control problems in simulation.\n","authors":["Kehan Long","Jorge Cortes","Nikolay Atanasov"],"pdf_url":"https://arxiv.org/pdf/2404.03017v2.pdf","comment":"Accepted to IEEE Open Journal of Control Systems"},{"id":"http://arxiv.org/abs/2404.06089v3","updated":"2024-08-03T12:11:12Z","published":"2024-04-09T07:48:49Z","title":"EVE: Enabling Anyone to Train Robots using Augmented Reality","summary":"  The increasing affordability of robot hardware is accelerating the\nintegration of robots into everyday activities. However, training a robot to\nautomate a task requires expensive trajectory data where a trained human\nannotator moves a physical robot to train it. Consequently, only those with\naccess to robots produce demonstrations to train robots. In this work, we\nremove this restriction with EVE, an iOS app that enables everyday users to\ntrain robots using intuitive augmented reality visualizations, without needing\na physical robot. With EVE, users can collect demonstrations by specifying\nwaypoints with their hands, visually inspecting the environment for obstacles,\nmodifying existing waypoints, and verifying collected trajectories. In a user\nstudy (N=14, D=30) consisting of three common tabletop tasks, EVE outperformed\nthree state-of-the-art interfaces in success rate and was comparable to\nkinesthetic teaching-physically moving a physical robot-in completion time,\nusability, motion intent communication, enjoyment, and preference (mean of\np=0.30). EVE allows users to train robots for personalized tasks, such as\nsorting desk supplies, organizing ingredients, or setting up board games. We\nconclude by enumerating limitations and design considerations for future\nAR-based demonstration collection systems for robotics.\n","authors":["Jun Wang","Chun-Cheng Chang","Jiafei Duan","Dieter Fox","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2404.06089v3.pdf","comment":"13 pages, UIST 2024"},{"id":"http://arxiv.org/abs/2408.01737v1","updated":"2024-08-03T10:39:42Z","published":"2024-08-03T10:39:42Z","title":"Real-time Localization and Mapping in Architectural Plans with\n  Deviations","summary":"  Having prior knowledge of an environment boosts the localization and mapping\naccuracy of robots. Several approaches in the literature have utilized\narchitectural plans in this regard. However, almost all of them overlook the\ndeviations between actual as-built environments and as-planned architectural\ndesigns, introducing bias in the estimations. To address this issue, we present\na novel localization and mapping method denoted as deviations-informed\nSituational Graphs or diS-Graphs that integrates prior knowledge from\narchitectural plans even in the presence of deviations. It is based on\nSituational Graphs (S-Graphs) that merge geometric models of the environment\nwith 3D scene graphs into a multi-layered jointly optimizable factor graph. Our\ndiS-Graph extracts information from architectural plans by first modeling them\nas a hierarchical factor graph, which we will call an Architectural Graph\n(A-Graph). While the robot explores the real environment, it estimates an\nS-Graph from its onboard sensors. We then use a novel matching algorithm to\nregister the A-Graph and S-Graph in the same reference, and merge both of them\nwith an explicit model of deviations. Finally, an alternating graph\noptimization strategy allows simultaneous global localization and mapping, as\nwell as deviation estimation between both the A-Graph and the S-Graph. We\nperform several experiments in simulated and real datasets in the presence of\ndeviations. On average, our diS-Graphs outperforms the baselines by a margin of\napproximately 43% in simulated environments and by 7% in real environments,\nwhile being able to estimate deviations up to 35 cm and 15 degrees.\n","authors":["Muhammad Shaheer","Jose Andres Millan-Romera","Hriday Bavle","Marco Giberna","Jose Luis Sanchez-Lopez","Javier Civera","Holger Voos"],"pdf_url":"https://arxiv.org/pdf/2408.01737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11537v3","updated":"2024-08-03T10:19:54Z","published":"2024-05-19T12:56:00Z","title":"VR-GPT: Visual Language Model for Intelligent Virtual Reality\n  Applications","summary":"  The advent of immersive Virtual Reality applications has transformed various\ndomains, yet their integration with advanced artificial intelligence\ntechnologies like Visual Language Models remains underexplored. This study\nintroduces a pioneering approach utilizing VLMs within VR environments to\nenhance user interaction and task efficiency. Leveraging the Unity engine and a\ncustom-developed VLM, our system facilitates real-time, intuitive user\ninteractions through natural language processing, without relying on visual\ntext instructions. The incorporation of speech-to-text and text-to-speech\ntechnologies allows for seamless communication between the user and the VLM,\nenabling the system to guide users through complex tasks effectively.\nPreliminary experimental results indicate that utilizing VLMs not only reduces\ntask completion times but also improves user comfort and task engagement\ncompared to traditional VR interaction methods.\n","authors":["Mikhail Konenkov","Artem Lykov","Daria Trinitatova","Dzmitry Tsetserukou"],"pdf_url":"https://arxiv.org/pdf/2405.11537v3.pdf","comment":"Updated version"},{"id":"http://arxiv.org/abs/2408.01729v1","updated":"2024-08-03T10:01:44Z","published":"2024-08-03T10:01:44Z","title":"A Survey on Robotic Prosthetics: Neuroprosthetics, Soft Actuators, and\n  Control Strategies","summary":"  The field of robotics is a quickly evolving feat of technology that accepts\ncontributions from various genres of science. Neuroscience, Physiology,\nChemistry, Material science, Computer science, and the wide umbrella of\nmechatronics have all simultaneously contributed to many innovations in the\nprosthetic applications of robotics. This review begins with a discussion of\nthe scope of the term robotic prosthetics and discusses the evolving domain of\nNeuroprosthetics. The discussion is then constrained to focus on various\nactuation and control strategies for robotic prosthetic limbs. This review\ndiscusses various soft robotic actuators such as EAP, SMA, FFA, etc., and the\nmerits of such actuators over conventional hard robotic actuators. Options in\ncontrol strategies for robotic prosthetics, that are in various states of\nresearch and development, are reviewed. This paper concludes the discussion\nwith an analysis regarding the prospective direction in which this field of\nrobotic prosthetics is evolving in terms of actuation, control, and other\nfeatures relevant to artificial limbs. This paper intends to review some of the\nemerging research and development trends in the field of robotic prosthetics\nand summarize many tangents that are represented under this broad domain in an\napproachable manner.\n","authors":["Kumar J. Jyothish","Subhankar Mishra"],"pdf_url":"https://arxiv.org/pdf/2408.01729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01716v1","updated":"2024-08-03T09:10:38Z","published":"2024-08-03T09:10:38Z","title":"Visual-Inertial SLAM for Agricultural Robotics: Benchmarking the\n  Benefits and Computational Costs of Loop Closing","summary":"  Simultaneous Localization and Mapping (SLAM) is essential for mobile\nrobotics, enabling autonomous navigation in dynamic, unstructured outdoor\nenvironments without relying on external positioning systems. In agricultural\napplications, where environmental conditions can be particularly challenging\ndue to variable lighting or weather conditions, Visual-Inertial SLAM has\nemerged as a potential solution. This paper benchmarks several open-source\nVisual-Inertial SLAM systems, including ORB-SLAM3, VINS-Fusion, OpenVINS,\nKimera, and SVO Pro, to evaluate their performance in agricultural settings. We\nfocus on the impact of loop closing on localization accuracy and computational\ndemands, providing a comprehensive analysis of these systems' effectiveness in\nreal-world environments and especially their application to embedded systems in\nagricultural robotics. Our contributions further include an assessment of\nvarying frame rates on localization accuracy and computational load. The\nfindings highlight the importance of loop closing in improving localization\naccuracy while managing computational resources efficiently, offering valuable\ninsights for optimizing Visual-Inertial SLAM systems for practical outdoor\napplications in mobile robotics.\n","authors":["Fabian Schmidt","Constantin Blessing","Markus Enzweiler","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2408.01716v1.pdf","comment":"18 pages, 8 figures, 5 tables"},{"id":"http://arxiv.org/abs/2408.01676v1","updated":"2024-08-03T06:25:10Z","published":"2024-08-03T06:25:10Z","title":"Prototyping of a multirotor UAV for precision landing under rotor\n  failures","summary":"  This work presents a prototype of a multirotor aerial vehicle capable of\nprecision landing, even under the effects of rotor failures. The manuscript\npresents the fault-tolerant techniques and mechanical designs to achieve a\nfault-tolerant multirotor, and a vision-based navigation system required to\nachieve a precision landing. Preliminary experimental results will be shown, to\nvalidate on one hand the fault-tolerant control vehicle and, on the other hand,\nthe autonomous landing algorithm. Also, a prototype of the fault-tolerant UAV\nis presented, capable of precise autonomous landing, which will be used in\nfuture experiments.\n","authors":["Alvaro J. Gaona","Claudio D. Pose","Juan I. Giribet","Roberto Bunge"],"pdf_url":"https://arxiv.org/pdf/2408.01676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01655v1","updated":"2024-08-03T03:53:05Z","published":"2024-08-03T03:53:05Z","title":"Stimulating Imagination: Towards General-purpose Object Rearrangement","summary":"  General-purpose object placement is a fundamental capability of an\nintelligent generalist robot, i.e., being capable of rearranging objects\nfollowing human instructions even in novel environments. To achieve this, we\nbreak the rearrangement down into three parts, including object localization,\ngoal imagination and robot control, and propose a framework named SPORT. SPORT\nleverages pre-trained large vision models for broad semantic reasoning about\nobjects, and learns a diffusion-based 3D pose estimator to ensure\nphysically-realistic results. Only object types (to be moved or reference) are\ncommunicated between these two parts, which brings two benefits. One is that we\ncan fully leverage the powerful ability of open-set object localization and\nrecognition since no specific fine-tuning is needed for robotic scenarios.\nFurthermore, the diffusion-based estimator only need to \"imagine\" the poses of\nthe moving and reference objects after the placement, while no necessity for\ntheir semantic information. Thus the training burden is greatly reduced and no\nmassive training is required. The training data for goal pose estimation is\ncollected in simulation and annotated with GPT-4. A set of simulation and\nreal-world experiments demonstrate the potential of our approach to accomplish\ngeneral-purpose object rearrangement, placing various objects following precise\ninstructions.\n","authors":["Jianyang Wu","Jie Gu","Xiaokang Ma","Chu Tang","Jingmin Chen"],"pdf_url":"https://arxiv.org/pdf/2408.01655v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2408.01649v1","updated":"2024-08-03T03:20:21Z","published":"2024-08-03T03:20:21Z","title":"LF-3PM: a LiDAR-based Framework for Perception-aware Planning with\n  Perturbation-induced Metric","summary":"  Just as humans can become disoriented in featureless deserts or thick fogs,\nnot all environments are conducive to the Localization Accuracy and Stability\n(LAS) of autonomous robots. This paper introduces an efficient framework\ndesigned to enhance LiDAR-based LAS through strategic trajectory generation,\nknown as Perception-aware Planning. Unlike vision-based frameworks, the\nLiDAR-based requires different considerations due to unique sensor attributes.\nOur approach focuses on two main aspects: firstly, assessing the impact of\nLiDAR observations on LAS. We introduce a perturbation-induced metric to\nprovide a comprehensive and reliable evaluation of LiDAR observations.\nSecondly, we aim to improve motion planning efficiency. By creating a Static\nObservation Loss Map (SOLM) as an intermediary, we logically separate the\ntime-intensive evaluation and motion planning phases, significantly boosting\nthe planning process. In the experimental section, we demonstrate the\neffectiveness of the proposed metrics across various scenes and the feature of\ntrajectories guided by different metrics. Ultimately, our framework is tested\nin a real-world scenario, enabling the robot to actively choose topologies and\norientations preferable for localization. The source code is accessible at\nhttps://github.com/ZJU-FAST-Lab/LF-3PM.\n","authors":["Kaixin Chai","Long Xu","Qianhao Wang","Chao Xu","Peng Yin","Fei Gao"],"pdf_url":"https://arxiv.org/pdf/2408.01649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01640v1","updated":"2024-08-03T02:57:37Z","published":"2024-08-03T02:57:37Z","title":"Leveraging GNSS and Onboard Visual Data from Consumer Vehicles for\n  Robust Road Network Estimation","summary":"  Maps are essential for diverse applications, such as vehicle navigation and\nautonomous robotics. Both require spatial models for effective route planning\nand localization. This paper addresses the challenge of road graph construction\nfor autonomous vehicles. Despite recent advances, creating a road graph remains\nlabor-intensive and has yet to achieve full automation. The goal of this paper\nis to generate such graphs automatically and accurately. Modern cars are\nequipped with onboard sensors used for today's advanced driver assistance\nsystems like lane keeping. We propose using global navigation satellite system\n(GNSS) traces and basic image data acquired from these standard sensors in\nconsumer vehicles to estimate road-level maps with minimal effort. We exploit\nthe spatial information in the data by framing the problem as a road centerline\nsemantic segmentation task using a convolutional neural network. We also\nutilize the data's time series nature to refine the neural network's output by\nusing map matching. We implemented and evaluated our method using a fleet of\nreal consumer vehicles, only using the deployed onboard sensors. Our evaluation\ndemonstrates that our approach not only matches existing methods on simpler\nroad configurations but also significantly outperforms them on more complex\nroad geometries and topologies. This work received the 2023 Woven by Toyota\nInvention Award.\n","authors":["Balázs Opra","Betty Le Dem","Jeffrey M. Walls","Dimitar Lukarski","Cyrill Stachniss"],"pdf_url":"https://arxiv.org/pdf/2408.01640v1.pdf","comment":"This work will be presented at IROS 2024. Supplementary website:\n  https://bazs.github.io/probe2road/"},{"id":"http://arxiv.org/abs/2408.01622v1","updated":"2024-08-03T01:09:48Z","published":"2024-08-03T01:09:48Z","title":"Positive-Unlabeled Constraint Learning (PUCL) for Inferring Nonlinear\n  Continuous Constraints Functions from Expert Demonstrations","summary":"  Planning for a wide range of real-world robotic tasks necessitates to know\nand write all constraints. However, instances exist where these constraints are\neither unknown or challenging to specify accurately. A possible solution is to\ninfer the unknown constraints from expert demonstration. This paper presents a\nnovel Positive-Unlabeled Constraint Learning (PUCL) algorithm to infer a\ncontinuous arbitrary constraint function from demonstration, without requiring\nprior knowledge of the true constraint parameterization or environmental model\nas existing works. Within our framework, we treat all data in demonstrations as\npositive (feasible) data, and learn a control policy to generate potentially\ninfeasible trajectories, which serve as unlabeled data. In each iteration, we\nfirst update the policy and then a two-step positive-unlabeled learning\nprocedure is applied, where it first identifies reliable infeasible data using\na distance metric, and secondly learns a binary feasibility classifier (i.e.,\nconstraint function) from the feasible demonstrations and reliable infeasible\ndata. The proposed framework is flexible to learn complex-shaped constraint\nboundary and will not mistakenly classify demonstrations as infeasible as\nprevious methods. The effectiveness of the proposed method is verified in three\nrobotic tasks, using a networked policy or a dynamical system policy. It\nsuccessfully infers and transfers the continuous nonlinear constraints and\noutperforms other baseline methods in terms of constraint accuracy and policy\nsafety.\n","authors":["Baiyu Peng","Aude Billard"],"pdf_url":"https://arxiv.org/pdf/2408.01622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01615v1","updated":"2024-08-03T00:45:01Z","published":"2024-08-03T00:45:01Z","title":"Three-dimensional Morphological Reconstruction of Millimeter-Scale Soft\n  Continuum Robots based on Dual Stereo Vision","summary":"  Continuum robots can be miniaturized to just a few millimeters in diameter.\nAmong these, notched tubular continuum robots (NTCR) show great potential in\nmany delicate applications. Existing works in robotic modeling focus on\nkinematics and dynamics but still face challenges in reproducing the robot's\nmorphology -- a significant factor that can expand the research landscape of\ncontinuum robots, especially for those with asymmetric continuum structures.\nThis paper proposes a dual stereo vision-based method for the three-dimensional\nmorphological reconstruction of millimeter-scale NTCRs. The method employs two\noppositely located stationary binocular cameras to capture the point cloud of\nthe NTCR, then utilizes predefined geometry as a reference for the KD tree\nmethod to relocate the capture point clouds, resulting in a morphologically\ncorrect NTCR despite the low-quality raw point cloud collection. The method has\nbeen proved feasible for an NTCR with a 3.5 mm diameter, capturing 14 out of 16\nnotch features, with the measurements generally centered around the standard of\n1.5 mm, demonstrating the capability of revealing morphological details. Our\nproposed method paves the way for 3D morphological reconstruction of\nmillimeter-scale soft robots for further self-modeling study.\n","authors":["Tian-Ao Ren","Wenyan Liu","Tao Zhang","Lei Zhao","Hongliang Ren","Jiewen Lai"],"pdf_url":"https://arxiv.org/pdf/2408.01615v1.pdf","comment":"6 pages, 6 figures, submitted to Robio 2024"}]},"2024-08-06T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2404.07950v3","updated":"2024-08-06T02:42:32Z","published":"2024-03-18T16:50:23Z","title":"Reinforcement Learning with Generalizable Gaussian Splatting","summary":"  An excellent representation is crucial for reinforcement learning (RL)\nperformance, especially in vision-based reinforcement learning tasks. The\nquality of the environment representation directly influences the achievement\nof the learning task. Previous vision-based RL typically uses explicit or\nimplicit ways to represent environments, such as images, points, voxels, and\nneural radiance fields. However, these representations contain several\ndrawbacks. They cannot either describe complex local geometries or generalize\nwell to unseen scenes, or require precise foreground masks. Moreover, these\nimplicit neural representations are akin to a ``black box\", significantly\nhindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit\nscene representation and differentiable rendering nature, is considered a\nrevolutionary change for reconstruction and representation methods. In this\npaper, we propose a novel Generalizable Gaussian Splatting framework to be the\nrepresentation of RL tasks, called GSRL. Through validation in the RoboMimic\nenvironment, our method achieves better results than other baselines in\nmultiple tasks, improving the performance by 10%, 44%, and 15% compared with\nbaselines on the hardest task. This work is the first attempt to leverage\ngeneralizable 3DGS as a representation for RL.\n","authors":["Jiaxu Wang","Qiang Zhang","Jingkai Sun","Jiahang Cao","Gang Han","Wen Zhao","Weining Zhang","Yecheng Shao","Yijie Guo","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2404.07950v3.pdf","comment":"7 pages,2 figures"},{"id":"http://arxiv.org/abs/2408.03326v1","updated":"2024-08-06T17:59:44Z","published":"2024-08-06T17:59:44Z","title":"LLaVA-OneVision: Easy Visual Task Transfer","summary":"  We present LLaVA-OneVision, a family of open large multimodal models (LMMs)\ndeveloped by consolidating our insights into data, models, and visual\nrepresentations in the LLaVA-NeXT blog series. Our experimental results\ndemonstrate that LLaVA-OneVision is the first single model that can\nsimultaneously push the performance boundaries of open LMMs in three important\ncomputer vision scenarios: single-image, multi-image, and video scenarios.\nImportantly, the design of LLaVA-OneVision allows strong transfer learning\nacross different modalities/scenarios, yielding new emerging capabilities. In\nparticular, strong video understanding and cross-scenario capabilities are\ndemonstrated through task transfer from images to videos.\n","authors":["Bo Li","Yuanhan Zhang","Dong Guo","Renrui Zhang","Feng Li","Hao Zhang","Kaichen Zhang","Yanwei Li","Ziwei Liu","Chunyuan Li"],"pdf_url":"https://arxiv.org/pdf/2408.03326v1.pdf","comment":"Project Homepage:\n  https://llava-vl.github.io/blog/2024-08-05-llava-onevision/"},{"id":"http://arxiv.org/abs/2408.03322v1","updated":"2024-08-06T17:58:18Z","published":"2024-08-06T17:58:18Z","title":"Segment Anything in Medical Images and Videos: Benchmark and Deployment","summary":"  Recent advances in segmentation foundation models have enabled accurate and\nefficient segmentation across a wide range of natural images and videos, but\ntheir utility to medical data remains unclear. In this work, we first present a\ncomprehensive benchmarking of the Segment Anything Model 2 (SAM2) across 11\nmedical image modalities and videos and point out its strengths and weaknesses\nby comparing it to SAM1 and MedSAM. Then, we develop a transfer learning\npipeline and demonstrate SAM2 can be quickly adapted to medical domain by\nfine-tuning. Furthermore, we implement SAM2 as a 3D slicer plugin and Gradio\nAPI for efficient 3D image and video segmentation. The code has been made\npublicly available at \\url{https://github.com/bowang-lab/MedSAM}.\n","authors":["Jun Ma","Sumin Kim","Feifei Li","Mohammed Baharoon","Reza Asakereh","Hongwei Lyu","Bo Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01197v2","updated":"2024-08-06T17:58:00Z","published":"2024-04-01T15:55:25Z","title":"Getting it Right: Improving Spatial Consistency in Text-to-Image Models","summary":"  One of the key shortcomings in current text-to-image (T2I) models is their\ninability to consistently generate images which faithfully follow the spatial\nrelationships specified in the text prompt. In this paper, we offer a\ncomprehensive investigation of this limitation, while also developing datasets\nand methods that support algorithmic solutions to improve spatial reasoning in\nT2I models. We find that spatial relationships are under-represented in the\nimage descriptions found in current vision-language datasets. To alleviate this\ndata bottleneck, we create SPRIGHT, the first spatially focused, large-scale\ndataset, by re-captioning 6 million images from 4 widely used vision datasets\nand through a 3-fold evaluation and analysis pipeline, show that SPRIGHT\nimproves the proportion of spatial relationships in existing datasets. We show\nthe efficacy of SPRIGHT data by showing that using only $\\sim$0.25% of SPRIGHT\nresults in a 22% improvement in generating spatially accurate images while also\nimproving FID and CMMD scores. We also find that training on images containing\na larger number of objects leads to substantial improvements in spatial\nconsistency, including state-of-the-art results on T2I-CompBench with a spatial\nscore of 0.2133, by fine-tuning on <500 images. Through a set of controlled\nexperiments and ablations, we document additional findings that could support\nfuture work that seeks to understand factors that affect spatial consistency in\ntext-to-image models.\n","authors":["Agneet Chatterjee","Gabriela Ben Melech Stan","Estelle Aflalo","Sayak Paul","Dhruba Ghosh","Tejas Gokhale","Ludwig Schmidt","Hannaneh Hajishirzi","Vasudev Lal","Chitta Baral","Yezhou Yang"],"pdf_url":"https://arxiv.org/pdf/2404.01197v2.pdf","comment":"Accepted to ECCV 2024. Project Page : https://spright-t2i.github.io/"},{"id":"http://arxiv.org/abs/2404.01282v2","updated":"2024-08-06T17:56:53Z","published":"2024-04-01T17:54:34Z","title":"LoSA: Long-Short-range Adapter for Scaling End-to-End Temporal Action\n  Localization","summary":"  Temporal Action Localization (TAL) involves localizing and classifying action\nsnippets in an untrimmed video. The emergence of large video foundation models\nhas led RGB-only video backbones to outperform previous methods needing both\nRGB and optical flow modalities. Leveraging these large models is often limited\nto training only the TAL head due to the prohibitively large GPU memory\nrequired to adapt the video backbone for TAL. To overcome this limitation, we\nintroduce LoSA, the first memory-and-parameter-efficient backbone adapter\ndesigned specifically for TAL to handle untrimmed videos. LoSA specializes for\nTAL by introducing Long-Short-range Adapters that adapt the intermediate layers\nof the video backbone over different temporal ranges. These adapters run\nparallel to the video backbone to significantly reduce memory footprint. LoSA\nalso includes Long-Short-range Gated Fusion that strategically combines the\noutput of these adapters from the video backbone layers to enhance the video\nfeatures provided to the TAL head. Experiments show that LoSA significantly\noutperforms all existing methods on standard TAL benchmarks, THUMOS-14 and\nActivityNet-v1.3, by scaling end-to-end backbone adaptation to\nbillion-parameter-plus models like VideoMAEv2~(ViT-g) and leveraging them\nbeyond head-only transfer learning.\n","authors":["Akshita Gupta","Gaurav Mittal","Ahmed Magooda","Ye Yu","Graham W. Taylor","Mei Chen"],"pdf_url":"https://arxiv.org/pdf/2404.01282v2.pdf","comment":"Under submission"},{"id":"http://arxiv.org/abs/2408.00756v2","updated":"2024-08-06T17:40:07Z","published":"2024-08-01T17:57:25Z","title":"Segment anything model 2: an application to 2D and 3D medical images","summary":"  Segment Anything Model (SAM) has gained significant attention because of its\nability to segment varous objects in images given a prompt. The recently\ndeveloped SAM 2 has extended this ability to video inputs. This opens an\nopportunity to apply SAM to 3D images, one of the fundamental tasks in the\nmedical imaging field. In this paper, we extensively evaluate SAM 2's ability\nto segment both 2D and 3D medical images by first collecting 18 medical imaging\ndatasets, including common 3D modalities such as computed tomography (CT),\nmagnetic resonance imaging (MRI), and positron emission tomography (PET) as\nwell as 2D modalities such as X-ray and ultrasound. Two evaluation pipelines of\nSAM 2 are considered: (1) multi-frame 3D segmentation, where prompts are\nprovided to one or multiple slice(s) selected from the volume, and (2)\nsingle-frame 2D segmentation, where prompts are provided to each slice. The\nformer is only applicable to 3D modalities, while the latter applies to both 2D\nand 3D modalities. Our results show that SAM 2 exhibits similar performance as\nSAM under single-frame 2D segmentation, and has variable performance under\nmulti-frame 3D segmentation depending on the choices of slices to annotate, the\ndirection of the propagation, the predictions utilized during the propagation,\netc.\n","authors":["Haoyu Dong","Hanxue Gu","Yaqian Chen","Jichen Yang","Maciej A. Mazurowski"],"pdf_url":"https://arxiv.org/pdf/2408.00756v2.pdf","comment":"12 pages, 9 figures. An updated version with new results and\n  corrections"},{"id":"http://arxiv.org/abs/2402.00035v4","updated":"2024-08-06T17:36:06Z","published":"2024-01-08T12:19:46Z","title":"Robustness Assessment of a Runway Object Classifier for Safe Aircraft\n  Taxiing","summary":"  As deep neural networks (DNNs) are becoming the prominent solution for many\ncomputational problems, the aviation industry seeks to explore their potential\nin alleviating pilot workload and in improving operational safety. However, the\nuse of DNNs in this type of safety-critical applications requires a thorough\ncertification process. This need can be addressed through formal verification,\nwhich provides rigorous assurances -- e.g.,~by proving the absence of certain\nmispredictions. In this case-study paper, we demonstrate this process using an\nimage-classifier DNN currently under development at Airbus and intended for use\nduring the aircraft taxiing phase. We use formal methods to assess this DNN's\nrobustness to three common image perturbation types: noise, brightness and\ncontrast, and some of their combinations. This process entails multiple\ninvocations of the underlying verifier, which might be computationally\nexpensive; and we therefore propose a method that leverages the monotonicity of\nthese robustness properties, as well as the results of past verification\nqueries, in order to reduce the overall number of verification queries required\nby nearly 60%. Our results provide an indication of the level of robustness\nachieved by the DNN classifier under study, and indicate that it is\nconsiderably more vulnerable to noise than to brightness or contrast\nperturbations.\n","authors":["Yizhak Elboher","Raya Elsaleh","Omri Isac","Mélanie Ducoffe","Audrey Galametz","Guillaume Povéda","Ryma Boumazouza","Noémie Cohen","Guy Katz"],"pdf_url":"https://arxiv.org/pdf/2402.00035v4.pdf","comment":"This is a preprint version of the paper in the proceedings of 43rd\n  Digital Avionics Systems Conference (DASC)"},{"id":"http://arxiv.org/abs/2402.04492v2","updated":"2024-08-06T17:31:33Z","published":"2024-02-07T00:31:49Z","title":"ColorSwap: A Color and Word Order Dataset for Multimodal Evaluation","summary":"  This paper introduces the ColorSwap dataset, designed to assess and improve\nthe proficiency of multimodal models in matching objects with their colors. The\ndataset is comprised of 2,000 unique image-caption pairs, grouped into 1,000\nexamples. Each example includes a caption-image pair, along with a\n``color-swapped'' pair. We follow the Winoground schema: the two captions in an\nexample have the same words, but the color words have been rearranged to modify\ndifferent objects. The dataset was created through a novel blend of automated\ncaption and image generation with humans in the loop. We evaluate image-text\nmatching (ITM) and visual language models (VLMs) and find that even the latest\nones are still not robust at this task. GPT-4V and LLaVA score 72% and 42% on\nour main VLM metric, although they may improve with more advanced prompting\ntechniques. On the main ITM metric, contrastive models such as CLIP and SigLIP\nperform close to chance (at 12% and 30%, respectively), although the\nnon-contrastive BLIP ITM model is stronger (87%). We also find that finetuning\non fewer than 2,000 examples yields significant performance gains on this\nout-of-distribution word-order understanding task. The dataset is here:\nhttps://github.com/Top34051/colorswap and here:\nhttps://huggingface.co/datasets/stanfordnlp/colorswap.\n","authors":["Jirayu Burapacheep","Ishan Gaur","Agam Bhatia","Tristan Thrush"],"pdf_url":"https://arxiv.org/pdf/2402.04492v2.pdf","comment":"ACL Findings 2024"},{"id":"http://arxiv.org/abs/2408.03312v1","updated":"2024-08-06T17:29:01Z","published":"2024-08-06T17:29:01Z","title":"MDT-A2G: Exploring Masked Diffusion Transformers for Co-Speech Gesture\n  Generation","summary":"  Recent advancements in the field of Diffusion Transformers have substantially\nimproved the generation of high-quality 2D images, 3D videos, and 3D shapes.\nHowever, the effectiveness of the Transformer architecture in the domain of\nco-speech gesture generation remains relatively unexplored, as prior\nmethodologies have predominantly employed the Convolutional Neural Network\n(CNNs) or simple a few transformer layers. In an attempt to bridge this\nresearch gap, we introduce a novel Masked Diffusion Transformer for co-speech\ngesture generation, referred to as MDT-A2G, which directly implements the\ndenoising process on gesture sequences. To enhance the contextual reasoning\ncapability of temporally aligned speech-driven gestures, we incorporate a novel\nMasked Diffusion Transformer. This model employs a mask modeling scheme\nspecifically designed to strengthen temporal relation learning among sequence\ngestures, thereby expediting the learning process and leading to coherent and\nrealistic motions. Apart from audio, Our MDT-A2G model also integrates\nmulti-modal information, encompassing text, emotion, and identity. Furthermore,\nwe propose an efficient inference strategy that diminishes the denoising\ncomputation by leveraging previously calculated results, thereby achieving a\nspeedup with negligible performance degradation. Experimental results\ndemonstrate that MDT-A2G excels in gesture generation, boasting a learning\nspeed that is over 6$\\times$ faster than traditional diffusion transformers and\nan inference speed that is 5.7$\\times$ than the standard diffusion model.\n","authors":["Xiaofeng Mao","Zhengkai Jiang","Qilin Wang","Chencan Fu","Jiangning Zhang","Jiafu Wu","Yabiao Wang","Chengjie Wang","Wei Li","Mingmin Chi"],"pdf_url":"https://arxiv.org/pdf/2408.03312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19308v2","updated":"2024-08-06T17:22:17Z","published":"2024-07-27T17:45:20Z","title":"Comprehensive Attribution: Inherently Explainable Vision Model with\n  Feature Detector","summary":"  As deep vision models' popularity rapidly increases, there is a growing\nemphasis on explanations for model predictions. The inherently explainable\nattribution method aims to enhance the understanding of model behavior by\nidentifying the important regions in images that significantly contribute to\npredictions. It is achieved by cooperatively training a selector (generating an\nattribution map to identify important features) and a predictor (making\npredictions using the identified features). Despite many advancements, existing\nmethods suffer from the incompleteness problem, where discriminative features\nare masked out, and the interlocking problem, where the non-optimized selector\ninitially selects noise, causing the predictor to fit on this noise and\nperpetuate the cycle. To address these problems, we introduce a new objective\nthat discourages the presence of discriminative features in the masked-out\nregions thus enhancing the comprehensiveness of feature selection. A\npre-trained detector is introduced to detect discriminative features in the\nmasked-out region. If the selector selects noise instead of discriminative\nfeatures, the detector can observe and break the interlocking situation by\npenalizing the selector. Extensive experiments show that our model makes\naccurate predictions with higher accuracy than the regular black-box model, and\nproduces attribution maps with high feature coverage, localization ability,\nfidelity and robustness. Our code will be available at\n\\href{https://github.com/Zood123/COMET}{https://github.com/Zood123/COMET}.\n","authors":["Xianren Zhang","Dongwon Lee","Suhang Wang"],"pdf_url":"https://arxiv.org/pdf/2407.19308v2.pdf","comment":"Accepted as a conference paper by ECCV 2024"},{"id":"http://arxiv.org/abs/2408.03304v1","updated":"2024-08-06T17:11:40Z","published":"2024-08-06T17:11:40Z","title":"Fusing Forces: Deep-Human-Guided Refinement of Segmentation Masks","summary":"  Etruscan mirrors constitute a significant category in Etruscan art,\ncharacterized by elaborate figurative illustrations featured on their backside.\nA laborious and costly aspect of their analysis and documentation is the task\nof manually tracing these illustrations. In previous work, a methodology has\nbeen proposed to automate this process, involving photometric-stereo scanning\nin combination with deep neural networks. While achieving quantitative\nperformance akin to an expert annotator, some results still lack qualitative\nprecision and, thus, require annotators for inspection and potential\ncorrection, maintaining resource intensity. In response, we propose a deep\nneural network trained to interactively refine existing annotations based on\nhuman guidance. Our human-in-the-loop approach streamlines annotation,\nachieving equal quality with up to 75% less manual input required. Moreover,\nduring the refinement process, the relative improvement of our methodology over\npure manual labeling reaches peak values of up to 26%, attaining drastically\nbetter quality quicker. By being tailored to the complex task of segmenting\nintricate lines, specifically distinguishing it from previous methods, our\napproach offers drastic improvements in efficacy, transferable to a broad\nspectrum of applications beyond Etruscan mirrors.\n","authors":["Rafael Sterzinger","Christian Stippel","Robert Sablatnig"],"pdf_url":"https://arxiv.org/pdf/2408.03304v1.pdf","comment":"16 pages, accepted at ICPR2024"},{"id":"http://arxiv.org/abs/2304.05339v2","updated":"2024-08-06T17:09:59Z","published":"2023-04-11T16:58:59Z","title":"Deep-learning Assisted Detection and Quantification of (oo)cysts of\n  Giardia and Cryptosporidium on Smartphone Microscopy Images","summary":"  The consumption of microbial-contaminated food and water is responsible for\nthe deaths of millions of people annually. Smartphone-based microscopy systems\nare portable, low-cost, and more accessible alternatives for the detection of\nGiardia and Cryptosporidium than traditional brightfield microscopes. However,\nthe images from smartphone microscopes are noisier and require manual cyst\nidentification by trained technicians, usually unavailable in resource-limited\nsettings. Automatic detection of (oo)cysts using deep-learning-based object\ndetection could offer a solution for this limitation. We evaluate the\nperformance of four state-of-the-art object detectors to detect (oo)cysts of\nGiardia and Cryptosporidium on a custom dataset that includes both smartphone\nand brightfield microscopic images from vegetable samples. Faster RCNN,\nRetinaNet, You Only Look Once (YOLOv8s), and Deformable Detection Transformer\n(Deformable DETR) deep-learning models were employed to explore their efficacy\nand limitations. Our results show that while the deep-learning models perform\nbetter with the brightfield microscopy image dataset than the smartphone\nmicroscopy image dataset, the smartphone microscopy predictions are still\ncomparable to the prediction performance of non-experts. Also, we publicly\nrelease brightfield and smartphone microscopy datasets with the benchmark\nresults for the detection of Giardia and Cryptosporidium, independently\ncaptured on reference (or standard lab setting) and vegetable samples. Our code\nand dataset are available at\nhttps://github.com/naamiinepal/smartphone_microscopy and\nhttps://doi.org/10.5281/zenodo.7813183, respectively.\n","authors":["Suprim Nakarmi","Sanam Pudasaini","Safal Thapaliya","Pratima Upretee","Retina Shrestha","Basant Giri","Bhanu Bhakta Neupane","Bishesh Khanal"],"pdf_url":"https://arxiv.org/pdf/2304.05339v2.pdf","comment":"21 pages (including supplementary information), 5 figures, 7 tables,\n  Accepted for publication at the Journal of Machine Learning for Biomedical\n  Imaging (MELBA) https://melba-journal.org/2024:014"},{"id":"http://arxiv.org/abs/2408.03302v1","updated":"2024-08-06T17:08:05Z","published":"2024-08-06T17:08:05Z","title":"TextIM: Part-aware Interactive Motion Synthesis from Text","summary":"  In this work, we propose TextIM, a novel framework for synthesizing\nTEXT-driven human Interactive Motions, with a focus on the precise alignment of\npart-level semantics. Existing methods often overlook the critical roles of\ninteractive body parts and fail to adequately capture and align part-level\nsemantics, resulting in inaccuracies and even erroneous movement outcomes. To\naddress these issues, TextIM utilizes a decoupled conditional diffusion\nframework to enhance the detailed alignment between interactive movements and\ncorresponding semantic intents from textual descriptions. Our approach\nleverages large language models, functioning as a human brain, to identify\ninteracting human body parts and to comprehend interaction semantics to\ngenerate complicated and subtle interactive motion. Guided by the refined\nmovements of the interacting parts, TextIM further extends these movements into\na coherent whole-body motion. We design a spatial coherence module to\ncomplement the entire body movements while maintaining consistency and harmony\nacross body parts using a part graph convolutional network. For training and\nevaluation, we carefully selected and re-labeled interactive motions from\nHUMANML3D to develop a specialized dataset. Experimental results demonstrate\nthat TextIM produces semantically accurate human interactive motions,\nsignificantly enhancing the realism and applicability of synthesized\ninteractive motions in diverse scenarios, even including interactions with\ndeformable and dynamically changing objects.\n","authors":["Siyuan Fan","Bo Du","Xiantao Cai","Bo Peng","Longling Sun"],"pdf_url":"https://arxiv.org/pdf/2408.03302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11914v2","updated":"2024-08-06T17:00:49Z","published":"2024-05-20T09:49:13Z","title":"PT43D: A Probabilistic Transformer for Generating 3D Shapes from Single\n  Highly-Ambiguous RGB Images","summary":"  Generating 3D shapes from single RGB images is essential in various\napplications such as robotics. Current approaches typically target images\ncontaining clear and complete visual descriptions of the object, without\nconsidering common realistic cases where observations of objects that are\nlargely occluded or truncated. We thus propose a transformer-based\nautoregressive model to generate the probabilistic distribution of 3D shapes\nconditioned on an RGB image containing potentially highly ambiguous\nobservations of the object. To handle realistic scenarios such as occlusion or\nfield-of-view truncation, we create simulated image-to-shape training pairs\nthat enable improved fine-tuning for real-world scenarios. We then adopt\ncross-attention to effectively identify the most relevant region of interest\nfrom the input image for shape generation. This enables inference of sampled\nshapes with reasonable diversity and strong alignment with the input image. We\ntrain and test our model on our synthetic data then fine-tune and test it on\nreal-world data. Experiments demonstrate that our model outperforms state of\nthe art in both scenarios.\n","authors":["Yiheng Xiong","Angela Dai"],"pdf_url":"https://arxiv.org/pdf/2405.11914v2.pdf","comment":"10 pages, 6 figures. Accepted to BMVC 2024"},{"id":"http://arxiv.org/abs/2408.03291v1","updated":"2024-08-06T16:40:04Z","published":"2024-08-06T16:40:04Z","title":"DopQ-ViT: Towards Distribution-Friendly and Outlier-Aware Post-Training\n  Quantization for Vision Transformers","summary":"  Vision transformers (ViTs) have garnered significant attention for their\nperformance in vision tasks; however, the high computational cost and\nsignificant latency issues have hinder widespread adoption. Post-training\nquantization (PTQ), a promising method for model compression, still faces\naccuracy degradation challenges with ViTs. There are two reasons for this: the\nexisting quantization paradigm does not fit the power-law distribution of\npost-Softmax activations well, and accuracy inevitably decreases after\nreparameterizing post-LayerNorm activations. We propose a Distribution-Friendly\nand Outlier-Aware Post-training Quantization method for Vision Transformers,\nnamed DopQ-ViT. DopQ-ViT analyzes the inefficiencies of current quantizers and\nintroduces a distribution-friendly Tan Quantizer called TanQ. TanQ focuses more\non values near 1, more accurately preserving the power-law distribution of\npost-Softmax activations, and achieves favorable results. Moreover, when\nreparameterizing post-LayerNorm activations from channel-wise to layer-wise\nquantization, the accuracy degradation is mainly due to the significant impact\nof outliers in the scaling factors. Therefore, DopQ-ViT proposes a method to\nSearch for the Optimal Scaling Factor, denoted as SOSF, which compensates for\nthe influence of outliers and preserves the performance of the quantization\nmodel. DopQ-ViT has undergone extensive validation and demonstrates significant\nperformance improvements in quantization models, particularly in low-bit\nsettings.\n","authors":["Lianwei Yang","Haisong Gong"],"pdf_url":"https://arxiv.org/pdf/2408.03291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14242v2","updated":"2024-08-06T16:36:11Z","published":"2023-11-24T01:15:57Z","title":"RSB-Pose: Robust Short-Baseline Binocular 3D Human Pose Estimation with\n  Occlusion Handling","summary":"  In the domain of 3D Human Pose Estimation, which finds widespread daily\napplications, the requirement for convenient acquisition equipment continues to\ngrow. To satisfy this demand, we set our sights on a short-baseline binocular\nsetting that offers both portability and a geometric measurement property that\nradically mitigates depth ambiguity. However, as the binocular baseline\nshortens, two serious challenges emerge: first, the robustness of 3D\nreconstruction against 2D errors deteriorates; and second, occlusion reoccurs\ndue to the limited visual differences between two views. To address the first\nchallenge, we propose the Stereo Co-Keypoints Estimation module to improve the\nview consistency of 2D keypoints and enhance the 3D robustness. In this module,\nthe disparity is utilized to represent the correspondence of binocular 2D\npoints and the Stereo Volume Feature is introduced to contain binocular\nfeatures across different disparities. Through the regression of SVF, two-view\n2D keypoints are simultaneously estimated in a collaborative way which\nrestricts their view consistency. Furthermore, to deal with occlusions, a\nPre-trained Pose Transformer module is introduced. Through this module, 3D\nposes are refined by perceiving pose coherence, a representation of joint\ncorrelations. This perception is injected by the Pose Transformer network and\nlearned through a pre-training task that recovers iterative masked joints.\nComprehensive experiments carried out on H36M and MHAD datasets, complemented\nby visualizations, validate the effectiveness of our approach in the\nshort-baseline binocular 3D Human Pose Estimation and occlusion handling.\n","authors":["Xiaoyue Wan","Zhuo Chen","Yiming Bao","Xu Zhao"],"pdf_url":"https://arxiv.org/pdf/2311.14242v2.pdf","comment":"13 pages, 8 figures, currently under review at IEEE Transactions on\n  Image Processing journal"},{"id":"http://arxiv.org/abs/2406.04485v3","updated":"2024-08-06T16:35:50Z","published":"2024-06-06T20:15:42Z","title":"GenAI Arena: An Open Evaluation Platform for Generative Models","summary":"  Generative AI has made remarkable strides to revolutionize fields such as\nimage and video generation. These advancements are driven by innovative\nalgorithms, architecture, and data. However, the rapid proliferation of\ngenerative models has highlighted a critical gap: the absence of trustworthy\nevaluation metrics. Current automatic assessments such as FID, CLIP, FVD, etc\noften fail to capture the nuanced quality and user satisfaction associated with\ngenerative outputs. This paper proposes an open platform GenAI-Arena to\nevaluate different image and video generative models, where users can actively\nparticipate in evaluating these models. By leveraging collective user feedback\nand votes, GenAI-Arena aims to provide a more democratic and accurate measure\nof model performance. It covers three arenas for text-to-image generation,\ntext-to-video generation, and image editing respectively. Currently, we cover a\ntotal of 27 open-source generative models. GenAI-Arena has been operating for\nfour months, amassing over 6000 votes from the community. We describe our\nplatform, analyze the data, and explain the statistical methods for ranking the\nmodels. To further promote the research in building model-based evaluation\nmetrics, we release a cleaned version of our preference data for the three\ntasks, namely GenAI-Bench. We prompt the existing multi-modal models like\nGemini, GPT-4o to mimic human voting. We compute the correlation between model\nvoting with human voting to understand their judging abilities. Our results\nshow existing multimodal models are still lagging in assessing the generated\nvisual content, even the best model GPT-4o only achieves a Pearson correlation\nof 0.22 in the quality subscore, and behaves like random guessing in others.\n","authors":["Dongfu Jiang","Max Ku","Tianle Li","Yuansheng Ni","Shizhuo Sun","Rongqi Fan","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2406.04485v3.pdf","comment":"9 pages,7 figures"},{"id":"http://arxiv.org/abs/2408.03286v1","updated":"2024-08-06T16:34:04Z","published":"2024-08-06T16:34:04Z","title":"Biomedical SAM 2: Segment Anything in Biomedical Images and Videos","summary":"  Medical image segmentation and video object segmentation are essential for\ndiagnosing and analyzing diseases by identifying and measuring biological\nstructures. Recent advances in natural domain have been driven by foundation\nmodels like the Segment Anything Model 2 (SAM 2). To explore the performance of\nSAM 2 in biomedical applications, we designed two evaluation pipelines for\nsingle-frame image segmentation and multi-frame video segmentation with varied\nprompt designs, revealing SAM 2's limitations in medical contexts.\nConsequently, we developed BioSAM 2, an enhanced foundation model optimized for\nbiomedical data based on SAM 2. Our experiments show that BioSAM 2 not only\nsurpasses the performance of existing state-of-the-art foundation models but\nalso matches or even exceeds specialist models, demonstrating its efficacy and\npotential in the medical domain.\n","authors":["Zhiling Yan","Weixiang Sun","Rong Zhou","Zhengqing Yuan","Kai Zhang","Yiwei Li","Tianming Liu","Quanzheng Li","Xiang Li","Lifang He","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2408.03286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03284v1","updated":"2024-08-06T16:31:45Z","published":"2024-08-06T16:31:45Z","title":"ReSyncer: Rewiring Style-based Generator for Unified Audio-Visually\n  Synced Facial Performer","summary":"  Lip-syncing videos with given audio is the foundation for various\napplications including the creation of virtual presenters or performers. While\nrecent studies explore high-fidelity lip-sync with different techniques, their\ntask-orientated models either require long-term videos for clip-specific\ntraining or retain visible artifacts. In this paper, we propose a unified and\neffective framework ReSyncer, that synchronizes generalized audio-visual facial\ninformation. The key design is revisiting and rewiring the Style-based\ngenerator to efficiently adopt 3D facial dynamics predicted by a principled\nstyle-injected Transformer. By simply re-configuring the information insertion\nmechanisms within the noise and style space, our framework fuses motion and\nappearance with unified training. Extensive experiments demonstrate that\nReSyncer not only produces high-fidelity lip-synced videos according to audio,\nbut also supports multiple appealing properties that are suitable for creating\nvirtual presenters and performers, including fast personalized fine-tuning,\nvideo-driven lip-syncing, the transfer of speaking styles, and even face\nswapping. Resources can be found at\nhttps://guanjz20.github.io/projects/ReSyncer.\n","authors":["Jiazhi Guan","Zhiliang Xu","Hang Zhou","Kaisiyuan Wang","Shengyi He","Zhanwang Zhang","Borong Liang","Haocheng Feng","Errui Ding","Jingtuo Liu","Jingdong Wang","Youjian Zhao","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2408.03284v1.pdf","comment":"Accepted to European Conference on Computer Vision (ECCV), 2024.\n  Project page: https://guanjz20.github.io/projects/ReSyncer"},{"id":"http://arxiv.org/abs/2408.03282v1","updated":"2024-08-06T16:29:51Z","published":"2024-08-06T16:29:51Z","title":"AMES: Asymmetric and Memory-Efficient Similarity Estimation for\n  Instance-level Retrieval","summary":"  This work investigates the problem of instance-level image retrieval\nre-ranking with the constraint of memory efficiency, ultimately aiming to limit\nmemory usage to 1KB per image. Departing from the prevalent focus on\nperformance enhancements, this work prioritizes the crucial trade-off between\nperformance and memory requirements. The proposed model uses a\ntransformer-based architecture designed to estimate image-to-image similarity\nby capturing interactions within and across images based on their local\ndescriptors. A distinctive property of the model is the capability for\nasymmetric similarity estimation. Database images are represented with a\nsmaller number of descriptors compared to query images, enabling performance\nimprovements without increasing memory consumption. To ensure adaptability\nacross different applications, a universal model is introduced that adjusts to\na varying number of local descriptors during the testing phase. Results on\nstandard benchmarks demonstrate the superiority of our approach over both\nhand-crafted and learned models. In particular, compared with current\nstate-of-the-art methods that overlook their memory footprint, our approach not\nonly attains superior performance but does so with a significantly reduced\nmemory footprint. The code and pretrained models are publicly available at:\nhttps://github.com/pavelsuma/ames\n","authors":["Pavel Suma","Giorgos Kordopatis-Zilos","Ahmet Iscen","Giorgos Tolias"],"pdf_url":"https://arxiv.org/pdf/2408.03282v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2403.15313v2","updated":"2024-08-06T15:58:35Z","published":"2024-03-22T16:06:05Z","title":"CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking","summary":"  To enable self-driving vehicles accurate detection and tracking of\nsurrounding objects is essential. While Light Detection and Ranging (LiDAR)\nsensors have set the benchmark for high-performance systems, the appeal of\ncamera-only solutions lies in their cost-effectiveness. Notably, despite the\nprevalent use of Radio Detection and Ranging (RADAR) sensors in automotive\nsystems, their potential in 3D detection and tracking has been largely\ndisregarded due to data sparsity and measurement noise. As a recent\ndevelopment, the combination of RADARs and cameras is emerging as a promising\nsolution. This paper presents Camera-RADAR 3D Detection and Tracking (CR3DT), a\ncamera-RADAR fusion model for 3D object detection, and Multi-Object Tracking\n(MOT). Building upon the foundations of the State-of-the-Art (SotA) camera-only\nBEVDet architecture, CR3DT demonstrates substantial improvements in both\ndetection and tracking capabilities, by incorporating the spatial and velocity\ninformation of the RADAR sensor. Experimental results demonstrate an absolute\nimprovement in detection performance of 5.3% in mean Average Precision (mAP)\nand a 14.9% increase in Average Multi-Object Tracking Accuracy (AMOTA) on the\nnuScenes dataset when leveraging both modalities. CR3DT bridges the gap between\nhigh-performance and cost-effective perception systems in autonomous driving,\nby capitalizing on the ubiquitous presence of RADAR in automotive applications.\nThe code is available at: https://github.com/ETH-PBL/CR3DT.\n","authors":["Nicolas Baumann","Michael Baumgartner","Edoardo Ghignone","Jonas Kühne","Tobias Fischer","Yung-Hsu Yang","Marc Pollefeys","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2403.15313v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10344v3","updated":"2024-08-06T15:39:03Z","published":"2024-02-15T22:17:17Z","title":"Evaluating Neural Radiance Fields (NeRFs) for 3D Plant Geometry\n  Reconstruction in Field Conditions","summary":"  We evaluate different Neural Radiance Fields (NeRFs) techniques for the 3D\nreconstruction of plants in varied environments, from indoor settings to\noutdoor fields. Traditional methods usually fail to capture the complex\ngeometric details of plants, which is crucial for phenotyping and breeding\nstudies. We evaluate the reconstruction fidelity of NeRFs in three scenarios\nwith increasing complexity and compare the results with the point cloud\nobtained using LiDAR as ground truth. In the most realistic field scenario, the\nNeRF models achieve a 74.6% F1 score after 30 minutes of training on the GPU,\nhighlighting the efficacy of NeRFs for 3D reconstruction in challenging\nenvironments. Additionally, we propose an early stopping technique for NeRF\ntraining that almost halves the training time while achieving only a reduction\nof 7.4% in the average F1 score. This optimization process significantly\nenhances the speed and efficiency of 3D reconstruction using NeRFs. Our\nfindings demonstrate the potential of NeRFs in detailed and realistic 3D plant\nreconstruction and suggest practical approaches for enhancing the speed and\nefficiency of NeRFs in the 3D reconstruction process.\n","authors":["Muhammad Arbab Arshad","Talukder Jubery","James Afful","Anushrut Jignasu","Aditya Balu","Baskar Ganapathysubramanian","Soumik Sarkar","Adarsh Krishnamurthy"],"pdf_url":"https://arxiv.org/pdf/2402.10344v3.pdf","comment":"Published in 'Plant Phenomics'"},{"id":"http://arxiv.org/abs/2402.15745v2","updated":"2024-08-06T15:28:30Z","published":"2024-02-24T06:57:15Z","title":"GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models\n  Evaluation","summary":"  The Large Vision-Language Models (LVLMs) have demonstrated great abilities in\nimage perception and language understanding. However, existing multimodal\nbenchmarks focus on primary perception abilities and commonsense knowledge\nwhich are insufficient to reflect the comprehensive capabilities of LVLMs. We\npropose GAOKAO-MM, a multimodal benchmark based on the Chinese College Entrance\nExamination (GAOKAO), comprising of 8 subjects and 12 types of images, such as\ndiagrams, function graphs, maps and photos. GAOKAO-MM derives from native\nChinese context and sets human-level requirements for the model's abilities,\nincluding perception, understanding, knowledge and reasoning. We evaluate 10\nLVLMs and find that the accuracies of all of them are lower than 50%, with\nGPT-4-Vison (48.1%), Qwen-VL-Plus (41.2%) and Gemini-Pro-Vision (35.1%) ranking\nin the top three positions. The results of our multi-dimension analysis\nindicate that LVLMs have moderate distance towards Artificial General\nIntelligence (AGI) and provide insights facilitating the development of\nmultilingual LVLMs.\n","authors":["Yi Zong","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2402.15745v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03291v2","updated":"2024-08-06T15:14:01Z","published":"2024-07-03T17:24:36Z","title":"VCHAR:Variance-Driven Complex Human Activity Recognition framework with\n  Generative Representation","summary":"  Complex human activity recognition (CHAR) remains a pivotal challenge within\nubiquitous computing, especially in the context of smart environments. Existing\nstudies typically require meticulous labeling of both atomic and complex\nactivities, a task that is labor-intensive and prone to errors due to the\nscarcity and inaccuracies of available datasets. Most prior research has\nfocused on datasets that either precisely label atomic activities or, at\nminimum, their sequence approaches that are often impractical in real world\nsettings.In response, we introduce VCHAR (Variance-Driven Complex Human\nActivity Recognition), a novel framework that treats the outputs of atomic\nactivities as a distribution over specified intervals. Leveraging generative\nmethodologies, VCHAR elucidates the reasoning behind complex activity\nclassifications through video-based explanations, accessible to users without\nprior machine learning expertise. Our evaluation across three publicly\navailable datasets demonstrates that VCHAR enhances the accuracy of complex\nactivity recognition without necessitating precise temporal or sequential\nlabeling of atomic activities. Furthermore, user studies confirm that VCHAR's\nexplanations are more intelligible compared to existing methods, facilitating a\nbroader understanding of complex activity recognition among non-experts.\n","authors":["Yuan Sun","Navid Salami Pargoo","Taqiya Ehsan","Zhao Zhang","Jorge Ortiz"],"pdf_url":"https://arxiv.org/pdf/2407.03291v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03238v1","updated":"2024-08-06T14:50:48Z","published":"2024-08-06T14:50:48Z","title":"LAC-Net: Linear-Fusion Attention-Guided Convolutional Network for\n  Accurate Robotic Grasping Under the Occlusion","summary":"  This paper addresses the challenge of perceiving complete object shapes\nthrough visual perception. While prior studies have demonstrated encouraging\noutcomes in segmenting the visible parts of objects within a scene, amodal\nsegmentation, in particular, has the potential to allow robots to infer the\noccluded parts of objects. To this end, this paper introduces a new framework\nthat explores amodal segmentation for robotic grasping in cluttered scenes,\nthus greatly enhancing robotic grasping abilities. Initially, we use a\nconventional segmentation algorithm to detect the visible segments of the\ntarget object, which provides shape priors for completing the full object mask.\nParticularly, to explore how to utilize semantic features from RGB images and\ngeometric information from depth images, we propose a Linear-fusion\nAttention-guided Convolutional Network (LAC-Net). LAC-Net utilizes the\nlinear-fusion strategy to effectively fuse this cross-modal data, and then uses\nthe prior visible mask as attention map to guide the network to focus on target\nfeature locations for further complete mask recovery. Using the amodal mask of\nthe target object provides advantages in selecting more accurate and robust\ngrasp points compared to relying solely on the visible segments. The results on\ndifferent datasets show that our method achieves state-of-the-art performance.\nFurthermore, the robot experiments validate the feasibility and robustness of\nthis method in the real world. Our code and demonstrations are available on the\nproject page: https://jrryzh.github.io/LAC-Net.\n","authors":["Jinyu Zhang","Yongchong Gu","Jianxiong Gao","Haitao Lin","Qiang Sun","Xinwei Sun","Xiangyang Xue","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2408.03238v1.pdf","comment":"accepted by IROS2024"},{"id":"http://arxiv.org/abs/2408.03230v1","updated":"2024-08-06T14:44:55Z","published":"2024-08-06T14:44:55Z","title":"Contrastive Learning for Image Complexity Representation","summary":"  Quantifying and evaluating image complexity can be instrumental in enhancing\nthe performance of various computer vision tasks. Supervised learning can\neffectively learn image complexity features from well-annotated datasets.\nHowever, creating such datasets requires expensive manual annotation costs. The\nmodels may learn human subjective biases from it. In this work, we introduce\nthe MoCo v2 framework. We utilize contrastive learning to represent image\ncomplexity, named CLIC (Contrastive Learning for Image Complexity). We find\nthat there are complexity differences between different local regions of an\nimage, and propose Random Crop and Mix (RCM), which can produce positive\nsamples consisting of multi-scale local crops. RCM can also expand the train\nset and increase data diversity without introducing additional data. We conduct\nextensive experiments with CLIC, comparing it with both unsupervised and\nsupervised methods. The results demonstrate that the performance of CLIC is\ncomparable to that of state-of-the-art supervised methods. In addition, we\nestablish the pipelines that can apply CLIC to computer vision tasks to\neffectively improve their performance.\n","authors":["Shipeng Liu","Liang Zhao","Dengfeng Chen","Zhanping Song"],"pdf_url":"https://arxiv.org/pdf/2408.03230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03225v1","updated":"2024-08-06T14:36:43Z","published":"2024-08-06T14:36:43Z","title":"Line-based 6-DoF Object Pose Estimation and Tracking With an Event\n  Camera","summary":"  Pose estimation and tracking of objects is a fundamental application in 3D\nvision. Event cameras possess remarkable attributes such as high dynamic range,\nlow latency, and resilience against motion blur, which enables them to address\nchallenging high dynamic range scenes or high-speed motion. These features make\nevent cameras an ideal complement over standard cameras for object pose\nestimation. In this work, we propose a line-based robust pose estimation and\ntracking method for planar or non-planar objects using an event camera.\nFirstly, we extract object lines directly from events, then provide an initial\npose using a globally-optimal Branch-and-Bound approach, where 2D-3D line\ncorrespondences are not known in advance. Subsequently, we utilize event-line\nmatching to establish correspondences between 2D events and 3D models.\nFurthermore, object poses are refined and continuously tracked by minimizing\nevent-line distances. Events are assigned different weights based on these\ndistances, employing robust estimation algorithms. To evaluate the precision of\nthe proposed methods in object pose estimation and tracking, we have devised\nand established an event-based moving object dataset. Compared against\nstate-of-the-art methods, the robustness and accuracy of our methods have been\nvalidated both on synthetic experiments and the proposed dataset. The source\ncode is available at https://github.com/Zibin6/LOPET.\n","authors":["Zibin Liu","Banglei Guan","Yang Shang","Qifeng Yu","Laurent Kneip"],"pdf_url":"https://arxiv.org/pdf/2408.03225v1.pdf","comment":"Accepted by IEEE Transactions on Image Processing,2024"},{"id":"http://arxiv.org/abs/2408.03219v1","updated":"2024-08-06T14:25:23Z","published":"2024-08-06T14:25:23Z","title":"Learning to Learn without Forgetting using Attention","summary":"  Continual learning (CL) refers to the ability to continually learn over time\nby accommodating new knowledge while retaining previously learned experience.\nWhile this concept is inherent in human learning, current machine learning\nmethods are highly prone to overwrite previously learned patterns and thus\nforget past experience. Instead, model parameters should be updated selectively\nand carefully, avoiding unnecessary forgetting while optimally leveraging\npreviously learned patterns to accelerate future learning. Since hand-crafting\neffective update mechanisms is difficult, we propose meta-learning a\ntransformer-based optimizer to enhance CL. This meta-learned optimizer uses\nattention to learn the complex relationships between model parameters across a\nstream of tasks, and is designed to generate effective weight updates for the\ncurrent task while preventing catastrophic forgetting on previously encountered\ntasks. Evaluations on benchmark datasets like SplitMNIST, RotatedMNIST, and\nSplitCIFAR-100 affirm the efficacy of the proposed approach in terms of both\nforward and backward transfer, even on small sets of labeled data, highlighting\nthe advantages of integrating a meta-learned optimizer within the continual\nlearning framework.\n","authors":["Anna Vettoruzzo","Joaquin Vanschoren","Mohamed-Rafik Bouguelia","Thorsteinn Rögnvaldsson"],"pdf_url":"https://arxiv.org/pdf/2408.03219v1.pdf","comment":"Published at 3rd Conference on Lifelong Learning Agents (CoLLAs),\n  2024"},{"id":"http://arxiv.org/abs/2408.03209v1","updated":"2024-08-06T14:08:22Z","published":"2024-08-06T14:08:22Z","title":"IPAdapter-Instruct: Resolving Ambiguity in Image-based Conditioning\n  using Instruct Prompts","summary":"  Diffusion models continuously push the boundary of state-of-the-art image\ngeneration, but the process is hard to control with any nuance: practice proves\nthat textual prompts are inadequate for accurately describing image style or\nfine structural details (such as faces). ControlNet and IPAdapter address this\nshortcoming by conditioning the generative process on imagery instead, but each\nindividual instance is limited to modeling a single conditional posterior: for\npractical use-cases, where multiple different posteriors are desired within the\nsame workflow, training and using multiple adapters is cumbersome. We propose\nIPAdapter-Instruct, which combines natural-image conditioning with ``Instruct''\nprompts to swap between interpretations for the same conditioning image: style\ntransfer, object extraction, both, or something else still? IPAdapterInstruct\nefficiently learns multiple tasks with minimal loss in quality compared to\ndedicated per-task models.\n","authors":["Ciara Rowles","Shimon Vainer","Dante De Nigris","Slava Elizarov","Konstantin Kutsy","Simon Donné"],"pdf_url":"https://arxiv.org/pdf/2408.03209v1.pdf","comment":"17 pages, 10 figures, Project page:\n  https://unity-research.github.io/IP-Adapter-Instruct.github.io/"},{"id":"http://arxiv.org/abs/2408.03208v1","updated":"2024-08-06T14:06:53Z","published":"2024-08-06T14:06:53Z","title":"Personalizing Federated Instrument Segmentation with Visual Trait Priors\n  in Robotic Surgery","summary":"  Personalized federated learning (PFL) for surgical instrument segmentation\n(SIS) is a promising approach. It enables multiple clinical sites to\ncollaboratively train a series of models in privacy, with each model tailored\nto the individual distribution of each site. Existing PFL methods rarely\nconsider the personalization of multi-headed self-attention, and do not account\nfor appearance diversity and instrument shape similarity, both inherent in\nsurgical scenes. We thus propose PFedSIS, a novel PFL method with visual trait\npriors for SIS, incorporating global-personalized disentanglement (GPD),\nappearance-regulation personalized enhancement (APE), and shape-similarity\nglobal enhancement (SGE), to boost SIS performance in each site. GPD represents\nthe first attempt at head-wise assignment for multi-headed self-attention\npersonalization. To preserve the unique appearance representation of each site\nand gradually leverage the inter-site difference, APE introduces appearance\nregulation and provides customized layer-wise aggregation solutions via\nhypernetworks for each site's personalized parameters. The mutual shape\ninformation of instruments is maintained and shared via SGE, which enhances the\ncross-style shape consistency on the image level and computes the\nshape-similarity contribution of each site on the prediction level for updating\nthe global parameters. PFedSIS outperforms state-of-the-art methods with +1.51%\nDice, +2.11% IoU, -2.79 ASSD, -15.55 HD95 performance gains. The corresponding\ncode and models will be released at https://github.com/wzjialang/PFedSIS.\n","authors":["Jialang Xu","Jiacheng Wang","Lequan Yu","Danail Stoyanov","Yueming Jin","Evangelos B. Mazomenos"],"pdf_url":"https://arxiv.org/pdf/2408.03208v1.pdf","comment":"9 pages, 3 figures, under review"},{"id":"http://arxiv.org/abs/2408.03194v1","updated":"2024-08-06T13:53:45Z","published":"2024-08-06T13:53:45Z","title":"SGSR: Structure-Guided Multi-Contrast MRI Super-Resolution via\n  Spatio-Frequency Co-Query Attention","summary":"  Magnetic Resonance Imaging (MRI) is a leading diagnostic modality for a wide\nrange of exams, where multiple contrast images are often acquired for\ncharacterizing different tissues. However, acquiring high-resolution MRI\ntypically extends scan time, which can introduce motion artifacts.\nSuper-resolution of MRI therefore emerges as a promising approach to mitigate\nthese challenges. Earlier studies have investigated the use of multiple\ncontrasts for MRI super-resolution (MCSR), whereas majority of them did not\nfully exploit the rich contrast-invariant structural information. To fully\nutilize such crucial prior knowledge of multi-contrast MRI, in this work, we\npropose a novel structure-guided MCSR (SGSR) framework based on a new\nspatio-frequency co-query attention (CQA) mechanism. Specifically, CQA performs\nattention on features of multiple contrasts with a shared structural query,\nwhich is particularly designed to extract, fuse, and refine the common\nstructures from different contrasts. We further propose a novel\nfrequency-domain CQA module in addition to the spatial domain, to enable more\nfine-grained structural refinement. Extensive experiments on fastMRI knee data\nand low-field brain MRI show that SGSR outperforms state-of-the-art MCSR\nmethods with statistical significance.\n","authors":["Shaoming Zheng","Yinsong Wang","Siyi Du","Chen Qin"],"pdf_url":"https://arxiv.org/pdf/2408.03194v1.pdf","comment":"The 15th International Workshop on Machine Learning in Medical\n  Imaging (MLMI 2024)"},{"id":"http://arxiv.org/abs/2408.03193v1","updated":"2024-08-06T13:49:01Z","published":"2024-08-06T13:49:01Z","title":"Efficient NeRF Optimization -- Not All Samples Remain Equally Hard","summary":"  We propose an application of online hard sample mining for efficient training\nof Neural Radiance Fields (NeRF). NeRF models produce state-of-the-art quality\nfor many 3D reconstruction and rendering tasks but require substantial\ncomputational resources. The encoding of the scene information within the NeRF\nnetwork parameters necessitates stochastic sampling. We observe that during the\ntraining, a major part of the compute time and memory usage is spent on\nprocessing already learnt samples, which no longer affect the model update\nsignificantly. We identify the backward pass on the stochastic samples as the\ncomputational bottleneck during the optimization. We thus perform the first\nforward pass in inference mode as a relatively low-cost search for hard\nsamples. This is followed by building the computational graph and updating the\nNeRF network parameters using only the hard samples. To demonstrate the\neffectiveness of the proposed approach, we apply our method to Instant-NGP,\nresulting in significant improvements of the view-synthesis quality over the\nbaseline (1 dB improvement on average per training time, or 2x speedup to reach\nthe same PSNR level) along with approx. 40% memory savings coming from using\nonly the hard samples to build the computational graph. As our method only\ninterfaces with the network module, we expect it to be widely applicable.\n","authors":["Juuso Korhonen","Goutham Rangu","Hamed R. Tavakoli","Juho Kannala"],"pdf_url":"https://arxiv.org/pdf/2408.03193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15890v3","updated":"2024-08-06T13:47:24Z","published":"2023-11-27T14:56:47Z","title":"Stability-Informed Initialization of Neural Ordinary Differential\n  Equations","summary":"  This paper addresses the training of Neural Ordinary Differential Equations\n(neural ODEs), and in particular explores the interplay between numerical\nintegration techniques, stability regions, step size, and initialization\ntechniques. It is shown how the choice of integration technique implicitly\nregularizes the learned model, and how the solver's corresponding stability\nregion affects training and prediction performance. From this analysis, a\nstability-informed parameter initialization technique is introduced. The\neffectiveness of the initialization method is displayed across several learning\nbenchmarks and industrial applications.\n","authors":["Theodor Westny","Arman Mohammadi","Daniel Jung","Erik Frisk"],"pdf_url":"https://arxiv.org/pdf/2311.15890v3.pdf","comment":"In Proceedings of the 41 st International Conference on Machine\n  Learning"},{"id":"http://arxiv.org/abs/2308.10015v2","updated":"2024-08-06T13:25:29Z","published":"2023-08-19T13:46:49Z","title":"DyFFPAD: Dynamic Fusion of Convolutional and Handcrafted Features for\n  Fingerprint Presentation Attack Detection","summary":"  Automatic fingerprint recognition systems suffer from the threat of\npresentation attacks due to their wide range of deployment in areas including\nnational borders and commercial applications. A presentation attack can be\nperformed by creating a spoof of a user's fingerprint with or without their\nconsent. This paper presents a dynamic ensemble of deep CNN and handcrafted\nfeatures to detect presentation attacks in known-material and unknown-material\nprotocols of the livness detection competition. The proposed presentation\nattack detection model, in this way, utilizes the capabilities of both deep CNN\nand handcrafted features techniques and exhibits better performance than their\nindividual performances. The proposed method is validated using benchmark\ndatabases from the Liveness Detection Competition in 2015, 2017, and 2019,\nyielding overall accuracy of 96.10\\%, 96.49\\%, and 94.99\\% on them,\nrespectively. The proposed method outperforms state-of-the-art methods in terms\nof classification accuracy.\n","authors":["Anuj Rai","Parsheel Kumar Tiwari","Jyotishna Baishya","Ram Prakash Sharma","Somnath Dey"],"pdf_url":"https://arxiv.org/pdf/2308.10015v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2305.09397"},{"id":"http://arxiv.org/abs/2408.03178v1","updated":"2024-08-06T13:22:51Z","published":"2024-08-06T13:22:51Z","title":"An Object is Worth 64x64 Pixels: Generating 3D Object via Image\n  Diffusion","summary":"  We introduce a new approach for generating realistic 3D models with UV maps\nthrough a representation termed \"Object Images.\" This approach encapsulates\nsurface geometry, appearance, and patch structures within a 64x64 pixel image,\neffectively converting complex 3D shapes into a more manageable 2D format. By\ndoing so, we address the challenges of both geometric and semantic irregularity\ninherent in polygonal meshes. This method allows us to use image generation\nmodels, such as Diffusion Transformers, directly for 3D shape generation.\nEvaluated on the ABO dataset, our generated shapes with patch structures\nachieve point cloud FID comparable to recent 3D generative models, while\nnaturally supporting PBR material generation.\n","authors":["Xingguang Yan","Han-Hung Lee","Ziyu Wan","Angel X. Chang"],"pdf_url":"https://arxiv.org/pdf/2408.03178v1.pdf","comment":"Project Page: https://omages.github.io/"},{"id":"http://arxiv.org/abs/2407.15706v5","updated":"2024-08-06T13:20:16Z","published":"2024-07-22T15:16:47Z","title":"Multi-Modality Co-Learning for Efficient Skeleton-based Action\n  Recognition","summary":"  Skeleton-based action recognition has garnered significant attention due to\nthe utilization of concise and resilient skeletons. Nevertheless, the absence\nof detailed body information in skeletons restricts performance, while other\nmultimodal methods require substantial inference resources and are inefficient\nwhen using multimodal data during both training and inference stages. To\naddress this and fully harness the complementary multimodal features, we\npropose a novel multi-modality co-learning (MMCL) framework by leveraging the\nmultimodal large language models (LLMs) as auxiliary networks for efficient\nskeleton-based action recognition, which engages in multi-modality co-learning\nduring the training stage and keeps efficiency by employing only concise\nskeletons in inference. Our MMCL framework primarily consists of two modules.\nFirst, the Feature Alignment Module (FAM) extracts rich RGB features from video\nframes and aligns them with global skeleton features via contrastive learning.\nSecond, the Feature Refinement Module (FRM) uses RGB images with temporal\ninformation and text instruction to generate instructive features based on the\npowerful generalization of multimodal LLMs. These instructive text features\nwill further refine the classification scores and the refined scores will\nenhance the model's robustness and generalization in a manner similar to soft\nlabels. Extensive experiments on NTU RGB+D, NTU RGB+D 120 and Northwestern-UCLA\nbenchmarks consistently verify the effectiveness of our MMCL, which outperforms\nthe existing skeleton-based action recognition methods. Meanwhile, experiments\non UTD-MHAD and SYSU-Action datasets demonstrate the commendable generalization\nof our MMCL in zero-shot and domain-adaptive action recognition. Our code is\npublicly available at: https://github.com/liujf69/MMCL-Action.\n","authors":["Jinfu Liu","Chen Chen","Mengyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2407.15706v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00956v3","updated":"2024-08-06T13:07:37Z","published":"2024-05-02T02:34:19Z","title":"SimEndoGS: Efficient Data-driven Scene Simulation using Robotic Surgery\n  Videos via Physics-embedded 3D Gaussians","summary":"  Surgical scene simulation plays a crucial role in surgical education and\nsimulator-based robot learning. Traditional approaches for creating these\nenvironments with surgical scene involve a labor-intensive process where\ndesigners hand-craft tissues models with textures and geometries for soft body\nsimulations. This manual approach is not only time-consuming but also limited\nin the scalability and realism. In contrast, data-driven simulation offers a\ncompelling alternative. It has the potential to automatically reconstruct 3D\nsurgical scenes from real-world surgical video data, followed by the\napplication of soft body physics. This area, however, is relatively uncharted.\nIn our research, we introduce 3D Gaussian as a learnable representation for\nsurgical scene, which is learned from stereo endoscopic video. To prevent\nover-fitting and ensure the geometrical correctness of these scenes, we\nincorporate depth supervision and anisotropy regularization into the Gaussian\nlearning process. Furthermore, we apply the Material Point Method, which is\nintegrated with physical properties, to the 3D Gaussians to achieve realistic\nscene deformations. Our method was evaluated on our collected in-house and\npublic surgical videos datasets. Results show that it can reconstruct and\nsimulate surgical scenes from endoscopic videos efficiently-taking only a few\nminutes to reconstruct the surgical scene-and produce both visually and\nphysically plausible deformations at a speed approaching real-time. The results\ndemonstrate great potential of our proposed method to enhance the efficiency\nand variety of simulations available for surgical education and robot learning.\n","authors":["Zhenya Yang","Kai Chen","Yonghao Long","Qi Dou"],"pdf_url":"https://arxiv.org/pdf/2405.00956v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12705v2","updated":"2024-08-06T13:06:26Z","published":"2024-07-17T16:26:30Z","title":"IMAGDressing-v1: Customizable Virtual Dressing","summary":"  Latest advances have achieved realistic virtual try-on (VTON) through\nlocalized garment inpainting using latent diffusion models, significantly\nenhancing consumers' online shopping experience. However, existing VTON\ntechnologies neglect the need for merchants to showcase garments\ncomprehensively, including flexible control over garments, optional faces,\nposes, and scenes. To address this issue, we define a virtual dressing (VD)\ntask focused on generating freely editable human images with fixed garments and\noptional conditions. Meanwhile, we design a comprehensive affinity metric index\n(CAMI) to evaluate the consistency between generated images and reference\ngarments. Then, we propose IMAGDressing-v1, which incorporates a garment UNet\nthat captures semantic features from CLIP and texture features from VAE. We\npresent a hybrid attention module, including a frozen self-attention and a\ntrainable cross-attention, to integrate garment features from the garment UNet\ninto a frozen denoising UNet, ensuring users can control different scenes\nthrough text. IMAGDressing-v1 can be combined with other extension plugins,\nsuch as ControlNet and IP-Adapter, to enhance the diversity and controllability\nof generated images. Furthermore, to address the lack of data, we release the\ninteractive garment pairing (IGPair) dataset, containing over 300,000 pairs of\nclothing and dressed images, and establish a standard pipeline for data\nassembly. Extensive experiments demonstrate that our IMAGDressing-v1 achieves\nstate-of-the-art human image synthesis performance under various controlled\nconditions. The code and model will be available at\nhttps://github.com/muzishen/IMAGDressing.\n","authors":["Fei Shen","Xin Jiang","Xin He","Hu Ye","Cong Wang","Xiaoyu Du","Zechao Li","Jinhui Tang"],"pdf_url":"https://arxiv.org/pdf/2407.12705v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03164v1","updated":"2024-08-06T13:05:32Z","published":"2024-08-06T13:05:32Z","title":"Dilated Convolution with Learnable Spacings makes visual models more\n  aligned with humans: a Grad-CAM study","summary":"  Dilated Convolution with Learnable Spacing (DCLS) is a recent advanced\nconvolution method that allows enlarging the receptive fields (RF) without\nincreasing the number of parameters, like the dilated convolution, yet without\nimposing a regular grid. DCLS has been shown to outperform the standard and\ndilated convolutions on several computer vision benchmarks. Here, we show that,\nin addition, DCLS increases the models' interpretability, defined as the\nalignment with human visual strategies. To quantify it, we use the Spearman\ncorrelation between the models' GradCAM heatmaps and the ClickMe dataset\nheatmaps, which reflect human visual attention. We took eight reference models\n- ResNet50, ConvNeXt (T, S and B), CAFormer, ConvFormer, and FastViT (sa 24 and\n36) - and drop-in replaced the standard convolution layers with DCLS ones. This\nimproved the interpretability score in seven of them. Moreover, we observed\nthat Grad-CAM generated random heatmaps for two models in our study: CAFormer\nand ConvFormer models, leading to low interpretability scores. We addressed\nthis issue by introducing Threshold-Grad-CAM, a modification built on top of\nGrad-CAM that enhanced interpretability across nearly all models. The code and\ncheckpoints to reproduce this study are available at:\nhttps://github.com/rabihchamas/DCLS-GradCAM-Eval.\n","authors":["Rabih Chamas","Ismail Khalfaoui-Hassani","Timothee Masquelier"],"pdf_url":"https://arxiv.org/pdf/2408.03164v1.pdf","comment":"Accepted at The Trustworthy AI Workshop, IJCAI 2024"},{"id":"http://arxiv.org/abs/2408.03156v1","updated":"2024-08-06T12:55:17Z","published":"2024-08-06T12:55:17Z","title":"Iterative CT Reconstruction via Latent Variable Optimization of Shallow\n  Diffusion Models","summary":"  Image generative AI has garnered significant attention in recent years. In\nparticular, the diffusion model, a core component of recent generative AI,\nproduces high-quality images with rich diversity. In this study, we propose a\nnovel CT reconstruction method by combining the denoising diffusion\nprobabilistic model with iterative CT reconstruction. In sharp contrast to\nprevious studies, we optimize the fidelity loss of CT reconstruction with\nrespect to the latent variable of the diffusion model, instead of the image and\nmodel parameters. To suppress anatomical structure changes produced by the\ndiffusion model, we shallow the diffusion and reverse processes, and fix a set\nof added noises in the reverse process to make it deterministic during\ninference. We demonstrate the effectiveness of the proposed method through\nsparse view CT reconstruction of 1/10 view projection data. Despite the\nsimplicity of the implementation, the proposed method shows the capability of\nreconstructing high-quality images while preserving the patient's anatomical\nstructure, and outperforms existing methods including iterative reconstruction,\niterative reconstruction with total variation, and the diffusion model alone in\nterms of quantitative indices such as SSIM and PSNR. We also explore further\nsparse view CT using 1/20 view projection data with the same trained diffusion\nmodel. As the number of iterations increases, image quality improvement\ncomparable to that of 1/10 sparse view CT reconstruction is achieved. In\nprinciple, the proposed method can be widely applied not only to CT but also to\nother imaging modalities such as MRI, PET, and SPECT.\n","authors":["Sho Ozaki","Shizuo Kaji","Toshikazu Imae","Kanabu Nawa","Hideomi Yamashita","Keiichi Nakagawa"],"pdf_url":"https://arxiv.org/pdf/2408.03156v1.pdf","comment":"19 pages, 9 figures"},{"id":"http://arxiv.org/abs/2407.15488v3","updated":"2024-08-06T12:54:41Z","published":"2024-07-22T09:05:16Z","title":"DiffX: Guide Your Layout to Cross-Modal Generative Modeling","summary":"  Diffusion models have made significant strides in language-driven and\nlayout-driven image generation. However, most diffusion models are limited to\nvisible RGB image generation. In fact, human perception of the world is\nenriched by diverse viewpoints, such as chromatic contrast, thermal\nillumination, and depth information. In this paper, we introduce a novel\ndiffusion model for general layout-guided cross-modal generation, called DiffX.\nNotably, DiffX presents a simple yet effective cross-modal generative modeling\npipeline, which conducts diffusion and denoising processes in the\nmodality-shared latent space. Moreover, we introduce the Joint-Modality\nEmbedder (JME) to enhance interaction between layout and text conditions by\nincorporating a gated attention mechanism. Meanwhile, the advanced Long-CLIP is\nemployed for long caption embedding for user instruction. To facilitate the\nuser-instructed generative training, we construct the cross-modal image\ndatasets with detailed text captions assisted by the Large-Multimodal Model\n(LMM). Through extensive experiments, DiffX demonstrates robustness in\ncross-modal generation across three ``RGB+X'' datasets: FLIR, MFNet, and\nCOME15K, guided by various layout conditions. It also shows the potential for\nthe adaptive generation of ``RGB+X+Y+Z'' images or more diverse modalities on\nCOME15K and MCXFace datasets. Our code and constructed cross-modal image\ndatasets are available at https://github.com/zeyuwang-zju/DiffX.\n","authors":["Zeyu Wang","Jingyu Lin","Yifei Qian","Yi Huang","Shicen Tian","Bosong Chai","Juncan Deng","Lan Du","Cunjian Chen","Yufei Guo","Kejie Huang"],"pdf_url":"https://arxiv.org/pdf/2407.15488v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01661v3","updated":"2024-08-06T12:54:26Z","published":"2024-05-02T18:31:47Z","title":"When a Relation Tells More Than a Concept: Exploring and Evaluating\n  Classifier Decisions with CoReX","summary":"  Explanations for Convolutional Neural Networks (CNNs) based on relevance of\ninput pixels might be too unspecific to evaluate which and how input features\nimpact model decisions. Especially in complex real-world domains like biology,\nthe presence of specific concepts and of relations between concepts might be\ndiscriminating between classes. Pixel relevance is not expressive enough to\nconvey this type of information. In consequence, model evaluation is limited\nand relevant aspects present in the data and influencing the model decisions\nmight be overlooked. This work presents a novel method to explain and evaluate\nCNN models, which uses a concept- and relation-based explainer (CoReX). It\nexplains the predictive behavior of a model on a set of images by masking\n(ir-)relevant concepts from the decision-making process and by constraining\nrelations in a learned interpretable surrogate model. We test our approach with\nseveral image data sets and CNN architectures. Results show that CoReX\nexplanations are faithful to the CNN model in terms of predictive outcomes. We\nfurther demonstrate through a human evaluation that CoReX is a suitable tool\nfor generating combined explanations that help assessing the classification\nquality of CNNs. We further show that CoReX supports the identification and\nre-classification of incorrect or ambiguous classifications.\n","authors":["Bettina Finzel","Patrick Hilme","Johannes Rabold","Ute Schmid"],"pdf_url":"https://arxiv.org/pdf/2405.01661v3.pdf","comment":"preliminary version, submitted to Machine Learning"},{"id":"http://arxiv.org/abs/2408.03149v1","updated":"2024-08-06T12:45:56Z","published":"2024-08-06T12:45:56Z","title":"Leveraging Entity Information for Cross-Modality Correlation Learning:\n  The Entity-Guided Multimodal Summarization","summary":"  The rapid increase in multimedia data has spurred advancements in Multimodal\nSummarization with Multimodal Output (MSMO), which aims to produce a multimodal\nsummary that integrates both text and relevant images. The inherent\nheterogeneity of content within multimodal inputs and outputs presents a\nsignificant challenge to the execution of MSMO. Traditional approaches\ntypically adopt a holistic perspective on coarse image-text data or individual\nvisual objects, overlooking the essential connections between objects and the\nentities they represent. To integrate the fine-grained entity knowledge, we\npropose an Entity-Guided Multimodal Summarization model (EGMS). Our model,\nbuilding on BART, utilizes dual multimodal encoders with shared weights to\nprocess text-image and entity-image information concurrently. A gating\nmechanism then combines visual data for enhanced textual summary generation,\nwhile image selection is refined through knowledge distillation from a\npre-trained vision-language model. Extensive experiments on public MSMO dataset\nvalidate the superiority of the EGMS method, which also prove the necessity to\nincorporate entity information into MSMO problem.\n","authors":["Yanghai Zhang","Ye Liu","Shiwei Wu","Kai Zhang","Xukai Liu","Qi Liu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2408.03149v1.pdf","comment":"In ACL-Findings 2024"},{"id":"http://arxiv.org/abs/2408.03143v1","updated":"2024-08-06T12:37:47Z","published":"2024-08-06T12:37:47Z","title":"SuperSimpleNet: Unifying Unsupervised and Supervised Learning for Fast\n  and Reliable Surface Defect Detection","summary":"  The aim of surface defect detection is to identify and localise abnormal\nregions on the surfaces of captured objects, a task that's increasingly\ndemanded across various industries. Current approaches frequently fail to\nfulfil the extensive demands of these industries, which encompass high\nperformance, consistency, and fast operation, along with the capacity to\nleverage the entirety of the available training data. Addressing these gaps, we\nintroduce SuperSimpleNet, an innovative discriminative model that evolved from\nSimpleNet. This advanced model significantly enhances its predecessor's\ntraining consistency, inference time, as well as detection performance.\nSuperSimpleNet operates in an unsupervised manner using only normal training\nimages but also benefits from labelled abnormal training images when they are\navailable. SuperSimpleNet achieves state-of-the-art results in both the\nsupervised and the unsupervised settings, as demonstrated by experiments across\nfour challenging benchmark datasets. Code:\nhttps://github.com/blaz-r/SuperSimpleNet .\n","authors":["Blaž Rolih","Matic Fučka","Danijel Skočaj"],"pdf_url":"https://arxiv.org/pdf/2408.03143v1.pdf","comment":"Accepted to ICPR 2024"},{"id":"http://arxiv.org/abs/2402.17485v2","updated":"2024-08-06T12:33:30Z","published":"2024-02-27T13:10:11Z","title":"EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with\n  Audio2Video Diffusion Model under Weak Conditions","summary":"  In this work, we tackle the challenge of enhancing the realism and\nexpressiveness in talking head video generation by focusing on the dynamic and\nnuanced relationship between audio cues and facial movements. We identify the\nlimitations of traditional techniques that often fail to capture the full\nspectrum of human expressions and the uniqueness of individual facial styles.\nTo address these issues, we propose EMO, a novel framework that utilizes a\ndirect audio-to-video synthesis approach, bypassing the need for intermediate\n3D models or facial landmarks. Our method ensures seamless frame transitions\nand consistent identity preservation throughout the video, resulting in highly\nexpressive and lifelike animations. Experimental results demonsrate that EMO is\nable to produce not only convincing speaking videos but also singing videos in\nvarious styles, significantly outperforming existing state-of-the-art\nmethodologies in terms of expressiveness and realism.\n","authors":["Linrui Tian","Qi Wang","Bang Zhang","Liefeng Bo"],"pdf_url":"https://arxiv.org/pdf/2402.17485v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20962v2","updated":"2024-08-06T12:25:48Z","published":"2024-07-30T16:43:24Z","title":"MMTrail: A Multimodal Trailer Video Dataset with Language and Music\n  Descriptions","summary":"  Massive multi-modality datasets play a significant role in facilitating the\nsuccess of large video-language models. However, current video-language\ndatasets primarily provide text descriptions for visual frames, considering\naudio to be weakly related information. They usually overlook exploring the\npotential of inherent audio-visual correlation, leading to monotonous\nannotation within each modality instead of comprehensive and precise\ndescriptions. Such ignorance results in the difficulty of multiple\ncross-modality studies. To fulfill this gap, we present MMTrail, a large-scale\nmulti-modality video-language dataset incorporating more than 20M trailer clips\nwith visual captions, and 2M high-quality clips with multimodal captions.\nTrailers preview full-length video works and integrate context, visual frames,\nand background music. In particular, the trailer has two main advantages: (1)\nthe topics are diverse, and the content characters are of various types, e.g.,\nfilm, news, and gaming. (2) the corresponding background music is\ncustom-designed, making it more coherent with the visual context. Upon these\ninsights, we propose a systemic captioning framework, achieving various\nmodality annotations with more than 27.1k hours of trailer videos. Here, to\nensure the caption retains music perspective while preserving the authority of\nvisual context, we leverage the advanced LLM to merge all annotations\nadaptively. In this fashion, our MMtrail dataset potentially paves the path for\nfine-grained large multimodal-language model training. In experiments, we\nprovide evaluation metrics and benchmark results on our dataset, demonstrating\nthe high quality of our annotation and its effectiveness for model training.\n","authors":["Xiaowei Chi","Yatian Wang","Aosong Cheng","Pengjun Fang","Zeyue Tian","Yingqing He","Zhaoyang Liu","Xingqun Qi","Jiahao Pan","Rongyu Zhang","Mengfei Li","Ruibin Yuan","Yanbing Jiang","Wei Xue","Wenhan Luo","Qifeng Chen","Shanghang Zhang","Qifeng Liu","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2407.20962v2.pdf","comment":"15 Pages. Dataset report"},{"id":"http://arxiv.org/abs/2408.00998v2","updated":"2024-08-06T12:01:17Z","published":"2024-08-02T04:13:38Z","title":"FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features\n  for Highly Controllable Text-Driven Image Translation","summary":"  Large-scale text-to-image diffusion models have been a revolutionary\nmilestone in the evolution of generative AI and multimodal technology, allowing\nwonderful image generation with natural-language text prompt. However, the\nissue of lacking controllability of such models restricts their practical\napplicability for real-life content creation. Thus, attention has been focused\non leveraging a reference image to control text-to-image synthesis, which is\nalso regarded as manipulating (or editing) a reference image as per a text\nprompt, namely, text-driven image-to-image translation. This paper contributes\na novel, concise, and efficient approach that adapts pre-trained large-scale\ntext-to-image (T2I) diffusion model to the image-to-image (I2I) paradigm in a\nplug-and-play manner, realizing high-quality and versatile text-driven I2I\ntranslation without any model training, model fine-tuning, or online\noptimization process. To guide T2I generation with a reference image, we\npropose to decompose diverse guiding factors with different frequency bands of\ndiffusion features in the DCT spectral space, and accordingly devise a novel\nfrequency band substitution layer which realizes dynamic control of the\nreference image to the T2I generation result in a plug-and-play manner. We\ndemonstrate that our method allows flexible control over both guiding factor\nand guiding intensity of the reference image simply by tuning the type and\nbandwidth of the substituted frequency band, respectively. Extensive\nqualitative and quantitative experiments verify superiority of our approach\nover related methods in I2I translation visual quality, versatility, and\ncontrollability. The code is publicly available at:\nhttps://github.com/XiangGao1102/FBSDiff.\n","authors":["Xiang Gao","Jiaying Liu"],"pdf_url":"https://arxiv.org/pdf/2408.00998v2.pdf","comment":"Accepted conference paper of ACM MM 2024"},{"id":"http://arxiv.org/abs/2408.03120v1","updated":"2024-08-06T11:49:13Z","published":"2024-08-06T11:49:13Z","title":"Benchmarking In-the-wild Multimodal Disease Recognition and A Versatile\n  Baseline","summary":"  Existing plant disease classification models have achieved remarkable\nperformance in recognizing in-laboratory diseased images. However, their\nperformance often significantly degrades in classifying in-the-wild images.\nFurthermore, we observed that in-the-wild plant images may exhibit similar\nappearances across various diseases (i.e., small inter-class discrepancy) while\nthe same diseases may look quite different (i.e., large intra-class variance).\nMotivated by this observation, we propose an in-the-wild multimodal plant\ndisease recognition dataset that contains the largest number of disease classes\nbut also text-based descriptions for each disease. Particularly, the newly\nprovided text descriptions are introduced to provide rich information in\ntextual modality and facilitate in-the-wild disease classification with small\ninter-class discrepancy and large intra-class variance issues. Therefore, our\nproposed dataset can be regarded as an ideal testbed for evaluating disease\nrecognition methods in the real world. In addition, we further present a strong\nyet versatile baseline that models text descriptions and visual data through\nmultiple prototypes for a given class. By fusing the contributions of\nmultimodal prototypes in classification, our baseline can effectively address\nthe small inter-class discrepancy and large intra-class variance issues.\nRemarkably, our baseline model can not only classify diseases but also\nrecognize diseases in few-shot or training-free scenarios. Extensive\nbenchmarking results demonstrate that our proposed in-the-wild multimodal\ndataset sets many new challenges to the plant disease recognition task and\nthere is a large space to improve for future works.\n","authors":["Tianqi Wei","Zhi Chen","Zi Huang","Xin Yu"],"pdf_url":"https://arxiv.org/pdf/2408.03120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02209v2","updated":"2024-08-06T11:46:39Z","published":"2024-08-05T03:18:58Z","title":"Source-Free Domain-Invariant Performance Prediction","summary":"  Accurately estimating model performance poses a significant challenge,\nparticularly in scenarios where the source and target domains follow different\ndata distributions. Most existing performance prediction methods heavily rely\non the source data in their estimation process, limiting their applicability in\na more realistic setting where only the trained model is accessible. The few\nmethods that do not require source data exhibit considerably inferior\nperformance. In this work, we propose a source-free approach centred on\nuncertainty-based estimation, using a generative model for calibration in the\nabsence of source data. We establish connections between our approach for\nunsupervised calibration and temperature scaling. We then employ a\ngradient-based strategy to evaluate the correctness of the calibrated\npredictions. Our experiments on benchmark object recognition datasets reveal\nthat existing source-based methods fall short with limited source sample\navailability. Furthermore, our approach significantly outperforms the current\nstate-of-the-art source-free and source-based methods, affirming its\neffectiveness in domain-invariant performance estimation.\n","authors":["Ekaterina Khramtsova","Mahsa Baktashmotlagh","Guido Zuccon","Xi Wang","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2408.02209v2.pdf","comment":"Accepted in ECCV 2024"},{"id":"http://arxiv.org/abs/2311.02558v3","updated":"2024-08-06T11:35:04Z","published":"2023-11-05T03:53:42Z","title":"Multi-Agent 3D Map Reconstruction and Change Detection in Microgravity\n  with Free-Flying Robots","summary":"  Assistive free-flyer robots autonomously caring for future crewed outposts --\nsuch as NASA's Astrobee robots on the International Space Station (ISS) -- must\nbe able to detect day-to-day interior changes to track inventory, detect and\ndiagnose faults, and monitor the outpost status. This work presents a framework\nfor multi-agent cooperative mapping and change detection to enable robotic\nmaintenance of space outposts. One agent is used to reconstruct a 3D model of\nthe environment from sequences of images and corresponding depth information.\nAnother agent is used to periodically scan the environment for inconsistencies\nagainst the 3D model. Change detection is validated after completing the\nsurveys using real image and pose data collected by Astrobee robots in a ground\ntesting environment and from microgravity aboard the ISS. This work outlines\nthe objectives, requirements, and algorithmic modules for the multi-agent\nreconstruction system, including recommendations for its use by assistive\nfree-flyers aboard future microgravity outposts.\n  *Denotes Equal Contribution\n","authors":["Holly Dinkel","Julia Di","Jamie Santos","Keenan Albee","Paulo Borges","Marina Moreira","Oleg Alexandrov","Brian Coltin","Trey Smith"],"pdf_url":"https://arxiv.org/pdf/2311.02558v3.pdf","comment":"11 pages, 8 figures, Manuscript presented at the 74th International\n  Astronautical Congress, IAC 2023, Baku, Azerbaijan, 2 - 6 October 2023. Video\n  presentation: [https://www.youtube.com/watch?v=VfjV-zwFEtU]. Code:\n  [https://github.com/hollydinkel/astrobeecd]"},{"id":"http://arxiv.org/abs/2408.03097v1","updated":"2024-08-06T10:56:53Z","published":"2024-08-06T10:56:53Z","title":"Prototype Learning for Micro-gesture Classification","summary":"  In this paper, we briefly introduce the solution developed by our team,\nHFUT-VUT, for the track of Micro-gesture Classification in the MiGA challenge\nat IJCAI 2024. The task of micro-gesture classification task involves\nrecognizing the category of a given video clip, which focuses on more\nfine-grained and subtle body movements compared to typical action recognition\ntasks. Given the inherent complexity of micro-gesture recognition, which\nincludes large intra-class variability and minimal inter-class differences, we\nutilize two innovative modules, i.e., the cross-modal fusion module and\nprototypical refinement module, to improve the discriminative ability of MG\nfeatures, thereby improving the classification accuracy. Our solution achieved\nsignificant success, ranking 1st in the track of Micro-gesture Classification.\nWe surpassed the performance of last year's leading team by a substantial\nmargin, improving Top-1 accuracy by 6.13%.\n","authors":["Guoliang Chen","Fei Wang","Kun Li","Zhiliang Wu","Hehe Fan","Yi Yang","Meng Wang","Dan Guo"],"pdf_url":"https://arxiv.org/pdf/2408.03097v1.pdf","comment":"1st Place in Micro-gesture Classification in MiGA at IJCAI-2024"},{"id":"http://arxiv.org/abs/2408.03078v1","updated":"2024-08-06T10:13:57Z","published":"2024-08-06T10:13:57Z","title":"BodySLAM: A Generalized Monocular Visual SLAM Framework for Surgical\n  Applications","summary":"  Endoscopic surgery relies on two-dimensional views, posing challenges for\nsurgeons in depth perception and instrument manipulation. While Simultaneous\nLocalization and Mapping (SLAM) has emerged as a promising solution to address\nthese limitations, its implementation in endoscopic procedures presents\nsignificant challenges due to hardware limitations, such as the use of a\nmonocular camera and the absence of odometry sensors. This study presents a\nrobust deep learning-based SLAM approach that combines state-of-the-art and\nnewly developed models. It consists of three main parts: the Monocular Pose\nEstimation Module that introduces a novel unsupervised method based on the\nCycleGAN architecture, the Monocular Depth Estimation Module that leverages the\nnovel Zoe architecture, and the 3D Reconstruction Module which uses information\nfrom the previous models to create a coherent surgical map. The performance of\nthe procedure was rigorously evaluated using three publicly available datasets\n(Hamlyn, EndoSLAM, and SCARED) and benchmarked against two state-of-the-art\nmethods, EndoSFMLearner and EndoDepth. The integration of Zoe in the MDEM\ndemonstrated superior performance compared to state-of-the-art depth estimation\nalgorithms in endoscopy, whereas the novel approach in the MPEM exhibited\ncompetitive performance and the lowest inference time. The results showcase the\nrobustness of our approach in laparoscopy, gastroscopy, and colonoscopy, three\ndifferent scenarios in endoscopic surgery. The proposed SLAM approach has the\npotential to improve the accuracy and efficiency of endoscopic procedures by\nproviding surgeons with enhanced depth perception and 3D reconstruction\ncapabilities.\n","authors":["G. Manni","C. Lauretti","F. Prata","R. Papalia","L. Zollo","P. Soda"],"pdf_url":"https://arxiv.org/pdf/2408.03078v1.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2406.02265v3","updated":"2024-08-06T10:10:58Z","published":"2024-06-04T12:41:54Z","title":"Understanding Retrieval Robustness for Retrieval-Augmented Image\n  Captioning","summary":"  Recent advances in retrieval-augmented models for image captioning highlight\nthe benefit of retrieving related captions for efficient, lightweight models\nwith strong domain-transfer capabilities. While these models demonstrate the\nsuccess of retrieval augmentation, retrieval models are still far from perfect\nin practice: the retrieved information can sometimes mislead the model,\nresulting in incorrect generation and worse performance. In this paper, we\nanalyze the robustness of a retrieval-augmented captioning model SmallCap. Our\nanalysis shows that the model is sensitive to tokens that appear in the\nmajority of the retrieved captions, and the input attribution shows that those\ntokens are likely copied into the generated output. Given these findings, we\npropose to train the model by sampling retrieved captions from more diverse\nsets. This decreases the chance that the model learns to copy majority tokens,\nand improves both in-domain and cross-domain performance.\n","authors":["Wenyan Li","Jiaang Li","Rita Ramos","Raphael Tang","Desmond Elliott"],"pdf_url":"https://arxiv.org/pdf/2406.02265v3.pdf","comment":"9 pages, long paper at ACL 2024"},{"id":"http://arxiv.org/abs/2407.14086v2","updated":"2024-08-06T09:56:36Z","published":"2024-07-19T07:48:45Z","title":"Temporal Correlation Meets Embedding: Towards a 2nd Generation of\n  JDE-based Real-Time Multi-Object Tracking","summary":"  Joint Detection and Embedding (JDE) trackers have demonstrated excellent\nperformance in Multi-Object Tracking (MOT) tasks by incorporating the\nextraction of appearance features as auxiliary tasks through embedding\nRe-Identification task (ReID) into the detector, achieving a balance between\ninference speed and tracking performance. However, solving the competition\nbetween the detector and the feature extractor has always been a challenge.\nMeanwhile, the issue of directly embedding the ReID task into MOT has remained\nunresolved. The lack of high discriminability in appearance features results in\ntheir limited utility. In this paper, a new learning approach using\ncross-correlation to capture temporal information of objects is proposed. The\nfeature extraction network is no longer trained solely on appearance features\nfrom each frame but learns richer motion features by utilizing feature heatmaps\nfrom consecutive frames, which addresses the challenge of inter-class feature\nsimilarity. Furthermore, our learning approach is applied to a more lightweight\nfeature extraction network, and treat the feature matching scores as strong\ncues rather than auxiliary cues, with an appropriate weight calculation to\nreflect the compatibility between our obtained features and the MOT task. Our\ntracker, named TCBTrack, achieves state-of-the-art performance on multiple\npublic benchmarks, i.e., MOT17, MOT20, and DanceTrack datasets. Specifically,\non the DanceTrack test set, we achieve 56.8 HOTA, 58.1 IDF1 and 92.5 MOTA,\nmaking it the best online tracker capable of achieving real-time performance.\nComparative evaluations with other trackers prove that our tracker achieves the\nbest balance between speed, robustness and accuracy. Code is available at\nhttps://github.com/yfzhang1214/TCBTrack.\n","authors":["Yunfei Zhang","Chao Liang","Jin Gao","Zhipeng Zhang","Weiming Hu","Stephen Maybank","Xue Zhou","Liang Li"],"pdf_url":"https://arxiv.org/pdf/2407.14086v2.pdf","comment":"A submission to IJCV"},{"id":"http://arxiv.org/abs/2408.03065v1","updated":"2024-08-06T09:35:50Z","published":"2024-08-06T09:35:50Z","title":"SCOPE: A Synthetic Multi-Modal Dataset for Collective Perception\n  Including Physical-Correct Weather Conditions","summary":"  Collective perception has received considerable attention as a promising\napproach to overcome occlusions and limited sensing ranges of vehicle-local\nperception in autonomous driving. In order to develop and test novel collective\nperception technologies, appropriate datasets are required. These datasets must\ninclude not only different environmental conditions, as they strongly influence\nthe perception capabilities, but also a wide range of scenarios with different\nroad users as well as realistic sensor models. Therefore, we propose the\nSynthetic COllective PErception (SCOPE) dataset. SCOPE is the first synthetic\nmulti-modal dataset that incorporates realistic camera and LiDAR models as well\nas parameterized and physically accurate weather simulations for both sensor\ntypes. The dataset contains 17,600 frames from over 40 diverse scenarios with\nup to 24 collaborative agents, infrastructure sensors, and passive traffic,\nincluding cyclists and pedestrians. In addition, recordings from two novel\ndigital-twin maps from Karlsruhe and T\\\"ubingen are included. The dataset is\navailable at https://ekut-es.github.io/scope\n","authors":["Jörg Gamerdinger","Sven Teufel","Patrick Schulz","Stephan Amann","Jan-Patrick Kirchner","Oliver Bringmann"],"pdf_url":"https://arxiv.org/pdf/2408.03065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03060v1","updated":"2024-08-06T09:23:24Z","published":"2024-08-06T09:23:24Z","title":"MGFs: Masked Gaussian Fields for Meshing Building based on Multi-View\n  Images","summary":"  Over the last few decades, image-based building surface reconstruction has\ngarnered substantial research interest and has been applied across various\nfields, such as heritage preservation, architectural planning, etc. Compared to\nthe traditional photogrammetric and NeRF-based solutions, recently, Gaussian\nfields-based methods have exhibited significant potential in generating surface\nmeshes due to their time-efficient training and detailed 3D information\npreservation. However, most gaussian fields-based methods are trained with all\nimage pixels, encompassing building and nonbuilding areas, which results in a\nsignificant noise for building meshes and degeneration in time efficiency. This\npaper proposes a novel framework, Masked Gaussian Fields (MGFs), designed to\ngenerate accurate surface reconstruction for building in a time-efficient way.\nThe framework first applies EfficientSAM and COLMAP to generate multi-level\nmasks of building and the corresponding masked point clouds. Subsequently, the\nmasked gaussian fields are trained by integrating two innovative losses: a\nmulti-level perceptual masked loss focused on constructing building regions and\na boundary loss aimed at enhancing the details of the boundaries between\ndifferent masks. Finally, we improve the tetrahedral surface mesh extraction\nmethod based on the masked gaussian spheres. Comprehensive experiments on UAV\nimages demonstrate that, compared to the traditional method and several\nNeRF-based and Gaussian-based SOTA solutions, our approach significantly\nimproves both the accuracy and efficiency of building surface reconstruction.\nNotably, as a byproduct, there is an additional gain in the novel view\nsynthesis of building.\n","authors":["Tengfei Wang","Zongqian Zhan","Rui Xia","Linxia Ji","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03046v1","updated":"2024-08-06T09:02:31Z","published":"2024-08-06T09:02:31Z","title":"Comb, Prune, Distill: Towards Unified Pruning for Vision Model\n  Compression","summary":"  Lightweight and effective models are essential for devices with limited\nresources, such as intelligent vehicles. Structured pruning offers a promising\napproach to model compression and efficiency enhancement. However, existing\nmethods often tie pruning techniques to specific model architectures or vision\ntasks. To address this limitation, we propose a novel unified pruning framework\nComb, Prune, Distill (CPD), which addresses both model-agnostic and\ntask-agnostic concerns simultaneously. Our framework employs a combing step to\nresolve hierarchical layer-wise dependency issues, enabling architecture\nindependence. Additionally, the pruning pipeline adaptively remove parameters\nbased on the importance scoring metrics regardless of vision tasks. To support\nthe model in retaining its learned information, we introduce knowledge\ndistillation during the pruning step. Extensive experiments demonstrate the\ngeneralizability of our framework, encompassing both convolutional neural\nnetwork (CNN) and transformer models, as well as image classification and\nsegmentation tasks. In image classification we achieve a speedup of up to x4.3\nwith a accuracy loss of 1.8% and in semantic segmentation up to x1.89 with a\n5.1% loss in mIoU.\n","authors":["Jonas Schmitt","Ruiping Liu","Junwei Zheng","Jiaming Zhang","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2408.03046v1.pdf","comment":"Accepted by ITSC 2024. Code is publicly available at:\n  https://github.com/Cranken/CPD"},{"id":"http://arxiv.org/abs/2408.03043v1","updated":"2024-08-06T08:58:20Z","published":"2024-08-06T08:58:20Z","title":"Targeted Visual Prompting for Medical Visual Question Answering","summary":"  With growing interest in recent years, medical visual question answering\n(Med-VQA) has rapidly evolved, with multimodal large language models (MLLMs)\nemerging as an alternative to classical model architectures. Specifically,\ntheir ability to add visual information to the input of pre-trained LLMs brings\nnew capabilities for image interpretation. However, simple visual errors cast\ndoubt on the actual visual understanding abilities of these models. To address\nthis, region-based questions have been proposed as a means to assess and\nenhance actual visual understanding through compositional evaluation. To\ncombine these two perspectives, this paper introduces targeted visual prompting\nto equip MLLMs with region-based questioning capabilities. By presenting the\nmodel with both the isolated region and the region in its context in a\ncustomized visual prompt, we show the effectiveness of our method across\nmultiple datasets while comparing it to several baseline models. Our code and\ndata are available at https://github.com/sergiotasconmorales/locvqallm.\n","authors":["Sergio Tascon-Morales","Pablo Márquez-Neila","Raphael Sznitman"],"pdf_url":"https://arxiv.org/pdf/2408.03043v1.pdf","comment":"Accepted at the MICCAI AMAI Workshop 2024"},{"id":"http://arxiv.org/abs/2407.20171v2","updated":"2024-08-06T08:42:47Z","published":"2024-07-29T17:00:09Z","title":"Diffusion Feedback Helps CLIP See Better","summary":"  Contrastive Language-Image Pre-training (CLIP), which excels at abstracting\nopen-world representations across domains and modalities, has become a\nfoundation for a variety of vision and multimodal tasks. However, recent\nstudies reveal that CLIP has severe visual shortcomings, such as which can\nhardly distinguish orientation, quantity, color, structure, etc. These visual\nshortcomings also limit the perception capabilities of multimodal large\nlanguage models (MLLMs) built on CLIP. The main reason could be that the\nimage-text pairs used to train CLIP are inherently biased, due to the lack of\nthe distinctiveness of the text and the diversity of images. In this work, we\npresent a simple post-training approach for CLIP models, which largely\novercomes its visual shortcomings via a self-supervised diffusion process. We\nintroduce DIVA, which uses the DIffusion model as a Visual Assistant for CLIP.\nSpecifically, DIVA leverages generative feedback from text-to-image diffusion\nmodels to optimize CLIP representations, with only images (without\ncorresponding text). We demonstrate that DIVA improves CLIP's performance on\nthe challenging MMVP-VLM benchmark which assesses fine-grained visual abilities\nto a large extent (e.g., 3-7%), and enhances the performance of MLLMs and\nvision models on multimodal understanding and segmentation tasks. Extensive\nevaluation on 29 image classification and retrieval benchmarks confirms that\nour framework preserves CLIP's strong zero-shot capabilities. The code is\navailable at https://github.com/baaivision/DIVA.\n","authors":["Wenxuan Wang","Quan Sun","Fan Zhang","Yepeng Tang","Jing Liu","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2407.20171v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03035v1","updated":"2024-08-06T08:31:34Z","published":"2024-08-06T08:31:34Z","title":"Training-Free Condition Video Diffusion Models for single frame\n  Spatial-Semantic Echocardiogram Synthesis","summary":"  Conditional video diffusion models (CDM) have shown promising results for\nvideo synthesis, potentially enabling the generation of realistic\nechocardiograms to address the problem of data scarcity. However, current CDMs\nrequire a paired segmentation map and echocardiogram dataset. We present a new\nmethod called Free-Echo for generating realistic echocardiograms from a single\nend-diastolic segmentation map without additional training data. Our method is\nbased on the 3D-Unet with Temporal Attention Layers model and is conditioned on\nthe segmentation map using a training-free conditioning method based on SDEdit.\nWe evaluate our model on two public echocardiogram datasets, CAMUS and\nEchoNet-Dynamic. We show that our model can generate plausible echocardiograms\nthat are spatially aligned with the input segmentation map, achieving\nperformance comparable to training-based CDMs. Our work opens up new\npossibilities for generating echocardiograms from a single segmentation map,\nwhich can be used for data augmentation, domain adaptation, and other\napplications in medical imaging. Our code is available at\n\\url{https://github.com/gungui98/echo-free}\n","authors":["Van Phi Nguyen","Tri Nhan Luong Ha","Huy Hieu Pham","Quoc Long Tran"],"pdf_url":"https://arxiv.org/pdf/2408.03035v1.pdf","comment":"Accepted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.09913v3","updated":"2024-08-06T08:27:55Z","published":"2024-06-14T10:47:52Z","title":"OpenECAD: An Efficient Visual Language Model for Editable 3D-CAD Design","summary":"  Computer-aided design (CAD) tools are utilized in the manufacturing industry\nfor modeling everything from cups to spacecraft. These programs are complex to\nuse and typically require years of training and experience to master.\nStructured and well-constrained 2D sketches and 3D constructions are crucial\ncomponents of CAD modeling. A well-executed CAD model can be seamlessly\nintegrated into the manufacturing process, thereby enhancing production\nefficiency. Deep generative models of 3D shapes and 3D object reconstruction\nmodels have garnered significant research interest. However, most of these\nmodels produce discrete forms of 3D objects that are not editable. Moreover,\nthe few models based on CAD operations often have substantial input\nrestrictions. In this work, we fine-tuned pre-trained models to create OpenECAD\nmodels (0.55B, 0.89B, 2.4B and 3.1B), leveraging the visual, logical, coding,\nand general capabilities of visual language models. OpenECAD models can process\nimages of 3D designs as input and generate highly structured 2D sketches and 3D\nconstruction commands, ensuring that the designs are editable. These outputs\ncan be directly used with existing CAD tools' APIs to generate project files.\nTo train our network, we created a series of OpenECAD datasets. These datasets\nare derived from existing public CAD datasets, adjusted and augmented to meet\nthe specific requirements of vision language model (VLM) training.\nAdditionally, we have introduced an approach that utilizes dependency\nrelationships to define and generate sketches, further enriching the content\nand functionality of the datasets.\n","authors":["Zhe Yuan","Jianqi Shi","Yanhong Huang"],"pdf_url":"https://arxiv.org/pdf/2406.09913v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03030v1","updated":"2024-08-06T08:24:47Z","published":"2024-08-06T08:24:47Z","title":"Nighttime Pedestrian Detection Based on Fore-Background Contrast\n  Learning","summary":"  The significance of background information is frequently overlooked in\ncontemporary research concerning channel attention mechanisms. This study\naddresses the issue of suboptimal single-spectral nighttime pedestrian\ndetection performance under low-light conditions by incorporating background\ninformation into the channel attention mechanism. Despite numerous studies\nfocusing on the development of efficient channel attention mechanisms, the\nrelevance of background information has been largely disregarded. By adopting a\ncontrast learning approach, we reexamine channel attention with regard to\npedestrian objects and background information for nighttime pedestrian\ndetection, resulting in the proposed Fore-Background Contrast Attention (FBCA).\nFBCA possesses two primary attributes: (1) channel descriptors form remote\ndependencies with global spatial feature information; (2) the integration of\nbackground information enhances the distinction between channels concentrating\non low-light pedestrian features and those focusing on background information.\nConsequently, the acquired channel descriptors exhibit a higher semantic level\nand spatial accuracy. Experimental outcomes demonstrate that FBCA significantly\noutperforms existing methods in single-spectral nighttime pedestrian detection,\nachieving state-of-the-art results on the NightOwls and TJU-DHD-pedestrian\ndatasets. Furthermore, this methodology also yields performance improvements\nfor the multispectral LLVIP dataset. These findings indicate that integrating\nbackground information into the channel attention mechanism effectively\nmitigates detector performance degradation caused by illumination factors in\nnighttime scenarios.\n","authors":["He Yao","Yongjun Zhang","Huachun Jian","Li Zhang","Ruzhong Cheng"],"pdf_url":"https://arxiv.org/pdf/2408.03030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03014v1","updated":"2024-08-06T07:51:20Z","published":"2024-08-06T07:51:20Z","title":"CKNN: Cleansed k-Nearest Neighbor for Unsupervised Video Anomaly\n  Detection","summary":"  In this paper, we address the problem of unsupervised video anomaly detection\n(UVAD). The task aims to detect abnormal events in test video using unlabeled\nvideos as training data. The presence of anomalies in the training data poses a\nsignificant challenge in this task, particularly because they form clusters in\nthe feature space. We refer to this property as the \"Anomaly Cluster\" issue.\nThe condensed nature of these anomalies makes it difficult to distinguish\nbetween normal and abnormal data in the training set. Consequently, training\nconventional anomaly detection techniques using an unlabeled dataset often\nleads to sub-optimal results. To tackle this difficulty, we propose a new\nmethod called Cleansed k-Nearest Neighbor (CKNN), which explicitly filters out\nthe Anomaly Clusters by cleansing the training dataset. Following the k-nearest\nneighbor algorithm in the feature space provides powerful anomaly detection\ncapability. Although the identified Anomaly Cluster issue presents a\nsignificant challenge to applying k-nearest neighbor in UVAD, our proposed\ncleansing scheme effectively addresses this problem. We evaluate the proposed\nmethod on various benchmark datasets and demonstrate that CKNN outperforms the\nprevious state-of-the-art UVAD method by up to 8.5% (from 82.0 to 89.0) in\nterms of AUROC. Moreover, we emphasize that the performance of the proposed\nmethod is comparable to that of the state-of-the-art method trained using\nanomaly-free data.\n","authors":["Jihun Yi","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2408.03014v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17572v3","updated":"2024-08-06T07:36:21Z","published":"2024-07-24T18:05:13Z","title":"CityX: Controllable Procedural Content Generation for Unbounded 3D\n  Cities","summary":"  Generating a realistic, large-scale 3D virtual city remains a complex\nchallenge due to the involvement of numerous 3D assets, various city styles,\nand strict layout constraints. Existing approaches provide promising attempts\nat procedural content generation to create large-scale scenes using Blender\nagents. However, they face crucial issues such as difficulties in scaling up\ngeneration capability and achieving fine-grained control at the semantic layout\nlevel. To address these problems, we propose a novel multi-modal controllable\nprocedural content generation method, named CityX, which enhances realistic,\nunbounded 3D city generation guided by multiple layout conditions, including\nOSM, semantic maps, and satellite images. Specifically, the proposed method\ncontains a general protocol for integrating various PCG plugins and a\nmulti-agent framework for transforming instructions into executable Blender\nactions. Through this effective framework, CityX shows the potential to build\nan innovative ecosystem for 3D scene generation by bridging the gap between the\nquality of generated assets and industrial requirements. Extensive experiments\nhave demonstrated the effectiveness of our method in creating high-quality,\ndiverse, and unbounded cities guided by multi-modal conditions. Our project\npage: https://cityx-lab.github.io.\n","authors":["Shougao Zhang","Mengqi Zhou","Yuxi Wang","Chuanchen Luo","Rongyu Wang","Yiwei Li","Xucheng Yin","Zhaoxiang Zhang","Junran Peng"],"pdf_url":"https://arxiv.org/pdf/2407.17572v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14362v3","updated":"2024-08-06T07:32:46Z","published":"2024-03-21T12:45:01Z","title":"Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen\n  Domains by Intrinsic Learning from Redundant LLM Semantics","summary":"  Generalized zero-shot learning (GZSL) focuses on recognizing seen and unseen\nclasses against domain shift problem (DSP) where data of unseen classes may be\nmisclassified as seen classes. However, existing GZSL is still limited to seen\ndomains. In the current work, we pioneer cross-domain GZSL (CDGZSL) which\naddresses GZSL towards unseen domains. Different from existing GZSL methods\nwhich alleviate DSP by generating features of unseen classes with semantics,\nCDGZSL needs to construct a common feature space across domains and acquire the\ncorresponding intrinsic semantics shared among domains to transfer from seen to\nunseen domains. Considering the information asymmetry problem caused by\nredundant class semantics annotated with large language models (LLMs), we\npresent Meta Domain Alignment Semantic Refinement (MDASR). Technically, MDASR\nconsists of two parts: Inter-class Similarity Alignment (ISA), which eliminates\nthe non-intrinsic semantics not shared across all domains under the guidance of\ninter-class feature relationships, and Unseen-class Meta Generation (UMG),\nwhich preserves intrinsic semantics to maintain connectivity between seen and\nunseen classes by simulating feature generation. MDASR effectively aligns the\nredundant semantic space with the common feature space, mitigating the\ninformation asymmetry in CDGZSL. The effectiveness of MDASR is demonstrated on\nthe Office-Home and Mini-DomainNet, and we have shared the LLM-based semantics\nfor these datasets as the benchmark.\n","authors":["Jiaqi Yue","Jiancheng Zhao","Chunhui Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.14362v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03006v1","updated":"2024-08-06T07:30:53Z","published":"2024-08-06T07:30:53Z","title":"Dual-path Collaborative Generation Network for Emotional Video\n  Captioning","summary":"  Emotional Video Captioning is an emerging task that aims to describe factual\ncontent with the intrinsic emotions expressed in videos. The essential of the\nEVC task is to effectively perceive subtle and ambiguous visual emotional cues\nduring the caption generation, which is neglected by the traditional video\ncaptioning. Existing emotional video captioning methods perceive global visual\nemotional cues at first, and then combine them with the video features to guide\nthe emotional caption generation, which neglects two characteristics of the EVC\ntask. Firstly, their methods neglect the dynamic subtle changes in the\nintrinsic emotions of the video, which makes it difficult to meet the needs of\ncommon scenes with diverse and changeable emotions. Secondly, as their methods\nincorporate emotional cues into each step, the guidance role of emotion is\noveremphasized, which makes factual content more or less ignored during\ngeneration. To this end, we propose a dual-path collaborative generation\nnetwork, which dynamically perceives visual emotional cues evolutions while\ngenerating emotional captions by collaborative learning. Specifically, in the\ndynamic emotion perception path, we propose a dynamic emotion evolution module,\nwhich first aggregates visual features and historical caption features to\nsummarize the global visual emotional cues, and then dynamically selects\nemotional cues required to be re-composed at each stage. Besides, in the\nadaptive caption generation path, to balance the description of factual content\nand emotional cues, we propose an emotion adaptive decoder. Thus, our methods\ncan generate emotion-related words at the necessary time step, and our caption\ngeneration balances the guidance of factual content and emotional cues well.\nExtensive experiments on three challenging datasets demonstrate the superiority\nof our approach and each proposed module.\n","authors":["Cheng Ye","Weidong Chen","Jingyu Li","Lei Zhang","Zhendong Mao"],"pdf_url":"https://arxiv.org/pdf/2408.03006v1.pdf","comment":"Acccepted by ACM Multimedia 2024, oral"},{"id":"http://arxiv.org/abs/2402.16907v2","updated":"2024-08-06T07:24:35Z","published":"2024-02-25T04:24:28Z","title":"Diffusion Posterior Proximal Sampling for Image Restoration","summary":"  Diffusion models have demonstrated remarkable efficacy in generating\nhigh-quality samples. Existing diffusion-based image restoration algorithms\nexploit pre-trained diffusion models to leverage data priors, yet they still\npreserve elements inherited from the unconditional generation paradigm. These\nstrategies initiate the denoising process with pure white noise and incorporate\nrandom noise at each generative step, leading to over-smoothed results. In this\npaper, we present a refined paradigm for diffusion-based image restoration.\nSpecifically, we opt for a sample consistent with the measurement identity at\neach generative step, exploiting the sampling selection as an avenue for output\nstability and enhancement. The number of candidate samples used for selection\nis adaptively determined based on the signal-to-noise ratio of the timestep.\nAdditionally, we start the restoration process with an initialization combined\nwith the measurement signal, providing supplementary information to better\nalign the generative process. Extensive experimental results and analyses\nvalidate that our proposed method significantly enhances image restoration\nperformance while consuming negligible additional computational resources.\n","authors":["Hongjie Wu","Linchao He","Mingqin Zhang","Dongdong Chen","Kunming Luo","Mengting Luo","Ji-Zhe Zhou","Hu Chen","Jiancheng Lv"],"pdf_url":"https://arxiv.org/pdf/2402.16907v2.pdf","comment":"ACM Multimedia 2024 Oral"},{"id":"http://arxiv.org/abs/2408.03001v1","updated":"2024-08-06T07:19:51Z","published":"2024-08-06T07:19:51Z","title":"Multitask and Multimodal Neural Tuning for Large Models","summary":"  In recent years, large-scale multimodal models have demonstrated impressive\ncapabilities across various domains. However, enabling these models to\neffectively perform multiple multimodal tasks simultaneously remains a\nsignificant challenge. To address this, we introduce a novel tuning method\ncalled neural tuning, designed to handle diverse multimodal tasks concurrently,\nincluding reasoning segmentation, referring segmentation, image captioning, and\ntext-to-image generation. Neural tuning emulates sparse distributed\nrepresentation in human brain, where only specific subsets of neurons are\nactivated for each task. Additionally, we present a new benchmark, MMUD, where\neach sample is annotated with multiple task labels. By applying neural tuning\nto pretrained large models on the MMUD benchmark, we achieve simultaneous task\nhandling in a streamlined and efficient manner. All models, code, and datasets\nwill be publicly available after publication, facilitating further research and\ndevelopment in this field.\n","authors":["Hao Sun","Yu Song","Jihong Hu","Yen-Wei Chen","Lanfen Lin"],"pdf_url":"https://arxiv.org/pdf/2408.03001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02993v1","updated":"2024-08-06T06:59:15Z","published":"2024-08-06T06:59:15Z","title":"DreamLCM: Towards High-Quality Text-to-3D Generation via Latent\n  Consistency Model","summary":"  Recently, the text-to-3D task has developed rapidly due to the appearance of\nthe SDS method. However, the SDS method always generates 3D objects with poor\nquality due to the over-smooth issue. This issue is attributed to two factors:\n1) the DDPM single-step inference produces poor guidance gradients; 2) the\nrandomness from the input noises and timesteps averages the details of the 3D\ncontents.In this paper, to address the issue, we propose DreamLCM which\nincorporates the Latent Consistency Model (LCM). DreamLCM leverages the\npowerful image generation capabilities inherent in LCM, enabling generating\nconsistent and high-quality guidance, i.e., predicted noises or images. Powered\nby the improved guidance, the proposed method can provide accurate and detailed\ngradients to optimize the target 3D models.In addition, we propose two\nstrategies to enhance the generation quality further. Firstly, we propose a\nguidance calibration strategy, utilizing Euler Solver to calibrate the guidance\ndistribution to accelerate 3D models to converge. Secondly, we propose a dual\ntimestep strategy, increasing the consistency of guidance and optimizing 3D\nmodels from geometry to appearance in DreamLCM. Experiments show that DreamLCM\nachieves state-of-the-art results in both generation quality and training\nefficiency. The code is available at https://github.com/1YimingZhong/DreamLCM.\n","authors":["Yiming Zhong","Xiaolin Zhang","Yao Zhao","Yunchao Wei"],"pdf_url":"https://arxiv.org/pdf/2408.02993v1.pdf","comment":"15 pages, 9 figures, ACM MM 2024"},{"id":"http://arxiv.org/abs/2408.02983v1","updated":"2024-08-06T06:33:24Z","published":"2024-08-06T06:33:24Z","title":"Diffusion Model Meets Non-Exemplar Class-Incremental Learning and Beyond","summary":"  Non-exemplar class-incremental learning (NECIL) is to resist catastrophic\nforgetting without saving old class samples. Prior methodologies generally\nemploy simple rules to generate features for replaying, suffering from large\ndistribution gap between replayed features and real ones. To address the\naforementioned issue, we propose a simple, yet effective\n\\textbf{Diff}usion-based \\textbf{F}eature \\textbf{R}eplay (\\textbf{DiffFR})\nmethod for NECIL. First, to alleviate the limited representational capacity\ncaused by fixing the feature extractor, we employ Siamese-based self-supervised\nlearning for initial generalizable features. Second, we devise diffusion models\nto generate class-representative features highly similar to real features,\nwhich provides an effective way for exemplar-free knowledge memorization.\nThird, we introduce prototype calibration to direct the diffusion model's focus\ntowards learning the distribution shapes of features, rather than the entire\ndistribution. Extensive experiments on public datasets demonstrate significant\nperformance gains of our DiffFR, outperforming the state-of-the-art NECIL\nmethods by 3.0\\% in average. The code will be made publicly available soon.\n","authors":["Jichuan Zhang","Yali Li","Xin Liu","Shengjin Wang"],"pdf_url":"https://arxiv.org/pdf/2408.02983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16499v2","updated":"2024-08-06T06:31:34Z","published":"2023-11-27T15:49:41Z","title":"InceptionHuman: Controllable Prompt-to-NeRF for Photorealistic 3D Human\n  Generation","summary":"  This paper presents InceptionHuman, a prompt-to-NeRF framework that allows\neasy control via a combination of prompts in different modalities (e.g., text,\nposes, edge, segmentation map, etc) as inputs to generate photorealistic 3D\nhumans. While many works have focused on generating 3D human models, they\nsuffer one or more of the following: lack of distinctive features, unnatural\nshading/shadows, unnatural poses/clothes, limited views, etc. InceptionHuman\nachieves consistent 3D human generation within a progressively refined NeRF\nspace with two novel modules, Iterative Pose-Aware Refinement (IPAR) and\nProgressive-Augmented Reconstruction (PAR). IPAR iteratively refines the\ndiffusion-generated images and synthesizes high-quality 3D-aware views\nconsidering the close-pose RGB values. PAR employs a pretrained diffusion prior\nto augment the generated synthetic views and adds regularization for\nview-independent appearance. Overall, the synthesis of photorealistic novel\nviews empowers the resulting 3D human NeRF from 360-degree perspectives.\nExtensive qualitative and quantitative experimental comparison show that our\nInceptionHuman models achieve state-of-the-art application quality.\n","authors":["Shiu-hong Kao","Xinhang Liu","Yu-Wing Tai","Chi-Keung Tang"],"pdf_url":"https://arxiv.org/pdf/2311.16499v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02980v1","updated":"2024-08-06T06:25:39Z","published":"2024-08-06T06:25:39Z","title":"Sample-agnostic Adversarial Perturbation for Vision-Language\n  Pre-training Models","summary":"  Recent studies on AI security have highlighted the vulnerability of\nVision-Language Pre-training (VLP) models to subtle yet intentionally designed\nperturbations in images and texts. Investigating multimodal systems' robustness\nvia adversarial attacks is crucial in this field. Most multimodal attacks are\nsample-specific, generating a unique perturbation for each sample to construct\nadversarial samples. To the best of our knowledge, it is the first work through\nmultimodal decision boundaries to explore the creation of a universal,\nsample-agnostic perturbation that applies to any image. Initially, we explore\nstrategies to move sample points beyond the decision boundaries of linear\nclassifiers, refining the algorithm to ensure successful attacks under the top\n$k$ accuracy metric. Based on this foundation, in visual-language tasks, we\ntreat visual and textual modalities as reciprocal sample points and decision\nhyperplanes, guiding image embeddings to traverse text-constructed decision\nboundaries, and vice versa. This iterative process consistently refines a\nuniversal perturbation, ultimately identifying a singular direction within the\ninput space which is exploitable to impair the retrieval performance of VLP\nmodels. The proposed algorithms support the creation of global perturbations or\nadversarial patches. Comprehensive experiments validate the effectiveness of\nour method, showcasing its data, task, and model transferability across various\nVLP models and datasets. Code: https://github.com/LibertazZ/MUAP\n","authors":["Haonan Zheng","Wen Jiang","Xinyang Deng","Wenrui Li"],"pdf_url":"https://arxiv.org/pdf/2408.02980v1.pdf","comment":"13 pages, 8 figures, published in ACMMM2024"},{"id":"http://arxiv.org/abs/2408.02978v1","updated":"2024-08-06T06:24:10Z","published":"2024-08-06T06:24:10Z","title":"ASR-enhanced Multimodal Representation Learning for Cross-Domain Product\n  Retrieval","summary":"  E-commerce is increasingly multimedia-enriched, with products exhibited in a\nbroad-domain manner as images, short videos, or live stream promotions. A\nunified and vectorized cross-domain production representation is essential. Due\nto large intra-product variance and high inter-product similarity in the\nbroad-domain scenario, a visual-only representation is inadequate. While\nAutomatic Speech Recognition (ASR) text derived from the short or live-stream\nvideos is readily accessible, how to de-noise the excessively noisy text for\nmultimodal representation learning is mostly untouched. We propose ASR-enhanced\nMultimodal Product Representation Learning (AMPere). In order to extract\nproduct-specific information from the raw ASR text, AMPere uses an\neasy-to-implement LLM-based ASR text summarizer. The LLM-summarized text,\ntogether with visual data, is then fed into a multi-branch network to generate\ncompact multimodal embeddings. Extensive experiments on a large-scale\ntri-domain dataset verify the effectiveness of AMPere in obtaining a unified\nmultimodal product representation that clearly improves cross-domain product\nretrieval.\n","authors":["Ruixiang Zhao","Jian Jia","Yan Li","Xuehan Bai","Quan Chen","Han Li","Peng Jiang","Xirong Li"],"pdf_url":"https://arxiv.org/pdf/2408.02978v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.18136v2","updated":"2024-08-06T06:04:41Z","published":"2024-04-28T10:16:35Z","title":"SafePaint: Anti-forensic Image Inpainting with Domain Adaptation","summary":"  Existing image inpainting methods have achieved remarkable accomplishments in\ngenerating visually appealing results, often accompanied by a trend toward\ncreating more intricate structural textures. However, while these models excel\nat creating more realistic image content, they often leave noticeable traces of\ntampering, posing a significant threat to security. In this work, we take the\nanti-forensic capabilities into consideration, firstly proposing an end-to-end\ntraining framework for anti-forensic image inpainting named SafePaint.\nSpecifically, we innovatively formulated image inpainting as two major tasks:\nsemantically plausible content completion and region-wise optimization. The\nformer is similar to current inpainting methods that aim to restore the missing\nregions of corrupted images. The latter, through domain adaptation, endeavors\nto reconcile the discrepancies between the inpainted region and the unaltered\narea to achieve anti-forensic goals. Through comprehensive theoretical\nanalysis, we validate the effectiveness of domain adaptation for anti-forensic\nperformance. Furthermore, we meticulously crafted a region-wise separated\nattention (RWSA) module, which not only aligns with our objective of\nanti-forensics but also enhances the performance of the model. Extensive\nqualitative and quantitative evaluations show our approach achieves comparable\nresults to existing image inpainting methods while offering anti-forensic\ncapabilities not available in other methods.\n","authors":["Dunyun Chen","Xin Liao","Xiaoshuai Wu","Shiwei Chen"],"pdf_url":"https://arxiv.org/pdf/2404.18136v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21317v2","updated":"2024-08-06T05:42:42Z","published":"2024-07-31T03:58:48Z","title":"Pathology Foundation Models","summary":"  Pathology has played a crucial role in the diagnosis and evaluation of\npatient tissue samples obtained from surgeries and biopsies for many years. The\nadvent of Whole Slide Scanners and the development of deep learning\ntechnologies have significantly advanced the field, leading to extensive\nresearch and development in pathology AI (Artificial Intelligence). These\nadvancements have contributed to reducing the workload of pathologists and\nsupporting decision-making in treatment plans. Recently, large-scale AI models\nknown as Foundation Models (FMs), which are more accurate and applicable to a\nwide range of tasks compared to traditional AI, have emerged, and expanded\ntheir application scope in the healthcare field. Numerous FMs have been\ndeveloped in pathology, and there are reported cases of their application in\nvarious tasks, such as disease diagnosis, rare cancer diagnosis, patient\nsurvival prognosis prediction, biomarker expression prediction, and the scoring\nof immunohistochemical expression intensity. However, several challenges remain\nfor the clinical application of FMs, which healthcare professionals, as users,\nmust be aware of. Research is ongoing to address these challenges. In the\nfuture, it is expected that the development of Generalist Medical AI, which\nintegrates pathology FMs with FMs from other medical domains, will progress,\nleading to the effective utilization of AI in real clinical settings to promote\nprecision and personalized medicine.\n","authors":["Mieko Ochi","Daisuke Komura","Shumpei Ishikawa"],"pdf_url":"https://arxiv.org/pdf/2407.21317v2.pdf","comment":"19 pages, 1 figure, 3 tables"},{"id":"http://arxiv.org/abs/2408.02966v1","updated":"2024-08-06T05:24:06Z","published":"2024-08-06T05:24:06Z","title":"Fast Point Cloud Geometry Compression with Context-based Residual Coding\n  and INR-based Refinement","summary":"  Compressing a set of unordered points is far more challenging than\ncompressing images/videos of regular sample grids, because of the difficulties\nin characterizing neighboring relations in an irregular layout of points. Many\nresearchers resort to voxelization to introduce regularity, but this approach\nsuffers from quantization loss. In this research, we use the KNN method to\ndetermine the neighborhoods of raw surface points. This gives us a means to\ndetermine the spatial context in which the latent features of 3D points are\ncompressed by arithmetic coding. As such, the conditional probability model is\nadaptive to local geometry, leading to significant rate reduction.\nAdditionally, we propose a dual-layer architecture where a non-learning base\nlayer reconstructs the main structures of the point cloud at low complexity,\nwhile a learned refinement layer focuses on preserving fine details. This\ndesign leads to reductions in model complexity and coding latency by two orders\nof magnitude compared to SOTA methods. Moreover, we incorporate an implicit\nneural representation (INR) into the refinement layer, allowing the decoder to\nsample points on the underlying surface at arbitrary densities. This work is\nthe first to effectively exploit content-aware local contexts for compressing\nirregular raw point clouds, achieving high rate-distortion performance, low\ncomplexity, and the ability to function as an arbitrary-scale upsampling\nnetwork simultaneously.\n","authors":["Hao Xu","Xi Zhang","Xiaolin Wu"],"pdf_url":"https://arxiv.org/pdf/2408.02966v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2405.05953v4","updated":"2024-08-06T05:19:06Z","published":"2024-05-09T17:46:22Z","title":"Frame Interpolation with Consecutive Brownian Bridge Diffusion","summary":"  Recent work in Video Frame Interpolation (VFI) tries to formulate VFI as a\ndiffusion-based conditional image generation problem, synthesizing the\nintermediate frame given a random noise and neighboring frames. Due to the\nrelatively high resolution of videos, Latent Diffusion Models (LDMs) are\nemployed as the conditional generation model, where the autoencoder compresses\nimages into latent representations for diffusion and then reconstructs images\nfrom these latent representations. Such a formulation poses a crucial\nchallenge: VFI expects that the output is deterministically equal to the ground\ntruth intermediate frame, but LDMs randomly generate a diverse set of different\nimages when the model runs multiple times. The reason for the diverse\ngeneration is that the cumulative variance (variance accumulated at each step\nof generation) of generated latent representations in LDMs is large. This makes\nthe sampling trajectory random, resulting in diverse rather than deterministic\ngenerations. To address this problem, we propose our unique solution: Frame\nInterpolation with Consecutive Brownian Bridge Diffusion. Specifically, we\npropose consecutive Brownian Bridge diffusion that takes a deterministic\ninitial value as input, resulting in a much smaller cumulative variance of\ngenerated latent representations. Our experiments suggest that our method can\nimprove together with the improvement of the autoencoder and achieve\nstate-of-the-art performance in VFI, leaving strong potential for further\nenhancement.\n","authors":["Zonglin Lyu","Ming Li","Jianbo Jiao","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2405.05953v4.pdf","comment":"corrected typo"},{"id":"http://arxiv.org/abs/2407.11652v4","updated":"2024-08-06T05:10:56Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v4.pdf","comment":"I found critical errors in the manuscript affecting its validity. I\n  need to correct these before resubmitting. Major changes to methodology and\n  results are underway, significantly altering the content. I will resubmit the\n  revised version"},{"id":"http://arxiv.org/abs/2408.02957v1","updated":"2024-08-06T04:55:33Z","published":"2024-08-06T04:55:33Z","title":"Online Temporal Action Localization with Memory-Augmented Transformer","summary":"  Online temporal action localization (On-TAL) is the task of identifying\nmultiple action instances given a streaming video. Since existing methods take\nas input only a video segment of fixed size per iteration, they are limited in\nconsidering long-term context and require tuning the segment size carefully. To\novercome these limitations, we propose memory-augmented transformer (MATR).\nMATR utilizes the memory queue that selectively preserves the past segment\nfeatures, allowing to leverage long-term context for inference. We also propose\na novel action localization method that observes the current input segment to\npredict the end time of the ongoing action and accesses the memory queue to\nestimate the start time of the action. Our method outperformed existing methods\non two datasets, THUMOS14 and MUSES, surpassing not only TAL methods in the\nonline setting but also some offline TAL methods.\n","authors":["Youngkil Song","Dongkeun Kim","Minsu Cho","Suha Kwak"],"pdf_url":"https://arxiv.org/pdf/2408.02957v1.pdf","comment":"Accepted to ECCV 2024, Project page:\n  https://cvlab.postech.ac.kr/research/MATR/"},{"id":"http://arxiv.org/abs/2408.02954v1","updated":"2024-08-06T04:44:10Z","published":"2024-08-06T04:44:10Z","title":"WWW: Where, Which and Whatever Enhancing Interpretability in Multimodal\n  Deepfake Detection","summary":"  All current benchmarks for multimodal deepfake detection manipulate entire\nframes using various generation techniques, resulting in oversaturated\ndetection accuracies exceeding 94% at the video-level classification. However,\nthese benchmarks struggle to detect dynamic deepfake attacks with challenging\nframe-by-frame alterations presented in real-world scenarios. To address this\nlimitation, we introduce FakeMix, a novel clip-level evaluation benchmark aimed\nat identifying manipulated segments within both video and audio, providing\ninsight into the origins of deepfakes. Furthermore, we propose novel evaluation\nmetrics, Temporal Accuracy (TA) and Frame-wise Discrimination Metric (FDM), to\nassess the robustness of deepfake detection models. Evaluating state-of-the-art\nmodels against diverse deepfake benchmarks, particularly FakeMix, demonstrates\nthe effectiveness of our approach comprehensively. Specifically, while\nachieving an Average Precision (AP) of 94.2% at the video-level, the evaluation\nof the existing models at the clip-level using the proposed metrics, TA and\nFDM, yielded sharp declines in accuracy to 53.1%, and 52.1%, respectively.\n","authors":["Juho Jung","Sangyoun Lee","Jooeon Kang","Yunjin Na"],"pdf_url":"https://arxiv.org/pdf/2408.02954v1.pdf","comment":"4 pages, 2 figures, 2 tables, Accepted as Oral Presentation at The\n  Trustworthy AI Workshop @ IJCAI 2024"},{"id":"http://arxiv.org/abs/2408.02336v2","updated":"2024-08-06T04:04:23Z","published":"2024-08-05T09:19:52Z","title":"Infusing Environmental Captions for Long-Form Video Language Grounding","summary":"  In this work, we tackle the problem of long-form video-language grounding\n(VLG). Given a long-form video and a natural language query, a model should\ntemporally localize the precise moment that answers the query. Humans can\neasily solve VLG tasks, even with arbitrarily long videos, by discarding\nirrelevant moments using extensive and robust knowledge gained from experience.\nUnlike humans, existing VLG methods are prone to fall into superficial cues\nlearned from small-scale datasets, even when they are within irrelevant frames.\nTo overcome this challenge, we propose EI-VLG, a VLG method that leverages\nricher textual information provided by a Multi-modal Large Language Model\n(MLLM) as a proxy for human experiences, helping to effectively exclude\nirrelevant frames. We validate the effectiveness of the proposed method via\nextensive experiments on a challenging EgoNLQ benchmark.\n","authors":["Hyogun Lee","Soyeon Hong","Mujeen Sung","Jinwoo Choi"],"pdf_url":"https://arxiv.org/pdf/2408.02336v2.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.16428v2","updated":"2024-08-06T03:44:00Z","published":"2024-03-25T05:12:21Z","title":"Benchmarks and Challenges in Pose Estimation for Egocentric Hand\n  Interactions with Objects","summary":"  We interact with the world with our hands and see it through our own\n(egocentric) perspective. A holistic 3Dunderstanding of such interactions from\negocentric views is important for tasks in robotics, AR/VR, action recognition\nand motion generation. Accurately reconstructing such interactions in 3D is\nchallenging due to heavy occlusion, viewpoint bias, camera distortion, and\nmotion blur from the head movement. To this end, we designed the HANDS23\nchallenge based on the AssemblyHands and ARCTIC datasets with carefully\ndesigned training and testing splits. Based on the results of the top submitted\nmethods and more recent baselines on the leaderboards, we perform a thorough\nanalysis on 3D hand(-object) reconstruction tasks. Our analysis demonstrates\nthe effectiveness of addressing distortion specific to egocentric cameras,\nadopting high-capacity transformers to learn complex hand-object interactions,\nand fusing predictions from different views. Our study further reveals\nchallenging scenarios intractable with state-of-the-art methods, such as fast\nhand motion, object reconstruction from narrow egocentric views, and close\ncontact between two hands and objects. Our efforts will enrich the community's\nknowledge foundation and facilitate future hand studies on egocentric\nhand-object interactions.\n","authors":["Zicong Fan","Takehiko Ohkawa","Linlin Yang","Nie Lin","Zhishan Zhou","Shihao Zhou","Jiajun Liang","Zhong Gao","Xuanyang Zhang","Xue Zhang","Fei Li","Zheng Liu","Feng Lu","Karim Abou Zeid","Bastian Leibe","Jeongwan On","Seungryul Baek","Aditya Prakash","Saurabh Gupta","Kun He","Yoichi Sato","Otmar Hilliges","Hyung Jin Chang","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2403.16428v2.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2404.18203v2","updated":"2024-08-06T03:37:31Z","published":"2024-04-28T14:47:09Z","title":"LMM-PCQA: Assisting Point Cloud Quality Assessment with LMM","summary":"  Although large multi-modality models (LMMs) have seen extensive exploration\nand application in various quality assessment studies, their integration into\nPoint Cloud Quality Assessment (PCQA) remains unexplored. Given LMMs'\nexceptional performance and robustness in low-level vision and quality\nassessment tasks, this study aims to investigate the feasibility of imparting\nPCQA knowledge to LMMs through text supervision. To achieve this, we transform\nquality labels into textual descriptions during the fine-tuning phase, enabling\nLMMs to derive quality rating logits from 2D projections of point clouds. To\ncompensate for the loss of perception in the 3D domain, structural features are\nextracted as well. These quality logits and structural features are then\ncombined and regressed into quality scores. Our experimental results affirm the\neffectiveness of our approach, showcasing a novel integration of LMMs into PCQA\nthat enhances model understanding and assessment accuracy. We hope our\ncontributions can inspire subsequent investigations into the fusion of LMMs\nwith PCQA, fostering advancements in 3D visual quality analysis and beyond. The\ncode is available at https://github.com/zzc-1998/LMM-PCQA.\n","authors":["Zicheng Zhang","Haoning Wu","Yingjie Zhou","Chunyi Li","Wei Sun","Chaofeng Chen","Xiongkuo Min","Xiaohong Liu","Weisi Lin","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2404.18203v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12794v2","updated":"2024-08-06T03:28:12Z","published":"2024-04-19T11:17:35Z","title":"MambaMOS: LiDAR-based 3D Moving Object Segmentation with Motion-aware\n  State Space Model","summary":"  LiDAR-based Moving Object Segmentation (MOS) aims to locate and segment\nmoving objects in point clouds of the current scan using motion information\nfrom previous scans. Despite the promising results achieved by previous MOS\nmethods, several key issues, such as the weak coupling of temporal and spatial\ninformation, still need further study. In this paper, we propose a novel\nLiDAR-based 3D Moving Object Segmentation with Motion-aware State Space Model,\ntermed MambaMOS. Firstly, we develop a novel embedding module, the Time Clue\nBootstrapping Embedding (TCBE), to enhance the coupling of temporal and spatial\ninformation in point clouds and alleviate the issue of overlooked temporal\nclues. Secondly, we introduce the Motion-aware State Space Model (MSSM) to\nendow the model with the capacity to understand the temporal correlations of\nthe same object across different time steps. Specifically, MSSM emphasizes the\nmotion states of the same object at different time steps through two distinct\ntemporal modeling and correlation steps. We utilize an improved state space\nmodel to represent these motion differences, significantly modeling the motion\nstates. Finally, extensive experiments on the SemanticKITTI-MOS and KITTI-Road\nbenchmarks demonstrate that the proposed MambaMOS achieves state-of-the-art\nperformance. The source code is publicly available at\nhttps://github.com/Terminal-K/MambaMOS.\n","authors":["Kang Zeng","Hao Shi","Jiacheng Lin","Siyu Li","Jintao Cheng","Kaiwei Wang","Zhiyong Li","Kailun Yang"],"pdf_url":"https://arxiv.org/pdf/2404.12794v2.pdf","comment":"Accepted to ACM MM 2024. The source code is publicly available at\n  https://github.com/Terminal-K/MambaMOS"},{"id":"http://arxiv.org/abs/2408.02929v1","updated":"2024-08-06T03:23:42Z","published":"2024-08-06T03:23:42Z","title":"Segmenting Small Stroke Lesions with Novel Labeling Strategies","summary":"  Deep neural networks have demonstrated exceptional efficacy in stroke lesion\nsegmentation. However, the delineation of small lesions, critical for stroke\ndiagnosis, remains a challenge. In this study, we propose two straightforward\nyet powerful approaches that can be seamlessly integrated into a variety of\nnetworks: Multi-Size Labeling (MSL) and Distance-Based Labeling (DBL), with the\naim of enhancing the segmentation accuracy of small lesions. MSL divides lesion\nmasks into various categories based on lesion volume while DBL emphasizes the\nlesion boundaries. Experimental evaluations on the Anatomical Tracings of\nLesions After Stroke (ATLAS) v2.0 dataset showcase that an ensemble of MSL and\nDBL achieves consistently better or equal performance on recall (3.6% and\n3.7%), F1 (2.4% and 1.5%), and Dice scores (1.3% and 0.0%) compared to the\ntop-1 winner of the 2022 MICCAI ATLAS Challenge on both the subset only\ncontaining small lesions and the entire dataset, respectively. Notably, on the\nmini-lesion subset, a single MSL model surpasses the previous best ensemble\nstrategy, with enhancements of 1.0% and 0.3% on F1 and Dice scores,\nrespectively. Our code is available at:\nhttps://github.com/nadluru/StrokeLesSeg.\n","authors":["Liang Shang","Zhengyang Lou","Andrew L. Alexander","Vivek Prabhakaran","William A. Sethares","Veena A. Nair","Nagesh Adluru"],"pdf_url":"https://arxiv.org/pdf/2408.02929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02924v1","updated":"2024-08-06T03:20:10Z","published":"2024-08-06T03:20:10Z","title":"Evaluation of Segment Anything Model 2: The Role of SAM2 in the\n  Underwater Environment","summary":"  With breakthroughs in large-scale modeling, the Segment Anything Model (SAM)\nand its extensions have been attempted for applications in various underwater\nvisualization tasks in marine sciences, and have had a significant impact on\nthe academic community. Recently, Meta has further developed the Segment\nAnything Model 2 (SAM2), which significantly improves running speed and\nsegmentation accuracy compared to its predecessor. This report aims to explore\nthe potential of SAM2 in marine science by evaluating it on the underwater\ninstance segmentation benchmark datasets UIIS and USIS10K. The experiments show\nthat the performance of SAM2 is extremely dependent on the type of\nuser-provided prompts. When using the ground truth bounding box as prompt, SAM2\nperformed excellently in the underwater instance segmentation domain. However,\nwhen running in automatic mode, SAM2's ability with point prompts to sense and\nsegment underwater instances is significantly degraded. It is hoped that this\npaper will inspire researchers to further explore the SAM model family in the\nunderwater domain. The results and evaluation codes in this paper are available\nat https://github.com/LiamLian0727/UnderwaterSAM2Eval.\n","authors":["Shijie Lian","Hua Li"],"pdf_url":"https://arxiv.org/pdf/2408.02924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02085v2","updated":"2024-08-06T03:19:25Z","published":"2024-08-04T16:50:07Z","title":"Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data\n  Assessment and Selection for Instruction Tuning of Language Models","summary":"  Instruction tuning plays a critical role in aligning large language models\n(LLMs) with human preference. Despite the vast amount of open instruction\ndatasets, naively training a LLM on all existing instructions may not be\noptimal and practical. To pinpoint the most beneficial datapoints, data\nassessment and selection methods have been proposed in the fields of natural\nlanguage processing (NLP) and deep learning. However, under the context of\ninstruction tuning, there still exists a gap in knowledge on what kind of data\nevaluation metrics can be employed and how they can be integrated into the\nselection mechanism. To bridge this gap, we present a comprehensive review on\nexisting literature of data assessment and selection especially for instruction\ntuning of LLMs. We systematically categorize all applicable methods into\nquality-based, diversity-based, and importance-based ones where a unified,\nfine-grained taxonomy is structured. For each category, representative methods\nare elaborated to describe the landscape of relevant research. In addition,\ncomparison between latest methods is conducted on their officially reported\nresults to provide in-depth discussions on their limitations. Finally, we\nsummarize the open challenges and propose the promosing avenues for future\nstudies. All related contents are available at\nhttps://github.com/yuleiqin/fantastic-data-engineering.\n","authors":["Yulei Qin","Yuncheng Yang","Pengcheng Guo","Gang Li","Hang Shao","Yuchen Shi","Zihan Xu","Yun Gu","Ke Li","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2408.02085v2.pdf","comment":"review, survey, 28 pages, 2 figures, 4 tables"},{"id":"http://arxiv.org/abs/2408.02922v1","updated":"2024-08-06T03:15:18Z","published":"2024-08-06T03:15:18Z","title":"Pose Magic: Efficient and Temporally Consistent Human Pose Estimation\n  with a Hybrid Mamba-GCN Network","summary":"  Current state-of-the-art (SOTA) methods in 3D Human Pose Estimation (HPE) are\nprimarily based on Transformers. However, existing Transformer-based 3D HPE\nbackbones often encounter a trade-off between accuracy and computational\nefficiency. To resolve the above dilemma, in this work, leveraging recent\nadvances in state space models, we utilize Mamba for high-quality and efficient\nlong-range modeling. Nonetheless, Mamba still faces challenges in precisely\nexploiting the local dependencies between joints. To address these issues, we\npropose a new attention-free hybrid spatiotemporal architecture named Hybrid\nMamba-GCN (Pose Magic). This architecture introduces local enhancement with GCN\nby capturing relationships between neighboring joints, thus producing new\nrepresentations to complement Mamba's outputs. By adaptively fusing\nrepresentations from Mamba and GCN, Pose Magic demonstrates superior capability\nin learning the underlying 3D structure. To meet the requirements of real-time\ninference, we also provide a fully causal version. Extensive experiments show\nthat Pose Magic achieves new SOTA results ($\\downarrow 0.9 mm$) while saving\n$74.1\\%$ FLOPs. In addition, Pose Magic exhibits optimal motion consistency and\nthe ability to generalize to unseen sequence lengths.\n","authors":["Xinyi Zhang","Qiqi Bao","Qinpeng Cui","Wenming Yang","Qingmin Liao"],"pdf_url":"https://arxiv.org/pdf/2408.02922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.01351v2","updated":"2024-08-06T02:58:13Z","published":"2023-09-04T04:29:01Z","title":"Adv3D: Generating 3D Adversarial Examples for 3D Object Detection in\n  Driving Scenarios with NeRF","summary":"  Deep neural networks (DNNs) have been proven extremely susceptible to\nadversarial examples, which raises special safety-critical concerns for\nDNN-based autonomous driving stacks (i.e., 3D object detection). Although there\nare extensive works on image-level attacks, most are restricted to 2D pixel\nspaces, and such attacks are not always physically realistic in our 3D world.\nHere we present Adv3D, the first exploration of modeling adversarial examples\nas Neural Radiance Fields (NeRFs). Advances in NeRF provide photorealistic\nappearances and 3D accurate generation, yielding a more realistic and\nrealizable adversarial example. We train our adversarial NeRF by minimizing the\nsurrounding objects' confidence predicted by 3D detectors on the training set.\nThen we evaluate Adv3D on the unseen validation set and show that it can cause\na large performance reduction when rendering NeRF in any sampled pose. To\ngenerate physically realizable adversarial examples, we propose primitive-aware\nsampling and semantic-guided regularization that enable 3D patch attacks with\ncamouflage adversarial texture. Experimental results demonstrate that the\ntrained adversarial NeRF generalizes well to different poses, scenes, and 3D\ndetectors. Finally, we provide a defense method to our attacks that involves\nadversarial training through data augmentation. Project page:\nhttps://len-li.github.io/adv3d-web\n","authors":["Leheng Li","Qing Lian","Ying-Cong Chen"],"pdf_url":"https://arxiv.org/pdf/2309.01351v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17379v2","updated":"2024-08-06T02:44:44Z","published":"2024-07-24T15:59:01Z","title":"MMRA: A Benchmark for Evaluating Multi-Granularity and Multi-Image\n  Relational Association Capabilities in Large Visual Language Models","summary":"  Given the remarkable success that large visual language models (LVLMs) have\nachieved in image perception tasks, the endeavor to make LVLMs perceive the\nworld like humans is drawing increasing attention. Current multi-modal\nbenchmarks primarily focus on facts or specific topic-related knowledge\ncontained within individual images. However, they often overlook the\nassociative relations between multiple images, which require the identification\nand analysis of similarities among entities or content present in different\nimages. Therefore, we propose the multi-image relation association task and a\nmeticulously curated Multi-granularity Multi-image Relational Association\n(MMRA) benchmark, comprising 1,024 samples. In order to systematically and\ncomprehensively evaluate current LVLMs, we establish an associational relation\nsystem among images that contain 11 subtasks (e.g, UsageSimilarity, SubEvent)\nat two granularity levels (i.e., image and entity) according to the relations\nin ConceptNet. Our experiments reveal that on the MMRA benchmark, current\nmulti-image LVLMs exhibit distinct advantages and disadvantages across various\nsubtasks. Notably, fine-grained, entity-level multi-image perception tasks pose\na greater challenge for LVLMs compared to image-level tasks. Moreover, LVLMs\nperform poorly on spatial-related tasks, indicating that LVLMs still have\nlimited spatial awareness. Additionally, our findings indicate that while LVLMs\ndemonstrate a strong capability to perceive image details, enhancing their\nability to associate information across multiple images hinges on improving the\nreasoning capabilities of their language model component. Moreover, we explored\nthe ability of LVLMs to perceive image sequences within the context of our\nmulti-image association task. Our experiments show that the majority of current\nLVLMs do not adequately model image sequences during the pre-training process.\n","authors":["Siwei Wu","Kang Zhu","Yu Bai","Yiming Liang","Yizhi Li","Haoning Wu","J. H. Liu","Ruibo Liu","Xingwei Qu","Xuxin Cheng","Ge Zhang","Wenhao Huang","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2407.17379v2.pdf","comment":"VLMs, Multi-Image Association"},{"id":"http://arxiv.org/abs/2408.01934v2","updated":"2024-08-06T02:39:46Z","published":"2024-08-04T05:22:08Z","title":"A Survey and Evaluation of Adversarial Attacks for Object Detection","summary":"  Deep learning models excel in various computer vision tasks but are\nsusceptible to adversarial examples-subtle perturbations in input data that\nlead to incorrect predictions. This vulnerability poses significant risks in\nsafety-critical applications such as autonomous vehicles, security\nsurveillance, and aircraft health monitoring. While numerous surveys focus on\nadversarial attacks in image classification, the literature on such attacks in\nobject detection is limited. This paper offers a comprehensive taxonomy of\nadversarial attacks specific to object detection, reviews existing adversarial\nrobustness evaluation metrics, and systematically assesses open-source attack\nmethods and model robustness. Key observations are provided to enhance the\nunderstanding of attack effectiveness and corresponding countermeasures.\nAdditionally, we identify crucial research challenges to guide future efforts\nin securing automated object detection systems.\n","authors":["Khoi Nguyen Tiet Nguyen","Wenyu Zhang","Kangkang Lu","Yuhuan Wu","Xingjian Zheng","Hui Li Tan","Liangli Zhen"],"pdf_url":"https://arxiv.org/pdf/2408.01934v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2303.00952v5","updated":"2024-08-06T02:39:05Z","published":"2023-03-02T04:12:53Z","title":"Towards Activated Muscle Group Estimation in the Wild","summary":"  In this paper, we tackle the new task of video-based Activated Muscle Group\nEstimation (AMGE) aiming at identifying active muscle regions during physical\nactivity in the wild. To this intent, we provide the MuscleMap dataset\nfeaturing >15K video clips with 135 different activities and 20 labeled muscle\ngroups. This dataset opens the vistas to multiple video-based applications in\nsports and rehabilitation medicine under flexible environment constraints. The\nproposed MuscleMap dataset is constructed with YouTube videos, specifically\ntargeting High-Intensity Interval Training (HIIT) physical exercise in the\nwild. To make the AMGE model applicable in real-life situations, it is crucial\nto ensure that the model can generalize well to numerous types of physical\nactivities not present during training and involving new combinations of\nactivated muscles. To achieve this, our benchmark also covers an evaluation\nsetting where the model is exposed to activity types excluded from the training\nset. Our experiments reveal that the generalizability of existing architectures\nadapted for the AMGE task remains a challenge. Therefore, we also propose a new\napproach, TransM3E, which employs a multi-modality feature fusion mechanism\nbetween both the video transformer model and the skeleton-based graph\nconvolution model with novel cross-modal knowledge distillation executed on\nmulti-classification tokens. The proposed method surpasses all popular video\nclassification models when dealing with both, previously seen and new types of\nphysical activities. The database and code can be found at\nhttps://github.com/KPeng9510/MuscleMap.\n","authors":["Kunyu Peng","David Schneider","Alina Roitberg","Kailun Yang","Jiaming Zhang","Chen Deng","Kaiyu Zhang","M. Saquib Sarfraz","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2303.00952v5.pdf","comment":"Accepted to ACM MM 2024. The database and code can be found at\n  https://github.com/KPeng9510/MuscleMap"},{"id":"http://arxiv.org/abs/2408.02906v1","updated":"2024-08-06T02:38:22Z","published":"2024-08-06T02:38:22Z","title":"Dual-View Pyramid Pooling in Deep Neural Networks for Improved Medical\n  Image Classification and Confidence Calibration","summary":"  Spatial pooling (SP) and cross-channel pooling (CCP) operators have been\napplied to aggregate spatial features and pixel-wise features from feature maps\nin deep neural networks (DNNs), respectively. Their main goal is to reduce\ncomputation and memory overhead without visibly weakening the performance of\nDNNs. However, SP often faces the problem of losing the subtle feature\nrepresentations, while CCP has a high possibility of ignoring salient feature\nrepresentations, which may lead to both miscalibration of confidence issues and\nsuboptimal medical classification results. To address these problems, we\npropose a novel dual-view framework, the first to systematically investigate\nthe relative roles of SP and CCP by analyzing the difference between spatial\nfeatures and pixel-wise features. Based on this framework, we propose a new\npooling method, termed dual-view pyramid pooling (DVPP), to aggregate\nmulti-scale dual-view features. DVPP aims to boost both medical image\nclassification and confidence calibration performance by fully leveraging the\nmerits of SP and CCP operators from a dual-axis perspective. Additionally, we\ndiscuss how to fulfill DVPP with five parameter-free implementations. Extensive\nexperiments on six 2D/3D medical image classification tasks show that our DVPP\nsurpasses state-of-the-art pooling methods in terms of medical image\nclassification results and confidence calibration across different DNNs.\n","authors":["Xiaoqing Zhang","Qiushi Nie","Zunjie Xiao","Jilu Zhao","Xiao Wu","Pengxin Guo","Runzhi Li","Jin Liu","Yanjie Wei","Yi Pan"],"pdf_url":"https://arxiv.org/pdf/2408.02906v1.pdf","comment":"27"},{"id":"http://arxiv.org/abs/2402.13699v4","updated":"2024-08-06T02:32:36Z","published":"2024-02-21T11:00:23Z","title":"Automation of Quantum Dot Measurement Analysis via Explainable Machine\n  Learning","summary":"  The rapid development of quantum dot (QD) devices for quantum computing has\nnecessitated more efficient and automated methods for device characterization\nand tuning. Many of the measurements acquired during the tuning process come in\nthe form of images that need to be properly analyzed to guide the subsequent\ntuning steps. By design, features present in such images capture certain\nbehaviors or states of the measured QD devices. When considered carefully, such\nfeatures can aid the control and calibration of QD devices. An important\nexample of such images are so-called \\textit{triangle plots}, which visually\nrepresent current flow and reveal characteristics important for QD device\ncalibration. While image-based classification tools, such as convolutional\nneural networks (CNNs), can be used to verify whether a given measurement is\n\\textit{good} and thus warrants the initiation of the next phase of tuning,\nthey do not provide any insights into how the device should be adjusted in the\ncase of \\textit{bad} images. This is because CNNs sacrifice prediction and\nmodel intelligibility for high accuracy. To ameliorate this trade-off, a recent\nstudy introduced an image vectorization approach that relies on the Gabor\nwavelet transform [1]. Here we propose an alternative vectorization method that\ninvolves mathematical modeling of synthetic triangles to mimic the experimental\ndata. Using explainable boosting machines, we show that this new method offers\nsuperior explainability of model prediction without sacrificing accuracy. This\nwork demonstrates the feasibility and advantages of applying explainable\nmachine learning techniques to the analysis of quantum dot measurements, paving\nthe way for further advances in automated and transparent QD device tuning.\n","authors":["Daniel Schug","Tyler J. Kovach","M. A. Wolfe","Jared Benson","Sanghyeok Park","J. P. Dodson","J. Corrigan","M. A. Eriksson","Justyna P. Zwolak"],"pdf_url":"https://arxiv.org/pdf/2402.13699v4.pdf","comment":"17 pages, 4 figures, abbreviated version published in Proceedings of\n  the XAI4Sci: Explainable machine learning for sciences workshop at AAAI 2024,\n  (Vancouver, Canada)"},{"id":"http://arxiv.org/abs/2408.02904v1","updated":"2024-08-06T02:27:54Z","published":"2024-08-06T02:27:54Z","title":"Enabling Intelligent Traffic Systems: A Deep Learning Method for\n  Accurate Arabic License Plate Recognition","summary":"  This paper introduces a novel two-stage framework for accurate Egyptian\nVehicle License Plate Recognition (EVLPR). The first stage employs image\nprocessing techniques to reliably localize license plates, while the second\nstage utilizes a custom-designed deep learning model for robust Arabic\ncharacter recognition. The proposed system achieves a remarkable 99.3% accuracy\non a diverse dataset, surpassing existing approaches. Its potential\napplications extend to intelligent traffic management, including traffic\nviolation detection and parking optimization. Future research will focus on\nenhancing the system's capabilities through architectural refinements, expanded\ndatasets, and addressing system dependencies.\n","authors":["M. A. Sayedelahl"],"pdf_url":"https://arxiv.org/pdf/2408.02904v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02901v1","updated":"2024-08-06T02:15:12Z","published":"2024-08-06T02:15:12Z","title":"Lighthouse: A User-Friendly Library for Reproducible Video Moment\n  Retrieval and Highlight Detection","summary":"  We propose Lighthouse, a user-friendly library for reproducible video moment\nretrieval and highlight detection (MR-HD). Although researchers proposed\nvarious MR-HD approaches, the research community holds two main issues. The\nfirst is a lack of comprehensive and reproducible experiments across various\nmethods, datasets, and video-text features. This is because no unified training\nand evaluation codebase covers multiple settings. The second is user-unfriendly\ndesign. Because previous works use different libraries, researchers set up\nindividual environments. In addition, most works release only the training\ncodes, requiring users to implement the whole inference process of MR-HD.\nLighthouse addresses these issues by implementing a unified reproducible\ncodebase that includes six models, three features, and five datasets. In\naddition, it provides an inference API and web demo to make these methods\neasily accessible for researchers and developers. Our experiments demonstrate\nthat Lighthouse generally reproduces the reported scores in the reference\npapers. The code is available at https://github.com/line/lighthouse.\n","authors":["Taichi Nishimura","Shota Nakada","Hokuto Munakata","Tatsuya Komatsu"],"pdf_url":"https://arxiv.org/pdf/2408.02901v1.pdf","comment":"6 pages; library tech report"},{"id":"http://arxiv.org/abs/2408.02900v1","updated":"2024-08-06T02:09:35Z","published":"2024-08-06T02:09:35Z","title":"MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular\n  Annotations for Medicine","summary":"  This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal\ndataset for medicine, covering over 25 million images across 10 modalities,\nwith multigranular annotations for more than 65 diseases. These enriched\nannotations encompass both global textual information, such as disease/lesion\ntype, modality, region-specific descriptions, and inter-regional relationships,\nas well as detailed local annotations for regions of interest (ROIs), including\nbounding boxes, segmentation masks. Unlike existing approach which is limited\nby the availability of image-text pairs, we have developed the first automated\npipeline that scales up multimodal data by generating multigranular visual and\ntexual annotations (in the form of image-ROI-description triplets) without the\nneed for any paired text descriptions. Specifically, data from over 90\ndifferent sources have been collected, preprocessed, and grounded using\ndomain-specific expert models to identify ROIs related to abnormal regions. We\nthen build a comprehensive knowledge base and prompt multimodal large language\nmodels to perform retrieval-augmented generation with the identified ROIs as\nguidance, resulting in multigranular texual descriptions. Compared to existing\ndatasets, MedTrinity-25M provides the most enriched annotations, supporting a\ncomprehensive range of multimodal tasks such as captioning and report\ngeneration, as well as vision-centric tasks like classification and\nsegmentation. Pretraining on MedTrinity-25M, our model achieves\nstate-of-the-art performance on VQA-RAD and PathVQA, surpassing both multimodal\nlarge language models and other representative SoTA approaches. This dataset\ncan also be utilized to support large-scale pre-training of multimodal medical\nAI models, contributing to the development of future foundation models in the\nmedical domain.\n","authors":["Yunfei Xie","Ce Zhou","Lang Gao","Juncheng Wu","Xianhang Li","Hong-Yu Zhou","Sheng Liu","Lei Xing","James Zou","Cihang Xie","Yuyin Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.02900v1.pdf","comment":"The project page is at https://yunfeixie233.github.io/MedTrinity-25M"},{"id":"http://arxiv.org/abs/2406.14643v2","updated":"2024-08-06T02:04:35Z","published":"2024-06-20T18:07:19Z","title":"Holistic Evaluation for Interleaved Text-and-Image Generation","summary":"  Interleaved text-and-image generation has been an intriguing research\ndirection, where the models are required to generate both images and text\npieces in an arbitrary order. Despite the emerging advancements in interleaved\ngeneration, the progress in its evaluation still significantly lags behind.\nExisting evaluation benchmarks do not support arbitrarily interleaved images\nand text for both inputs and outputs, and they only cover a limited number of\ndomains and use cases. Also, current works predominantly use similarity-based\nmetrics which fall short in assessing the quality in open-ended scenarios. To\nthis end, we introduce InterleavedBench, the first benchmark carefully curated\nfor the evaluation of interleaved text-and-image generation. InterleavedBench\nfeatures a rich array of tasks to cover diverse real-world use cases. In\naddition, we present InterleavedEval, a strong reference-free metric powered by\nGPT-4o to deliver accurate and explainable evaluation. We carefully define five\nessential evaluation aspects for InterleavedEval, including text quality,\nperceptual quality, image coherence, text-image coherence, and helpfulness, to\nensure a comprehensive and fine-grained assessment. Through extensive\nexperiments and rigorous human evaluation, we show that our benchmark and\nmetric can effectively evaluate the existing models with a strong correlation\nwith human judgments surpassing previous reference-based metrics. We also\nprovide substantial findings and insights to foster future research in\ninterleaved generation and its evaluation.\n","authors":["Minqian Liu","Zhiyang Xu","Zihao Lin","Trevor Ashby","Joy Rimchala","Jiaxin Zhang","Lifu Huang"],"pdf_url":"https://arxiv.org/pdf/2406.14643v2.pdf","comment":"13 pages, 6 figures, 6 tables. Website:\n  https://vt-nlp.github.io/InterleavedEval/. Dataset:\n  https://huggingface.co/mqliu/InterleavedBench"},{"id":"http://arxiv.org/abs/2408.02891v1","updated":"2024-08-06T01:41:40Z","published":"2024-08-06T01:41:40Z","title":"Diverse Generation while Maintaining Semantic Coordination: A\n  Diffusion-Based Data Augmentation Method for Object Detection","summary":"  Recent studies emphasize the crucial role of data augmentation in enhancing\nthe performance of object detection models. However,existing methodologies\noften struggle to effectively harmonize dataset diversity with semantic\ncoordination.To bridge this gap, we introduce an innovative augmentation\ntechnique leveraging pre-trained conditional diffusion models to mediate this\nbalance. Our approach encompasses the development of a Category Affinity\nMatrix, meticulously designed to enhance dataset diversity, and a Surrounding\nRegion Alignment strategy, which ensures the preservation of semantic\ncoordination in the augmented images. Extensive experimental evaluations\nconfirm the efficacy of our method in enriching dataset diversity while\nseamlessly maintaining semantic coordination. Our method yields substantial\naverage improvements of +1.4AP, +0.9AP, and +3.4AP over existing alternatives\non three distinct object detection models, respectively.\n","authors":["Sen Nie","Zhuo Wang","Xinxin Wang","Kun He"],"pdf_url":"https://arxiv.org/pdf/2408.02891v1.pdf","comment":"15 pages, 7 figures, ICPR2024"},{"id":"http://arxiv.org/abs/2408.02888v1","updated":"2024-08-06T01:34:43Z","published":"2024-08-06T01:34:43Z","title":"VizECGNet: Visual ECG Image Network for Cardiovascular Diseases\n  Classification with Multi-Modal Training and Knowledge Distillation","summary":"  An electrocardiogram (ECG) captures the heart's electrical signal to assess\nvarious heart conditions. In practice, ECG data is stored as either digitized\nsignals or printed images. Despite the emergence of numerous deep learning\nmodels for digitized signals, many hospitals prefer image storage due to cost\nconsiderations. Recognizing the unavailability of raw ECG signals in many\nclinical settings, we propose VizECGNet, which uses only printed ECG graphics\nto determine the prognosis of multiple cardiovascular diseases. During\ntraining, cross-modal attention modules (CMAM) are used to integrate\ninformation from two modalities - image and signal, while self-modality\nattention modules (SMAM) capture inherent long-range dependencies in ECG data\nof each modality. Additionally, we utilize knowledge distillation to improve\nthe similarity between two distinct predictions from each modality stream. This\ninnovative multi-modal deep learning architecture enables the utilization of\nonly ECG images during inference. VizECGNet with image input achieves higher\nperformance in precision, recall, and F1-Score compared to signal-based ECG\nclassification models, with improvements of 3.50%, 8.21%, and 7.38%,\nrespectively.\n","authors":["Ju-Hyeon Nam","Seo-Hyung Park","Su Jung Kim","Sang-Chul Lee"],"pdf_url":"https://arxiv.org/pdf/2408.02888v1.pdf","comment":"Accepted in International Conference on Image Processing (ICIP) 2024"}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2404.07950v3","updated":"2024-08-06T02:42:32Z","published":"2024-03-18T16:50:23Z","title":"Reinforcement Learning with Generalizable Gaussian Splatting","summary":"  An excellent representation is crucial for reinforcement learning (RL)\nperformance, especially in vision-based reinforcement learning tasks. The\nquality of the environment representation directly influences the achievement\nof the learning task. Previous vision-based RL typically uses explicit or\nimplicit ways to represent environments, such as images, points, voxels, and\nneural radiance fields. However, these representations contain several\ndrawbacks. They cannot either describe complex local geometries or generalize\nwell to unseen scenes, or require precise foreground masks. Moreover, these\nimplicit neural representations are akin to a ``black box\", significantly\nhindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit\nscene representation and differentiable rendering nature, is considered a\nrevolutionary change for reconstruction and representation methods. In this\npaper, we propose a novel Generalizable Gaussian Splatting framework to be the\nrepresentation of RL tasks, called GSRL. Through validation in the RoboMimic\nenvironment, our method achieves better results than other baselines in\nmultiple tasks, improving the performance by 10%, 44%, and 15% compared with\nbaselines on the hardest task. This work is the first attempt to leverage\ngeneralizable 3DGS as a representation for RL.\n","authors":["Jiaxu Wang","Qiang Zhang","Jingkai Sun","Jiahang Cao","Gang Han","Wen Zhao","Weining Zhang","Yecheng Shao","Yijie Guo","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2404.07950v3.pdf","comment":"7 pages,2 figures"},{"id":"http://arxiv.org/abs/2408.02085v2","updated":"2024-08-06T03:19:25Z","published":"2024-08-04T16:50:07Z","title":"Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data\n  Assessment and Selection for Instruction Tuning of Language Models","summary":"  Instruction tuning plays a critical role in aligning large language models\n(LLMs) with human preference. Despite the vast amount of open instruction\ndatasets, naively training a LLM on all existing instructions may not be\noptimal and practical. To pinpoint the most beneficial datapoints, data\nassessment and selection methods have been proposed in the fields of natural\nlanguage processing (NLP) and deep learning. However, under the context of\ninstruction tuning, there still exists a gap in knowledge on what kind of data\nevaluation metrics can be employed and how they can be integrated into the\nselection mechanism. To bridge this gap, we present a comprehensive review on\nexisting literature of data assessment and selection especially for instruction\ntuning of LLMs. We systematically categorize all applicable methods into\nquality-based, diversity-based, and importance-based ones where a unified,\nfine-grained taxonomy is structured. For each category, representative methods\nare elaborated to describe the landscape of relevant research. In addition,\ncomparison between latest methods is conducted on their officially reported\nresults to provide in-depth discussions on their limitations. Finally, we\nsummarize the open challenges and propose the promosing avenues for future\nstudies. All related contents are available at\nhttps://github.com/yuleiqin/fantastic-data-engineering.\n","authors":["Yulei Qin","Yuncheng Yang","Pengcheng Guo","Gang Li","Hang Shao","Yuchen Shi","Zihan Xu","Yun Gu","Ke Li","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2408.02085v2.pdf","comment":"review, survey, 28 pages, 2 figures, 4 tables"},{"id":"http://arxiv.org/abs/2408.03326v1","updated":"2024-08-06T17:59:44Z","published":"2024-08-06T17:59:44Z","title":"LLaVA-OneVision: Easy Visual Task Transfer","summary":"  We present LLaVA-OneVision, a family of open large multimodal models (LMMs)\ndeveloped by consolidating our insights into data, models, and visual\nrepresentations in the LLaVA-NeXT blog series. Our experimental results\ndemonstrate that LLaVA-OneVision is the first single model that can\nsimultaneously push the performance boundaries of open LMMs in three important\ncomputer vision scenarios: single-image, multi-image, and video scenarios.\nImportantly, the design of LLaVA-OneVision allows strong transfer learning\nacross different modalities/scenarios, yielding new emerging capabilities. In\nparticular, strong video understanding and cross-scenario capabilities are\ndemonstrated through task transfer from images to videos.\n","authors":["Bo Li","Yuanhan Zhang","Dong Guo","Renrui Zhang","Feng Li","Hao Zhang","Kaichen Zhang","Yanwei Li","Ziwei Liu","Chunyuan Li"],"pdf_url":"https://arxiv.org/pdf/2408.03326v1.pdf","comment":"Project Homepage:\n  https://llava-vl.github.io/blog/2024-08-05-llava-onevision/"},{"id":"http://arxiv.org/abs/2407.21770v2","updated":"2024-08-06T17:57:41Z","published":"2024-07-31T17:46:51Z","title":"MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware\n  Experts","summary":"  We introduce MoMa, a novel modality-aware mixture-of-experts (MoE)\narchitecture designed for pre-training mixed-modal, early-fusion language\nmodels. MoMa processes images and text in arbitrary sequences by dividing\nexpert modules into modality-specific groups. These groups exclusively process\ndesignated tokens while employing learned routing within each group to maintain\nsemantically informed adaptivity. Our empirical results reveal substantial\npre-training efficiency gains through this modality-specific parameter\nallocation. Under a 1-trillion-token training budget, the MoMa 1.4B model,\nfeaturing 4 text experts and 4 image experts, achieves impressive FLOPs\nsavings: 3.7x overall, with 2.6x for text and 5.2x for image processing\ncompared to a compute-equivalent dense baseline, measured by pre-training loss.\nThis outperforms the standard expert-choice MoE with 8 mixed-modal experts,\nwhich achieves 3x overall FLOPs savings (3x for text, 2.8x for image).\nCombining MoMa with mixture-of-depths (MoD) further improves pre-training FLOPs\nsavings to 4.2x overall (text: 3.4x, image: 5.3x), although this combination\nhurts performance in causal inference due to increased sensitivity to router\naccuracy. These results demonstrate MoMa's potential to significantly advance\nthe efficiency of mixed-modal, early-fusion language model pre-training, paving\nthe way for more resource-efficient and capable multimodal AI systems.\n","authors":["Xi Victoria Lin","Akshat Shrivastava","Liang Luo","Srinivasan Iyer","Mike Lewis","Gargi Gosh","Luke Zettlemoyer","Armen Aghajanyan"],"pdf_url":"https://arxiv.org/pdf/2407.21770v2.pdf","comment":"v2 -> update related work section"},{"id":"http://arxiv.org/abs/2408.03319v1","updated":"2024-08-06T17:51:42Z","published":"2024-08-06T17:51:42Z","title":"Training LLMs to Recognize Hedges in Spontaneous Narratives","summary":"  Hedges allow speakers to mark utterances as provisional, whether to signal\nnon-prototypicality or \"fuzziness\", to indicate a lack of commitment to an\nutterance, to attribute responsibility for a statement to someone else, to\ninvite input from a partner, or to soften critical feedback in the service of\nface-management needs. Here we focus on hedges in an experimentally\nparameterized corpus of 63 Roadrunner cartoon narratives spontaneously produced\nfrom memory by 21 speakers for co-present addressees, transcribed to text\n(Galati and Brennan, 2010). We created a gold standard of hedges annotated by\nhuman coders (the Roadrunner-Hedge corpus) and compared three LLM-based\napproaches for hedge detection: fine-tuning BERT, and zero and few-shot\nprompting with GPT-4o and LLaMA-3. The best-performing approach was a\nfine-tuned BERT model, followed by few-shot GPT-4o. After an error analysis on\nthe top performing approaches, we used an LLM-in-the-Loop approach to improve\nthe gold standard coding, as well as to highlight cases in which hedges are\nambiguous in linguistically interesting ways that will guide future research.\nThis is the first step in our research program to train LLMs to interpret and\ngenerate collateral signals appropriately and meaningfully in conversation.\n","authors":["Amie J. Paige","Adil Soubki","John Murzaku","Owen Rambow","Susan E. Brennan"],"pdf_url":"https://arxiv.org/pdf/2408.03319v1.pdf","comment":"Amie Paige, Adil Soubki, and John Murzaku contributed equally to this\n  study"},{"id":"http://arxiv.org/abs/2408.00756v2","updated":"2024-08-06T17:40:07Z","published":"2024-08-01T17:57:25Z","title":"Segment anything model 2: an application to 2D and 3D medical images","summary":"  Segment Anything Model (SAM) has gained significant attention because of its\nability to segment varous objects in images given a prompt. The recently\ndeveloped SAM 2 has extended this ability to video inputs. This opens an\nopportunity to apply SAM to 3D images, one of the fundamental tasks in the\nmedical imaging field. In this paper, we extensively evaluate SAM 2's ability\nto segment both 2D and 3D medical images by first collecting 18 medical imaging\ndatasets, including common 3D modalities such as computed tomography (CT),\nmagnetic resonance imaging (MRI), and positron emission tomography (PET) as\nwell as 2D modalities such as X-ray and ultrasound. Two evaluation pipelines of\nSAM 2 are considered: (1) multi-frame 3D segmentation, where prompts are\nprovided to one or multiple slice(s) selected from the volume, and (2)\nsingle-frame 2D segmentation, where prompts are provided to each slice. The\nformer is only applicable to 3D modalities, while the latter applies to both 2D\nand 3D modalities. Our results show that SAM 2 exhibits similar performance as\nSAM under single-frame 2D segmentation, and has variable performance under\nmulti-frame 3D segmentation depending on the choices of slices to annotate, the\ndirection of the propagation, the predictions utilized during the propagation,\netc.\n","authors":["Haoyu Dong","Hanxue Gu","Yaqian Chen","Jichen Yang","Maciej A. Mazurowski"],"pdf_url":"https://arxiv.org/pdf/2408.00756v2.pdf","comment":"12 pages, 9 figures. An updated version with new results and\n  corrections"},{"id":"http://arxiv.org/abs/2408.03304v1","updated":"2024-08-06T17:11:40Z","published":"2024-08-06T17:11:40Z","title":"Fusing Forces: Deep-Human-Guided Refinement of Segmentation Masks","summary":"  Etruscan mirrors constitute a significant category in Etruscan art,\ncharacterized by elaborate figurative illustrations featured on their backside.\nA laborious and costly aspect of their analysis and documentation is the task\nof manually tracing these illustrations. In previous work, a methodology has\nbeen proposed to automate this process, involving photometric-stereo scanning\nin combination with deep neural networks. While achieving quantitative\nperformance akin to an expert annotator, some results still lack qualitative\nprecision and, thus, require annotators for inspection and potential\ncorrection, maintaining resource intensity. In response, we propose a deep\nneural network trained to interactively refine existing annotations based on\nhuman guidance. Our human-in-the-loop approach streamlines annotation,\nachieving equal quality with up to 75% less manual input required. Moreover,\nduring the refinement process, the relative improvement of our methodology over\npure manual labeling reaches peak values of up to 26%, attaining drastically\nbetter quality quicker. By being tailored to the complex task of segmenting\nintricate lines, specifically distinguishing it from previous methods, our\napproach offers drastic improvements in efficacy, transferable to a broad\nspectrum of applications beyond Etruscan mirrors.\n","authors":["Rafael Sterzinger","Christian Stippel","Robert Sablatnig"],"pdf_url":"https://arxiv.org/pdf/2408.03304v1.pdf","comment":"16 pages, accepted at ICPR2024"},{"id":"http://arxiv.org/abs/2408.03303v1","updated":"2024-08-06T17:09:56Z","published":"2024-08-06T17:09:56Z","title":"Understanding How Blind Users Handle Object Recognition Errors:\n  Strategies and Challenges","summary":"  Object recognition technologies hold the potential to support blind and\nlow-vision people in navigating the world around them. However, the gap between\nbenchmark performances and practical usability remains a significant challenge.\nThis paper presents a study aimed at understanding blind users' interaction\nwith object recognition systems for identifying and avoiding errors. Leveraging\na pre-existing object recognition system, URCam, fine-tuned for our experiment,\nwe conducted a user study involving 12 blind and low-vision participants.\nThrough in-depth interviews and hands-on error identification tasks, we gained\ninsights into users' experiences, challenges, and strategies for identifying\nerrors in camera-based assistive technologies and object recognition systems.\nDuring interviews, many participants preferred independent error review, while\nexpressing apprehension toward misrecognitions. In the error identification\ntask, participants varied viewpoints, backgrounds, and object sizes in their\nimages to avoid and overcome errors. Even after repeating the task,\nparticipants identified only half of the errors, and the proportion of errors\nidentified did not significantly differ from their first attempts. Based on\nthese insights, we offer implications for designing accessible interfaces\ntailored to the needs of blind and low-vision users in identifying object\nrecognition errors.\n","authors":["Jonggi Hong","Hernisa Kacorri"],"pdf_url":"https://arxiv.org/pdf/2408.03303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03297v1","updated":"2024-08-06T16:55:54Z","published":"2024-08-06T16:55:54Z","title":"KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge\n  Selection in Retrieval-Augmented Language Models","summary":"  By integrating external knowledge, Retrieval-Augmented Generation (RAG) has\nbecome an effective strategy for mitigating the hallucination problems that\nlarge language models (LLMs) encounter when dealing with knowledge-intensive\ntasks. However, in the process of integrating external non-parametric\nsupporting evidence with internal parametric knowledge, inevitable knowledge\nconflicts may arise, leading to confusion in the model's responses. To enhance\nthe knowledge selection of LLMs in various contexts, some research has focused\non refining their behavior patterns through instruction-tuning. Nonetheless,\ndue to the absence of explicit negative signals and comparative objectives,\nmodels fine-tuned in this manner may still exhibit undesirable behaviors in the\nintricate and realistic retrieval scenarios. To this end, we propose a\nKnowledge-aware Preference Optimization, dubbed KaPO, aimed at achieving\ncontrollable knowledge selection in real retrieval scenarios. Concretely, we\nexplore and simulate error types across diverse context combinations and learn\nhow to avoid these negative signals through preference optimization methods.\nSimultaneously, by adjusting the balance between response length and the\nproportion of preference data representing different behavior patterns, we\nenhance the adherence capabilities and noise robustness of LLMs in a balanced\nmanner. Experimental results show that KaPO outperforms previous methods for\nhandling knowledge conflicts by over 37%, while also exhibiting robust\ngeneralization across various out-of-distribution datasets.\n","authors":["Ruizhe Zhang","Yongxin Xu","Yuzhen Xiao","Runchuan Zhu","Xinke Jiang","Xu Chu","Junfeng Zhao","Yasha Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03292v1","updated":"2024-08-06T16:41:33Z","published":"2024-08-06T16:41:33Z","title":"Static IR Drop Prediction with Attention U-Net and Saliency-Based\n  Explainability","summary":"  There has been significant recent progress to reduce the computational effort\nof static IR drop analysis using neural networks, and modeling as an\nimage-to-image translation task. A crucial issue is the lack of sufficient data\nfrom real industry designs to train these networks. Additionally, there is no\nmethodology to explain a high-drop pixel in a predicted IR drop image to its\nspecific root-causes. In this work, we first propose a U-Net neural network\nmodel with attention gates which is specifically tailored to achieve fast and\naccurate image-based static IR drop prediction. Attention gates allow selective\nemphasis on relevant parts of the input data without supervision which is\ndesired because of the often sparse nature of the IR drop map. We propose a\ntwo-phase training process which utilizes a mix of artificially-generated data\nand a limited number of points from real designs. The results are, on-average,\n18% (53%) better in MAE and 14% (113%) in F1 score compared to the winner of\nthe ICCAD 2023 contest (and U-Net only) when tested on real designs. Second, we\npropose a fast method using saliency maps which can explain a predicted IR drop\nin terms of specific input pixels contributing the most to a drop. In our\nexperiments, we show the number of high IR drop pixels can be reduced\non-average by 18% by mimicking upsize of a tiny portion of PDN's resistive\nedges.\n","authors":["Lizi Zhang","Azadeh Davoodi"],"pdf_url":"https://arxiv.org/pdf/2408.03292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04485v3","updated":"2024-08-06T16:35:50Z","published":"2024-06-06T20:15:42Z","title":"GenAI Arena: An Open Evaluation Platform for Generative Models","summary":"  Generative AI has made remarkable strides to revolutionize fields such as\nimage and video generation. These advancements are driven by innovative\nalgorithms, architecture, and data. However, the rapid proliferation of\ngenerative models has highlighted a critical gap: the absence of trustworthy\nevaluation metrics. Current automatic assessments such as FID, CLIP, FVD, etc\noften fail to capture the nuanced quality and user satisfaction associated with\ngenerative outputs. This paper proposes an open platform GenAI-Arena to\nevaluate different image and video generative models, where users can actively\nparticipate in evaluating these models. By leveraging collective user feedback\nand votes, GenAI-Arena aims to provide a more democratic and accurate measure\nof model performance. It covers three arenas for text-to-image generation,\ntext-to-video generation, and image editing respectively. Currently, we cover a\ntotal of 27 open-source generative models. GenAI-Arena has been operating for\nfour months, amassing over 6000 votes from the community. We describe our\nplatform, analyze the data, and explain the statistical methods for ranking the\nmodels. To further promote the research in building model-based evaluation\nmetrics, we release a cleaned version of our preference data for the three\ntasks, namely GenAI-Bench. We prompt the existing multi-modal models like\nGemini, GPT-4o to mimic human voting. We compute the correlation between model\nvoting with human voting to understand their judging abilities. Our results\nshow existing multimodal models are still lagging in assessing the generated\nvisual content, even the best model GPT-4o only achieves a Pearson correlation\nof 0.22 in the quality subscore, and behaves like random guessing in others.\n","authors":["Dongfu Jiang","Max Ku","Tianle Li","Yuansheng Ni","Shizhuo Sun","Rongqi Fan","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2406.04485v3.pdf","comment":"9 pages,7 figures"},{"id":"http://arxiv.org/abs/2408.03281v1","updated":"2024-08-06T16:28:30Z","published":"2024-08-06T16:28:30Z","title":"StructEval: Deepen and Broaden Large Language Model Assessment via\n  Structured Evaluation","summary":"  Evaluation is the baton for the development of large language models. Current\nevaluations typically employ a single-item assessment paradigm for each atomic\ntest objective, which struggles to discern whether a model genuinely possesses\nthe required capabilities or merely memorizes/guesses the answers to specific\nquestions. To this end, we propose a novel evaluation framework referred to as\nStructEval. Starting from an atomic test objective, StructEval deepens and\nbroadens the evaluation by conducting a structured assessment across multiple\ncognitive levels and critical concepts, and therefore offers a comprehensive,\nrobust and consistent evaluation for LLMs. Experiments on three widely-used\nbenchmarks demonstrate that StructEval serves as a reliable tool for resisting\nthe risk of data contamination and reducing the interference of potential\nbiases, thereby providing more reliable and consistent conclusions regarding\nmodel capabilities. Our framework also sheds light on the design of future\nprincipled and trustworthy LLM evaluation protocols.\n","authors":["Boxi Cao","Mengjie Ren","Hongyu Lin","Xianpei Han","Feng Zhang","Junfeng Zhan","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2408.03281v1.pdf","comment":"ACL 2024;Benchmark at https://github.com/c-box/StructEval;Leaderboard\n  at https://huggingface.co/spaces/Bowieee/StructEval_leaderboard"},{"id":"http://arxiv.org/abs/2408.03274v1","updated":"2024-08-06T16:17:51Z","published":"2024-08-06T16:17:51Z","title":"Compress and Compare: Interactively Evaluating Efficiency and Behavior\n  Across ML Model Compression Experiments","summary":"  To deploy machine learning models on-device, practitioners use compression\nalgorithms to shrink and speed up models while maintaining their high-quality\noutput. A critical aspect of compression in practice is model comparison,\nincluding tracking many compression experiments, identifying subtle changes in\nmodel behavior, and negotiating complex accuracy-efficiency trade-offs.\nHowever, existing compression tools poorly support comparison, leading to\ntedious and, sometimes, incomplete analyses spread across disjoint tools. To\nsupport real-world comparative workflows, we develop an interactive visual\nsystem called Compress and Compare. Within a single interface, Compress and\nCompare surfaces promising compression strategies by visualizing provenance\nrelationships between compressed models and reveals compression-induced\nbehavior changes by comparing models' predictions, weights, and activations. We\ndemonstrate how Compress and Compare supports common compression analysis tasks\nthrough two case studies, debugging failed compression on generative language\nmodels and identifying compression artifacts in image classification models. We\nfurther evaluate Compress and Compare in a user study with eight compression\nexperts, illustrating its potential to provide structure to compression\nworkflows, help practitioners build intuition about compression, and encourage\nthorough analysis of compression's effect on model behavior. Through these\nevaluations, we identify compression-specific challenges that future visual\nanalytics tools should consider and Compress and Compare visualizations that\nmay generalize to broader model comparison tasks.\n","authors":["Angie Boggust","Venkatesh Sivaraman","Yannick Assogba","Donghao Ren","Dominik Moritz","Fred Hohman"],"pdf_url":"https://arxiv.org/pdf/2408.03274v1.pdf","comment":"Accepted to VIS 2024"},{"id":"http://arxiv.org/abs/2403.15313v2","updated":"2024-08-06T15:58:35Z","published":"2024-03-22T16:06:05Z","title":"CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking","summary":"  To enable self-driving vehicles accurate detection and tracking of\nsurrounding objects is essential. While Light Detection and Ranging (LiDAR)\nsensors have set the benchmark for high-performance systems, the appeal of\ncamera-only solutions lies in their cost-effectiveness. Notably, despite the\nprevalent use of Radio Detection and Ranging (RADAR) sensors in automotive\nsystems, their potential in 3D detection and tracking has been largely\ndisregarded due to data sparsity and measurement noise. As a recent\ndevelopment, the combination of RADARs and cameras is emerging as a promising\nsolution. This paper presents Camera-RADAR 3D Detection and Tracking (CR3DT), a\ncamera-RADAR fusion model for 3D object detection, and Multi-Object Tracking\n(MOT). Building upon the foundations of the State-of-the-Art (SotA) camera-only\nBEVDet architecture, CR3DT demonstrates substantial improvements in both\ndetection and tracking capabilities, by incorporating the spatial and velocity\ninformation of the RADAR sensor. Experimental results demonstrate an absolute\nimprovement in detection performance of 5.3% in mean Average Precision (mAP)\nand a 14.9% increase in Average Multi-Object Tracking Accuracy (AMOTA) on the\nnuScenes dataset when leveraging both modalities. CR3DT bridges the gap between\nhigh-performance and cost-effective perception systems in autonomous driving,\nby capitalizing on the ubiquitous presence of RADAR in automotive applications.\nThe code is available at: https://github.com/ETH-PBL/CR3DT.\n","authors":["Nicolas Baumann","Michael Baumgartner","Edoardo Ghignone","Jonas Kühne","Tobias Fischer","Yung-Hsu Yang","Marc Pollefeys","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2403.15313v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08565v2","updated":"2024-08-06T15:40:04Z","published":"2024-02-13T16:05:51Z","title":"Artificial Intelligence for Literature Reviews: Opportunities and\n  Challenges","summary":"  This manuscript presents a comprehensive review of the use of Artificial\nIntelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous\nand organised methodology that assesses and integrates previous research on a\ngiven topic. Numerous tools have been developed to assist and partially\nautomate the SLR process. The increasing role of AI in this field shows great\npotential in providing more effective support for researchers, moving towards\nthe semi-automatic creation of literature reviews. Our study focuses on how AI\ntechniques are applied in the semi-automation of SLRs, specifically in the\nscreening and extraction phases. We examine 21 leading SLR tools using a\nframework that combines 23 traditional features with 11 AI features. We also\nanalyse 11 recent tools that leverage large language models for searching the\nliterature and assisting academic writing. Finally, the paper discusses current\ntrends in the field, outlines key research challenges, and suggests directions\nfor future research.\n","authors":["Francisco Bolanos","Angelo Salatino","Francesco Osborne","Enrico Motta"],"pdf_url":"https://arxiv.org/pdf/2402.08565v2.pdf","comment":"Updated with the reviewers comments. This version is now accepted at\n  the Artificial Intelligence Review journal"},{"id":"http://arxiv.org/abs/2402.15745v2","updated":"2024-08-06T15:28:30Z","published":"2024-02-24T06:57:15Z","title":"GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models\n  Evaluation","summary":"  The Large Vision-Language Models (LVLMs) have demonstrated great abilities in\nimage perception and language understanding. However, existing multimodal\nbenchmarks focus on primary perception abilities and commonsense knowledge\nwhich are insufficient to reflect the comprehensive capabilities of LVLMs. We\npropose GAOKAO-MM, a multimodal benchmark based on the Chinese College Entrance\nExamination (GAOKAO), comprising of 8 subjects and 12 types of images, such as\ndiagrams, function graphs, maps and photos. GAOKAO-MM derives from native\nChinese context and sets human-level requirements for the model's abilities,\nincluding perception, understanding, knowledge and reasoning. We evaluate 10\nLVLMs and find that the accuracies of all of them are lower than 50%, with\nGPT-4-Vison (48.1%), Qwen-VL-Plus (41.2%) and Gemini-Pro-Vision (35.1%) ranking\nin the top three positions. The results of our multi-dimension analysis\nindicate that LVLMs have moderate distance towards Artificial General\nIntelligence (AGI) and provide insights facilitating the development of\nmultilingual LVLMs.\n","authors":["Yi Zong","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2402.15745v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.08354v3","updated":"2024-08-06T15:14:36Z","published":"2023-04-17T15:16:10Z","title":"Tool Learning with Foundation Models","summary":"  Humans possess an extraordinary ability to create and utilize tools, allowing\nthem to overcome physical limitations and explore new frontiers. With the\nadvent of foundation models, AI systems have the potential to be equally adept\nin tool use as humans. This paradigm, i.e., tool learning with foundation\nmodels, combines the strengths of specialized tools and foundation models to\nachieve enhanced accuracy, efficiency, and automation in problem-solving.\nDespite its immense potential, there is still a lack of a comprehensive\nunderstanding of key challenges, opportunities, and future endeavors in this\nfield. To this end, we present a systematic investigation of tool learning in\nthis paper. We first introduce the background of tool learning, including its\ncognitive origins, the paradigm shift of foundation models, and the\ncomplementary roles of tools and models. Then we recapitulate existing tool\nlearning research into tool-augmented and tool-oriented learning. We formulate\na general tool learning framework: starting from understanding the user\ninstruction, models should learn to decompose a complex task into several\nsubtasks, dynamically adjust their plan through reasoning, and effectively\nconquer each sub-task by selecting appropriate tools. We also discuss how to\ntrain models for improved tool-use capabilities and facilitate the\ngeneralization in tool learning. Considering the lack of a systematic tool\nlearning evaluation in prior works, we experiment with 18 representative tools\nand show the potential of current foundation models in skillfully utilizing\ntools. Finally, we discuss several open problems that require further\ninvestigation for tool learning. In general, we hope this paper could inspire\nfuture research in integrating tools with foundation models.\n","authors":["Yujia Qin","Shengding Hu","Yankai Lin","Weize Chen","Ning Ding","Ganqu Cui","Zheni Zeng","Yufei Huang","Chaojun Xiao","Chi Han","Yi Ren Fung","Yusheng Su","Huadong Wang","Cheng Qian","Runchu Tian","Kunlun Zhu","Shihao Liang","Xingyu Shen","Bokai Xu","Zhen Zhang","Yining Ye","Bowen Li","Ziwei Tang","Jing Yi","Yuzhang Zhu","Zhenning Dai","Lan Yan","Xin Cong","Yaxi Lu","Weilin Zhao","Yuxiang Huang","Junxi Yan","Xu Han","Xian Sun","Dahai Li","Jason Phang","Cheng Yang","Tongshuang Wu","Heng Ji","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2304.08354v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03291v2","updated":"2024-08-06T15:14:01Z","published":"2024-07-03T17:24:36Z","title":"VCHAR:Variance-Driven Complex Human Activity Recognition framework with\n  Generative Representation","summary":"  Complex human activity recognition (CHAR) remains a pivotal challenge within\nubiquitous computing, especially in the context of smart environments. Existing\nstudies typically require meticulous labeling of both atomic and complex\nactivities, a task that is labor-intensive and prone to errors due to the\nscarcity and inaccuracies of available datasets. Most prior research has\nfocused on datasets that either precisely label atomic activities or, at\nminimum, their sequence approaches that are often impractical in real world\nsettings.In response, we introduce VCHAR (Variance-Driven Complex Human\nActivity Recognition), a novel framework that treats the outputs of atomic\nactivities as a distribution over specified intervals. Leveraging generative\nmethodologies, VCHAR elucidates the reasoning behind complex activity\nclassifications through video-based explanations, accessible to users without\nprior machine learning expertise. Our evaluation across three publicly\navailable datasets demonstrates that VCHAR enhances the accuracy of complex\nactivity recognition without necessitating precise temporal or sequential\nlabeling of atomic activities. Furthermore, user studies confirm that VCHAR's\nexplanations are more intelligible compared to existing methods, facilitating a\nbroader understanding of complex activity recognition among non-experts.\n","authors":["Yuan Sun","Navid Salami Pargoo","Taqiya Ehsan","Zhao Zhang","Jorge Ortiz"],"pdf_url":"https://arxiv.org/pdf/2407.03291v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03247v1","updated":"2024-08-06T15:07:08Z","published":"2024-08-06T15:07:08Z","title":"Unveiling Factual Recall Behaviors of Large Language Models through\n  Knowledge Neurons","summary":"  In this paper, we investigate whether Large Language Models (LLMs) actively\nrecall or retrieve their internal repositories of factual knowledge when faced\nwith reasoning tasks. Through an analysis of LLMs' internal factual recall at\neach reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness\nthe critical factual associations under certain circumstances. Instead, they\ntend to opt for alternative, shortcut-like pathways to answer reasoning\nquestions. By manually manipulating the recall process of parametric knowledge\nin LLMs, we demonstrate that enhancing this recall process directly improves\nreasoning performance whereas suppressing it leads to notable degradation.\nFurthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a\npowerful technique for addressing complex reasoning tasks. Our findings\nindicate that CoT can intensify the recall of factual knowledge by encouraging\nLLMs to engage in orderly and reliable reasoning. Furthermore, we explored how\ncontextual conflicts affect the retrieval of facts during the reasoning process\nto gain a comprehensive understanding of the factual recall behaviors of LLMs.\nCode and data will be available soon.\n","authors":["Yifei Wang","Yuheng Chen","Wanting Wen","Yu Sheng","Linjing Li","Daniel Dajun Zeng"],"pdf_url":"https://arxiv.org/pdf/2408.03247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03623v2","updated":"2024-08-06T15:02:33Z","published":"2024-04-04T17:45:59Z","title":"Unveiling LLMs: The Evolution of Latent Representations in a Dynamic\n  Knowledge Graph","summary":"  Large Language Models (LLMs) demonstrate an impressive capacity to recall a\nvast range of factual knowledge. However, understanding their underlying\nreasoning and internal mechanisms in exploiting this knowledge remains a key\nresearch area. This work unveils the factual information an LLM represents\ninternally for sentence-level claim verification. We propose an end-to-end\nframework to decode factual knowledge embedded in token representations from a\nvector space to a set of ground predicates, showing its layer-wise evolution\nusing a dynamic knowledge graph. Our framework employs activation patching, a\nvector-level technique that alters a token representation during inference, to\nextract encoded knowledge. Accordingly, we neither rely on training nor\nexternal models. Using factual and common-sense claims from two claim\nverification datasets, we showcase interpretability analyses at local and\nglobal levels. The local analysis highlights entity centrality in LLM\nreasoning, from claim-related information and multi-hop reasoning to\nrepresentation errors causing erroneous evaluation. On the other hand, the\nglobal reveals trends in the underlying evolution, such as word-based knowledge\nevolving into claim-related facts. By interpreting semantics from LLM latent\nrepresentations and enabling graph-related analyses, this work enhances the\nunderstanding of the factual knowledge resolution process.\n","authors":["Marco Bronzini","Carlo Nicolini","Bruno Lepri","Jacopo Staiano","Andrea Passerini"],"pdf_url":"https://arxiv.org/pdf/2404.03623v2.pdf","comment":"Accepted at COLM 2024"},{"id":"http://arxiv.org/abs/2402.19135v2","updated":"2024-08-06T14:53:43Z","published":"2024-02-29T13:12:31Z","title":"Think Fast, Think Slow, Think Critical: Designing an Automated\n  Propaganda Detection Tool","summary":"  In today's digital age, characterized by rapid news consumption and\nincreasing vulnerability to propaganda, fostering citizens' critical thinking\nis crucial for stable democracies. This paper introduces the design of\nClarifAI, a novel automated propaganda detection tool designed to nudge readers\ntowards more critical news consumption by activating the analytical mode of\nthinking, following Kahneman's dual-system theory of cognition. Using Large\nLanguage Models, ClarifAI detects propaganda in news articles and provides\ncontext-rich explanations, enhancing users' understanding and critical\nthinking. Our contribution is threefold: first, we propose the design of\nClarifAI; second, in an online experiment, we demonstrate that this design\neffectively encourages news readers to engage in more critical reading; and\nthird, we emphasize the value of explanations for fostering critical thinking.\nThe study thus offers both a practical tool and useful design knowledge for\nmitigating propaganda in digital news.\n","authors":["Liudmila Zavolokina","Kilian Sprenkamp","Zoya Katashinskaya","Daniel Gordon Jones","Gerhard Schwabe"],"pdf_url":"https://arxiv.org/pdf/2402.19135v2.pdf","comment":"The paper is accepted for publication in proceedings of the CHI\n  Conference on Human Factors in Computing Systems (2024)"},{"id":"http://arxiv.org/abs/2404.14712v2","updated":"2024-08-06T14:46:11Z","published":"2024-04-23T03:39:57Z","title":"ORBIT: Oak Ridge Base Foundation Model for Earth System Predictability","summary":"  Earth system predictability is challenged by the complexity of environmental\ndynamics and the multitude of variables involved. Current AI foundation models,\nalthough advanced by leveraging large and heterogeneous data, are often\nconstrained by their size and data integration, limiting their effectiveness in\naddressing the full range of Earth system prediction challenges. To overcome\nthese limitations, we introduce the Oak Ridge Base Foundation Model for Earth\nSystem Predictability (ORBIT), an advanced vision transformer model that scales\nup to 113 billion parameters using a novel hybrid tensor-data orthogonal\nparallelism technique. As the largest model of its kind, ORBIT surpasses the\ncurrent climate AI foundation model size by a thousandfold. Performance scaling\ntests conducted on the Frontier supercomputer have demonstrated that ORBIT\nachieves 684 petaFLOPS to 1.6 exaFLOPS sustained throughput, with scaling\nefficiency maintained at 41% to 85% across 49,152 AMD GPUs. These breakthroughs\nestablish new advances in AI-driven climate modeling and demonstrate promise to\nsignificantly improve the Earth system predictability.\n","authors":["Xiao Wang","Siyan Liu","Aristeidis Tsaris","Jong-Youl Choi","Ashwin Aji","Ming Fan","Wei Zhang","Junqi Yin","Moetasim Ashfaq","Dan Lu","Prasanna Balaprakash"],"pdf_url":"https://arxiv.org/pdf/2404.14712v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05530v2","updated":"2024-08-06T14:30:31Z","published":"2024-04-08T13:59:02Z","title":"Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data","summary":"  Reinforcement Learning from Human Feedback (RLHF) is a popular method for\naligning Language Models (LM) with human values and preferences. RLHF requires\na large number of preference pairs as training data, which are often used in\nboth the Supervised Fine-Tuning and Reward Model training and therefore\npublicly available datasets are commonly used. In this work, we study to what\nextent a malicious actor can manipulate the LMs generations by poisoning the\npreferences, i.e., injecting poisonous preference pairs into these datasets and\nthe RLHF training process. We propose strategies to build poisonous preference\npairs and test their performance by poisoning two widely used preference\ndatasets. Our results show that preference poisoning is highly effective:\ninjecting a small amount of poisonous data (1-5\\% of the original dataset), we\ncan effectively manipulate the LM to generate a target entity in a target\nsentiment (positive or negative). The findings from our experiments also shed\nlight on strategies to defend against the preference poisoning attack.\n","authors":["Tim Baumgärtner","Yang Gao","Dana Alon","Donald Metzler"],"pdf_url":"https://arxiv.org/pdf/2404.05530v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09939v2","updated":"2024-08-06T14:30:11Z","published":"2024-04-15T17:07:55Z","title":"A Survey on Deep Learning for Theorem Proving","summary":"  Theorem proving is a fundamental aspect of mathematics, spanning from\ninformal reasoning in natural language to rigorous derivations in formal\nsystems. In recent years, the advancement of deep learning, especially the\nemergence of large language models, has sparked a notable surge of research\nexploring these techniques to enhance the process of theorem proving. This\npaper presents a comprehensive survey of deep learning for theorem proving by\noffering (i) a thorough review of existing approaches across various tasks such\nas autoformalization, premise selection, proofstep generation, and proof\nsearch; (ii) an extensive summary of curated datasets and strategies for\nsynthetic data generation; (iii) a detailed analysis of evaluation metrics and\nthe performance of state-of-the-art methods; and (iv) a critical discussion on\nthe persistent challenges and the promising avenues for future exploration. Our\nsurvey aims to serve as a foundational reference for deep learning approaches\nin theorem proving, inspiring and catalyzing further research endeavors in this\nrapidly growing field. A curated list of papers is available at\nhttps://github.com/zhaoyu-li/DL4TP.\n","authors":["Zhaoyu Li","Jialiang Sun","Logan Murphy","Qidong Su","Zenan Li","Xian Zhang","Kaiyu Yang","Xujie Si"],"pdf_url":"https://arxiv.org/pdf/2404.09939v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11647v3","updated":"2024-08-06T14:12:26Z","published":"2024-05-19T18:57:25Z","title":"Hummer: Towards Limited Competitive Preference Dataset","summary":"  Preference datasets are essential for incorporating human preferences into\npre-trained language models, playing a key role in the success of Reinforcement\nLearning from Human Feedback. However, these datasets often demonstrate\nconflicting alignment objectives, leading to increased vulnerability to\njailbreak attacks and challenges in adapting downstream tasks to prioritize\nspecific alignment objectives without negatively impacting others. In this\nwork, we introduce a novel statistical metric, Alignment Dimension Conflict, to\nquantify the degree of conflict within preference datasets. We then present\n\\texttt{Hummer} and its fine-grained variant, \\texttt{Hummer-F}, as innovative\npairwise preference datasets with reduced-conflict alignment objectives.\n\\texttt{Hummer} is built based on UltraFeedback and is enhanced by AI feedback\nfrom GPT-4, marking as the first preference dataset aimed at reducing the\ncompetition between alignment objectives. Furthermore, we develop reward\nmodels, HummerRM and HummerRM-F, which employ a hybrid sampling approach to\nbalance diverse alignment objectives effectively. This sampling method\npositions HummerRM as an ideal model for domain-specific further fine-tuning\nand reducing vulnerabilities to attacks.\n","authors":["Li Jiang","Yusen Wu","Junwu Xiong","Jingqing Ruan","Yichuan Ding","Qingpei Guo","Zujie Wen","Jun Zhou","Xiaotie Deng"],"pdf_url":"https://arxiv.org/pdf/2405.11647v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03208v1","updated":"2024-08-06T14:06:53Z","published":"2024-08-06T14:06:53Z","title":"Personalizing Federated Instrument Segmentation with Visual Trait Priors\n  in Robotic Surgery","summary":"  Personalized federated learning (PFL) for surgical instrument segmentation\n(SIS) is a promising approach. It enables multiple clinical sites to\ncollaboratively train a series of models in privacy, with each model tailored\nto the individual distribution of each site. Existing PFL methods rarely\nconsider the personalization of multi-headed self-attention, and do not account\nfor appearance diversity and instrument shape similarity, both inherent in\nsurgical scenes. We thus propose PFedSIS, a novel PFL method with visual trait\npriors for SIS, incorporating global-personalized disentanglement (GPD),\nappearance-regulation personalized enhancement (APE), and shape-similarity\nglobal enhancement (SGE), to boost SIS performance in each site. GPD represents\nthe first attempt at head-wise assignment for multi-headed self-attention\npersonalization. To preserve the unique appearance representation of each site\nand gradually leverage the inter-site difference, APE introduces appearance\nregulation and provides customized layer-wise aggregation solutions via\nhypernetworks for each site's personalized parameters. The mutual shape\ninformation of instruments is maintained and shared via SGE, which enhances the\ncross-style shape consistency on the image level and computes the\nshape-similarity contribution of each site on the prediction level for updating\nthe global parameters. PFedSIS outperforms state-of-the-art methods with +1.51%\nDice, +2.11% IoU, -2.79 ASSD, -15.55 HD95 performance gains. The corresponding\ncode and models will be released at https://github.com/wzjialang/PFedSIS.\n","authors":["Jialang Xu","Jiacheng Wang","Lequan Yu","Danail Stoyanov","Yueming Jin","Evangelos B. Mazomenos"],"pdf_url":"https://arxiv.org/pdf/2408.03208v1.pdf","comment":"9 pages, 3 figures, under review"},{"id":"http://arxiv.org/abs/2408.03200v1","updated":"2024-08-06T13:58:56Z","published":"2024-08-06T13:58:56Z","title":"Adversarial Safety-Critical Scenario Generation using Naturalistic Human\n  Driving Priors","summary":"  Evaluating the decision-making system is indispensable in developing\nautonomous vehicles, while realistic and challenging safety-critical test\nscenarios play a crucial role. Obtaining these scenarios is non-trivial, thanks\nto the long-tailed distribution, sparsity, and rarity in real-world data sets.\nTo tackle this problem, in this paper, we introduce a natural adversarial\nscenario generation solution using naturalistic human driving priors and\nreinforcement learning techniques. By doing this, we can obtain large-scale\ntest scenarios that are both diverse and realistic. Specifically, we build a\nsimulation environment that mimics natural traffic interaction scenarios.\nInformed by this environment, we implement a two-stage procedure. The first\nstage incorporates conventional rule-based models, e.g., IDM~(Intelligent\nDriver Model) and MOBIL~(Minimizing Overall Braking Induced by Lane changes)\nmodel, to coarsely and discretely capture and calibrate key control parameters\nfrom the real-world dataset. Next, we leverage GAIL~(Generative Adversarial\nImitation Learning) to represent driver behaviors continuously. The derived\nGAIL can be further used to design a PPO~(Proximal Policy Optimization)-based\nactor-critic network framework to fine-tune the reward function, and then\noptimizes our natural adversarial scenario generation solution. Extensive\nexperiments have been conducted in the NGSIM dataset including the trajectory\nof 3,000 vehicles. Essential traffic parameters were measured in comparison\nwith the baseline model, e.g., the collision rate, accelerations, steering, and\nthe number of lane changes. Our findings demonstrate that the proposed model\ncan generate realistic safety-critical test scenarios covering both naturalness\nand adversariality, which can be a cornerstone for the development of\nautonomous vehicles.\n","authors":["Kunkun Hao","Yonggang Luo","Wen Cui","Yuqiao Bai","Jucheng Yang","Songyang Yan","Yuxi Pan","Zijiang Yang"],"pdf_url":"https://arxiv.org/pdf/2408.03200v1.pdf","comment":"Published in IEEE Transactions on Intelligent Vehicles, 2023"},{"id":"http://arxiv.org/abs/2408.00655v3","updated":"2024-08-06T13:38:50Z","published":"2024-08-01T15:45:19Z","title":"SentenceVAE: Enable Next-sentence Prediction for Large Language Models\n  with Faster Speed, Higher Accuracy and Longer Context","summary":"  Current large language models (LLMs) primarily utilize next-token prediction\nmethod for inference, which significantly impedes their processing speed. In\nthis paper, we introduce a novel inference methodology termed next-sentence\nprediction, aimed at enhancing the inference efficiency of LLMs. We present\nSentence Variational Autoencoder (SentenceVAE), a tiny model consisting of a\nSentence Encoder and a Sentence Decoder. The Sentence Encoder can effectively\ncondense the information within a sentence into a singular token, while the\nSentence Decoder can reconstruct this compressed token back into sentence. By\nintegrating SentenceVAE into the input and output layers of LLMs, we develop\nSentence-level LLMs (SLLMs) that employ a sentence-by-sentence inference\nmethod. In addition, the SentenceVAE module of SLLMs can maintain the integrity\nof the original semantic content by segmenting the context into sentences,\nthereby improving accuracy while boosting inference speed. Moreover, compared\nto previous LLMs, SLLMs process fewer tokens over equivalent context length,\nsignificantly reducing memory demands for self-attention computation and\nfacilitating the handling of longer context. Extensive experiments on Wanjuan\ndataset have reveal that the proposed method can accelerate inference speed by\n204~365%, reduce perplexity (PPL) to 46~75% of its original metric, and\ndecrease memory overhead by 86~91% for the equivalent context length, compared\nto the token-by-token method.\n","authors":["Hongjun An","Yifan Chen","Xiaozhen Qiao","Zhe Sun","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.00655v3.pdf","comment":"update the article"},{"id":"http://arxiv.org/abs/2408.03168v1","updated":"2024-08-06T13:11:36Z","published":"2024-08-06T13:11:36Z","title":"Training on the Fly: On-device Self-supervised Learning aboard\n  Nano-drones within 20 mW","summary":"  Miniaturized cyber-physical systems (CPSes) powered by tiny machine learning\n(TinyML), such as nano-drones, are becoming an increasingly attractive\ntechnology. Their small form factor (i.e., ~10cm diameter) ensures vast\napplicability, ranging from the exploration of narrow disaster scenarios to\nsafe human-robot interaction. Simple electronics make these CPSes inexpensive,\nbut strongly limit the computational, memory, and sensing resources available\non board. In real-world applications, these limitations are further exacerbated\nby domain shift. This fundamental machine learning problem implies that model\nperception performance drops when moving from the training domain to a\ndifferent deployment one. To cope with and mitigate this general problem, we\npresent a novel on-device fine-tuning approach that relies only on the limited\nultra-low power resources available aboard nano-drones. Then, to overcome the\nlack of ground-truth training labels aboard our CPS, we also employ a\nself-supervised method based on ego-motion consistency. Albeit our work builds\non top of a specific real-world vision-based human pose estimation task, it is\nwidely applicable for many embedded TinyML use cases. Our 512-image on-device\ntraining procedure is fully deployed aboard an ultra-low power GWT GAP9\nSystem-on-Chip and requires only 1MB of memory while consuming as low as 19mW\nor running in just 510ms (at 38mW). Finally, we demonstrate the benefits of our\non-device learning approach by field-testing our closed-loop CPS, showing a\nreduction in horizontal position error of up to 26% vs. a non-fine-tuned\nstate-of-the-art baseline. In the most challenging never-seen-before\nenvironment, our on-device learning procedure makes the difference between\nsucceeding or failing the mission.\n","authors":["Elia Cereda","Alessandro Giusti","Daniele Palossi"],"pdf_url":"https://arxiv.org/pdf/2408.03168v1.pdf","comment":"This paper has been accepted for publication in the IEEE Transactions\n  on Computer-Aided Design of Integrated Circuits and Systems. Copyright 2024\n  IEEE"},{"id":"http://arxiv.org/abs/2408.03164v1","updated":"2024-08-06T13:05:32Z","published":"2024-08-06T13:05:32Z","title":"Dilated Convolution with Learnable Spacings makes visual models more\n  aligned with humans: a Grad-CAM study","summary":"  Dilated Convolution with Learnable Spacing (DCLS) is a recent advanced\nconvolution method that allows enlarging the receptive fields (RF) without\nincreasing the number of parameters, like the dilated convolution, yet without\nimposing a regular grid. DCLS has been shown to outperform the standard and\ndilated convolutions on several computer vision benchmarks. Here, we show that,\nin addition, DCLS increases the models' interpretability, defined as the\nalignment with human visual strategies. To quantify it, we use the Spearman\ncorrelation between the models' GradCAM heatmaps and the ClickMe dataset\nheatmaps, which reflect human visual attention. We took eight reference models\n- ResNet50, ConvNeXt (T, S and B), CAFormer, ConvFormer, and FastViT (sa 24 and\n36) - and drop-in replaced the standard convolution layers with DCLS ones. This\nimproved the interpretability score in seven of them. Moreover, we observed\nthat Grad-CAM generated random heatmaps for two models in our study: CAFormer\nand ConvFormer models, leading to low interpretability scores. We addressed\nthis issue by introducing Threshold-Grad-CAM, a modification built on top of\nGrad-CAM that enhanced interpretability across nearly all models. The code and\ncheckpoints to reproduce this study are available at:\nhttps://github.com/rabihchamas/DCLS-GradCAM-Eval.\n","authors":["Rabih Chamas","Ismail Khalfaoui-Hassani","Timothee Masquelier"],"pdf_url":"https://arxiv.org/pdf/2408.03164v1.pdf","comment":"Accepted at The Trustworthy AI Workshop, IJCAI 2024"},{"id":"http://arxiv.org/abs/2407.21483v3","updated":"2024-08-06T12:34:16Z","published":"2024-07-31T09:48:27Z","title":"eSPARQL: Representing and Reconciling Agnostic and Atheistic Beliefs in\n  RDF-star Knowledge Graphs","summary":"  Over the past few years, we have seen the emergence of large knowledge graphs\ncombining information from multiple sources. Sometimes, this information is\nprovided in the form of assertions about other assertions, defining contexts\nwhere assertions are valid. A recent extension to RDF which admits statements\nover statements, called RDF-star, is in revision to become a W3C standard.\nHowever, there is no proposal for a semantics of these RDF-star statements nor\na built-in facility to operate over them. In this paper, we propose a query\nlanguage for epistemic RDF-star metadata based on a four-valued logic, called\neSPARQL. Our proposed query language extends SPARQL-star, the query language\nfor RDF-star, with a new type of FROM clause to facilitate operating with\nmultiple and sometimes conflicting beliefs. We show that the proposed query\nlanguage can express four use case queries, including the following features:\n(i) querying the belief of an individual, (ii) the aggregating of beliefs,\n(iii) querying who is conflicting with somebody, and (iv) beliefs about beliefs\n(i.e., nesting of beliefs).\n","authors":["Xinyi Pan","Daniel Hernández","Philipp Seifer","Ralf Lämmel","Steffen Staab"],"pdf_url":"https://arxiv.org/pdf/2407.21483v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03780v4","updated":"2024-08-06T12:25:33Z","published":"2023-10-05T17:02:59Z","title":"Automating Human Tutor-Style Programming Feedback: Leveraging GPT-4\n  Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation","summary":"  Generative AI and large language models hold great promise in enhancing\nprogramming education by automatically generating individualized feedback for\nstudents. We investigate the role of generative AI models in providing human\ntutor-style programming hints to help students resolve errors in their buggy\nprograms. Recent works have benchmarked state-of-the-art models for various\nfeedback generation scenarios; however, their overall quality is still inferior\nto human tutors and not yet ready for real-world deployment. In this paper, we\nseek to push the limits of generative AI models toward providing high-quality\nprogramming hints and develop a novel technique, GPT4Hints-GPT3.5Val. As a\nfirst step, our technique leverages GPT-4 as a ``tutor'' model to generate\nhints -- it boosts the generative quality by using symbolic information of\nfailing test cases and fixes in prompts. As a next step, our technique\nleverages GPT-3.5, a weaker model, as a ``student'' model to further validate\nthe hint quality -- it performs an automatic quality validation by simulating\nthe potential utility of providing this feedback. We show the efficacy of our\ntechnique via extensive evaluation using three real-world datasets of Python\nprograms covering a variety of concepts ranging from basic algorithms to\nregular expressions and data analysis using pandas library.\n","authors":["Tung Phung","Victor-Alexandru Pădurean","Anjali Singh","Christopher Brooks","José Cambronero","Sumit Gulwani","Adish Singla","Gustavo Soares"],"pdf_url":"https://arxiv.org/pdf/2310.03780v4.pdf","comment":"Published in Learning Analytics and Knowledge Conference (LAK) 2024"},{"id":"http://arxiv.org/abs/2408.00998v2","updated":"2024-08-06T12:01:17Z","published":"2024-08-02T04:13:38Z","title":"FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features\n  for Highly Controllable Text-Driven Image Translation","summary":"  Large-scale text-to-image diffusion models have been a revolutionary\nmilestone in the evolution of generative AI and multimodal technology, allowing\nwonderful image generation with natural-language text prompt. However, the\nissue of lacking controllability of such models restricts their practical\napplicability for real-life content creation. Thus, attention has been focused\non leveraging a reference image to control text-to-image synthesis, which is\nalso regarded as manipulating (or editing) a reference image as per a text\nprompt, namely, text-driven image-to-image translation. This paper contributes\na novel, concise, and efficient approach that adapts pre-trained large-scale\ntext-to-image (T2I) diffusion model to the image-to-image (I2I) paradigm in a\nplug-and-play manner, realizing high-quality and versatile text-driven I2I\ntranslation without any model training, model fine-tuning, or online\noptimization process. To guide T2I generation with a reference image, we\npropose to decompose diverse guiding factors with different frequency bands of\ndiffusion features in the DCT spectral space, and accordingly devise a novel\nfrequency band substitution layer which realizes dynamic control of the\nreference image to the T2I generation result in a plug-and-play manner. We\ndemonstrate that our method allows flexible control over both guiding factor\nand guiding intensity of the reference image simply by tuning the type and\nbandwidth of the substituted frequency band, respectively. Extensive\nqualitative and quantitative experiments verify superiority of our approach\nover related methods in I2I translation visual quality, versatility, and\ncontrollability. The code is publicly available at:\nhttps://github.com/XiangGao1102/FBSDiff.\n","authors":["Xiang Gao","Jiaying Liu"],"pdf_url":"https://arxiv.org/pdf/2408.00998v2.pdf","comment":"Accepted conference paper of ACM MM 2024"},{"id":"http://arxiv.org/abs/2404.01869v2","updated":"2024-08-06T11:58:53Z","published":"2024-04-02T11:46:31Z","title":"Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language\n  Models -- A Survey","summary":"  Large language models (LLMs) have recently shown impressive performance on\ntasks involving reasoning, leading to a lively debate on whether these models\npossess reasoning capabilities similar to humans. However, despite these\nsuccesses, the depth of LLMs' reasoning abilities remains uncertain. This\nuncertainty partly stems from the predominant focus on task performance,\nmeasured through shallow accuracy metrics, rather than a thorough investigation\nof the models' reasoning behavior. This paper seeks to address this gap by\nproviding a comprehensive review of studies that go beyond task accuracy,\noffering deeper insights into the models' reasoning processes. Furthermore, we\nsurvey prevalent methodologies to evaluate the reasoning behavior of LLMs,\nemphasizing current trends and efforts towards more nuanced reasoning analyses.\nOur review suggests that LLMs tend to rely on surface-level patterns and\ncorrelations in their training data, rather than on sophisticated reasoning\nabilities. Additionally, we identify the need for further research that\ndelineates the key differences between human and LLM-based reasoning. Through\nthis survey, we aim to shed light on the complex reasoning processes within\nLLMs.\n","authors":["Philipp Mondorf","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2404.01869v2.pdf","comment":"COLM 2024, 27 pages, 2 figures"},{"id":"http://arxiv.org/abs/2408.03125v1","updated":"2024-08-06T11:56:26Z","published":"2024-08-06T11:56:26Z","title":"COMMENTATOR: A Code-mixed Multilingual Text Annotation Framework","summary":"  As the NLP community increasingly addresses challenges associated with\nmultilingualism, robust annotation tools are essential to handle multilingual\ndatasets efficiently. In this paper, we introduce a code-mixed multilingual\ntext annotation framework, COMMENTATOR, specifically designed for annotating\ncode-mixed text. The tool demonstrates its effectiveness in token-level and\nsentence-level language annotation tasks for Hinglish text. We perform robust\nqualitative human-based evaluations to showcase COMMENTATOR led to 5x faster\nannotations than the best baseline. Our code is publicly available at\n\\url{https://github.com/lingo-iitgn/commentator}. The demonstration video is\navailable at \\url{https://bit.ly/commentator_video}.\n","authors":["Rajvee Sheth","Shubh Nisar","Heenaben Prajapati","Himanshu Beniwal","Mayank Singh"],"pdf_url":"https://arxiv.org/pdf/2408.03125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03119v1","updated":"2024-08-06T11:49:11Z","published":"2024-08-06T11:49:11Z","title":"Evaluating the Translation Performance of Large Language Models Based on\n  Euas-20","summary":"  In recent years, with the rapid development of deep learning technology,\nlarge language models (LLMs) such as BERT and GPT have achieved breakthrough\nresults in natural language processing tasks. Machine translation (MT), as one\nof the core tasks of natural language processing, has also benefited from the\ndevelopment of large language models and achieved a qualitative leap. Despite\nthe significant progress in translation performance achieved by large language\nmodels, machine translation still faces many challenges. Therefore, in this\npaper, we construct the dataset Euas-20 to evaluate the performance of large\nlanguage models on translation tasks, the translation ability on different\nlanguages, and the effect of pre-training data on the translation ability of\nLLMs for researchers and developers.\n","authors":["Yan Huang","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2408.03119v1.pdf","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2401.06824v3","updated":"2024-08-06T11:46:37Z","published":"2024-01-12T00:50:04Z","title":"Rethinking Jailbreaking through the Lens of Representation Engineering","summary":"  The recent surge in jailbreaking methods has revealed the vulnerability of\nLarge Language Models (LLMs) to malicious inputs. While earlier research has\nprimarily concentrated on increasing the success rates of jailbreaking attacks,\nthe underlying mechanism for safeguarding LLMs remains underexplored. This\nstudy investigates the vulnerability of safety-aligned LLMs by uncovering\nspecific activity patterns within the representation space generated by LLMs.\nSuch ``safety patterns'' can be identified with only a few pairs of contrastive\nqueries in a simple method and function as ``keys'' (used as a metaphor for\nsecurity defense capability) that can be used to open or lock Pandora's Box of\nLLMs. Extensive experiments demonstrate that the robustness of LLMs against\njailbreaking can be lessened or augmented by attenuating or strengthening the\nidentified safety patterns. These findings deepen our understanding of\njailbreaking phenomena and call for the LLM community to address the potential\nmisuse of open-source LLMs.\n","authors":["Tianlong Li","Shihan Dou","Wenhao Liu","Muling Wu","Changze Lv","Rui Zheng","Xiaoqing Zheng","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2401.06824v3.pdf","comment":"21 pages, 20 figures, 6 tables"},{"id":"http://arxiv.org/abs/2401.11044v2","updated":"2024-08-06T11:29:06Z","published":"2024-01-19T22:11:54Z","title":"Preservation of Feature Stability in Machine Learning Under Data\n  Uncertainty for Decision Support in Critical Domains","summary":"  In a world where Machine Learning (ML) is increasingly deployed to support\ndecision-making in critical domains, providing decision-makers with\nexplainable, stable, and relevant inputs becomes fundamental. Understanding how\nmachine learning works under missing data and how this affects feature\nvariability is paramount. This is even more relevant as machine learning\napproaches focus on standardising decision-making approaches that rely on an\nidealised set of features. However, decision-making in human activities often\nrelies on incomplete data, even in critical domains. This paper addresses this\ngap by conducting a set of experiments using traditional machine learning\nmethods that look for optimal decisions in comparison to a recently deployed\nmachine learning method focused on a classification that is more descriptive\nand mimics human decision making, allowing for the natural integration of\nexplainability. We found that the ML descriptive approach maintains higher\nclassification accuracy while ensuring the stability of feature selection as\ndata incompleteness increases. This suggests that descriptive classification\nmethods can be helpful in uncertain decision-making scenarios.\n","authors":["Karol Capała","Paulina Tworek","Jose Sousa"],"pdf_url":"https://arxiv.org/pdf/2401.11044v2.pdf","comment":"30 pages, 6 figures, supplementary materials"},{"id":"http://arxiv.org/abs/2408.03093v1","updated":"2024-08-06T10:48:15Z","published":"2024-08-06T10:48:15Z","title":"Learning Provably Robust Policies in Uncertain Parametric Environments","summary":"  We present a data-driven approach for learning MDP policies that are robust\nacross stochastic environments whose transition probabilities are defined by\nparameters with an unknown distribution. We produce probably approximately\ncorrect (PAC) guarantees for the performance of these learned policies in a\nnew, unseen environment over the unknown distribution. Our approach is based on\nfinite samples of the MDP environments, for each of which we build an\napproximation of the model as an interval MDP, by exploring a set of generated\ntrajectories. We use the built approximations to synthesise a single policy\nthat performs well (meets given requirements) across the sampled environments,\nand furthermore bound its risk (of not meeting the given requirements) when\ndeployed in an unseen environment. Our procedure offers a trade-off between the\nguaranteed performance of the learned policy and the risk of not meeting the\nguarantee in an unseen environment. Our approach exploits knowledge of the\nenvironment's state space and graph structure, and we show how additional\nknowledge of its parametric structure can be leveraged to optimize learning and\nto obtain tighter guarantees from less samples. We evaluate our approach on a\ndiverse range of established benchmarks, demonstrating that we can generate\nhighly performing and robust policies, along with guarantees that tightly\nquantify their performance and the associated risk.\n","authors":["Yannik Schnitzer","Alessandro Abate","David Parker"],"pdf_url":"https://arxiv.org/pdf/2408.03093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03088v1","updated":"2024-08-06T10:41:46Z","published":"2024-08-06T10:41:46Z","title":"QADQN: Quantum Attention Deep Q-Network for Financial Market Prediction","summary":"  Financial market prediction and optimal trading strategy development remain\nchallenging due to market complexity and volatility. Our research in quantum\nfinance and reinforcement learning for decision-making demonstrates the\napproach of quantum-classical hybrid algorithms to tackling real-world\nfinancial challenges. In this respect, we corroborate the concept with rigorous\nbacktesting and validate the framework's performance under realistic market\nconditions, by including fixed transaction cost per trade. This paper\nintroduces a Quantum Attention Deep Q-Network (QADQN) approach to address these\nchallenges through quantum-enhanced reinforcement learning. Our QADQN\narchitecture uses a variational quantum circuit inside a traditional deep\nQ-learning framework to take advantage of possible quantum advantages in\ndecision-making. We gauge the QADQN agent's performance on historical data from\nmajor market indices, including the S&P 500. We evaluate the agent's learning\nprocess by examining its reward accumulation and the effectiveness of its\nexperience replay mechanism. Our empirical results demonstrate the QADQN's\nsuperior performance, achieving better risk-adjusted returns with Sortino\nratios of 1.28 and 1.19 for non-overlapping and overlapping test periods\nrespectively, indicating effective downside risk management.\n","authors":["Siddhant Dutta","Nouhaila Innan","Alberto Marchisio","Sadok Ben Yahia","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2408.03088v1.pdf","comment":"Accepted at the 2024 IEEE International Conference on Quantum\n  Computing and Engineering (QCE24), QCRL, September 2024"},{"id":"http://arxiv.org/abs/2408.02402v2","updated":"2024-08-06T10:19:26Z","published":"2024-08-05T11:52:34Z","title":"Enhancing AI-based Generation of Software Exploits with Contextual\n  Information","summary":"  This practical experience report explores Neural Machine Translation (NMT)\nmodels' capability to generate offensive security code from natural language\n(NL) descriptions, highlighting the significance of contextual understanding\nand its impact on model performance. Our study employs a dataset comprising\nreal shellcodes to evaluate the models across various scenarios, including\nmissing information, necessary context, and unnecessary context. The\nexperiments are designed to assess the models' resilience against incomplete\ndescriptions, their proficiency in leveraging context for enhanced accuracy,\nand their ability to discern irrelevant information. The findings reveal that\nthe introduction of contextual data significantly improves performance.\nHowever, the benefits of additional context diminish beyond a certain point,\nindicating an optimal level of contextual information for model training.\nMoreover, the models demonstrate an ability to filter out unnecessary context,\nmaintaining high levels of accuracy in the generation of offensive security\ncode. This study paves the way for future research on optimizing context use in\nAI-driven code generation, particularly for applications requiring a high\ndegree of technical precision such as the generation of offensive code.\n","authors":["Pietro Liguori","Cristina Improta","Roberto Natella","Bojan Cukic","Domenico Cotroneo"],"pdf_url":"https://arxiv.org/pdf/2408.02402v2.pdf","comment":"Accepted for publication at The 35th IEEE International Symposium on\n  Software Reliability Engineering"},{"id":"http://arxiv.org/abs/2408.03079v1","updated":"2024-08-06T10:15:15Z","published":"2024-08-06T10:15:15Z","title":"Enhancing Complex Causality Extraction via Improved Subtask Interaction\n  and Knowledge Fusion","summary":"  Event Causality Extraction (ECE) aims at extracting causal event pairs from\ntexts. Despite ChatGPT's recent success, fine-tuning small models remains the\nbest approach for the ECE task. However, existing fine-tuning based ECE methods\ncannot address all three key challenges in ECE simultaneously: 1) Complex\nCausality Extraction, where multiple causal-effect pairs occur within a single\nsentence; 2) Subtask~ Interaction, which involves modeling the mutual\ndependence between the two subtasks of ECE, i.e., extracting events and\nidentifying the causal relationship between extracted events; and 3) Knowledge\nFusion, which requires effectively fusing the knowledge in two modalities,\ni.e., the expressive pretrained language models and the structured knowledge\ngraphs. In this paper, we propose a unified ECE framework (UniCE to address all\nthree issues in ECE simultaneously. Specifically, we design a subtask\ninteraction mechanism to enable mutual interaction between the two ECE\nsubtasks. Besides, we design a knowledge fusion mechanism to fuse knowledge in\nthe two modalities. Furthermore, we employ separate decoders for each subtask\nto facilitate complex causality extraction. Experiments on three benchmark\ndatasets demonstrate that our method achieves state-of-the-art performance and\noutperforms ChatGPT with a margin of at least 30% F1-score. More importantly,\nour model can also be used to effectively improve the ECE performance of\nChatGPT via in-context learning.\n","authors":["Jinglong Gao","Chen Lu","Xiao Ding","Zhongyang Li","Ting Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2408.03079v1.pdf","comment":"NLPCC 2024 Oral"},{"id":"http://arxiv.org/abs/2408.03078v1","updated":"2024-08-06T10:13:57Z","published":"2024-08-06T10:13:57Z","title":"BodySLAM: A Generalized Monocular Visual SLAM Framework for Surgical\n  Applications","summary":"  Endoscopic surgery relies on two-dimensional views, posing challenges for\nsurgeons in depth perception and instrument manipulation. While Simultaneous\nLocalization and Mapping (SLAM) has emerged as a promising solution to address\nthese limitations, its implementation in endoscopic procedures presents\nsignificant challenges due to hardware limitations, such as the use of a\nmonocular camera and the absence of odometry sensors. This study presents a\nrobust deep learning-based SLAM approach that combines state-of-the-art and\nnewly developed models. It consists of three main parts: the Monocular Pose\nEstimation Module that introduces a novel unsupervised method based on the\nCycleGAN architecture, the Monocular Depth Estimation Module that leverages the\nnovel Zoe architecture, and the 3D Reconstruction Module which uses information\nfrom the previous models to create a coherent surgical map. The performance of\nthe procedure was rigorously evaluated using three publicly available datasets\n(Hamlyn, EndoSLAM, and SCARED) and benchmarked against two state-of-the-art\nmethods, EndoSFMLearner and EndoDepth. The integration of Zoe in the MDEM\ndemonstrated superior performance compared to state-of-the-art depth estimation\nalgorithms in endoscopy, whereas the novel approach in the MPEM exhibited\ncompetitive performance and the lowest inference time. The results showcase the\nrobustness of our approach in laparoscopy, gastroscopy, and colonoscopy, three\ndifferent scenarios in endoscopic surgery. The proposed SLAM approach has the\npotential to improve the accuracy and efficiency of endoscopic procedures by\nproviding surgeons with enhanced depth perception and 3D reconstruction\ncapabilities.\n","authors":["G. Manni","C. Lauretti","F. Prata","R. Papalia","L. Zollo","P. Soda"],"pdf_url":"https://arxiv.org/pdf/2408.03078v1.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.03076v1","updated":"2024-08-06T10:07:43Z","published":"2024-08-06T10:07:43Z","title":"Solving QUBO on the Loihi 2 Neuromorphic Processor","summary":"  In this article, we describe an algorithm for solving Quadratic Unconstrained\nBinary Optimization problems on the Intel Loihi 2 neuromorphic processor. The\nsolver is based on a hardware-aware fine-grained parallel simulated annealing\nalgorithm developed for Intel's neuromorphic research chip Loihi 2. Preliminary\nresults show that our approach can generate feasible solutions in as little as\n1 ms and up to 37x more energy efficient compared to two baseline solvers\nrunning on a CPU. These advantages could be especially relevant for size-,\nweight-, and power-constrained edge computing applications.\n","authors":["Alessandro Pierro","Philipp Stratmann","Gabriel Andres Fonseca Guerra","Sumedh Risbud","Timothy Shea","Ashish Rao Mangalore","Andreas Wild"],"pdf_url":"https://arxiv.org/pdf/2408.03076v1.pdf","comment":"12 pages, 3 figures. Shared first authorship: Alessandro Pierro,\n  Philipp Stratmann, and Gabriel Andres Fonseca Guerra"},{"id":"http://arxiv.org/abs/2403.14972v2","updated":"2024-08-06T09:45:27Z","published":"2024-03-22T06:03:07Z","title":"A Picture Is Worth a Graph: A Blueprint Debate Paradigm for Multimodal\n  Reasoning","summary":"  This paper presents a pilot study aimed at introducing multi-agent debate\ninto multimodal reasoning. The study addresses two key challenges: the\ntrivialization of opinions resulting from excessive summarization and the\ndiversion of focus caused by distractor concepts introduced from images. These\nchallenges stem from the inductive (bottom-up) nature of existing debating\nschemes. To address the issue, we propose a deductive (top-down) debating\napproach called Blueprint Debate on Graphs (BDoG). In BDoG, debates are\nconfined to a blueprint graph to prevent opinion trivialization through\nworld-level summarization. Moreover, by storing evidence in branches within the\ngraph, BDoG mitigates distractions caused by frequent but irrelevant concepts.\nExtensive experiments validate that BDoG is able to achieve state-of-the-art\nresults in ScienceQA and MMBench with significant improvements over previous\nmethods. The source code can be accessed at https://github.com/thecharm/BDoG.\n","authors":["Changmeng Zheng","Dayong Liang","Wengyu Zhang","Xiao-Yong Wei","Tat-Seng Chua","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2403.14972v2.pdf","comment":"Accepted by ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2407.04221v2","updated":"2024-08-06T09:39:14Z","published":"2024-07-05T02:18:02Z","title":"Autoverse: An Evolvable Game Language for Learning Robust Embodied\n  Agents","summary":"  We introduce Autoverse, an evolvable, domain-specific language for\nsingle-player 2D grid-based games, and demonstrate its use as a scalable\ntraining ground for Open-Ended Learning (OEL) algorithms. Autoverse uses\ncellular-automaton-like rewrite rules to describe game mechanics, allowing it\nto express various game environments (e.g. mazes, dungeons, sokoban puzzles)\nthat are popular testbeds for Reinforcement Learning (RL) agents. Each rewrite\nrule can be expressed as a series of simple convolutions, allowing for\nenvironments to be parallelized on the GPU, thereby drastically accelerating RL\ntraining. Using Autoverse, we propose jump-starting open-ended learning by\nimitation learning from search. In such an approach, we first evolve Autoverse\nenvironments (their rules and initial map topology) to maximize the number of\niterations required by greedy tree search to discover a new best solution,\nproducing a curriculum of increasingly complex environments and playtraces. We\nthen distill these expert playtraces into a neural-network-based policy using\nimitation learning. Finally, we use the learned policy as a starting point for\nopen-ended RL, where new training environments are continually evolved to\nmaximize the RL player agent's value function error (a proxy for its regret, or\nthe learnability of generated environments), finding that this approach\nimproves the performance and generality of resultant player agents.\n","authors":["Sam Earle","Julian Togelius"],"pdf_url":"https://arxiv.org/pdf/2407.04221v2.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.03131v3","updated":"2024-08-06T09:21:47Z","published":"2024-07-03T14:13:00Z","title":"MVGT: A Multi-view Graph Transformer Based on Spatial Relations for EEG\n  Emotion Recognition","summary":"  Electroencephalography (EEG), a medical imaging technique that captures scalp\nelectrical activity of brain structures via electrodes, has been widely used in\naffective computing. The spatial domain of EEG is rich in affective\ninformation. However, few of the existing studies have simultaneously analyzed\nEEG signals from multiple perspectives of geometric and anatomical structures\nin spatial domain. In this paper, we propose a multi-view Graph Transformer\n(MVGT) based on spatial relations, which integrates information from the\ntemporal, frequency and spatial domains, including geometric and anatomical\nstructures, so as to enhance the expressive power of the model comprehensively.\nWe incorporate the spatial information of EEG channels into the model as\nencoding, thereby improving its ability to perceive the spatial structure of\nthe channels. Meanwhile, experimental results based on publicly available\ndatasets demonstrate that our proposed model outperforms state-of-the-art\nmethods in recent years. In addition, the results also show that the MVGT could\nextract information from multiple domains and capture inter-channel\nrelationships in EEG emotion recognition tasks effectively.\n","authors":["Yanjie Cui","Xiaohong Liu","Jing Liang","Yamin Fu"],"pdf_url":"https://arxiv.org/pdf/2407.03131v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11757v3","updated":"2024-08-06T09:17:59Z","published":"2024-06-17T17:16:45Z","title":"STAR: SocioTechnical Approach to Red Teaming Language Models","summary":"  This research introduces STAR, a sociotechnical framework that improves on\ncurrent best practices for red teaming safety of large language models. STAR\nmakes two key contributions: it enhances steerability by generating\nparameterised instructions for human red teamers, leading to improved coverage\nof the risk surface. Parameterised instructions also provide more detailed\ninsights into model failures at no increased cost. Second, STAR improves signal\nquality by matching demographics to assess harms for specific groups, resulting\nin more sensitive annotations. STAR further employs a novel step of arbitration\nto leverage diverse viewpoints and improve label reliability, treating\ndisagreement not as noise but as a valuable contribution to signal quality.\n","authors":["Laura Weidinger","John Mellor","Bernat Guillen Pegueroles","Nahema Marchal","Ravin Kumar","Kristian Lum","Canfer Akbulut","Mark Diaz","Stevie Bergman","Mikel Rodriguez","Verena Rieser","William Isaac"],"pdf_url":"https://arxiv.org/pdf/2406.11757v3.pdf","comment":"8 pages, 5 figures, 5 pages appendix. * denotes equal contribution"},{"id":"http://arxiv.org/abs/2408.03047v1","updated":"2024-08-06T09:02:53Z","published":"2024-08-06T09:02:53Z","title":"OpenOmni: A Collaborative Open Source Tool for Building Future-Ready\n  Multimodal Conversational Agents","summary":"  Multimodal conversational agents are highly desirable because they offer\nnatural and human-like interaction. However, there is a lack of comprehensive\nend-to-end solutions to support collaborative development and benchmarking.\nWhile proprietary systems like GPT-4o and Gemini demonstrating impressive\nintegration of audio, video, and text with response times of 200-250ms,\nchallenges remain in balancing latency, accuracy, cost, and data privacy. To\nbetter understand and quantify these issues, we developed OpenOmni, an\nopen-source, end-to-end pipeline benchmarking tool that integrates advanced\ntechnologies such as Speech-to-Text, Emotion Detection, Retrieval Augmented\nGeneration, Large Language Models, along with the ability to integrate\ncustomized models. OpenOmni supports local and cloud deployment, ensuring data\nprivacy and supporting latency and accuracy benchmarking. This flexible\nframework allows researchers to customize the pipeline, focusing on real\nbottlenecks and facilitating rapid proof-of-concept development. OpenOmni can\nsignificantly enhance applications like indoor assistance for visually impaired\nindividuals, advancing human-computer interaction. Our demonstration video is\navailable https://www.youtube.com/watch?v=zaSiT3clWqY, demo is available via\nhttps://openomni.ai4wa.com, code is available via\nhttps://github.com/AI4WA/OpenOmniFramework.\n","authors":["Qiang Sun","Yuanyi Luo","Sirui Li","Wenxiao Zhang","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2408.03047v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17454v3","updated":"2024-08-06T08:52:18Z","published":"2024-07-24T17:41:32Z","title":"Automated Explanation Selection for Scientific Discovery","summary":"  Automated reasoning is a key technology in the young but rapidly growing\nfield of Explainable Artificial Intelligence (XAI). Explanability helps build\ntrust in artificial intelligence systems beyond their mere predictive accuracy\nand robustness. In this paper, we propose a cycle of scientific discovery that\ncombines machine learning with automated reasoning for the generation and the\nselection of explanations. We present a taxonomy of explanation selection\nproblems that draws on insights from sociology and cognitive science. These\nselection criteria subsume existing notions and extend them with new\nproperties.\n","authors":["Markus Iser"],"pdf_url":"https://arxiv.org/pdf/2407.17454v3.pdf","comment":"Composite AI Workshop at ECAI 2024 (accepted for publication)"},{"id":"http://arxiv.org/abs/2401.10660v2","updated":"2024-08-06T08:23:30Z","published":"2024-01-19T12:26:57Z","title":"Accelerating Multilingual Language Model for Excessively Tokenized\n  Languages","summary":"  Recent advancements in large language models (LLMs) have remarkably enhanced\nperformances on a variety of tasks in multiple languages. However, tokenizers\nin LLMs trained primarily on English-centric corpora often overly fragment a\ntext into character or Unicode-level tokens in non-Roman alphabetic languages,\nleading to inefficient text generation. We introduce a simple yet effective\nframework to accelerate text generation in such languages. Our approach\ninvolves employing a new language model head with a vocabulary set tailored to\na specific target language for a pre-trained LLM. This is followed by\nfine-tuning the new head while incorporating a verification step to ensure the\nmodel's performance is preserved. We show that this targeted fine-tuning, while\nfreezing other model parameters, effectively reduces token fragmentation for\nthe target language. Our extensive experiments demonstrate that the proposed\nframework increases the generation speed by a factor of 1.7 while maintaining\nthe performance of pre-trained multilingual models on target monolingual tasks.\n","authors":["Jimin Hong","Gibbeum Lee","Jaewoong Cho"],"pdf_url":"https://arxiv.org/pdf/2401.10660v2.pdf","comment":"Accepted to ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2408.03029v1","updated":"2024-08-06T08:22:16Z","published":"2024-08-06T08:22:16Z","title":"Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning","summary":"  Reward shaping addresses the challenge of sparse rewards in reinforcement\nlearning by constructing denser and more informative reward signals. To achieve\nself-adaptive and highly efficient reward shaping, we propose a novel method\nthat incorporates success rates derived from historical experiences into shaped\nrewards. Our approach utilizes success rates sampled from Beta distributions,\nwhich dynamically evolve from uncertain to reliable values as more data is\ncollected. Initially, the self-adaptive success rates exhibit more randomness\nto encourage exploration. Over time, they become more certain to enhance\nexploitation, thus achieving a better balance between exploration and\nexploitation. We employ Kernel Density Estimation (KDE) combined with Random\nFourier Features (RFF) to derive the Beta distributions, resulting in a\ncomputationally efficient implementation in high-dimensional continuous state\nspaces. This method provides a non-parametric and learning-free approach. The\nproposed method is evaluated on a wide range of continuous control tasks with\nsparse and delayed rewards, demonstrating significant improvements in sample\nefficiency and convergence stability compared to several baselines.\n","authors":["Haozhe Ma","Zhengding Luo","Thanh Vinh Vo","Kuankuan Sima","Tze-Yun Leong"],"pdf_url":"https://arxiv.org/pdf/2408.03029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14244v2","updated":"2024-08-06T08:11:34Z","published":"2024-04-22T14:57:17Z","title":"AI-Generated Faces in the Real World: A Large-Scale Case Study of\n  Twitter Profile Images","summary":"  Recent advances in the field of generative artificial intelligence (AI) have\nblurred the lines between authentic and machine-generated content, making it\nalmost impossible for humans to distinguish between such media. One notable\nconsequence is the use of AI-generated images for fake profiles on social\nmedia. While several types of disinformation campaigns and similar incidents\nhave been reported in the past, a systematic analysis has been lacking. In this\nwork, we conduct the first large-scale investigation of the prevalence of\nAI-generated profile pictures on Twitter. We tackle the challenges of a\nreal-world measurement study by carefully integrating various data sources and\ndesigning a multi-stage detection pipeline. Our analysis of nearly 15 million\nTwitter profile pictures shows that 0.052% were artificially generated,\nconfirming their notable presence on the platform. We comprehensively examine\nthe characteristics of these accounts and their tweet content, and uncover\npatterns of coordinated inauthentic behavior. The results also reveal several\nmotives, including spamming and political amplification campaigns. Our research\nreaffirms the need for effective detection and mitigation strategies to cope\nwith the potential negative effects of generative AI in the future.\n","authors":["Jonas Ricker","Dennis Assenmacher","Thorsten Holz","Asja Fischer","Erwin Quiring"],"pdf_url":"https://arxiv.org/pdf/2404.14244v2.pdf","comment":"Accepted to RAID 2024"},{"id":"http://arxiv.org/abs/2408.03018v1","updated":"2024-08-06T08:01:02Z","published":"2024-08-06T08:01:02Z","title":"Integrating Controllable Motion Skills from Demonstrations","summary":"  The expanding applications of legged robots require their mastery of\nversatile motion skills. Correspondingly, researchers must address the\nchallenge of integrating multiple diverse motion skills into controllers. While\nexisting reinforcement learning (RL)-based approaches have achieved notable\nsuccess in multi-skill integration for legged robots, these methods often\nrequire intricate reward engineering or are restricted to integrating a\npredefined set of motion skills constrained by specific task objectives,\nresulting in limited flexibility. In this work, we introduce a flexible\nmulti-skill integration framework named Controllable Skills Integration (CSI).\nCSI enables the integration of a diverse set of motion skills with varying\nstyles into a single policy without the need for complex reward tuning.\nFurthermore, in a hierarchical control manner, the trained low-level policy can\nbe coupled with a high-level Natural Language Inference (NLI) module to enable\npreliminary language-directed skill control. Our experiments demonstrate that\nCSI can flexibly integrate a diverse array of motion skills more\ncomprehensively and facilitate the transitions between different skills.\nAdditionally, CSI exhibits good scalability as the number of motion skills to\nbe integrated increases significantly.\n","authors":["Honghao Liao","Zhiheng Li","Ziyu Meng","Ran Song","Yibin Li","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.03018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.13274v3","updated":"2024-08-06T07:55:44Z","published":"2024-04-20T05:14:52Z","title":"Augmented Object Intelligence with XR-Objects","summary":"  Seamless integration of physical objects as interactive digital entities\nremains a challenge for spatial computing. This paper explores Artificial\nObject Intelligence (AOI) in the context of XR, an interaction paradigm that\naims to blur the lines between digital and physical by equipping real-world\nobjects with the ability to interact as if they were digital, where every\nobject has the potential to serve as a portal to digital functionalities. Our\napproach utilizes real-time object segmentation and classification, combined\nwith the power of Multimodal Large Language Models (MLLMs), to facilitate these\ninteractions without the need for object pre-registration. We implement the AOI\nconcept in the form of XR-Objects, an open-source prototype system that\nprovides a platform for users to engage with their physical environment in\ncontextually relevant ways using object-based context menus. This system\nenables analog objects to not only convey information but also to initiate\ndigital actions, such as querying for details or executing tasks. Our\ncontributions are threefold: (1) we define the AOI concept and detail its\nadvantages over traditional AI assistants, (2) detail the XR-Objects system's\nopen-source design and implementation, and (3) show its versatility through\nvarious use cases and a user study.\n","authors":["Mustafa Doga Dogan","Eric J. Gonzalez","Karan Ahuja","Ruofei Du","Andrea Colaço","Johnny Lee","Mar Gonzalez-Franco","David Kim"],"pdf_url":"https://arxiv.org/pdf/2404.13274v3.pdf","comment":"To appear in the Proceedings of the 2024 ACM Symposium on User\n  Interface Software and Technology (UIST)"},{"id":"http://arxiv.org/abs/2408.03013v1","updated":"2024-08-06T07:48:51Z","published":"2024-08-06T07:48:51Z","title":"NeurDB: On the Design and Implementation of an AI-powered Autonomous\n  Database","summary":"  Databases are increasingly embracing AI to provide autonomous system\noptimization and intelligent in-database analytics, aiming to relieve end-user\nburdens across various industry sectors. Nonetheless, most existing approaches\nfail to account for the dynamic nature of databases, which renders them\nineffective for real-world applications characterized by evolving data and\nworkloads. This paper introduces NeurDB, an AI-powered autonomous database that\ndeepens the fusion of AI and databases with adaptability to data and workload\ndrift. NeurDB establishes a new in-database AI ecosystem that seamlessly\nintegrates AI workflows within the database. This integration enables efficient\nand effective in-database AI analytics and fast-adaptive learned system\ncomponents. Empirical evaluations demonstrate that NeurDB substantially\noutperforms existing solutions in managing AI analytics tasks, with the\nproposed learned components more effectively handling environmental dynamism\nthan state-of-the-art approaches.\n","authors":["Zhanhao Zhao","Shaofeng Cai","Haotian Gao","Hexiang Pan","Siqi Xiang","Naili Xing","Gang Chen","Beng Chin Ooi","Yanyan Shen","Yuncheng Wu","Meihui Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.03013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17572v3","updated":"2024-08-06T07:36:21Z","published":"2024-07-24T18:05:13Z","title":"CityX: Controllable Procedural Content Generation for Unbounded 3D\n  Cities","summary":"  Generating a realistic, large-scale 3D virtual city remains a complex\nchallenge due to the involvement of numerous 3D assets, various city styles,\nand strict layout constraints. Existing approaches provide promising attempts\nat procedural content generation to create large-scale scenes using Blender\nagents. However, they face crucial issues such as difficulties in scaling up\ngeneration capability and achieving fine-grained control at the semantic layout\nlevel. To address these problems, we propose a novel multi-modal controllable\nprocedural content generation method, named CityX, which enhances realistic,\nunbounded 3D city generation guided by multiple layout conditions, including\nOSM, semantic maps, and satellite images. Specifically, the proposed method\ncontains a general protocol for integrating various PCG plugins and a\nmulti-agent framework for transforming instructions into executable Blender\nactions. Through this effective framework, CityX shows the potential to build\nan innovative ecosystem for 3D scene generation by bridging the gap between the\nquality of generated assets and industrial requirements. Extensive experiments\nhave demonstrated the effectiveness of our method in creating high-quality,\ndiverse, and unbounded cities guided by multi-modal conditions. Our project\npage: https://cityx-lab.github.io.\n","authors":["Shougao Zhang","Mengqi Zhou","Yuxi Wang","Chuanchen Luo","Rongyu Wang","Yiwei Li","Xucheng Yin","Zhaoxiang Zhang","Junran Peng"],"pdf_url":"https://arxiv.org/pdf/2407.17572v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02529v2","updated":"2024-08-06T07:32:02Z","published":"2024-08-05T14:49:12Z","title":"Explaining Reinforcement Learning: A Counterfactual Shapley Values\n  Approach","summary":"  This paper introduces a novel approach Counterfactual Shapley Values (CSV),\nwhich enhances explainability in reinforcement learning (RL) by integrating\ncounterfactual analysis with Shapley Values. The approach aims to quantify and\ncompare the contributions of different state dimensions to various action\nchoices. To more accurately analyze these impacts, we introduce new\ncharacteristic value functions, the ``Counterfactual Difference Characteristic\nValue\" and the ``Average Counterfactual Difference Characteristic Value.\" These\nfunctions help calculate the Shapley values to evaluate the differences in\ncontributions between optimal and non-optimal actions. Experiments across\nseveral RL domains, such as GridWorld, FrozenLake, and Taxi, demonstrate the\neffectiveness of the CSV method. The results show that this method not only\nimproves transparency in complex RL systems but also quantifies the differences\nacross various decisions.\n","authors":["Yiwei Shi","Qi Zhang","Kevin McAreavey","Weiru Liu"],"pdf_url":"https://arxiv.org/pdf/2408.02529v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03003v1","updated":"2024-08-06T07:28:59Z","published":"2024-08-06T07:28:59Z","title":"Cross-cultural analysis of pedestrian group behaviour influence on\n  crossing decisions in interactions with autonomous vehicles","summary":"  Understanding cultural backgrounds is crucial for the seamless integration of\nautonomous driving into daily life as it ensures that systems are attuned to\ndiverse societal norms and behaviours, enhancing acceptance and safety in\nvaried cultural contexts. In this work, we investigate the impact of co-located\npedestrians on crossing behaviour, considering cultural and situational\nfactors. To accomplish this, a full-scale virtual reality (VR) environment was\ncreated in the CARLA simulator, enabling the identical experiment to be\nreplicated in both Spain and Australia. Participants (N=30) attempted to cross\nthe road at an urban crosswalk alongside other pedestrians exhibiting\nconservative to more daring behaviours, while an autonomous vehicle (AV)\napproached with different driving styles. For the analysis of interactions, we\nutilized questionnaires and direct measures of the moment when participants\nentered the lane.\n  Our findings indicate that pedestrians tend to cross the same traffic gap\ntogether, even though reckless behaviour by the group reduces confidence and\nmakes the situation perceived as more complex. Australian participants were\nwilling to take fewer risks than Spanish participants, adopting more cautious\nbehaviour when it was uncertain whether the AV would yield.\n","authors":["Sergio Martín Serrano","Óscar Méndez Blanco","Stewart Worrall","Miguel Ángel Sotelo","David Fernández-Llorca"],"pdf_url":"https://arxiv.org/pdf/2408.03003v1.pdf","comment":"Paper accepted at the 27th IEEE International Conference on\n  Intelligent Transportation Systems (ITSC 2024)"},{"id":"http://arxiv.org/abs/2407.15520v2","updated":"2024-08-06T07:25:16Z","published":"2024-07-22T10:13:46Z","title":"Future-Proofing Mobile Networks: A Digital Twin Approach to Multi-Signal\n  Management","summary":"  Digital Twins (DTs) are set to become a key enabling technology in future\nwireless networks, with their use in network management increasing\nsignificantly. We developed a DT framework that leverages the heterogeneity of\nnetwork access technologies as a resource for enhanced network performance and\nmanagement, enabling smart data handling in the physical network. Tested in a\nCampus Area Network environment, our framework integrates diverse data sources\nto provide real-time, holistic insights into network performance and\nenvironmental sensing. We also envision that traditional analytics will evolve\nto rely on emerging AI models, such as Generative AI (GenAI), while leveraging\ncurrent analytics capabilities. This capacity can simplify analytics processes\nthrough advanced ML models, enabling descriptive, diagnostic, predictive, and\nprescriptive analytics in a unified fashion. Finally, we present specific\nresearch opportunities concerning interoperability aspects and envision\naligning advancements in DT technology with evolved AI integration.\n","authors":["Roberto Morabito","Bivek Pandey","Paulius Daubaris","Yasith R Wanigarathna","Sasu Tarkoma"],"pdf_url":"https://arxiv.org/pdf/2407.15520v2.pdf","comment":"A shortened version of this paper is currently under review for\n  publication in an IEEE magazine. If accepted, the copyright will be\n  transferred to IEEE"},{"id":"http://arxiv.org/abs/2408.02999v1","updated":"2024-08-06T07:12:09Z","published":"2024-08-06T07:12:09Z","title":"LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning","summary":"  The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.\n","authors":["Lekai Chen","Ashutosh Trivedi","Alvaro Velasquez"],"pdf_url":"https://arxiv.org/pdf/2408.02999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10805v3","updated":"2024-08-06T06:47:06Z","published":"2024-07-15T15:20:40Z","title":"Think-on-Graph 2.0: Deep and Interpretable Large Language Model\n  Reasoning with Knowledge Graph-guided Retrieval","summary":"  Retrieval-augmented generation (RAG) has significantly advanced large\nlanguage models (LLMs) by enabling dynamic information retrieval to mitigate\nknowledge gaps and hallucinations in generated content. However, these systems\noften falter with complex reasoning and consistency across diverse queries. In\nthis work, we present Think-on-Graph 2.0, an enhanced RAG framework that aligns\nquestions with the knowledge graph and uses it as a navigational tool, which\ndeepens and refines the RAG paradigm for information collection and\nintegration. The KG-guided navigation fosters deep and long-range associations\nto uphold logical consistency and optimize the scope of retrieval for precision\nand interoperability. In conjunction, factual consistency can be better ensured\nthrough semantic similarity guided by precise directives. ToG${2.0}$ not only\nimproves the accuracy and reliability of LLMs' responses but also demonstrates\nthe potential of hybrid structured knowledge systems to significantly advance\nLLM reasoning, aligning it closer to human-like performance. We conducted\nextensive experiments on four public datasets to demonstrate the advantages of\nour method compared to the baseline.\n","authors":["Shengjie Ma","Chengjin Xu","Xuhui Jiang","Muzhi Li","Huaren Qu","Jian Guo"],"pdf_url":"https://arxiv.org/pdf/2407.10805v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02514v3","updated":"2024-08-06T06:39:02Z","published":"2024-06-22T12:50:41Z","title":"LOGIC-LM++: Multi-Step Refinement for Symbolic Formulations","summary":"  In this paper we examine the limitations of Large Language Models (LLMs) for\ncomplex reasoning tasks. Although recent works have started to employ formal\nlanguages as an intermediate representation for reasoning tasks, they often\nface challenges in accurately generating and refining these formal\nspecifications to ensure correctness. To address these issues, this paper\nproposes Logic-LM++, an improvement on Logic-LM . It uses the ability of LLMs\nto do pairwise comparisons, allowing the evaluation of the refinements\nsuggested by the LLM. The paper demonstrates that Logic-LM++ outperforms\nLogic-LM and other contemporary techniques across natural language reasoning\ntasks on three datasets, FOLIO, ProofWriter and AR-LSAT, with an average\nimprovement of 18.5% on standard prompting, 12.3% on chain of thought prompting\nand 5% on Logic-LM.\n","authors":["Shashank Kirtania","Priyanshu Gupta","Arjun Radhakirshna"],"pdf_url":"https://arxiv.org/pdf/2407.02514v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02978v1","updated":"2024-08-06T06:24:10Z","published":"2024-08-06T06:24:10Z","title":"ASR-enhanced Multimodal Representation Learning for Cross-Domain Product\n  Retrieval","summary":"  E-commerce is increasingly multimedia-enriched, with products exhibited in a\nbroad-domain manner as images, short videos, or live stream promotions. A\nunified and vectorized cross-domain production representation is essential. Due\nto large intra-product variance and high inter-product similarity in the\nbroad-domain scenario, a visual-only representation is inadequate. While\nAutomatic Speech Recognition (ASR) text derived from the short or live-stream\nvideos is readily accessible, how to de-noise the excessively noisy text for\nmultimodal representation learning is mostly untouched. We propose ASR-enhanced\nMultimodal Product Representation Learning (AMPere). In order to extract\nproduct-specific information from the raw ASR text, AMPere uses an\neasy-to-implement LLM-based ASR text summarizer. The LLM-summarized text,\ntogether with visual data, is then fed into a multi-branch network to generate\ncompact multimodal embeddings. Extensive experiments on a large-scale\ntri-domain dataset verify the effectiveness of AMPere in obtaining a unified\nmultimodal product representation that clearly improves cross-domain product\nretrieval.\n","authors":["Ruixiang Zhao","Jian Jia","Yan Li","Xuehan Bai","Quan Chen","Han Li","Peng Jiang","Xirong Li"],"pdf_url":"https://arxiv.org/pdf/2408.02978v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2109.03445v6","updated":"2024-08-06T06:19:46Z","published":"2021-09-08T06:06:28Z","title":"Convergence of Batch Asynchronous Stochastic Approximation With\n  Applications to Reinforcement Learning","summary":"  We begin by briefly surveying some results on the convergence of the\nStochastic Gradient Descent (SGD) Method, proved in a companion paper by the\npresent authors. These results are based on viewing SGD as a version of\nStochastic Approximation (SA). Ever since its introduction in the classic paper\nof Robbins and Monro in 1951, SA has become a standard tool for finding a\nsolution of an equation of the form $f(\\theta) = 0$, when only noisy\nmeasurements of $f(\\cdot)$ are available. In most situations, \\textit{every\ncomponent} of the putative solution $\\theta_t$ is updated at each step $t$. In\nsome applications in Reinforcement Learning (RL), \\textit{only one component}\nof $\\theta_t$ is updated at each $t$. This is known as \\textbf{asynchronous}\nSA. In this paper, we study \\textbf{Block Asynchronous SA (BASA)}, in which, at\neach step $t$, \\textit{some but not necessarily all} components of $\\theta_t$\nare updated. The theory presented here embraces both conventional (synchronous)\nSA as well as asynchronous SA, and all in-between possibilities. We provide\nsufficient conditions for the convergence of BASA, and also prove bounds on the\n\\textit{rate} of convergence of $\\theta_t$ to the solution. For the case of\nconventional SGD, these results reduce to those proved in our companion paper.\nThen we apply these results to the problem of finding a fixed point of a map\nwith only noisy measurements. This problem arises frequently in RL. We prove\nsufficient conditions for convergence as well as estimates for the rate of\nconvergence.\n","authors":["Rajeeva L. Karandikar","M. Vidyasagar"],"pdf_url":"https://arxiv.org/pdf/2109.03445v6.pdf","comment":"34 pages, 1 figure"},{"id":"http://arxiv.org/abs/2408.02976v1","updated":"2024-08-06T06:16:00Z","published":"2024-08-06T06:16:00Z","title":"Empathy Level Alignment via Reinforcement Learning for Empathetic\n  Response Generation","summary":"  Empathetic response generation, aiming at understanding the user's situation\nand feelings and respond empathically, is crucial in building human-like\ndialogue systems. Previous methods mainly focus on using maximum likelihood\nestimation as the optimization objective for training response generation\nmodels, without taking into account the empathy level alignment between\ngenerated responses and target responses. To this end, we propose an empathetic\nresponse generation using reinforcement learning (EmpRL) framework. The\nframework designs an effective empathy reward function and generates empathetic\nresponses by maximizing the expected reward through reinforcement learning.\nGiven the powerful text generation capability of pre-trained language models,\nEmpRL utilizes the pre-trained T5 model as the generator and conducts further\ntraining to initialize the policy. To align the empathy level between generated\nresponses and target responses in the context, an empathy reward function\ncontaining three empathy communication mechanisms, i.e., emotional reaction,\ninterpretation, and exploration, is constructed using pre-designed and\npre-trained empathy identifiers. Finally, the proximal policy optimization\nalgorithm is used to further train the policy to produce empathetic responses.\nBoth automatic and manual evaluations demonstrate that the proposed EmpRL\nframework can improve the quality of generated responses, enhance the empathy\nlevel similarity between generated and target responses, and produce empathetic\nresponses covering both affective and cognitive aspects.\n","authors":["Hui Ma","Bo Zhang","Bo Xu","Jian Wang","Hongfei Lin","Xiao Sun"],"pdf_url":"https://arxiv.org/pdf/2408.02976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02960v1","updated":"2024-08-06T05:15:35Z","published":"2024-08-06T05:15:35Z","title":"Anytime Multi-Agent Path Finding with an Adaptive Delay-Based Heuristic","summary":"  Anytime multi-agent path finding (MAPF) is a promising approach to scalable\npath optimization in multi-agent systems. MAPF-LNS, based on Large Neighborhood\nSearch (LNS), is the current state-of-the-art approach where a fast initial\nsolution is iteratively optimized by destroying and repairing selected paths of\nthe solution. Current MAPF-LNS variants commonly use an adaptive selection\nmechanism to choose among multiple destroy heuristics. However, to determine\npromising destroy heuristics, MAPF-LNS requires a considerable amount of\nexploration time. As common destroy heuristics are non-adaptive, any\nperformance bottleneck caused by these heuristics cannot be overcome via\nadaptive heuristic selection alone, thus limiting the overall effectiveness of\nMAPF-LNS in terms of solution cost. In this paper, we propose Adaptive\nDelay-based Destroy-and-Repair Enhanced with Success-based Self-Learning\n(ADDRESS) as a single-destroy-heuristic variant of MAPF-LNS. ADDRESS applies\nrestricted Thompson Sampling to the top-K set of the most delayed agents to\nselect a seed agent for adaptive LNS neighborhood generation. We evaluate\nADDRESS in multiple maps from the MAPF benchmark set and demonstrate cost\nimprovements by at least 50% in large-scale scenarios with up to a thousand\nagents, compared with the original MAPF-LNS and other state-of-the-art methods.\n","authors":["Thomy Phan","Benran Zhang","Shao-Hung Chan","Sven Koenig"],"pdf_url":"https://arxiv.org/pdf/2408.02960v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2312.16767"},{"id":"http://arxiv.org/abs/2407.11652v4","updated":"2024-08-06T05:10:56Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v4.pdf","comment":"I found critical errors in the manuscript affecting its validity. I\n  need to correct these before resubmitting. Major changes to methodology and\n  results are underway, significantly altering the content. I will resubmit the\n  revised version"},{"id":"http://arxiv.org/abs/2408.01933v2","updated":"2024-08-06T04:28:01Z","published":"2024-08-04T05:15:02Z","title":"DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language\n  Models","summary":"  Large language models (LLMs) have recently showcased remarkable capabilities,\nspanning a wide range of tasks and applications, including those in the medical\ndomain. Models like GPT-4 excel in medical question answering but may face\nchallenges in the lack of interpretability when handling complex tasks in real\nclinical settings. We thus introduce the diagnostic reasoning dataset for\nclinical notes (DiReCT), aiming at evaluating the reasoning ability and\ninterpretability of LLMs compared to human doctors. It contains 511 clinical\nnotes, each meticulously annotated by physicians, detailing the diagnostic\nreasoning process from observations in a clinical note to the final diagnosis.\nAdditionally, a diagnostic knowledge graph is provided to offer essential\nknowledge for reasoning, which may not be covered in the training data of\nexisting LLMs. Evaluations of leading LLMs on DiReCT bring out a significant\ngap between their reasoning ability and that of human doctors, highlighting the\ncritical need for models that can reason effectively in real-world clinical\nscenarios.\n","authors":["Bowen Wang","Jiuyang Chang","Yiming Qian","Guoxin Chen","Junhao Chen","Zhouqiang Jiang","Jiahao Zhang","Yuta Nakashima","Hajime Nagahara"],"pdf_url":"https://arxiv.org/pdf/2408.01933v2.pdf","comment":"9 pages,6 figures"},{"id":"http://arxiv.org/abs/2408.02949v1","updated":"2024-08-06T04:25:09Z","published":"2024-08-06T04:25:09Z","title":"Few-shot Scooping Under Domain Shift via Simulated Maximal Deployment\n  Gaps","summary":"  Autonomous lander missions on extraterrestrial bodies need to sample granular\nmaterials while coping with domain shifts, even when sampling strategies are\nextensively tuned on Earth. To tackle this challenge, this paper studies the\nfew-shot scooping problem and proposes a vision-based adaptive scooping\nstrategy that uses the deep kernel Gaussian process method trained with a novel\nmeta-training strategy to learn online from very limited experience on\nout-of-distribution target terrains. Our Deep Kernel Calibration with Maximal\nDeployment Gaps (kCMD) strategy explicitly trains a deep kernel model to adapt\nto large domain shifts by creating simulated maximal deployment gaps from an\noffline training dataset and training models to overcome these deployment gaps\nduring training. Employed in a Bayesian Optimization sequential decision-making\nframework, the proposed method allows the robot to perform high-quality\nscooping actions on out-of-distribution terrains after a few attempts,\nsignificantly outperforming non-adaptive methods proposed in the excavation\nliterature as well as other state-of-the-art meta-learning methods. The\nproposed method also demonstrates zero-shot transfer capability, successfully\nadapting to the NASA OWLAT platform, which serves as a state-of-the-art\nsimulator for potential future planetary missions. These results demonstrate\nthe potential of training deep models with simulated deployment gaps for more\ngeneralizable meta-learning in high-capacity models. Furthermore, they\nhighlight the promise of our method in autonomous lander sampling missions by\nenabling landers to overcome the deployment gap between Earth and\nextraterrestrial bodies.\n","authors":["Yifan Zhu","Pranay Thangeda","Erica L Tevere","Ashish Goel","Erik Kramer","Hari D Nayar","Melkior Ornik","Kris Hauser"],"pdf_url":"https://arxiv.org/pdf/2408.02949v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2303.02893"},{"id":"http://arxiv.org/abs/2408.02946v1","updated":"2024-08-06T04:14:29Z","published":"2024-08-06T04:14:29Z","title":"Scaling Laws for Data Poisoning in LLMs","summary":"  Recent work shows that LLMs are vulnerable to data poisoning, in which they\nare trained on partially corrupted or harmful data. Poisoned data is hard to\ndetect, breaks guardrails, and leads to undesirable and harmful behavior. Given\nthe intense efforts by leading labs to train and deploy increasingly larger and\nmore capable LLMs, it is critical to ask if the risk of data poisoning will be\nnaturally mitigated by scale, or if it is an increasing threat. We consider\nthree threat models by which data poisoning can occur: malicious fine-tuning,\nimperfect data curation, and intentional data contamination. Our experiments\nevaluate the effects of data poisoning on 23 frontier LLMs ranging from 1.5-72\nbillion parameters on three datasets which speak to each of our threat models.\nWe find that larger LLMs are increasingly vulnerable, learning harmful behavior\n-- including sleeper agent behavior -- significantly more quickly than smaller\nLLMs with even minimal data poisoning. These results underscore the need for\nrobust safeguards against data poisoning in larger LLMs.\n","authors":["Dillon Bowen","Brendan Murphy","Will Cai","David Khachaturov","Adam Gleave","Kellin Pelrine"],"pdf_url":"https://arxiv.org/pdf/2408.02946v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2408.02944v1","updated":"2024-08-06T04:08:26Z","published":"2024-08-06T04:08:26Z","title":"LLM-Empowered Resource Allocation in Wireless Communications Systems","summary":"  The recent success of large language models (LLMs) has spurred their\napplication in various fields. In particular, there have been efforts to\nintegrate LLMs into various aspects of wireless communication systems. The use\nof LLMs in wireless communication systems has the potential to realize\nartificial general intelligence (AGI)-enabled wireless networks. In this paper,\nwe investigate an LLM-based resource allocation scheme for wireless\ncommunication systems. Specifically, we formulate a simple resource allocation\nproblem involving two transmit pairs and develop an LLM-based resource\nallocation approach that aims to maximize either energy efficiency or spectral\nefficiency. Additionally, we consider the joint use of low-complexity resource\nallocation techniques to compensate for the reliability shortcomings of the\nLLM-based scheme. After confirming the applicability and feasibility of\nLLM-based resource allocation, we address several key technical challenges that\nremain in applying LLMs in practice.\n","authors":["Woongsup Lee","Jeonghun Park"],"pdf_url":"https://arxiv.org/pdf/2408.02944v1.pdf","comment":"submitted to possible IEEE journal"},{"id":"http://arxiv.org/abs/2404.18203v2","updated":"2024-08-06T03:37:31Z","published":"2024-04-28T14:47:09Z","title":"LMM-PCQA: Assisting Point Cloud Quality Assessment with LMM","summary":"  Although large multi-modality models (LMMs) have seen extensive exploration\nand application in various quality assessment studies, their integration into\nPoint Cloud Quality Assessment (PCQA) remains unexplored. Given LMMs'\nexceptional performance and robustness in low-level vision and quality\nassessment tasks, this study aims to investigate the feasibility of imparting\nPCQA knowledge to LMMs through text supervision. To achieve this, we transform\nquality labels into textual descriptions during the fine-tuning phase, enabling\nLMMs to derive quality rating logits from 2D projections of point clouds. To\ncompensate for the loss of perception in the 3D domain, structural features are\nextracted as well. These quality logits and structural features are then\ncombined and regressed into quality scores. Our experimental results affirm the\neffectiveness of our approach, showcasing a novel integration of LMMs into PCQA\nthat enhances model understanding and assessment accuracy. We hope our\ncontributions can inspire subsequent investigations into the fusion of LMMs\nwith PCQA, fostering advancements in 3D visual quality analysis and beyond. The\ncode is available at https://github.com/zzc-1998/LMM-PCQA.\n","authors":["Zicheng Zhang","Haoning Wu","Yingjie Zhou","Chunyi Li","Wei Sun","Chaofeng Chen","Xiongkuo Min","Xiaohong Liu","Weisi Lin","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2404.18203v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02932v1","updated":"2024-08-06T03:34:43Z","published":"2024-08-06T03:34:43Z","title":"Doubly Stochastic Adaptive Neighbors Clustering via the Marcus Mapping","summary":"  Clustering is a fundamental task in machine learning and data science, and\nsimilarity graph-based clustering is an important approach within this domain.\nDoubly stochastic symmetric similarity graphs provide numerous benefits for\nclustering problems and downstream tasks, yet learning such graphs remains a\nsignificant challenge. Marcus theorem states that a strictly positive symmetric\nmatrix can be transformed into a doubly stochastic symmetric matrix by diagonal\nmatrices. However, in clustering, learning sparse matrices is crucial for\ncomputational efficiency. We extend Marcus theorem by proposing the Marcus\nmapping, which indicates that certain sparse matrices can also be transformed\ninto doubly stochastic symmetric matrices via diagonal matrices. Additionally,\nwe introduce rank constraints into the clustering problem and propose the\nDoubly Stochastic Adaptive Neighbors Clustering algorithm based on the Marcus\nMapping (ANCMM). This ensures that the learned graph naturally divides into the\ndesired number of clusters. We validate the effectiveness of our algorithm\nthrough extensive comparisons with state-of-the-art algorithms. Finally, we\nexplore the relationship between the Marcus mapping and optimal transport. We\nprove that the Marcus mapping solves a specific type of optimal transport\nproblem and demonstrate that solving this problem through Marcus mapping is\nmore efficient than directly applying optimal transport methods.\n","authors":["Jinghui Yuan","Chusheng Zeng","Fangyuan Xie","Zhe Cao","Rong Wang","Feiping Nie","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.02932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21276v2","updated":"2024-08-06T03:34:03Z","published":"2024-07-31T01:51:24Z","title":"Multi-Level Querying using A Knowledge Pyramid","summary":"  This paper addresses the need for improved precision in existing\nRetrieval-Augmented Generation (RAG) methods that primarily focus on enhancing\nrecall. We propose a multi-layer knowledge pyramid approach within the RAG\nframework to achieve a better balance between precision and recall. The\nknowledge pyramid consists of three layers: Ontologies, Knowledge Graphs (KGs),\nand chunk-based raw text. We employ cross-layer augmentation techniques for\ncomprehensive knowledge coverage and dynamic updates of the Ontology schema and\ninstances. To ensure compactness, we utilize cross-layer filtering methods for\nknowledge condensation in KGs. Our approach, named PolyRAG, follows a waterfall\nmodel for retrieval, starting from the top of the pyramid and progressing down\nuntil a confident answer is obtained. We introduce two benchmarks for\ndomain-specific knowledge retrieval, one in the academic domain and the other\nin the financial domain. The effectiveness of the methods has been validated\nthrough comprehensive experiments by outperforming 19 SOTA methods. An\nencouraging observation is that the proposed method has augmented the GPT-4,\nproviding 395\\% F1 gain by improving its performance from 0.1636 to 0.8109.\n","authors":["Rubing Chen","Xulu Zhang","Jiaxin Wu","Wenqi Fan","Xiao-Yong Wei","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2407.21276v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02930v1","updated":"2024-08-06T03:26:01Z","published":"2024-08-06T03:26:01Z","title":"The Need for a Big World Simulator: A Scientific Challenge for Continual\n  Learning","summary":"  The \"small agent, big world\" frame offers a conceptual view that motivates\nthe need for continual learning. The idea is that a small agent operating in a\nmuch bigger world cannot store all information that the world has to offer. To\nperform well, the agent must be carefully designed to ingest, retain, and eject\nthe right information. To enable the development of performant continual\nlearning agents, a number of synthetic environments have been proposed.\nHowever, these benchmarks suffer from limitations, including unnatural\ndistribution shifts and a lack of fidelity to the \"small agent, big world\"\nframing. This paper aims to formalize two desiderata for the design of future\nsimulated environments. These two criteria aim to reflect the objectives and\ncomplexity of continual learning in practical settings while enabling rapid\nprototyping of algorithms on a smaller scale.\n","authors":["Saurabh Kumar","Hong Jun Jeon","Alex Lewandowski","Benjamin Van Roy"],"pdf_url":"https://arxiv.org/pdf/2408.02930v1.pdf","comment":"Accepted to the Finding the Frame Workshop at RLC 2024"},{"id":"http://arxiv.org/abs/2408.02927v1","updated":"2024-08-06T03:21:13Z","published":"2024-08-06T03:21:13Z","title":"HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy\n  Protection","summary":"  Data serves as the fundamental foundation for advancing deep learning,\nparticularly tabular data presented in a structured format, which is highly\nconducive to modeling. However, even in the era of LLM, obtaining tabular data\nfrom sensitive domains remains a challenge due to privacy or copyright\nconcerns. Hence, exploring how to effectively use models like LLMs to generate\nrealistic and privacy-preserving synthetic tabular data is urgent. In this\npaper, we take a step forward to explore LLMs for tabular data synthesis and\nprivacy protection, by introducing a new framework HARMONIC for tabular data\ngeneration and evaluation. In the tabular data generation of our framework,\nunlike previous small-scale LLM-based methods that rely on continued\npre-training, we explore the larger-scale LLMs with fine-tuning to generate\ntabular data and enhance privacy. Based on idea of the k-nearest neighbors\nalgorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to\ndiscover inter-row relationships. Then, with fine-tuning, LLMs are trained to\nremember the format and connections of the data rather than the data itself,\nwhich reduces the risk of privacy leakage. In the evaluation part of our\nframework, we develop specific privacy risk metrics DLT for LLM synthetic data\ngeneration, as well as performance evaluation metrics LLE for downstream LLM\ntasks. Our experiments find that this tabular data generation framework\nachieves equivalent performance to existing methods with better privacy, which\nalso demonstrates our evaluation framework for the effectiveness of synthetic\ndata and privacy risks in LLM scenarios.\n","authors":["Yuxin Wang","Duanyu Feng","Yongfu Dai","Zhengyu Chen","Jimin Huang","Sophia Ananiadou","Qianqian Xie","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2408.02927v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02920v1","updated":"2024-08-06T03:10:52Z","published":"2024-08-06T03:10:52Z","title":"A Taxonomy of Architecture Options for Foundation Model-based Agents:\n  Analysis and Decision Model","summary":"  The rapid advancement of AI technology has led to widespread applications of\nagent systems across various domains. However, the need for detailed\narchitecture design poses significant challenges in designing and operating\nthese systems. This paper introduces a taxonomy focused on the architectures of\nfoundation-model-based agents, addressing critical aspects such as functional\ncapabilities and non-functional qualities. We also discuss the operations\ninvolved in both design-time and run-time phases, providing a comprehensive\nview of architectural design and operational characteristics. By unifying and\ndetailing these classifications, our taxonomy aims to improve the design of\nfoundation-model-based agents. Additionally, the paper establishes a decision\nmodel that guides critical design and runtime decisions, offering a structured\napproach to enhance the development of foundation-model-based agents. Our\ncontributions include providing a structured architecture design option and\nguiding the development process of foundation-model-based agents, thereby\naddressing current fragmentation in the field.\n","authors":["Jingwen Zhou","Qinghua Lu","Jieshan Chen","Liming Zhu","Xiwei Xu","Zhenchang Xing","Stefan Harrer"],"pdf_url":"https://arxiv.org/pdf/2408.02920v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2408.02912v1","updated":"2024-08-06T02:53:55Z","published":"2024-08-06T02:53:55Z","title":"KOI: Accelerating Online Imitation Learning via Hybrid Key-state\n  Guidance","summary":"  Online Imitation Learning methods struggle with the gap between extensive\nonline exploration space and limited expert trajectories, which hinder\nefficient exploration due to inaccurate task-aware reward estimation. Inspired\nby the findings from cognitive neuroscience that task decomposition could\nfacilitate cognitive processing for efficient learning, we hypothesize that an\nagent could estimate precise task-aware imitation rewards for efficient online\nexploration by decomposing the target task into the objectives of \"what to do\"\nand the mechanisms of \"how to do\". In this work, we introduce the hybrid\nKey-state guided Online Imitation (KOI) learning approach, which leverages the\nintegration of semantic and motion key states as guidance for task-aware reward\nestimation. Initially, we utilize the visual-language models to segment the\nexpert trajectory into semantic key states, indicating the objectives of \"what\nto do\". Within the intervals between semantic key states, optical flow is\nemployed to capture motion key states to understand the process of \"how to do\".\nBy integrating a thorough grasp of both semantic and motion key states, we\nrefine the trajectory-matching reward computation, encouraging task-aware\nexploration for efficient online imitation learning. Our experiment results\nprove that our method is more sample efficient in the Meta-World and LIBERO\nenvironments. We also conduct real-world robotic manipulation experiments to\nvalidate the efficacy of our method, demonstrating the practical applicability\nof our KOI method.\n","authors":["Jingxian Lu","Wenke Xia","Dong Wang","Zhigang Wang","Bin Zhao","Di Hu","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.02912v1.pdf","comment":"Submitted to Corl 2024"},{"id":"http://arxiv.org/abs/2407.16110v3","updated":"2024-08-06T02:37:51Z","published":"2024-07-23T00:52:12Z","title":"Analyzing Polysemy Evolution Using Semantic Cells","summary":"  The senses of words evolve. The sense of the same word may change from today\nto tomorrow, and multiple senses of the same word may be the result of the\nevolution of each other, that is, they may be parents and children. If we view\nJuba as an evolving ecosystem, the paradigm of learning the correct answer,\nwhich does not move with the sense of a word, is no longer valid. This paper is\na case study that shows that word polysemy is an evolutionary consequence of\nthe modification of Semantic Cells, which has al-ready been presented by the\nauthor, by introducing a small amount of diversity in its initial state as an\nexample of analyzing the current set of short sentences. In particular, the\nanalysis of a sentence sequence of 1000 sentences in some order for each of the\nfour senses of the word Spring, collected using Chat GPT, shows that the word\nacquires the most polysemy monotonically in the analysis when the senses are\narranged in the order in which they have evolved. In other words, we present a\nmethod for analyzing the dynamism of a word's acquiring polysemy with evolution\nand, at the same time, a methodology for viewing polysemy from an evolutionary\nframework rather than a learning-based one.\n","authors":["Yukio Ohsawa","Dingming Xue","Kaira Sekiguchi"],"pdf_url":"https://arxiv.org/pdf/2407.16110v3.pdf","comment":"11 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:2404.14749"},{"id":"http://arxiv.org/abs/2408.02904v1","updated":"2024-08-06T02:27:54Z","published":"2024-08-06T02:27:54Z","title":"Enabling Intelligent Traffic Systems: A Deep Learning Method for\n  Accurate Arabic License Plate Recognition","summary":"  This paper introduces a novel two-stage framework for accurate Egyptian\nVehicle License Plate Recognition (EVLPR). The first stage employs image\nprocessing techniques to reliably localize license plates, while the second\nstage utilizes a custom-designed deep learning model for robust Arabic\ncharacter recognition. The proposed system achieves a remarkable 99.3% accuracy\non a diverse dataset, surpassing existing approaches. Its potential\napplications extend to intelligent traffic management, including traffic\nviolation detection and parking optimization. Future research will focus on\nenhancing the system's capabilities through architectural refinements, expanded\ndatasets, and addressing system dependencies.\n","authors":["M. A. Sayedelahl"],"pdf_url":"https://arxiv.org/pdf/2408.02904v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02897v1","updated":"2024-08-06T02:06:04Z","published":"2024-08-06T02:06:04Z","title":"A Metric Driven Approach to Mixed Precision Training","summary":"  As deep learning methodologies have developed, it has been generally agreed\nthat increasing neural network size improves model quality. However, this is at\nthe expense of memory and compute requirements, which also need to be\nincreased. Various efficiency techniques have been proposed to rein in hardware\ncosts, one being the use of low precision numerics. Recent accelerators have\nintroduced several different 8-bit data types to help accommodate DNNs in terms\nof numerics. In this paper, we identify a metric driven methodology to aid in\nthe choice of numerics. We demonstrate how such a methodology can help scale\ntraining of a language representation model. The technique can be generalized\nto other model architectures.\n","authors":["Mitchelle Rasquinha","Gil Tabak"],"pdf_url":"https://arxiv.org/pdf/2408.02897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14643v2","updated":"2024-08-06T02:04:35Z","published":"2024-06-20T18:07:19Z","title":"Holistic Evaluation for Interleaved Text-and-Image Generation","summary":"  Interleaved text-and-image generation has been an intriguing research\ndirection, where the models are required to generate both images and text\npieces in an arbitrary order. Despite the emerging advancements in interleaved\ngeneration, the progress in its evaluation still significantly lags behind.\nExisting evaluation benchmarks do not support arbitrarily interleaved images\nand text for both inputs and outputs, and they only cover a limited number of\ndomains and use cases. Also, current works predominantly use similarity-based\nmetrics which fall short in assessing the quality in open-ended scenarios. To\nthis end, we introduce InterleavedBench, the first benchmark carefully curated\nfor the evaluation of interleaved text-and-image generation. InterleavedBench\nfeatures a rich array of tasks to cover diverse real-world use cases. In\naddition, we present InterleavedEval, a strong reference-free metric powered by\nGPT-4o to deliver accurate and explainable evaluation. We carefully define five\nessential evaluation aspects for InterleavedEval, including text quality,\nperceptual quality, image coherence, text-image coherence, and helpfulness, to\nensure a comprehensive and fine-grained assessment. Through extensive\nexperiments and rigorous human evaluation, we show that our benchmark and\nmetric can effectively evaluate the existing models with a strong correlation\nwith human judgments surpassing previous reference-based metrics. We also\nprovide substantial findings and insights to foster future research in\ninterleaved generation and its evaluation.\n","authors":["Minqian Liu","Zhiyang Xu","Zihao Lin","Trevor Ashby","Joy Rimchala","Jiaxin Zhang","Lifu Huang"],"pdf_url":"https://arxiv.org/pdf/2406.14643v2.pdf","comment":"13 pages, 6 figures, 6 tables. Website:\n  https://vt-nlp.github.io/InterleavedEval/. Dataset:\n  https://huggingface.co/mqliu/InterleavedBench"},{"id":"http://arxiv.org/abs/2310.12428v3","updated":"2024-08-06T01:45:44Z","published":"2023-10-19T02:42:20Z","title":"Enhanced Local Explainability and Trust Scores with Random Forest\n  Proximities","summary":"  We initiate a novel approach to explain the predictions and out of sample\nperformance of random forest (RF) regression and classification models by\nexploiting the fact that any RF can be mathematically formulated as an adaptive\nweighted K nearest-neighbors model. Specifically, we employ a recent result\nthat, for both regression and classification tasks, any RF prediction can be\nrewritten exactly as a weighted sum of the training targets, where the weights\nare RF proximities between the corresponding pairs of data points. We show that\nthis linearity facilitates a local notion of explainability of RF predictions\nthat generates attributions for any model prediction across observations in the\ntraining set, and thereby complements established feature-based methods like\nSHAP, which generate attributions for a model prediction across input features.\nWe show how this proximity-based approach to explainability can be used in\nconjunction with SHAP to explain not just the model predictions, but also\nout-of-sample performance, in the sense that proximities furnish a novel means\nof assessing when a given model prediction is more or less likely to be\ncorrect. We demonstrate this approach in the modeling of US corporate bond\nprices and returns in both regression and classification cases.\n","authors":["Joshua Rosaler","Dhruv Desai","Bhaskarjit Sarmah","Dimitrios Vamvourellis","Deran Onay","Dhagash Mehta","Stefano Pasquali"],"pdf_url":"https://arxiv.org/pdf/2310.12428v3.pdf","comment":"5 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.02888v1","updated":"2024-08-06T01:34:43Z","published":"2024-08-06T01:34:43Z","title":"VizECGNet: Visual ECG Image Network for Cardiovascular Diseases\n  Classification with Multi-Modal Training and Knowledge Distillation","summary":"  An electrocardiogram (ECG) captures the heart's electrical signal to assess\nvarious heart conditions. In practice, ECG data is stored as either digitized\nsignals or printed images. Despite the emergence of numerous deep learning\nmodels for digitized signals, many hospitals prefer image storage due to cost\nconsiderations. Recognizing the unavailability of raw ECG signals in many\nclinical settings, we propose VizECGNet, which uses only printed ECG graphics\nto determine the prognosis of multiple cardiovascular diseases. During\ntraining, cross-modal attention modules (CMAM) are used to integrate\ninformation from two modalities - image and signal, while self-modality\nattention modules (SMAM) capture inherent long-range dependencies in ECG data\nof each modality. Additionally, we utilize knowledge distillation to improve\nthe similarity between two distinct predictions from each modality stream. This\ninnovative multi-modal deep learning architecture enables the utilization of\nonly ECG images during inference. VizECGNet with image input achieves higher\nperformance in precision, recall, and F1-Score compared to signal-based ECG\nclassification models, with improvements of 3.50%, 8.21%, and 7.38%,\nrespectively.\n","authors":["Ju-Hyeon Nam","Seo-Hyung Park","Su Jung Kim","Sang-Chul Lee"],"pdf_url":"https://arxiv.org/pdf/2408.02888v1.pdf","comment":"Accepted in International Conference on Image Processing (ICIP) 2024"},{"id":"http://arxiv.org/abs/2408.02882v1","updated":"2024-08-06T01:20:12Z","published":"2024-08-06T01:20:12Z","title":"Compromising Embodied Agents with Contextual Backdoor Attacks","summary":"  Large language models (LLMs) have transformed the development of embodied\nintelligence. By providing a few contextual demonstrations, developers can\nutilize the extensive internal knowledge of LLMs to effortlessly translate\ncomplex tasks described in abstract language into sequences of code snippets,\nwhich will serve as the execution logic for embodied agents. However, this\npaper uncovers a significant backdoor security threat within this process and\nintroduces a novel method called \\method{}. By poisoning just a few contextual\ndemonstrations, attackers can covertly compromise the contextual environment of\na black-box LLM, prompting it to generate programs with context-dependent\ndefects. These programs appear logically sound but contain defects that can\nactivate and induce unintended behaviors when the operational agent encounters\nspecific triggers in its interactive environment. To compromise the LLM's\ncontextual environment, we employ adversarial in-context generation to optimize\npoisoned demonstrations, where an LLM judge evaluates these poisoned prompts,\nreporting to an additional LLM that iteratively optimizes the demonstration in\na two-player adversarial game using chain-of-thought reasoning. To enable\ncontext-dependent behaviors in downstream agents, we implement a dual-modality\nactivation strategy that controls both the generation and execution of program\ndefects through textual and visual triggers. We expand the scope of our attack\nby developing five program defect modes that compromise key aspects of\nconfidentiality, integrity, and availability in embodied agents. To validate\nthe effectiveness of our approach, we conducted extensive experiments across\nvarious tasks, including robot planning, robot manipulation, and compositional\nvisual reasoning. Additionally, we demonstrate the potential impact of our\napproach by successfully attacking real-world autonomous driving systems.\n","authors":["Aishan Liu","Yuguang Zhou","Xianglong Liu","Tianyuan Zhang","Siyuan Liang","Jiakai Wang","Yanjun Pu","Tianlin Li","Junqi Zhang","Wenbo Zhou","Qing Guo","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2408.02882v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02871v1","updated":"2024-08-06T00:13:10Z","published":"2024-08-06T00:13:10Z","title":"Hide and Seek: Fingerprinting Large Language Models with Evolutionary\n  Learning","summary":"  As content generated by Large Language Model (LLM) has grown exponentially,\nthe ability to accurately identify and fingerprint such text has become\nincreasingly crucial. In this work, we introduce a novel black-box approach for\nfingerprinting LLMs, achieving an impressive 72% accuracy in identifying the\ncorrect family of models (Such as Llama, Mistral, Gemma, etc) among a lineup of\nLLMs. We present an evolutionary strategy that leverages the capabilities of\none LLM to discover the most salient features for identifying other LLMs. Our\nmethod employs a unique \"Hide and Seek\" algorithm, where an Auditor LLM\ngenerates discriminative prompts, and a Detective LLM analyzes the responses to\nfingerprint the target models. This approach not only demonstrates the\nfeasibility of LLM-driven model identification but also reveals insights into\nthe semantic manifolds of different LLM families. By iteratively refining\nprompts through in-context learning, our system uncovers subtle distinctions\nbetween model outputs, providing a powerful tool for LLM analysis and\nverification. This research opens new avenues for understanding LLM behavior\nand has significant implications for model attribution, security, and the\nbroader field of AI transparency.\n","authors":["Dmitri Iourovitski","Sanat Sharma","Rakshak Talwar"],"pdf_url":"https://arxiv.org/pdf/2408.02871v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00965v2","updated":"2024-08-06T00:12:50Z","published":"2024-08-02T00:58:01Z","title":"Integrating ESG and AI: A Comprehensive Responsible AI Assessment\n  Framework","summary":"  Artificial Intelligence (AI) is a widely developed and adopted technology\nacross entire industry sectors. Integrating environmental, social, and\ngovernance (ESG) considerations with AI investments is crucial for ensuring\nethical and sustainable technological advancement. Particularly from an\ninvestor perspective, this integration not only mitigates risks but also\nenhances long-term value creation by aligning AI initiatives with broader\nsocietal goals. Yet, this area has been less explored in both academia and\nindustry. To bridge the gap, we introduce a novel ESG-AI framework, which is\ndeveloped based on insights from engagements with 28 companies and comprises\nthree key components. The framework provides a structured approach to this\nintegration, developed in collaboration with industry practitioners. The ESG-AI\nframework provides an overview of the environmental and social impacts of AI\napplications, helping users such as investors assess the materiality of AI use.\nMoreover, it enables investors to evaluate a company's commitment to\nresponsible AI through structured engagements and thorough assessment of\nspecific risk areas. We have publicly released the framework and toolkit in\nApril 2024, which has received significant attention and positive feedback from\nthe investment community. This paper details each component of the framework,\ndemonstrating its applicability in real-world contexts and its potential to\nguide ethical AI investments.\n","authors":["Sung Une Lee","Harsha Perera","Yue Liu","Boming Xia","Qinghua Lu","Liming Zhu","Jessica Cairns","Moana Nottage"],"pdf_url":"https://arxiv.org/pdf/2408.00965v2.pdf","comment":"23 pages, 8 tables, 10 figures"},{"id":"http://arxiv.org/abs/2401.00391v3","updated":"2024-08-06T23:58:07Z","published":"2023-12-31T04:14:43Z","title":"SAFE-SIM: Safety-Critical Closed-Loop Traffic Simulation with\n  Diffusion-Controllable Adversaries","summary":"  Evaluating the performance of autonomous vehicle planning algorithms\nnecessitates simulating long-tail safety-critical traffic scenarios. However,\ntraditional methods for generating such scenarios often fall short in terms of\ncontrollability and realism; they also neglect the dynamics of agent\ninteractions. To address these limitations, we introduce SAFE-SIM, a novel\ndiffusion-based controllable closed-loop safety-critical simulation framework.\nOur approach yields two distinct advantages: 1) generating realistic long-tail\nsafety-critical scenarios that closely reflect real-world conditions, and 2)\nproviding controllable adversarial behavior for more comprehensive and\ninteractive evaluations. We develop a novel approach to simulate\nsafety-critical scenarios through an adversarial term in the denoising process\nof diffusion models, which allows an adversarial agent to challenge a planner\nwith plausible maneuvers while all agents in the scene exhibit reactive and\nrealistic behaviors. Furthermore, we propose novel guidance objectives and a\npartial diffusion process that enables users to control key aspects of the\nscenarios, such as the collision type and aggressiveness of the adversarial\nagent, while maintaining the realism of the behavior. We validate our framework\nempirically using the nuScenes and nuPlan datasets across multiple planners,\ndemonstrating improvements in both realism and controllability. These findings\naffirm that diffusion models provide a robust and versatile foundation for\nsafety-critical, interactive traffic simulation, extending their utility across\nthe broader autonomous driving landscape. Project website:\nhttps://safe-sim.github.io/.\n","authors":["Wei-Jer Chang","Francesco Pittaluga","Masayoshi Tomizuka","Wei Zhan","Manmohan Chandraker"],"pdf_url":"https://arxiv.org/pdf/2401.00391v3.pdf","comment":"Accepted by ECCV2024; Project website: https://safe-sim.github.io/"},{"id":"http://arxiv.org/abs/2408.03475v1","updated":"2024-08-06T23:14:39Z","published":"2024-08-06T23:14:39Z","title":"Can LLMs Serve As Time Series Anomaly Detectors?","summary":"  An emerging topic in large language models (LLMs) is their application to\ntime series forecasting, characterizing mainstream and patternable\ncharacteristics of time series. A relevant but rarely explored and more\nchallenging question is whether LLMs can detect and explain time series\nanomalies, a critical task across various real-world applications. In this\npaper, we investigate the capabilities of LLMs, specifically GPT-4 and LLaMA3,\nin detecting and explaining anomalies in time series. Our studies reveal that:\n1) LLMs cannot be directly used for time series anomaly detection. 2) By\ndesigning prompt strategies such as in-context learning and chain-of-thought\nprompting, GPT-4 can detect time series anomalies with results competitive to\nbaseline methods. 3) We propose a synthesized dataset to automatically generate\ntime series anomalies with corresponding explanations. By applying instruction\nfine-tuning on this dataset, LLaMA3 demonstrates improved performance in time\nseries anomaly detection tasks. In summary, our exploration shows the promising\npotential of LLMs as time series anomaly detectors.\n","authors":["Manqing Dong","Hao Huang","Longbing Cao"],"pdf_url":"https://arxiv.org/pdf/2408.03475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03463v1","updated":"2024-08-06T22:38:14Z","published":"2024-08-06T22:38:14Z","title":"Identifying treatment response subgroups in observational time-to-event\n  data","summary":"  Identifying patient subgroups with different treatment responses is an\nimportant task to inform medical recommendations, guidelines, and the design of\nfuture clinical trials. Existing approaches for subgroup analysis primarily\nfocus on Randomised Controlled Trials (RCTs), in which treatment assignment is\nrandomised. Furthermore, the patient cohort of an RCT is often constrained by\ncost, and is not representative of the heterogeneity of patients likely to\nreceive treatment in real-world clinical practice. Therefore, when applied to\nobservational studies, such approaches suffer from significant statistical\nbiases because of the non-randomisation of treatment. Our work introduces a\nnovel, outcome-guided method for identifying treatment response subgroups in\nobservational studies. Our approach assigns each patient to a subgroup\nassociated with two time-to-event distributions: one under treatment and one\nunder control regime. It hence positions itself in between individualised and\naverage treatment effect estimation. The assumptions of our model result in a\nsimple correction of the statistical bias from treatment non-randomisation\nthrough inverse propensity weighting. In experiments, our approach\nsignificantly outperforms the current state-of-the-art method for\noutcome-guided subgroup analysis in both randomised and observational treatment\nregimes.\n","authors":["Vincent Jeanselme","Chang Ho Yoon","Fabian Falck","Brian Tom","Jessica Barrett"],"pdf_url":"https://arxiv.org/pdf/2408.03463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11704v2","updated":"2024-08-06T22:37:06Z","published":"2024-06-17T16:25:04Z","title":"Nemotron-4 340B Technical Report","summary":"  We release the Nemotron-4 340B model family, including Nemotron-4-340B-Base,\nNemotron-4-340B-Instruct, and Nemotron-4-340B-Reward. Our models are open\naccess under the NVIDIA Open Model License Agreement, a permissive model\nlicense that allows distribution, modification, and use of the models and its\noutputs. These models perform competitively to open access models on a wide\nrange of evaluation benchmarks, and were sized to fit on a single DGX H100 with\n8 GPUs when deployed in FP8 precision. We believe that the community can\nbenefit from these models in various research studies and commercial\napplications, especially for generating synthetic data to train smaller\nlanguage models. Notably, over 98% of data used in our model alignment process\nis synthetically generated, showcasing the effectiveness of these models in\ngenerating synthetic data. To further support open research and facilitate\nmodel development, we are also open-sourcing the synthetic data generation\npipeline used in our model alignment process.\n","authors":[" Nvidia"," :","Bo Adler","Niket Agarwal","Ashwath Aithal","Dong H. Anh","Pallab Bhattacharya","Annika Brundyn","Jared Casper","Bryan Catanzaro","Sharon Clay","Jonathan Cohen","Sirshak Das","Ayush Dattagupta","Olivier Delalleau","Leon Derczynski","Yi Dong","Daniel Egert","Ellie Evans","Aleksander Ficek","Denys Fridman","Shaona Ghosh","Boris Ginsburg","Igor Gitman","Tomasz Grzegorzek","Robert Hero","Jining Huang","Vibhu Jawa","Joseph Jennings","Aastha Jhunjhunwala","John Kamalu","Sadaf Khan","Oleksii Kuchaiev","Patrick LeGresley","Hui Li","Jiwei Liu","Zihan Liu","Eileen Long","Ameya Sunil Mahabaleshwarkar","Somshubra Majumdar","James Maki","Miguel Martinez","Maer Rodrigues de Melo","Ivan Moshkov","Deepak Narayanan","Sean Narenthiran","Jesus Navarro","Phong Nguyen","Osvald Nitski","Vahid Noroozi","Guruprasad Nutheti","Christopher Parisien","Jupinder Parmar","Mostofa Patwary","Krzysztof Pawelec","Wei Ping","Shrimai Prabhumoye","Rajarshi Roy","Trisha Saar","Vasanth Rao Naik Sabavat","Sanjeev Satheesh","Jane Polak Scowcroft","Jason Sewall","Pavel Shamis","Gerald Shen","Mohammad Shoeybi","Dave Sizer","Misha Smelyanskiy","Felipe Soares","Makesh Narsimhan Sreedhar","Dan Su","Sandeep Subramanian","Shengyang Sun","Shubham Toshniwal","Hao Wang","Zhilin Wang","Jiaxuan You","Jiaqi Zeng","Jimmy Zhang","Jing Zhang","Vivienne Zhang","Yian Zhang","Chen Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.11704v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18742v5","updated":"2024-08-06T22:33:26Z","published":"2024-03-27T16:39:28Z","title":"Understanding the Learning Dynamics of Alignment with Human Feedback","summary":"  Aligning large language models (LLMs) with human intentions has become a\ncritical task for safely deploying models in real-world systems. While existing\nalignment approaches have seen empirical success, theoretically understanding\nhow these methods affect model behavior remains an open question. Our work\nprovides an initial attempt to theoretically analyze the learning dynamics of\nhuman preference alignment. We formally show how the distribution of preference\ndatasets influences the rate of model updates and provide rigorous guarantees\non the training accuracy. Our theory also reveals an intricate phenomenon where\nthe optimization is prone to prioritizing certain behaviors with higher\npreference distinguishability. We empirically validate our findings on\ncontemporary LLMs and alignment tasks, reinforcing our theoretical insights and\nshedding light on considerations for future alignment approaches. Disclaimer:\nThis paper contains potentially offensive text; reader discretion is advised.\n","authors":["Shawn Im","Yixuan Li"],"pdf_url":"https://arxiv.org/pdf/2403.18742v5.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2305.09958v3","updated":"2024-08-06T02:32:05Z","published":"2023-05-17T05:35:49Z","title":"SIGMA: Similarity-based Efficient Global Aggregation for Heterophilous\n  Graph Neural Networks","summary":"  Graph neural networks (GNNs) realize great success in graph learning but\nsuffer from performance loss when meeting heterophily, i.e. neighboring nodes\nare dissimilar, due to their local and uniform aggregation. Existing attempts\nof heterophilous GNNs incorporate long-range or global aggregations to\ndistinguish nodes in the graph. However, these aggregations usually require\niteratively maintaining and updating full-graph information, which limits their\nefficiency when applying to large-scale graphs. In this paper, we propose\nSIGMA, an efficient global heterophilous GNN aggregation integrating the\nstructural similarity measurement SimRank. Our theoretical analysis illustrates\nthat SIGMA inherently captures distant global similarity even under\nheterophily, that conventional approaches can only achieve after iterative\naggregations. Furthermore, it enjoys efficient one-time computation with a\ncomplexity only linear to the node set size $\\mathcal{O}(n)$. Comprehensive\nevaluation demonstrates that SIGMA achieves state-of-the-art performance with\nsuperior aggregation and overall efficiency. Notably, it obtains 5$\\times$\nacceleration on the large-scale heterophily dataset \\emph{pokec} with over 30\nmillion edges compared to the best baseline aggregation.\n","authors":["Haoyu Liu","Ningyi Liao","Siqiang Luo"],"pdf_url":"https://arxiv.org/pdf/2305.09958v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07950v3","updated":"2024-08-06T02:42:32Z","published":"2024-03-18T16:50:23Z","title":"Reinforcement Learning with Generalizable Gaussian Splatting","summary":"  An excellent representation is crucial for reinforcement learning (RL)\nperformance, especially in vision-based reinforcement learning tasks. The\nquality of the environment representation directly influences the achievement\nof the learning task. Previous vision-based RL typically uses explicit or\nimplicit ways to represent environments, such as images, points, voxels, and\nneural radiance fields. However, these representations contain several\ndrawbacks. They cannot either describe complex local geometries or generalize\nwell to unseen scenes, or require precise foreground masks. Moreover, these\nimplicit neural representations are akin to a ``black box\", significantly\nhindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit\nscene representation and differentiable rendering nature, is considered a\nrevolutionary change for reconstruction and representation methods. In this\npaper, we propose a novel Generalizable Gaussian Splatting framework to be the\nrepresentation of RL tasks, called GSRL. Through validation in the RoboMimic\nenvironment, our method achieves better results than other baselines in\nmultiple tasks, improving the performance by 10%, 44%, and 15% compared with\nbaselines on the hardest task. This work is the first attempt to leverage\ngeneralizable 3DGS as a representation for RL.\n","authors":["Jiaxu Wang","Qiang Zhang","Jingkai Sun","Jiahang Cao","Gang Han","Wen Zhao","Weining Zhang","Yecheng Shao","Yijie Guo","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2404.07950v3.pdf","comment":"7 pages,2 figures"},{"id":"http://arxiv.org/abs/2408.03323v1","updated":"2024-08-06T17:58:29Z","published":"2024-08-06T17:58:29Z","title":"ClassiFIM: An Unsupervised Method To Detect Phase Transitions","summary":"  Estimation of the Fisher Information Metric (FIM-estimation) is an important\ntask that arises in unsupervised learning of phase transitions, a problem\nproposed by physicists. This work completes the definition of the task by\ndefining rigorous evaluation metrics distMSE, distMSEPS, and distRE and\nintroduces ClassiFIM, a novel machine learning method designed to solve the\nFIM-estimation task. Unlike existing methods for unsupervised learning of phase\ntransitions, ClassiFIM directly estimates a well-defined quantity (the FIM),\nallowing it to be rigorously compared to any present and future other methods\nthat estimate the same. ClassiFIM transforms a dataset for the FIM-estimation\ntask into a dataset for an auxiliary binary classification task and involves\nselecting and training a model for the latter. We prove that the output of\nClassiFIM approaches the exact FIM in the limit of infinite dataset size and\nunder certain regularity conditions. We implement ClassiFIM on multiple\ndatasets, including datasets describing classical and quantum phase\ntransitions, and find that it achieves a good ground truth approximation with\nmodest computational resources. Furthermore, we independently implement two\nalternative state-of-the-art methods for unsupervised estimation of phase\ntransition locations on the same datasets and find that ClassiFIM predicts such\nlocations at least as well as these other methods. To emphasize the generality\nof our method, we also propose and generate the MNIST-CNN dataset, which\nconsists of the output of CNNs trained on MNIST for different hyperparameter\nchoices. Using ClassiFIM on this dataset suggests there is a phase transition\nin the distribution of image-prediction pairs for CNNs trained on MNIST,\ndemonstrating the broad scope of FIM-estimation beyond physics.\n","authors":["Victor Kasatkin","Evgeny Mozgunov","Nicholas Ezzell","Utkarsh Mishra","Itay Hen","Daniel Lidar"],"pdf_url":"https://arxiv.org/pdf/2408.03323v1.pdf","comment":"23 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.21770v2","updated":"2024-08-06T17:57:41Z","published":"2024-07-31T17:46:51Z","title":"MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware\n  Experts","summary":"  We introduce MoMa, a novel modality-aware mixture-of-experts (MoE)\narchitecture designed for pre-training mixed-modal, early-fusion language\nmodels. MoMa processes images and text in arbitrary sequences by dividing\nexpert modules into modality-specific groups. These groups exclusively process\ndesignated tokens while employing learned routing within each group to maintain\nsemantically informed adaptivity. Our empirical results reveal substantial\npre-training efficiency gains through this modality-specific parameter\nallocation. Under a 1-trillion-token training budget, the MoMa 1.4B model,\nfeaturing 4 text experts and 4 image experts, achieves impressive FLOPs\nsavings: 3.7x overall, with 2.6x for text and 5.2x for image processing\ncompared to a compute-equivalent dense baseline, measured by pre-training loss.\nThis outperforms the standard expert-choice MoE with 8 mixed-modal experts,\nwhich achieves 3x overall FLOPs savings (3x for text, 2.8x for image).\nCombining MoMa with mixture-of-depths (MoD) further improves pre-training FLOPs\nsavings to 4.2x overall (text: 3.4x, image: 5.3x), although this combination\nhurts performance in causal inference due to increased sensitivity to router\naccuracy. These results demonstrate MoMa's potential to significantly advance\nthe efficiency of mixed-modal, early-fusion language model pre-training, paving\nthe way for more resource-efficient and capable multimodal AI systems.\n","authors":["Xi Victoria Lin","Akshat Shrivastava","Liang Luo","Srinivasan Iyer","Mike Lewis","Gargi Gosh","Luke Zettlemoyer","Armen Aghajanyan"],"pdf_url":"https://arxiv.org/pdf/2407.21770v2.pdf","comment":"v2 -> update related work section"},{"id":"http://arxiv.org/abs/2408.03320v1","updated":"2024-08-06T17:55:58Z","published":"2024-08-06T17:55:58Z","title":"Hedge Fund Portfolio Construction Using PolyModel Theory and\n  iTransformer","summary":"  When constructing portfolios, a key problem is that a lot of financial time\nseries data are sparse, making it challenging to apply machine learning\nmethods. Polymodel theory can solve this issue and demonstrate superiority in\nportfolio construction from various aspects. To implement the PolyModel theory\nfor constructing a hedge fund portfolio, we begin by identifying an asset pool,\nutilizing over 10,000 hedge funds for the past 29 years' data. PolyModel theory\nalso involves choosing a wide-ranging set of risk factors, which includes\nvarious financial indices, currencies, and commodity prices. This comprehensive\nselection mirrors the complexities of the real-world environment. Leveraging on\nthe PolyModel theory, we create quantitative measures such as Long-term Alpha,\nLong-term Ratio, and SVaR. We also use more classical measures like the Sharpe\nratio or Morningstar's MRAR. To enhance the performance of the constructed\nportfolio, we also employ the latest deep learning techniques (iTransformer) to\ncapture the upward trend, while efficiently controlling the downside, using all\nthe features. The iTransformer model is specifically designed to address the\nchallenges in high-dimensional time series forecasting and could largely\nimprove our strategies. More precisely, our strategies achieve better Sharpe\nratio and annualized return. The above process enables us to create multiple\nportfolio strategies aiming for high returns and low risks when compared to\nvarious benchmarks.\n","authors":["Siqiao Zhao","Zhikang Dong","Zeyu Cao","Raphael Douady"],"pdf_url":"https://arxiv.org/pdf/2408.03320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.17403v2","updated":"2024-08-06T17:41:52Z","published":"2023-09-29T17:04:06Z","title":"Maximal Volume Matrix Cross Approximation for Image Compression and\n  Least Squares Solution","summary":"  We study the classic matrix cross approximation based on the maximal volume\nsubmatrices. Our main results consist of an improvement of the classic estimate\nfor matrix cross approximation and a greedy approach for finding the maximal\nvolume submatrices. More precisely, we present a new proof of the classic\nestimate of the inequality with an improved constant. Also, we present a family\nof greedy maximal volume algorithms to improve the computational efficiency of\nmatrix cross approximation. The proposed algorithms are shown to have\ntheoretical guarantees of convergence. Finally, we present two applications:\nimage compression and the least squares approximation of continuous functions.\nOur numerical results at the end of the paper demonstrate the effective\nperformance of our approach.\n","authors":["Kenneth Allen","Ming-Jun Lai","Zhaiming Shen"],"pdf_url":"https://arxiv.org/pdf/2309.17403v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00035v4","updated":"2024-08-06T17:36:06Z","published":"2024-01-08T12:19:46Z","title":"Robustness Assessment of a Runway Object Classifier for Safe Aircraft\n  Taxiing","summary":"  As deep neural networks (DNNs) are becoming the prominent solution for many\ncomputational problems, the aviation industry seeks to explore their potential\nin alleviating pilot workload and in improving operational safety. However, the\nuse of DNNs in this type of safety-critical applications requires a thorough\ncertification process. This need can be addressed through formal verification,\nwhich provides rigorous assurances -- e.g.,~by proving the absence of certain\nmispredictions. In this case-study paper, we demonstrate this process using an\nimage-classifier DNN currently under development at Airbus and intended for use\nduring the aircraft taxiing phase. We use formal methods to assess this DNN's\nrobustness to three common image perturbation types: noise, brightness and\ncontrast, and some of their combinations. This process entails multiple\ninvocations of the underlying verifier, which might be computationally\nexpensive; and we therefore propose a method that leverages the monotonicity of\nthese robustness properties, as well as the results of past verification\nqueries, in order to reduce the overall number of verification queries required\nby nearly 60%. Our results provide an indication of the level of robustness\nachieved by the DNN classifier under study, and indicate that it is\nconsiderably more vulnerable to noise than to brightness or contrast\nperturbations.\n","authors":["Yizhak Elboher","Raya Elsaleh","Omri Isac","Mélanie Ducoffe","Audrey Galametz","Guillaume Povéda","Ryma Boumazouza","Noémie Cohen","Guy Katz"],"pdf_url":"https://arxiv.org/pdf/2402.00035v4.pdf","comment":"This is a preprint version of the paper in the proceedings of 43rd\n  Digital Avionics Systems Conference (DASC)"},{"id":"http://arxiv.org/abs/2408.03314v1","updated":"2024-08-06T17:35:05Z","published":"2024-08-06T17:35:05Z","title":"Scaling LLM Test-Time Compute Optimally can be More Effective than\n  Scaling Model Parameters","summary":"  Enabling LLMs to improve their outputs by using more test-time computation is\na critical step towards building generally self-improving agents that can\noperate on open-ended natural language. In this paper, we study the scaling of\ninference-time computation in LLMs, with a focus on answering the question: if\nan LLM is allowed to use a fixed but non-trivial amount of inference-time\ncompute, how much can it improve its performance on a challenging prompt?\nAnswering this question has implications not only on the achievable performance\nof LLMs, but also on the future of LLM pretraining and how one should tradeoff\ninference-time and pre-training compute. Despite its importance, little\nresearch attempted to understand the scaling behaviors of various test-time\ninference methods. Moreover, current work largely provides negative results for\na number of these strategies. In this work, we analyze two primary mechanisms\nto scale test-time computation: (1) searching against dense, process-based\nverifier reward models; and (2) updating the model's distribution over a\nresponse adaptively, given the prompt at test time. We find that in both cases,\nthe effectiveness of different approaches to scaling test-time compute\ncritically varies depending on the difficulty of the prompt. This observation\nmotivates applying a \"compute-optimal\" scaling strategy, which acts to most\neffectively allocate test-time compute adaptively per prompt. Using this\ncompute-optimal strategy, we can improve the efficiency of test-time compute\nscaling by more than 4x compared to a best-of-N baseline. Additionally, in a\nFLOPs-matched evaluation, we find that on problems where a smaller base model\nattains somewhat non-trivial success rates, test-time compute can be used to\noutperform a 14x larger model.\n","authors":["Charlie Snell","Jaehoon Lee","Kelvin Xu","Aviral Kumar"],"pdf_url":"https://arxiv.org/pdf/2408.03314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03720v3","updated":"2024-08-06T17:26:55Z","published":"2023-10-05T17:40:09Z","title":"SteP: Stacked LLM Policies for Web Actions","summary":"  Performing tasks on the web presents fundamental challenges to large language\nmodels (LLMs), including combinatorially large open-world tasks and variations\nacross web interfaces. Simply specifying a large prompt to handle all possible\nbehaviors and states is extremely complex, and results in behavior leaks\nbetween unrelated behaviors. Decomposition to distinct policies can address\nthis challenge, but requires carefully handing off control between policies. We\npropose Stacked LLM Policies for Web Actions (SteP), an approach to dynamically\ncompose policies to solve a diverse set of web tasks. SteP defines a Markov\nDecision Process where the state is a stack of policies representing the\ncontrol state, i.e., the chain of policy calls. Unlike traditional methods that\nare restricted to static hierarchies, SteP enables dynamic control that adapts\nto the complexity of the task. We evaluate SteP against multiple baselines and\nweb environments including WebArena, MiniWoB++, and a CRM. On WebArena, SteP\nimproves (14.9\\% to 33.5\\%) over SOTA that use GPT-4 policies, while on\nMiniWob++, SteP is competitive with prior works while using significantly less\ndata. Our code and data are available at\nhttps://asappresearch.github.io/webagents-step.\n","authors":["Paloma Sodhi","S. R. K. Branavan","Yoav Artzi","Ryan McDonald"],"pdf_url":"https://arxiv.org/pdf/2310.03720v3.pdf","comment":"Accepted at Conference on Language Modeling (COLM) 2024. 30 pages, 15\n  figures"},{"id":"http://arxiv.org/abs/2408.03307v1","updated":"2024-08-06T17:16:10Z","published":"2024-08-06T17:16:10Z","title":"Pre-training and in-context learning IS Bayesian inference a la De\n  Finetti","summary":"  Accurately gauging uncertainty on the underlying environment is a\nlongstanding goal of intelligent systems. We characterize which latent concepts\npre-trained sequence models are naturally able to reason with. We go back to De\nFinetti's predictive view of Bayesian reasoning: instead of modeling latent\nparameters through priors and likelihoods like topic models do, De Finetti has\nlong advocated for modeling exchangeable (permutation invariant) sequences of\nobservables. According to this view, pre-training autoregressive models\nformulates informed beliefs based on prior observations (\"empirical Bayes\"),\nand forward generation is a simulated instantiation of an environment\n(\"posterior inference\"). This connection allows extending in-context learning\n(ICL) beyond predictive settings, highlighting sequence models' ability to\nperform explicit statistical inference. In particular, we show the sequence\nprediction loss over exchangeable documents controls performance on downstream\ntasks where uncertainty quantification is key. Empirically, we propose and\ndemonstrate several approaches for encoding exchangeability in sequence model\narchitectures: data augmentation, regularization, and causal masking.\n","authors":["Naimeng Ye","Hanming Yang","Andrew Siah","Hongseok Namkoong"],"pdf_url":"https://arxiv.org/pdf/2408.03307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03304v1","updated":"2024-08-06T17:11:40Z","published":"2024-08-06T17:11:40Z","title":"Fusing Forces: Deep-Human-Guided Refinement of Segmentation Masks","summary":"  Etruscan mirrors constitute a significant category in Etruscan art,\ncharacterized by elaborate figurative illustrations featured on their backside.\nA laborious and costly aspect of their analysis and documentation is the task\nof manually tracing these illustrations. In previous work, a methodology has\nbeen proposed to automate this process, involving photometric-stereo scanning\nin combination with deep neural networks. While achieving quantitative\nperformance akin to an expert annotator, some results still lack qualitative\nprecision and, thus, require annotators for inspection and potential\ncorrection, maintaining resource intensity. In response, we propose a deep\nneural network trained to interactively refine existing annotations based on\nhuman guidance. Our human-in-the-loop approach streamlines annotation,\nachieving equal quality with up to 75% less manual input required. Moreover,\nduring the refinement process, the relative improvement of our methodology over\npure manual labeling reaches peak values of up to 26%, attaining drastically\nbetter quality quicker. By being tailored to the complex task of segmenting\nintricate lines, specifically distinguishing it from previous methods, our\napproach offers drastic improvements in efficacy, transferable to a broad\nspectrum of applications beyond Etruscan mirrors.\n","authors":["Rafael Sterzinger","Christian Stippel","Robert Sablatnig"],"pdf_url":"https://arxiv.org/pdf/2408.03304v1.pdf","comment":"16 pages, accepted at ICPR2024"},{"id":"http://arxiv.org/abs/2304.05339v2","updated":"2024-08-06T17:09:59Z","published":"2023-04-11T16:58:59Z","title":"Deep-learning Assisted Detection and Quantification of (oo)cysts of\n  Giardia and Cryptosporidium on Smartphone Microscopy Images","summary":"  The consumption of microbial-contaminated food and water is responsible for\nthe deaths of millions of people annually. Smartphone-based microscopy systems\nare portable, low-cost, and more accessible alternatives for the detection of\nGiardia and Cryptosporidium than traditional brightfield microscopes. However,\nthe images from smartphone microscopes are noisier and require manual cyst\nidentification by trained technicians, usually unavailable in resource-limited\nsettings. Automatic detection of (oo)cysts using deep-learning-based object\ndetection could offer a solution for this limitation. We evaluate the\nperformance of four state-of-the-art object detectors to detect (oo)cysts of\nGiardia and Cryptosporidium on a custom dataset that includes both smartphone\nand brightfield microscopic images from vegetable samples. Faster RCNN,\nRetinaNet, You Only Look Once (YOLOv8s), and Deformable Detection Transformer\n(Deformable DETR) deep-learning models were employed to explore their efficacy\nand limitations. Our results show that while the deep-learning models perform\nbetter with the brightfield microscopy image dataset than the smartphone\nmicroscopy image dataset, the smartphone microscopy predictions are still\ncomparable to the prediction performance of non-experts. Also, we publicly\nrelease brightfield and smartphone microscopy datasets with the benchmark\nresults for the detection of Giardia and Cryptosporidium, independently\ncaptured on reference (or standard lab setting) and vegetable samples. Our code\nand dataset are available at\nhttps://github.com/naamiinepal/smartphone_microscopy and\nhttps://doi.org/10.5281/zenodo.7813183, respectively.\n","authors":["Suprim Nakarmi","Sanam Pudasaini","Safal Thapaliya","Pratima Upretee","Retina Shrestha","Basant Giri","Bhanu Bhakta Neupane","Bishesh Khanal"],"pdf_url":"https://arxiv.org/pdf/2304.05339v2.pdf","comment":"21 pages (including supplementary information), 5 figures, 7 tables,\n  Accepted for publication at the Journal of Machine Learning for Biomedical\n  Imaging (MELBA) https://melba-journal.org/2024:014"},{"id":"http://arxiv.org/abs/2408.03290v1","updated":"2024-08-06T16:39:42Z","published":"2024-08-06T16:39:42Z","title":"SARA: Singular-Value Based Adaptive Low-Rank Adaption","summary":"  With the increasing number of parameters in large pre-trained models, LoRA as\na parameter-efficient fine-tuning(PEFT) method is widely used for not adding\ninference overhead. The LoRA method assumes that weight changes during\nfine-tuning can be approximated by low-rank matrices. However, the rank values\nneed to be manually verified to match different downstream tasks, and they\ncannot accommodate the varying importance of different layers in the model. In\nthis work, we first analyze the relationship between the performance of\ndifferent layers and their ranks using SVD. Based on this, we design the\nSingular-Value Based Adaptive Low-Rank Adaption(SARA), which adaptively finds\nthe rank during initialization by performing SVD on the pre-trained weights.\nAdditionally, we explore the Mixture-of-SARA(Mo-SARA), which significantly\nreduces the number of parameters by fine-tuning only multiple parallel sets of\nsingular values controlled by a router. Extensive experiments on various\ncomplex tasks demonstrate the simplicity and parameter efficiency of our\nmethods. They can effectively and adaptively find the most suitable rank for\neach layer of each model.\n","authors":["Jihao Gu","Shuai Chen","Zelin Wang","Yibo Zhang","Ping Gong"],"pdf_url":"https://arxiv.org/pdf/2408.03290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08871v3","updated":"2024-08-06T16:38:41Z","published":"2024-02-14T00:35:10Z","title":"Position: Topological Deep Learning is the New Frontier for Relational\n  Learning","summary":"  Topological deep learning (TDL) is a rapidly evolving field that uses\ntopological features to understand and design deep learning models. This paper\nposits that TDL is the new frontier for relational learning. TDL may complement\ngraph representation learning and geometric deep learning by incorporating\ntopological concepts, and can thus provide a natural choice for various machine\nlearning settings. To this end, this paper discusses open problems in TDL,\nranging from practical benefits to theoretical foundations. For each problem,\nit outlines potential solutions and future research opportunities. At the same\ntime, this paper serves as an invitation to the scientific community to\nactively participate in TDL research to unlock the potential of this emerging\nfield.\n","authors":["Theodore Papamarkou","Tolga Birdal","Michael Bronstein","Gunnar Carlsson","Justin Curry","Yue Gao","Mustafa Hajij","Roland Kwitt","Pietro Liò","Paolo Di Lorenzo","Vasileios Maroulas","Nina Miolane","Farzana Nasrin","Karthikeyan Natesan Ramamurthy","Bastian Rieck","Simone Scardapane","Michael T. Schaub","Petar Veličković","Bei Wang","Yusu Wang","Guo-Wei Wei","Ghada Zamzmi"],"pdf_url":"https://arxiv.org/pdf/2402.08871v3.pdf","comment":"Proceedings of the 41st International Conference on Machine Learning,\n  Vienna, Austria. PMLR 235, 2024"},{"id":"http://arxiv.org/abs/2408.03287v1","updated":"2024-08-06T16:35:25Z","published":"2024-08-06T16:35:25Z","title":"Malicious Internet Entity Detection Using Local Graph Inference","summary":"  Detection of malicious behavior in a large network is a challenging problem\nfor machine learning in computer security, since it requires a model with high\nexpressive power and scalable inference. Existing solutions struggle to achieve\nthis feat -- current cybersec-tailored approaches are still limited in\nexpressivity, and methods successful in other domains do not scale well for\nlarge volumes of data, rendering frequent retraining impossible. This work\nproposes a new perspective for learning from graph data that is modeling\nnetwork entity interactions as a large heterogeneous graph. High expressivity\nof the method is achieved with neural network architecture HMILnet that\nnaturally models this type of data and provides theoretical guarantees. The\nscalability is achieved by pursuing local graph inference, i.e., classifying\nindividual vertices and their neighborhood as independent samples. Our\nexperiments exhibit improvement over the state-of-the-art Probabilistic Threat\nPropagation (PTP) algorithm, show a further threefold accuracy improvement when\nadditional data is used, which is not possible with the PTP algorithm, and\ndemonstrate the generalization capabilities of the method to new, previously\nunseen entities.\n","authors":["Simon Mandlik","Tomas Pevny","Vaclav Smidl","Lukas Bajer"],"pdf_url":"https://arxiv.org/pdf/2408.03287v1.pdf","comment":"A preprint. Full publication:\n  https://ieeexplore.ieee.org/document/10418120"},{"id":"http://arxiv.org/abs/2402.00809v5","updated":"2024-08-06T16:32:38Z","published":"2024-02-01T17:45:26Z","title":"Position: Bayesian Deep Learning is Needed in the Age of Large-Scale AI","summary":"  In the current landscape of deep learning research, there is a predominant\nemphasis on achieving high predictive accuracy in supervised tasks involving\nlarge image and language datasets. However, a broader perspective reveals a\nmultitude of overlooked metrics, tasks, and data types, such as uncertainty,\nactive and continual learning, and scientific data, that demand attention.\nBayesian deep learning (BDL) constitutes a promising avenue, offering\nadvantages across these diverse settings. This paper posits that BDL can\nelevate the capabilities of deep learning. It revisits the strengths of BDL,\nacknowledges existing challenges, and highlights some exciting research avenues\naimed at addressing these obstacles. Looking ahead, the discussion focuses on\npossible ways to combine large-scale foundation models with BDL to unlock their\nfull potential.\n","authors":["Theodore Papamarkou","Maria Skoularidou","Konstantina Palla","Laurence Aitchison","Julyan Arbel","David Dunson","Maurizio Filippone","Vincent Fortuin","Philipp Hennig","José Miguel Hernández-Lobato","Aliaksandr Hubin","Alexander Immer","Theofanis Karaletsos","Mohammad Emtiyaz Khan","Agustinus Kristiadi","Yingzhen Li","Stephan Mandt","Christopher Nemeth","Michael A. Osborne","Tim G. J. Rudner","David Rügamer","Yee Whye Teh","Max Welling","Andrew Gordon Wilson","Ruqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.00809v5.pdf","comment":"Proceedings of the 41st International Conference on Machine Learning,\n  Vienna, Austria. PMLR 235, 2024"},{"id":"http://arxiv.org/abs/2408.03281v1","updated":"2024-08-06T16:28:30Z","published":"2024-08-06T16:28:30Z","title":"StructEval: Deepen and Broaden Large Language Model Assessment via\n  Structured Evaluation","summary":"  Evaluation is the baton for the development of large language models. Current\nevaluations typically employ a single-item assessment paradigm for each atomic\ntest objective, which struggles to discern whether a model genuinely possesses\nthe required capabilities or merely memorizes/guesses the answers to specific\nquestions. To this end, we propose a novel evaluation framework referred to as\nStructEval. Starting from an atomic test objective, StructEval deepens and\nbroadens the evaluation by conducting a structured assessment across multiple\ncognitive levels and critical concepts, and therefore offers a comprehensive,\nrobust and consistent evaluation for LLMs. Experiments on three widely-used\nbenchmarks demonstrate that StructEval serves as a reliable tool for resisting\nthe risk of data contamination and reducing the interference of potential\nbiases, thereby providing more reliable and consistent conclusions regarding\nmodel capabilities. Our framework also sheds light on the design of future\nprincipled and trustworthy LLM evaluation protocols.\n","authors":["Boxi Cao","Mengjie Ren","Hongyu Lin","Xianpei Han","Feng Zhang","Junfeng Zhan","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2408.03281v1.pdf","comment":"ACL 2024;Benchmark at https://github.com/c-box/StructEval;Leaderboard\n  at https://huggingface.co/spaces/Bowieee/StructEval_leaderboard"},{"id":"http://arxiv.org/abs/2408.03274v1","updated":"2024-08-06T16:17:51Z","published":"2024-08-06T16:17:51Z","title":"Compress and Compare: Interactively Evaluating Efficiency and Behavior\n  Across ML Model Compression Experiments","summary":"  To deploy machine learning models on-device, practitioners use compression\nalgorithms to shrink and speed up models while maintaining their high-quality\noutput. A critical aspect of compression in practice is model comparison,\nincluding tracking many compression experiments, identifying subtle changes in\nmodel behavior, and negotiating complex accuracy-efficiency trade-offs.\nHowever, existing compression tools poorly support comparison, leading to\ntedious and, sometimes, incomplete analyses spread across disjoint tools. To\nsupport real-world comparative workflows, we develop an interactive visual\nsystem called Compress and Compare. Within a single interface, Compress and\nCompare surfaces promising compression strategies by visualizing provenance\nrelationships between compressed models and reveals compression-induced\nbehavior changes by comparing models' predictions, weights, and activations. We\ndemonstrate how Compress and Compare supports common compression analysis tasks\nthrough two case studies, debugging failed compression on generative language\nmodels and identifying compression artifacts in image classification models. We\nfurther evaluate Compress and Compare in a user study with eight compression\nexperts, illustrating its potential to provide structure to compression\nworkflows, help practitioners build intuition about compression, and encourage\nthorough analysis of compression's effect on model behavior. Through these\nevaluations, we identify compression-specific challenges that future visual\nanalytics tools should consider and Compress and Compare visualizations that\nmay generalize to broader model comparison tasks.\n","authors":["Angie Boggust","Venkatesh Sivaraman","Yannick Assogba","Donghao Ren","Dominik Moritz","Fred Hohman"],"pdf_url":"https://arxiv.org/pdf/2408.03274v1.pdf","comment":"Accepted to VIS 2024"},{"id":"http://arxiv.org/abs/2312.03179v4","updated":"2024-08-06T16:11:29Z","published":"2023-12-05T23:05:36Z","title":"CaloQVAE : Simulating high-energy particle-calorimeter interactions\n  using hybrid quantum-classical generative models","summary":"  The Large Hadron Collider's high luminosity era presents major computational\nchallenges in the analysis of collision events. Large amounts of Monte Carlo\n(MC) simulation will be required to constrain the statistical uncertainties of\nthe simulated datasets below these of the experimental data. Modelling of\nhigh-energy particles propagating through the calorimeter section of the\ndetector is the most computationally intensive MC simulation task. We introduce\na technique combining recent advancements in generative models and quantum\nannealing for fast and efficient simulation of high-energy particle-calorimeter\ninteractions.\n","authors":["Sehmimul Hoque","Hao Jia","Abhishek Abhishek","Mojde Fadaie","J. Quetzalcoatl Toledo-Marín","Tiago Vale","Roger G. Melko","Maximilian Swiatlowski","Wojciech T. Fedorko"],"pdf_url":"https://arxiv.org/pdf/2312.03179v4.pdf","comment":"6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2212.03559v3","updated":"2024-08-06T15:56:31Z","published":"2022-12-07T10:19:39Z","title":"GraphLearner: Graph Node Clustering with Fully Learnable Augmentation","summary":"  Contrastive deep graph clustering (CDGC) leverages the power of contrastive\nlearning to group nodes into different clusters. The quality of contrastive\nsamples is crucial for achieving better performance, making augmentation\ntechniques a key factor in the process. However, the augmentation samples in\nexisting methods are always predefined by human experiences, and agnostic from\nthe downstream task clustering, thus leading to high human resource costs and\npoor performance. To overcome these limitations, we propose a Graph Node\nClustering with Fully Learnable Augmentation, termed GraphLearner. It\nintroduces learnable augmentors to generate high-quality and task-specific\naugmented samples for CDGC. GraphLearner incorporates two learnable augmentors\nspecifically designed for capturing attribute and structural information.\nMoreover, we introduce two refinement matrices, including the high-confidence\npseudo-label matrix and the cross-view sample similarity matrix, to enhance the\nreliability of the learned affinity matrix. During the training procedure, we\nnotice the distinct optimization goals for training learnable augmentors and\ncontrastive learning networks. In other words, we should both guarantee the\nconsistency of the embeddings as well as the diversity of the augmented\nsamples. To address this challenge, we propose an adversarial learning\nmechanism within our method. Besides, we leverage a two-stage training strategy\nto refine the high-confidence matrices. Extensive experimental results on six\nbenchmark datasets validate the effectiveness of GraphLearner.The code and\nappendix of GraphLearner are available at\nhttps://github.com/xihongyang1999/GraphLearner on Github.\n","authors":["Xihong Yang","Erxue Min","Ke Liang","Yue Liu","Siwei Wang","Sihang Zhou","Huijun Wu","Xinwang Liu","En Zhu"],"pdf_url":"https://arxiv.org/pdf/2212.03559v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10799v2","updated":"2024-08-06T15:33:20Z","published":"2024-05-17T14:10:24Z","title":"Training Compute Thresholds: Features and Functions in AI Regulation","summary":"  Regulators in the US and EU are using thresholds based on training\ncompute--the number of computational operations used in training--to identify\ngeneral-purpose artificial intelligence (GPAI) models that may pose risks of\nlarge-scale societal harm. We argue that training compute currently is the most\nsuitable metric to identify GPAI models that deserve regulatory oversight and\nfurther scrutiny. Training compute correlates with model capabilities and\nrisks, is quantifiable, can be measured early in the AI lifecycle, and can be\nverified by external actors, among other advantageous features. These features\nmake compute thresholds considerably more suitable than other proposed metrics\nto serve as an initial filter to trigger additional regulatory requirements and\nscrutiny. However, training compute is an imperfect proxy for risk. As such,\ncompute thresholds should not be used in isolation to determine appropriate\nmitigation measures. Instead, they should be used to detect potentially risky\nGPAI models that warrant regulatory oversight, such as through notification\nrequirements, and further scrutiny, such as via model evaluations and risk\nassessments, the results of which may inform which mitigation measures are\nappropriate. In fact, this appears largely consistent with how compute\nthresholds are used today. As GPAI technology and market structures evolve,\nregulators should update compute thresholds and complement them with other\nmetrics into regulatory review processes.\n","authors":["Lennart Heim","Leonie Koessler"],"pdf_url":"https://arxiv.org/pdf/2405.10799v2.pdf","comment":"v2: Major revision of earlier working paper"},{"id":"http://arxiv.org/abs/2405.06605v3","updated":"2024-08-06T15:20:00Z","published":"2024-05-10T17:12:48Z","title":"Calo-VQ: Vector-Quantized Two-Stage Generative Model in Calorimeter\n  Simulation","summary":"  We introduce a novel machine learning method developed for the fast\nsimulation of calorimeter detector response, adapting vector-quantized\nvariational autoencoder (VQ-VAE). Our model adopts a two-stage generation\nstrategy: initially compressing geometry-aware calorimeter data into a discrete\nlatent space, followed by the application of a sequence model to learn and\ngenerate the latent tokens. Extensive experimentation on the Calo-challenge\ndataset underscores the efficiency of our approach, showcasing a remarkable\nimprovement in the generation speed compared with conventional method by a\nfactor of 2000. Remarkably, our model achieves the generation of calorimeter\nshowers within milliseconds. Furthermore, comprehensive quantitative\nevaluations across various metrics are performed to validate physics\nperformance of generation.\n","authors":["Qibin Liu","Chase Shimmin","Xiulong Liu","Eli Shlizerman","Shu Li","Shih-Chieh Hsu"],"pdf_url":"https://arxiv.org/pdf/2405.06605v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.08354v3","updated":"2024-08-06T15:14:36Z","published":"2023-04-17T15:16:10Z","title":"Tool Learning with Foundation Models","summary":"  Humans possess an extraordinary ability to create and utilize tools, allowing\nthem to overcome physical limitations and explore new frontiers. With the\nadvent of foundation models, AI systems have the potential to be equally adept\nin tool use as humans. This paradigm, i.e., tool learning with foundation\nmodels, combines the strengths of specialized tools and foundation models to\nachieve enhanced accuracy, efficiency, and automation in problem-solving.\nDespite its immense potential, there is still a lack of a comprehensive\nunderstanding of key challenges, opportunities, and future endeavors in this\nfield. To this end, we present a systematic investigation of tool learning in\nthis paper. We first introduce the background of tool learning, including its\ncognitive origins, the paradigm shift of foundation models, and the\ncomplementary roles of tools and models. Then we recapitulate existing tool\nlearning research into tool-augmented and tool-oriented learning. We formulate\na general tool learning framework: starting from understanding the user\ninstruction, models should learn to decompose a complex task into several\nsubtasks, dynamically adjust their plan through reasoning, and effectively\nconquer each sub-task by selecting appropriate tools. We also discuss how to\ntrain models for improved tool-use capabilities and facilitate the\ngeneralization in tool learning. Considering the lack of a systematic tool\nlearning evaluation in prior works, we experiment with 18 representative tools\nand show the potential of current foundation models in skillfully utilizing\ntools. Finally, we discuss several open problems that require further\ninvestigation for tool learning. In general, we hope this paper could inspire\nfuture research in integrating tools with foundation models.\n","authors":["Yujia Qin","Shengding Hu","Yankai Lin","Weize Chen","Ning Ding","Ganqu Cui","Zheni Zeng","Yufei Huang","Chaojun Xiao","Chi Han","Yi Ren Fung","Yusheng Su","Huadong Wang","Cheng Qian","Runchu Tian","Kunlun Zhu","Shihao Liang","Xingyu Shen","Bokai Xu","Zhen Zhang","Yining Ye","Bowen Li","Ziwei Tang","Jing Yi","Yuzhang Zhu","Zhenning Dai","Lan Yan","Xin Cong","Yaxi Lu","Weilin Zhao","Yuxiang Huang","Junxi Yan","Xu Han","Xian Sun","Dahai Li","Jason Phang","Cheng Yang","Tongshuang Wu","Heng Ji","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2304.08354v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17216v2","updated":"2024-08-06T15:08:44Z","published":"2024-07-24T12:15:59Z","title":"An Adaptive Second-order Method for a Class of Nonconvex Nonsmooth\n  Composite Optimization","summary":"  This paper explores a specific type of nonconvex sparsity-promoting\nregularization problems, namely those involving $\\ell_p$-norm regularization,\nin conjunction with a twice continuously differentiable loss function. We\npropose a novel second-order algorithm designed to effectively address this\nclass of challenging nonconvex and nonsmooth problems, showcasing several\ninnovative features: (i) The use of an alternating strategy to solve a\nreweighted $\\ell_1$ regularized subproblem and the subspace approximate Newton\nstep. (ii) The reweighted $\\ell_1$ regularized subproblem relies on a convex\napproximation to the nonconvex regularization term, enabling a closed-form\nsolution characterized by the soft-thresholding operator. This feature allows\nour method to be applied to various nonconvex regularization problems. (iii)\nOur algorithm ensures that the iterates maintain their sign values and that\nnonzero components are kept away from 0 for a sufficient number of iterations,\neventually transitioning to a perturbed Newton method. (iv) We provide\ntheoretical guarantees of global convergence, local superlinear convergence in\nthe presence of the Kurdyka-\\L ojasiewicz (KL) property, and local quadratic\nconvergence when employing the exact Newton step in our algorithm. We also\nshowcase the effectiveness of our approach through experiments on a diverse set\nof model prediction problems.\n","authors":["Hao Wang","Xiangyu Yang","Yichen Zhu"],"pdf_url":"https://arxiv.org/pdf/2407.17216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16495v2","updated":"2024-08-06T15:03:50Z","published":"2024-04-25T10:40:49Z","title":"T-Explainer: A Model-Agnostic Explainability Framework Based on\n  Gradients","summary":"  The development of machine learning applications has increased significantly\nin recent years, motivated by the remarkable ability of learning-powered\nsystems to discover and generalize intricate patterns hidden in massive\ndatasets. Modern learning models, while powerful, often have a level of\ncomplexity that renders them opaque black boxes, resulting in a notable lack of\ntransparency that hinders our ability to decipher their reasoning. Opacity\nchallenges the interpretability and practical application of machine learning,\nespecially in critical domains where understanding the underlying reasons is\nessential for informed decision-making. Explainable Artificial Intelligence\n(XAI) rises to address that challenge, unraveling the complexity of black boxes\nby providing elucidating explanations. Among the various XAI approaches,\nfeature attribution/importance stands out for its capacity to delineate the\nsignificance of input features in the prediction process. However, most\nexisting attribution methods have limitations, such as instability, when\ndivergent explanations may result from similar or even the same instance. This\nwork introduces T-Explainer, a novel local additive attribution explainer based\non Taylor expansion. It has desirable properties, such as local accuracy and\nconsistency, making T-Explainer stable over multiple runs. We demonstrate\nT-Explainer's effectiveness in quantitative benchmark experiments against\nwell-known attribution methods. Additionally, we provide several tools to\nevaluate and visualize explanations, turning T-Explainer into a comprehensive\nXAI framework.\n","authors":["Evandro S. Ortigossa","Fábio F. Dias","Brian Barr","Claudio T. Silva","Luis Gustavo Nonato"],"pdf_url":"https://arxiv.org/pdf/2404.16495v2.pdf","comment":"16 pages -- 2 figures and 20 tables -- Under review. This work has\n  been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2407.13605v2","updated":"2024-08-06T14:55:04Z","published":"2024-07-18T15:44:23Z","title":"Physics-guided Active Sample Reweighting for Urban Flow Prediction","summary":"  Urban flow prediction is a spatio-temporal modeling task that estimates the\nthroughput of transportation services like buses, taxis, and ride-sharing,\nwhere data-driven models have become the most popular solution in the past\ndecade. Meanwhile, the implicitly learned mapping between historical\nobservations to the prediction targets tend to over-simplify the dynamics of\nreal-world urban flows, leading to suboptimal predictions. Some recent\nspatio-temporal prediction solutions bring remedies with the notion of\nphysics-guided machine learning (PGML), which describes spatio-temporal data\nwith nuanced and principled physics laws, thus enhancing both the prediction\naccuracy and interpretability. However, these spatio-temporal PGML methods are\nbuilt upon a strong assumption that the observed data fully conforms to the\ndifferential equations that define the physical system, which can quickly\nbecome ill-posed in urban flow prediction tasks. The observed urban flow data,\nespecially when sliced into time-dependent snapshots to facilitate predictions,\nis typically incomplete and sparse, and prone to inherent noise incurred in the\ncollection process. As a result, such physical inconsistency between the data\nand PGML model significantly limits the predictive power and robustness of the\nsolution. Moreover, due to the interval-based predictions and intermittent\nnature of data filing in many transportation services, the instantaneous\ndynamics of urban flows can hardly be captured, rendering differential\nequation-based continuous modeling a loose fit for this setting. To overcome\nthe challenges, we develop a discretized physics-guided network (PN), and\npropose a data-aware framework Physics-guided Active Sample Reweighting\n(P-GASR) to enhance PN. Experimental results in four real-world datasets\ndemonstrate that our method achieves state-of-the-art performance with a\ndemonstrable improvement in robustness.\n","authors":["Wei Jiang","Tong Chen","Guanhua Ye","Wentao Zhang","Lizhen Cui","Zi Huang","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2407.13605v2.pdf","comment":"This paper is accepted by Proceedings of the 33nd ACM International\n  Conference on Information and Knowledge Management (CIKM '24)"},{"id":"http://arxiv.org/abs/2408.03236v1","updated":"2024-08-06T14:48:34Z","published":"2024-08-06T14:48:34Z","title":"Analysis of Partially-Calibrated Sparse Subarrays for Direction Finding\n  with Extended Degrees of Freedom","summary":"  This paper investigates the problem of direction-of-arrival (DOA) estimation\nusing multiple partially-calibrated sparse subarrays. In particular, we present\nthe Generalized Coarray Multiple Signal Classification (GCA-MUSIC) DOA\nestimation algorithm to scenarios with partially-calibrated sparse subarrays.\nThe proposed GCA-MUSIC algorithm exploits the difference coarray for each\nsubarray, followed by a specific pseudo-spectrum merging rule that is based on\nthe intersection of the signal subspaces associated to each subarray. This rule\nassumes that there is no a priori knowledge about the cross-covariance between\nsubarrays. In that way, only the second-order statistics of each subarray are\nused to estimate the directions with increased degrees of freedom, i.e., the\nestimation procedure preserves the coarray Multiple Signal Classification and\nsparse arrays properties to estimate more sources than the number of physical\nsensors in each subarray. Numerical simulations show that the proposed\nGCA-MUSIC has better performance than other similar strategies.\n","authors":["W. S. Leite","R. C. de Lamare"],"pdf_url":"https://arxiv.org/pdf/2408.03236v1.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.03223v1","updated":"2024-08-06T14:36:29Z","published":"2024-08-06T14:36:29Z","title":"Don't Think It Twice: Exploit Shift Invariance for Efficient Online\n  Streaming Inference of CNNs","summary":"  Deep learning time-series processing often relies on convolutional neural\nnetworks with overlapping windows. This overlap allows the network to produce\nan output faster than the window length. However, it introduces additional\ncomputations. This work explores the potential to optimize computational\nefficiency during inference by exploiting convolution's shift-invariance\nproperties to skip the calculation of layer activations between successive\noverlapping windows. Although convolutions are shift-invariant, zero-padding\nand pooling operations, widely used in such networks, are not efficient and\ncomplicate efficient streaming inference. We introduce StreamiNNC, a strategy\nto deploy Convolutional Neural Networks for online streaming inference. We\nexplore the adverse effects of zero padding and pooling on the accuracy of\nstreaming inference, deriving theoretical error upper bounds for pooling during\nstreaming. We address these limitations by proposing signal padding and pooling\nalignment and provide guidelines for designing and deploying models for\nStreamiNNC. We validate our method in simulated data and on three real-world\nbiomedical signal processing applications. StreamiNNC achieves a low deviation\nbetween streaming output and normal inference for all three networks (2.03 -\n3.55% NRMSE). This work demonstrates that it is possible to linearly speed up\nthe inference of streaming CNNs processing overlapping windows, negating the\nadditional computation typically incurred by overlapping windows.\n","authors":["Christodoulos Kechris","Jonathan Dan","Jose Miranda","David Atienza"],"pdf_url":"https://arxiv.org/pdf/2408.03223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09146v4","updated":"2024-08-06T14:30:52Z","published":"2024-02-14T12:55:28Z","title":"ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural\n  Networks","summary":"  In this paper, we present a novel framework for enhancing the performance of\nQuanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional\nlayers and addressing the critical challenges associated with them. Traditional\nquanvolutional layers, although beneficial for feature extraction, have largely\nbeen static, offering limited adaptability. Unlike state-of-the-art, our\nresearch overcomes this limitation by enabling training within these layers,\nsignificantly increasing the flexibility and potential of QuNNs. However, the\nintroduction of multiple trainable quanvolutional layers induces complexities\nin gradient-based optimization, primarily due to the difficulty in accessing\ngradients across these layers. To resolve this, we propose a novel\narchitecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging\nthe concept of residual learning, which facilitates the flow of gradients by\nadding skip connections between layers. By inserting residual blocks between\nquanvolutional layers, we ensure enhanced gradient access throughout the\nnetwork, leading to improved training performance. Moreover, we provide\nempirical evidence on the strategic placement of these residual blocks within\nQuNNs. Through extensive experimentation, we identify an efficient\nconfiguration of residual blocks, which enables gradients across all the layers\nin the network that eventually results in efficient training. Our findings\nsuggest that the precise location of residual blocks plays a crucial role in\nmaximizing the performance gains in QuNNs. Our results mark a substantial step\nforward in the evolution of quantum deep learning, offering new avenues for\nboth theoretical development and practical quantum computing applications.\n","authors":["Muhammad Kashif","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2402.09146v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05530v2","updated":"2024-08-06T14:30:31Z","published":"2024-04-08T13:59:02Z","title":"Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data","summary":"  Reinforcement Learning from Human Feedback (RLHF) is a popular method for\naligning Language Models (LM) with human values and preferences. RLHF requires\na large number of preference pairs as training data, which are often used in\nboth the Supervised Fine-Tuning and Reward Model training and therefore\npublicly available datasets are commonly used. In this work, we study to what\nextent a malicious actor can manipulate the LMs generations by poisoning the\npreferences, i.e., injecting poisonous preference pairs into these datasets and\nthe RLHF training process. We propose strategies to build poisonous preference\npairs and test their performance by poisoning two widely used preference\ndatasets. Our results show that preference poisoning is highly effective:\ninjecting a small amount of poisonous data (1-5\\% of the original dataset), we\ncan effectively manipulate the LM to generate a target entity in a target\nsentiment (positive or negative). The findings from our experiments also shed\nlight on strategies to defend against the preference poisoning attack.\n","authors":["Tim Baumgärtner","Yang Gao","Dana Alon","Donald Metzler"],"pdf_url":"https://arxiv.org/pdf/2404.05530v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03220v1","updated":"2024-08-06T14:26:09Z","published":"2024-08-06T14:26:09Z","title":"Masked Random Noise for Communication Efficient Federaetd Learning","summary":"  Federated learning is a promising distributed training paradigm that\neffectively safeguards data privacy. However, it may involve significant\ncommunication costs, which hinders training efficiency. In this paper, we aim\nto enhance communication efficiency from a new perspective. Specifically, we\nrequest the distributed clients to find optimal model updates relative to\nglobal model parameters within predefined random noise. For this purpose, we\npropose Federated Masked Random Noise (FedMRN), a novel framework that enables\nclients to learn a 1-bit mask for each model parameter and apply masked random\nnoise (i.e., the Hadamard product of random noise and masks) to represent model\nupdates. To make FedMRN feasible, we propose an advanced mask training\nstrategy, called progressive stochastic masking (PSM). After local training,\neach client only need to transmit local masks and a random seed to the server.\nAdditionally, we provide theoretical guarantees for the convergence of FedMRN\nunder both strongly convex and non-convex assumptions. Extensive experiments\nare conducted on four popular datasets. The results show that FedMRN exhibits\nsuperior convergence speed and test accuracy compared to relevant baselines,\nwhile attaining a similar level of accuracy as FedAvg.\n","authors":["Shiwei Li","Yingyi Cheng","Haozhao Wang","Xing Tang","Shijie Xu","Weihong Luo","Yuhua Li","Dugang Liu","Xiuqiang He","and Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2408.03220v1.pdf","comment":"Accepted by MM 2024"},{"id":"http://arxiv.org/abs/2408.03219v1","updated":"2024-08-06T14:25:23Z","published":"2024-08-06T14:25:23Z","title":"Learning to Learn without Forgetting using Attention","summary":"  Continual learning (CL) refers to the ability to continually learn over time\nby accommodating new knowledge while retaining previously learned experience.\nWhile this concept is inherent in human learning, current machine learning\nmethods are highly prone to overwrite previously learned patterns and thus\nforget past experience. Instead, model parameters should be updated selectively\nand carefully, avoiding unnecessary forgetting while optimally leveraging\npreviously learned patterns to accelerate future learning. Since hand-crafting\neffective update mechanisms is difficult, we propose meta-learning a\ntransformer-based optimizer to enhance CL. This meta-learned optimizer uses\nattention to learn the complex relationships between model parameters across a\nstream of tasks, and is designed to generate effective weight updates for the\ncurrent task while preventing catastrophic forgetting on previously encountered\ntasks. Evaluations on benchmark datasets like SplitMNIST, RotatedMNIST, and\nSplitCIFAR-100 affirm the efficacy of the proposed approach in terms of both\nforward and backward transfer, even on small sets of labeled data, highlighting\nthe advantages of integrating a meta-learned optimizer within the continual\nlearning framework.\n","authors":["Anna Vettoruzzo","Joaquin Vanschoren","Mohamed-Rafik Bouguelia","Thorsteinn Rögnvaldsson"],"pdf_url":"https://arxiv.org/pdf/2408.03219v1.pdf","comment":"Published at 3rd Conference on Lifelong Learning Agents (CoLLAs),\n  2024"},{"id":"http://arxiv.org/abs/2408.03215v1","updated":"2024-08-06T14:19:06Z","published":"2024-08-06T14:19:06Z","title":"FedBAT: Communication-Efficient Federated Learning via Learnable\n  Binarization","summary":"  Federated learning is a promising distributed machine learning paradigm that\ncan effectively exploit large-scale data without exposing users' privacy.\nHowever, it may incur significant communication overhead, thereby potentially\nimpairing the training efficiency. To address this challenge, numerous studies\nsuggest binarizing the model updates. Nonetheless, traditional methods usually\nbinarize model updates in a post-training manner, resulting in significant\napproximation errors and consequent degradation in model accuracy. To this end,\nwe propose Federated Binarization-Aware Training (FedBAT), a novel framework\nthat directly learns binary model updates during the local training process,\nthus inherently reducing the approximation errors. FedBAT incorporates an\ninnovative binarization operator, along with meticulously designed derivatives\nto facilitate efficient learning. In addition, we establish theoretical\nguarantees regarding the convergence of FedBAT. Extensive experiments are\nconducted on four popular datasets. The results show that FedBAT significantly\naccelerates the convergence and exceeds the accuracy of baselines by up to 9\\%,\neven surpassing that of FedAvg in some cases.\n","authors":["Shiwei Li","Wenchao Xu","Haozhao Wang","Xing Tang","Yining Qi","Shijie Xu","Weihong Luo","Yuhua Li","Xiuqiang He","Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2408.03215v1.pdf","comment":"Accepted by ICML 2024"},{"id":"http://arxiv.org/abs/2405.11647v3","updated":"2024-08-06T14:12:26Z","published":"2024-05-19T18:57:25Z","title":"Hummer: Towards Limited Competitive Preference Dataset","summary":"  Preference datasets are essential for incorporating human preferences into\npre-trained language models, playing a key role in the success of Reinforcement\nLearning from Human Feedback. However, these datasets often demonstrate\nconflicting alignment objectives, leading to increased vulnerability to\njailbreak attacks and challenges in adapting downstream tasks to prioritize\nspecific alignment objectives without negatively impacting others. In this\nwork, we introduce a novel statistical metric, Alignment Dimension Conflict, to\nquantify the degree of conflict within preference datasets. We then present\n\\texttt{Hummer} and its fine-grained variant, \\texttt{Hummer-F}, as innovative\npairwise preference datasets with reduced-conflict alignment objectives.\n\\texttt{Hummer} is built based on UltraFeedback and is enhanced by AI feedback\nfrom GPT-4, marking as the first preference dataset aimed at reducing the\ncompetition between alignment objectives. Furthermore, we develop reward\nmodels, HummerRM and HummerRM-F, which employ a hybrid sampling approach to\nbalance diverse alignment objectives effectively. This sampling method\npositions HummerRM as an ideal model for domain-specific further fine-tuning\nand reducing vulnerabilities to attacks.\n","authors":["Li Jiang","Yusen Wu","Junwu Xiong","Jingqing Ruan","Yichuan Ding","Qingpei Guo","Zujie Wen","Jun Zhou","Xiaotie Deng"],"pdf_url":"https://arxiv.org/pdf/2405.11647v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03199v1","updated":"2024-08-06T13:58:37Z","published":"2024-08-06T13:58:37Z","title":"Convergence Conditions for Stochastic Line Search Based Optimization of\n  Over-parametrized Models","summary":"  In this paper, we deal with algorithms to solve the finite-sum problems\nrelated to fitting over-parametrized models, that typically satisfy the\ninterpolation condition. In particular, we focus on approaches based on\nstochastic line searches and employing general search directions. We define\nconditions on the sequence of search directions that guarantee finite\ntermination and bounds for the backtracking procedure. Moreover, we shed light\non the additional property of directions needed to prove fast (linear)\nconvergence of the general class of algorithms when applied to PL functions in\nthe interpolation regime. From the point of view of algorithms design, the\nproposed analysis identifies safeguarding conditions that could be employed in\nrelevant algorithmic framework. In particular, it could be of interest to\nintegrate stochastic line searches within momentum, conjugate gradient or\nadaptive preconditioning methods.\n","authors":["Matteo Lapucci","Davide Pucci"],"pdf_url":"https://arxiv.org/pdf/2408.03199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03195v1","updated":"2024-08-06T13:55:51Z","published":"2024-08-06T13:55:51Z","title":"RELIEF: Reinforcement Learning Empowered Graph Feature Prompt Tuning","summary":"  The advent of the \"pre-train, prompt\" paradigm has recently extended its\ngeneralization ability and data efficiency to graph representation learning,\nfollowing its achievements in Natural Language Processing (NLP). Initial graph\nprompt tuning approaches tailored specialized prompting functions for Graph\nNeural Network (GNN) models pre-trained with specific strategies, such as edge\nprediction, thus limiting their applicability. In contrast, another pioneering\nline of research has explored universal prompting via adding prompts to the\ninput graph's feature space, thereby removing the reliance on specific\npre-training strategies. However, the necessity to add feature prompts to all\nnodes remains an open question. Motivated by findings from prompt tuning\nresearch in the NLP domain, which suggest that highly capable pre-trained\nmodels need less conditioning signal to achieve desired behaviors, we advocate\nfor strategically incorporating necessary and lightweight feature prompts to\ncertain graph nodes to enhance downstream task performance. This introduces a\ncombinatorial optimization problem, requiring a policy to decide 1) which nodes\nto prompt and 2) what specific feature prompts to attach. We then address the\nproblem by framing the prompt incorporation process as a sequential\ndecision-making problem and propose our method, RELIEF, which employs\nReinforcement Learning (RL) to optimize it. At each step, the RL agent selects\na node (discrete action) and determines the prompt content (continuous action),\naiming to maximize cumulative performance gain. Extensive experiments on graph\nand node-level tasks with various pre-training strategies in few-shot scenarios\ndemonstrate that our RELIEF outperforms fine-tuning and other prompt-based\napproaches in classification performance and data efficiency.\n","authors":["Jiapeng Zhu","Zichen Ding","Jianxiang Yu","Jiaqi Tan","Xiang Li","Weining Qian"],"pdf_url":"https://arxiv.org/pdf/2408.03195v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07186v5","updated":"2024-08-06T13:47:50Z","published":"2023-12-12T11:48:56Z","title":"Towards Optimal Sobolev Norm Rates for the Vector-Valued Regularized\n  Least-Squares Algorithm","summary":"  We present the first optimal rates for infinite-dimensional vector-valued\nridge regression on a continuous scale of norms that interpolate between $L_2$\nand the hypothesis space, which we consider as a vector-valued reproducing\nkernel Hilbert space. These rates allow to treat the misspecified case in which\nthe true regression function is not contained in the hypothesis space. We\ncombine standard assumptions on the capacity of the hypothesis space with a\nnovel tensor product construction of vector-valued interpolation spaces in\norder to characterize the smoothness of the regression function. Our upper\nbound not only attains the same rate as real-valued kernel ridge regression,\nbut also removes the assumption that the target regression function is bounded.\nFor the lower bound, we reduce the problem to the scalar setting using a\nprojection argument. We show that these rates are optimal in most cases and\nindependent of the dimension of the output space. We illustrate our results for\nthe special case of vector-valued Sobolev spaces.\n","authors":["Zhu Li","Dimitri Meunier","Mattes Mollenhauer","Arthur Gretton"],"pdf_url":"https://arxiv.org/pdf/2312.07186v5.pdf","comment":"Published JMLR version. arXiv admin note: text overlap with\n  arXiv:2208.01711"},{"id":"http://arxiv.org/abs/2311.15890v3","updated":"2024-08-06T13:47:24Z","published":"2023-11-27T14:56:47Z","title":"Stability-Informed Initialization of Neural Ordinary Differential\n  Equations","summary":"  This paper addresses the training of Neural Ordinary Differential Equations\n(neural ODEs), and in particular explores the interplay between numerical\nintegration techniques, stability regions, step size, and initialization\ntechniques. It is shown how the choice of integration technique implicitly\nregularizes the learned model, and how the solver's corresponding stability\nregion affects training and prediction performance. From this analysis, a\nstability-informed parameter initialization technique is introduced. The\neffectiveness of the initialization method is displayed across several learning\nbenchmarks and industrial applications.\n","authors":["Theodor Westny","Arman Mohammadi","Daniel Jung","Erik Frisk"],"pdf_url":"https://arxiv.org/pdf/2311.15890v3.pdf","comment":"In Proceedings of the 41 st International Conference on Machine\n  Learning"},{"id":"http://arxiv.org/abs/2408.03178v1","updated":"2024-08-06T13:22:51Z","published":"2024-08-06T13:22:51Z","title":"An Object is Worth 64x64 Pixels: Generating 3D Object via Image\n  Diffusion","summary":"  We introduce a new approach for generating realistic 3D models with UV maps\nthrough a representation termed \"Object Images.\" This approach encapsulates\nsurface geometry, appearance, and patch structures within a 64x64 pixel image,\neffectively converting complex 3D shapes into a more manageable 2D format. By\ndoing so, we address the challenges of both geometric and semantic irregularity\ninherent in polygonal meshes. This method allows us to use image generation\nmodels, such as Diffusion Transformers, directly for 3D shape generation.\nEvaluated on the ABO dataset, our generated shapes with patch structures\nachieve point cloud FID comparable to recent 3D generative models, while\nnaturally supporting PBR material generation.\n","authors":["Xingguang Yan","Han-Hung Lee","Ziyu Wan","Angel X. Chang"],"pdf_url":"https://arxiv.org/pdf/2408.03178v1.pdf","comment":"Project Page: https://omages.github.io/"},{"id":"http://arxiv.org/abs/2408.03172v1","updated":"2024-08-06T13:16:16Z","published":"2024-08-06T13:16:16Z","title":"Leveraging Parameter Efficient Training Methods for Low Resource Text\n  Classification: A Case Study in Marathi","summary":"  With the surge in digital content in low-resource languages, there is an\nescalating demand for advanced Natural Language Processing (NLP) techniques\ntailored to these languages. BERT (Bidirectional Encoder Representations from\nTransformers), serving as the foundational framework for numerous NLP\narchitectures and language models, is increasingly employed for the development\nof low-resource NLP models. Parameter Efficient Fine-Tuning (PEFT) is a method\nfor fine-tuning Large Language Models (LLMs) and reducing the training\nparameters to some extent to decrease the computational costs needed for\ntraining the model and achieve results comparable to a fully fine-tuned model.\nIn this work, we present a study of PEFT methods for the Indic low-resource\nlanguage Marathi. We conduct a comprehensive analysis of PEFT methods applied\nto various monolingual and multilingual Marathi BERT models. These approaches\nare evaluated on prominent text classification datasets like MahaSent,\nMahaHate, and MahaNews. The incorporation of PEFT techniques is demonstrated to\nsignificantly expedite the training speed of the models, addressing a critical\naspect of model development and deployment. In this study, we explore Low-Rank\nAdaptation of Large Language Models (LoRA) and adapter methods for low-resource\ntext classification. We show that these methods are competitive with full\nfine-tuning and can be used without loss in accuracy. This study contributes\nvaluable insights into the effectiveness of Marathi BERT models, offering a\nfoundation for the continued advancement of NLP capabilities in Marathi and\nsimilar Indic languages.\n","authors":["Pranita Deshmukh","Nikita Kulkarni","Sanhita Kulkarni","Kareena Manghani","Raviraj Joshi"],"pdf_url":"https://arxiv.org/pdf/2408.03172v1.pdf","comment":"Accepted at I2CT 2024"},{"id":"http://arxiv.org/abs/2408.03156v1","updated":"2024-08-06T12:55:17Z","published":"2024-08-06T12:55:17Z","title":"Iterative CT Reconstruction via Latent Variable Optimization of Shallow\n  Diffusion Models","summary":"  Image generative AI has garnered significant attention in recent years. In\nparticular, the diffusion model, a core component of recent generative AI,\nproduces high-quality images with rich diversity. In this study, we propose a\nnovel CT reconstruction method by combining the denoising diffusion\nprobabilistic model with iterative CT reconstruction. In sharp contrast to\nprevious studies, we optimize the fidelity loss of CT reconstruction with\nrespect to the latent variable of the diffusion model, instead of the image and\nmodel parameters. To suppress anatomical structure changes produced by the\ndiffusion model, we shallow the diffusion and reverse processes, and fix a set\nof added noises in the reverse process to make it deterministic during\ninference. We demonstrate the effectiveness of the proposed method through\nsparse view CT reconstruction of 1/10 view projection data. Despite the\nsimplicity of the implementation, the proposed method shows the capability of\nreconstructing high-quality images while preserving the patient's anatomical\nstructure, and outperforms existing methods including iterative reconstruction,\niterative reconstruction with total variation, and the diffusion model alone in\nterms of quantitative indices such as SSIM and PSNR. We also explore further\nsparse view CT using 1/20 view projection data with the same trained diffusion\nmodel. As the number of iterations increases, image quality improvement\ncomparable to that of 1/10 sparse view CT reconstruction is achieved. In\nprinciple, the proposed method can be widely applied not only to CT but also to\nother imaging modalities such as MRI, PET, and SPECT.\n","authors":["Sho Ozaki","Shizuo Kaji","Toshikazu Imae","Kanabu Nawa","Hideomi Yamashita","Keiichi Nakagawa"],"pdf_url":"https://arxiv.org/pdf/2408.03156v1.pdf","comment":"19 pages, 9 figures"},{"id":"http://arxiv.org/abs/2405.01661v3","updated":"2024-08-06T12:54:26Z","published":"2024-05-02T18:31:47Z","title":"When a Relation Tells More Than a Concept: Exploring and Evaluating\n  Classifier Decisions with CoReX","summary":"  Explanations for Convolutional Neural Networks (CNNs) based on relevance of\ninput pixels might be too unspecific to evaluate which and how input features\nimpact model decisions. Especially in complex real-world domains like biology,\nthe presence of specific concepts and of relations between concepts might be\ndiscriminating between classes. Pixel relevance is not expressive enough to\nconvey this type of information. In consequence, model evaluation is limited\nand relevant aspects present in the data and influencing the model decisions\nmight be overlooked. This work presents a novel method to explain and evaluate\nCNN models, which uses a concept- and relation-based explainer (CoReX). It\nexplains the predictive behavior of a model on a set of images by masking\n(ir-)relevant concepts from the decision-making process and by constraining\nrelations in a learned interpretable surrogate model. We test our approach with\nseveral image data sets and CNN architectures. Results show that CoReX\nexplanations are faithful to the CNN model in terms of predictive outcomes. We\nfurther demonstrate through a human evaluation that CoReX is a suitable tool\nfor generating combined explanations that help assessing the classification\nquality of CNNs. We further show that CoReX supports the identification and\nre-classification of incorrect or ambiguous classifications.\n","authors":["Bettina Finzel","Patrick Hilme","Johannes Rabold","Ute Schmid"],"pdf_url":"https://arxiv.org/pdf/2405.01661v3.pdf","comment":"preliminary version, submitted to Machine Learning"},{"id":"http://arxiv.org/abs/2408.03152v1","updated":"2024-08-06T12:52:03Z","published":"2024-08-06T12:52:03Z","title":"TSC: A Simple Two-Sided Constraint against Over-Smoothing","summary":"  Graph Convolutional Neural Network (GCN), a widely adopted method for\nanalyzing relational data, enhances node discriminability through the\naggregation of neighboring information. Usually, stacking multiple layers can\nimprove the performance of GCN by leveraging information from high-order\nneighbors. However, the increase of the network depth will induce the\nover-smoothing problem, which can be attributed to the quality and quantity of\nneighbors changing: (a) neighbor quality, node's neighbors become overlapping\nin high order, leading to aggregated information becoming indistinguishable,\n(b) neighbor quantity, the exponentially growing aggregated neighbors submerges\nthe node's initial feature by recursively aggregating operations. Current\nsolutions mainly focus on one of the above causes and seldom consider both at\nonce.\n  Aiming at tackling both causes of over-smoothing in one shot, we introduce a\nsimple Two-Sided Constraint (TSC) for GCNs, comprising two straightforward yet\npotent techniques: random masking and contrastive constraint. The random\nmasking acts on the representation matrix's columns to regulate the degree of\ninformation aggregation from neighbors, thus preventing the convergence of node\nrepresentations. Meanwhile, the contrastive constraint, applied to the\nrepresentation matrix's rows, enhances the discriminability of the nodes.\nDesigned as a plug-in module, TSC can be easily coupled with GCN or SGC\narchitectures. Experimental analyses on diverse real-world graph datasets\nverify that our approach markedly reduces the convergence of node's\nrepresentation and the performance degradation in deeper GCN.\n","authors":["Furong Peng","Kang Liu","Xuan Lu","Yuhua Qian","Hongren Yan","Chao Ma"],"pdf_url":"https://arxiv.org/pdf/2408.03152v1.pdf","comment":"accept by KDD2024"},{"id":"http://arxiv.org/abs/2408.03150v1","updated":"2024-08-06T12:49:33Z","published":"2024-08-06T12:49:33Z","title":"Conditioning LLMs with Emotion in Neural Machine Translation","summary":"  Large Language Models (LLMs) have shown remarkable performance in Natural\nLanguage Processing tasks, including Machine Translation (MT). In this work, we\npropose a novel MT pipeline that integrates emotion information extracted from\na Speech Emotion Recognition (SER) model into LLMs to enhance translation\nquality. We first fine-tune five existing LLMs on the Libri-trans dataset and\nselect the most performant model. Subsequently, we augment LLM prompts with\ndifferent dimensional emotions and train the selected LLM under these different\nconfigurations. Our experiments reveal that integrating emotion information,\nespecially arousal, into LLM prompts leads to notable improvements in\ntranslation quality.\n","authors":["Charles Brazier","Jean-Luc Rouas"],"pdf_url":"https://arxiv.org/pdf/2408.03150v1.pdf","comment":"6 pages, In Proceedings of the 21st International Conference on\n  Spoken Language Translation (IWSLT), Bangkok, Thailand, 2024"},{"id":"http://arxiv.org/abs/2408.03144v1","updated":"2024-08-06T12:39:12Z","published":"2024-08-06T12:39:12Z","title":"Active Learning for Level Set Estimation Using Randomized Straddle\n  Algorithms","summary":"  Level set estimation (LSE), the problem of identifying the set of input\npoints where a function takes value above (or below) a given threshold, is\nimportant in practical applications. When the function is expensive-to-evaluate\nand black-box, the \\textit{straddle} algorithm, which is a representative\nheuristic for LSE based on Gaussian process models, and its extensions having\ntheoretical guarantees have been developed. However, many of existing methods\ninclude a confidence parameter $\\beta^{1/2}_t$ that must be specified by the\nuser, and methods that choose $\\beta^{1/2}_t$ heuristically do not provide\ntheoretical guarantees. In contrast, theoretically guaranteed values of\n$\\beta^{1/2}_t$ need to be increased depending on the number of iterations and\ncandidate points, and are conservative and not good for practical performance.\nIn this study, we propose a novel method, the \\textit{randomized straddle}\nalgorithm, in which $\\beta_t$ in the straddle algorithm is replaced by a random\nsample from the chi-squared distribution with two degrees of freedom. The\nconfidence parameter in the proposed method has the advantages of not needing\nadjustment, not depending on the number of iterations and candidate points, and\nnot being conservative. Furthermore, we show that the proposed method has\ntheoretical guarantees that depend on the sample complexity and the number of\niterations. Finally, we confirm the usefulness of the proposed method through\nnumerical experiments using synthetic and real data.\n","authors":["Yu Inatsu","Shion Takeno","Kentaro Kutsukake","Ichiro Takeuchi"],"pdf_url":"https://arxiv.org/pdf/2408.03144v1.pdf","comment":"21 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.00573v2","updated":"2024-08-06T12:36:57Z","published":"2024-08-01T14:06:34Z","title":"Convergence Analysis of Natural Gradient Descent for Over-parameterized\n  Physics-Informed Neural Networks","summary":"  First-order methods, such as gradient descent (GD) and stochastic gradient\ndescent (SGD), have been proven effective in training neural networks. In the\ncontext of over-parameterization, there is a line of work demonstrating that\nrandomly initialized (stochastic) gradient descent converges to a globally\noptimal solution at a linear convergence rate for the quadratic loss function.\nHowever, the learning rate of GD for training two-layer neural networks\nexhibits poor dependence on the sample size and the Gram matrix, leading to a\nslow training process. In this paper, we show that for the $L^2$ regression\nproblems, the learning rate can be improved from $\\mathcal{O}(\\lambda_0/n^2)$\nto $\\mathcal{O}(1/\\|\\bm{H}^{\\infty}\\|_2)$, which implies that GD actually\nenjoys a faster convergence rate. Furthermore, we generalize the method to GD\nin training two-layer Physics-Informed Neural Networks (PINNs), showing a\nsimilar improvement for the learning rate. Although the improved learning rate\nhas a mild dependence on the Gram matrix, we still need to set it small enough\nin practice due to the unknown eigenvalues of the Gram matrix. More\nimportantly, the convergence rate is tied to the least eigenvalue of the Gram\nmatrix, which can lead to slow convergence. In this work, we provide the\nconvergence analysis of natural gradient descent (NGD) in training two-layer\nPINNs, demonstrating that the learning rate can be $\\mathcal{O}(1)$, and at\nthis rate, the convergence rate is independent of the Gram matrix.\n","authors":["Xianliang Xu","Ting Du","Wang Kong","Ye Li","Zhongyi Huang"],"pdf_url":"https://arxiv.org/pdf/2408.00573v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01294v2","updated":"2024-08-06T12:24:07Z","published":"2024-08-02T14:31:37Z","title":"Feature Clock: High-Dimensional Effects in Two-Dimensional Plots","summary":"  Humans struggle to perceive and interpret high-dimensional data. Therefore,\nhigh-dimensional data are often projected into two dimensions for\nvisualization. Many applications benefit from complex nonlinear dimensionality\nreduction techniques, but the effects of individual high-dimensional features\nare hard to explain in the two-dimensional space. Most visualization solutions\nuse multiple two-dimensional plots, each showing the effect of one\nhigh-dimensional feature in two dimensions; this approach creates a need for a\nvisual inspection of k plots for a k-dimensional input space. Our solution,\nFeature Clock, provides a novel approach that eliminates the need to inspect\nthese k plots to grasp the influence of original features on the data structure\ndepicted in two dimensions. Feature Clock enhances the explainability and\ncompactness of visualizations of embedded data and is available in an\nopen-source Python library.\n","authors":["Olga Ovcharenko","Rita Sevastjanova","Valentina Boeva"],"pdf_url":"https://arxiv.org/pdf/2408.01294v2.pdf","comment":"To be published in IEEE VIS 2024"},{"id":"http://arxiv.org/abs/2407.09064v2","updated":"2024-08-06T11:43:53Z","published":"2024-07-12T07:34:10Z","title":"Multi-Modal Dataset Creation for Federated Learning with DICOM\n  Structured Reports","summary":"  Purpose: Federated training is often hindered by heterogeneous datasets due\nto divergent data storage options, inconsistent naming schemes, varied\nannotation procedures, and disparities in label quality. This is particularly\nevident in the emerging multi-modal learning paradigms, where dataset\nharmonization including a uniform data representation and filtering options are\nof paramount importance.\n  Methods: DICOM structured reports enable the standardized linkage of\narbitrary information beyond the imaging domain and can be used within Python\ndeep learning pipelines with highdicom. Building on this, we developed an open\nplatform for data integration and interactive filtering capabilities that\nsimplifies the process of assembling multi-modal datasets.\n  Results: In this study, we extend our prior work by showing its applicability\nto more and divergent data types, as well as streamlining datasets for\nfederated training within an established consortium of eight university\nhospitals in Germany. We prove its concurrent filtering ability by creating\nharmonized multi-modal datasets across all locations for predicting the outcome\nafter minimally invasive heart valve replacement. The data includes DICOM data\n(i.e. computed tomography images, electrocardiography scans) as well as\nannotations (i.e. calcification segmentations, pointsets and pacemaker\ndependency), and metadata (i.e. prosthesis and diagnoses).\n  Conclusion: Structured reports bridge the traditional gap between imaging\nsystems and information systems. Utilizing the inherent DICOM reference system\narbitrary data types can be queried concurrently to create meaningful cohorts\nfor clinical studies. The graphical interface as well as example structured\nreport templates will be made publicly available.\n","authors":["Malte Tölle","Lukas Burger","Halvar Kelm","Florian André","Peter Bannas","Gerhard Diller","Norbert Frey","Philipp Garthe","Stefan Groß","Anja Hennemuth","Lars Kaderali","Nina Krüger","Andreas Leha","Simon Martin","Alexander Meyer","Eike Nagel","Stefan Orwat","Clemens Scherer","Moritz Seiffert","Jan Moritz Seliger","Stefan Simm","Tim Friede","Tim Seidler","Sandy Engelhardt"],"pdf_url":"https://arxiv.org/pdf/2407.09064v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11044v2","updated":"2024-08-06T11:29:06Z","published":"2024-01-19T22:11:54Z","title":"Preservation of Feature Stability in Machine Learning Under Data\n  Uncertainty for Decision Support in Critical Domains","summary":"  In a world where Machine Learning (ML) is increasingly deployed to support\ndecision-making in critical domains, providing decision-makers with\nexplainable, stable, and relevant inputs becomes fundamental. Understanding how\nmachine learning works under missing data and how this affects feature\nvariability is paramount. This is even more relevant as machine learning\napproaches focus on standardising decision-making approaches that rely on an\nidealised set of features. However, decision-making in human activities often\nrelies on incomplete data, even in critical domains. This paper addresses this\ngap by conducting a set of experiments using traditional machine learning\nmethods that look for optimal decisions in comparison to a recently deployed\nmachine learning method focused on a classification that is more descriptive\nand mimics human decision making, allowing for the natural integration of\nexplainability. We found that the ML descriptive approach maintains higher\nclassification accuracy while ensuring the stability of feature selection as\ndata incompleteness increases. This suggests that descriptive classification\nmethods can be helpful in uncertain decision-making scenarios.\n","authors":["Karol Capała","Paulina Tworek","Jose Sousa"],"pdf_url":"https://arxiv.org/pdf/2401.11044v2.pdf","comment":"30 pages, 6 figures, supplementary materials"},{"id":"http://arxiv.org/abs/2407.03234v3","updated":"2024-08-06T11:15:00Z","published":"2024-07-03T16:03:42Z","title":"Self-Evaluation as a Defense Against Adversarial Attacks on LLMs","summary":"  We introduce a defense against adversarial attacks on LLMs utilizing\nself-evaluation. Our method requires no model fine-tuning, instead using\npre-trained models to evaluate the inputs and outputs of a generator model,\nsignificantly reducing the cost of implementation in comparison to other,\nfinetuning-based methods. Our method can significantly reduce the attack\nsuccess rate of attacks on both open and closed-source LLMs, beyond the\nreductions demonstrated by Llama-Guard2 and commonly used content moderation\nAPIs. We present an analysis of the effectiveness of our method, including\nattempts to attack the evaluator in various settings, demonstrating that it is\nalso more resilient to attacks than existing methods. Code and data will be\nmade available at https://github.com/Linlt-leon/self-eval.\n","authors":["Hannah Brown","Leon Lin","Kenji Kawaguchi","Michael Shieh"],"pdf_url":"https://arxiv.org/pdf/2407.03234v3.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.03099v1","updated":"2024-08-06T11:04:07Z","published":"2024-08-06T11:04:07Z","title":"Topic Modeling with Fine-tuning LLMs and Bag of Sentences","summary":"  Large language models (LLM)'s are increasingly used for topic modeling\noutperforming classical topic models such as LDA. Commonly, pre-trained LLM\nencoders such as BERT are used out-of-the-box despite the fact that fine-tuning\nis known to improve LLMs considerably. The challenge lies in obtaining a\nsuitable (labeled) dataset for fine-tuning. In this paper, we use the recent\nidea to use bag of sentences as the elementary unit in computing topics. In\nturn, we derive an approach FT-Topic to perform unsupervised fine-tuning\nrelying primarily on two steps for constructing a training dataset in an\nautomatic fashion. First, a heuristic method to identifies pairs of sentence\ngroups that are either assumed to be of the same or different topics. Second,\nwe remove sentence pairs that are likely labeled incorrectly. The dataset is\nthen used to fine-tune an encoder LLM, which can be leveraged by any topic\nmodeling approach using embeddings. However, in this work, we demonstrate its\neffectiveness by deriving a novel state-of-the-art topic modeling method called\nSenClu, which achieves fast inference through an expectation-maximization\nalgorithm and hard assignments of sentence groups to a single topic, while\ngiving users the possibility to encode prior knowledge on the topic-document\ndistribution. Code is at \\url{https://github.com/JohnTailor/FT-Topic}\n","authors":["Johannes Schneider"],"pdf_url":"https://arxiv.org/pdf/2408.03099v1.pdf","comment":"This is the submitted journal version of enhanced with the novel\n  fine-tuning part of \"Efficient and Flexible Topic Modeling using Pretrained\n  Embeddings and Bag of Sentences'' which appeared at the International\n  Conference on Agents and Artificial Intelligence(ICAART) in 2024"},{"id":"http://arxiv.org/abs/2211.16237v4","updated":"2024-08-06T10:51:40Z","published":"2022-11-29T14:21:34Z","title":"Closing the gap between SVRG and TD-SVRG with Gradient Splitting","summary":"  Temporal difference (TD) learning is a policy evaluation in reinforcement\nlearning whose performance can be enhanced by variance reduction methods.\nRecently, multiple works have sought to fuse TD learning with Stochastic\nVariance Reduced Gradient (SVRG) method to achieve a geometric rate of\nconvergence. However, the resulting convergence rate is significantly weaker\nthan what is achieved by SVRG in the setting of convex optimization. In this\nwork we utilize a recent interpretation of TD-learning as the splitting of the\ngradient of an appropriately chosen function, thus simplifying the algorithm\nand fusing TD with SVRG. Our main result is a geometric convergence bound with\npredetermined learning rate of $1/8$, which is identical to the convergence\nbound available for SVRG in the convex setting. Our theoretical findings are\nsupported by a set of experiments.\n","authors":["Arsenii Mustafin","Alex Olshevsky","Ioannis Ch. Paschalidis"],"pdf_url":"https://arxiv.org/pdf/2211.16237v4.pdf","comment":"42 pages, 8 figures, 6 tables"},{"id":"http://arxiv.org/abs/2408.03093v1","updated":"2024-08-06T10:48:15Z","published":"2024-08-06T10:48:15Z","title":"Learning Provably Robust Policies in Uncertain Parametric Environments","summary":"  We present a data-driven approach for learning MDP policies that are robust\nacross stochastic environments whose transition probabilities are defined by\nparameters with an unknown distribution. We produce probably approximately\ncorrect (PAC) guarantees for the performance of these learned policies in a\nnew, unseen environment over the unknown distribution. Our approach is based on\nfinite samples of the MDP environments, for each of which we build an\napproximation of the model as an interval MDP, by exploring a set of generated\ntrajectories. We use the built approximations to synthesise a single policy\nthat performs well (meets given requirements) across the sampled environments,\nand furthermore bound its risk (of not meeting the given requirements) when\ndeployed in an unseen environment. Our procedure offers a trade-off between the\nguaranteed performance of the learned policy and the risk of not meeting the\nguarantee in an unseen environment. Our approach exploits knowledge of the\nenvironment's state space and graph structure, and we show how additional\nknowledge of its parametric structure can be leveraged to optimize learning and\nto obtain tighter guarantees from less samples. We evaluate our approach on a\ndiverse range of established benchmarks, demonstrating that we can generate\nhighly performing and robust policies, along with guarantees that tightly\nquantify their performance and the associated risk.\n","authors":["Yannik Schnitzer","Alessandro Abate","David Parker"],"pdf_url":"https://arxiv.org/pdf/2408.03093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20800v2","updated":"2024-08-06T10:45:42Z","published":"2024-05-31T14:01:12Z","title":"Shape Constraints in Symbolic Regression using Penalized Least Squares","summary":"  We study the addition of shape constraints (SC) and their consideration\nduring the parameter identification step of symbolic regression (SR). SC serve\nas a means to introduce prior knowledge about the shape of the otherwise\nunknown model function into SR. Unlike previous works that have explored SC in\nSR, we propose minimizing SC violations during parameter identification using\ngradient-based numerical optimization. We test three algorithm variants to\nevaluate their performance in identifying three symbolic expressions from\nsynthetically generated data sets. This paper examines two benchmark scenarios:\none with varying noise levels and another with reduced amounts of training\ndata. The results indicate that incorporating SC into the expression search is\nparticularly beneficial when data is scarce. Compared to using SC only in the\nselection process, our approach of minimizing violations during parameter\nidentification shows a statistically significant benefit in some of our test\ncases, without being significantly worse in any instance.\n","authors":["Viktor Martinek","Julia Reuter","Ophelia Frotscher","Sanaz Mostaghim","Markus Richter","Roland Herzog"],"pdf_url":"https://arxiv.org/pdf/2405.20800v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03088v1","updated":"2024-08-06T10:41:46Z","published":"2024-08-06T10:41:46Z","title":"QADQN: Quantum Attention Deep Q-Network for Financial Market Prediction","summary":"  Financial market prediction and optimal trading strategy development remain\nchallenging due to market complexity and volatility. Our research in quantum\nfinance and reinforcement learning for decision-making demonstrates the\napproach of quantum-classical hybrid algorithms to tackling real-world\nfinancial challenges. In this respect, we corroborate the concept with rigorous\nbacktesting and validate the framework's performance under realistic market\nconditions, by including fixed transaction cost per trade. This paper\nintroduces a Quantum Attention Deep Q-Network (QADQN) approach to address these\nchallenges through quantum-enhanced reinforcement learning. Our QADQN\narchitecture uses a variational quantum circuit inside a traditional deep\nQ-learning framework to take advantage of possible quantum advantages in\ndecision-making. We gauge the QADQN agent's performance on historical data from\nmajor market indices, including the S&P 500. We evaluate the agent's learning\nprocess by examining its reward accumulation and the effectiveness of its\nexperience replay mechanism. Our empirical results demonstrate the QADQN's\nsuperior performance, achieving better risk-adjusted returns with Sortino\nratios of 1.28 and 1.19 for non-overlapping and overlapping test periods\nrespectively, indicating effective downside risk management.\n","authors":["Siddhant Dutta","Nouhaila Innan","Alberto Marchisio","Sadok Ben Yahia","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2408.03088v1.pdf","comment":"Accepted at the 2024 IEEE International Conference on Quantum\n  Computing and Engineering (QCE24), QCRL, September 2024"},{"id":"http://arxiv.org/abs/2308.09310v3","updated":"2024-08-06T10:38:22Z","published":"2023-08-18T05:11:50Z","title":"Variance reduction techniques for stochastic proximal point algorithms","summary":"  In the context of finite sums minimization, variance reduction techniques are\nwidely used to improve the performance of state-of-the-art stochastic gradient\nmethods. Their practical impact is clear, as well as their theoretical\nproperties. Stochastic proximal point algorithms have been studied as an\nalternative to stochastic gradient algorithms since they are more stable with\nrespect to the choice of the step size. However, their variance-reduced\nversions are not as well studied as the gradient ones. In this work, we propose\nthe first unified study of variance reduction techniques for stochastic\nproximal point algorithms. We introduce a generic stochastic proximal-based\nalgorithm that can be specified to give the proximal version of SVRG, SAGA, and\nsome of their variants. For this algorithm, in the smooth setting, we provide\nseveral convergence rates for the iterates and the objective function values,\nwhich are faster than those of the vanilla stochastic proximal point algorithm.\nMore specifically, for convex functions, we prove a sublinear convergence rate\nof $O(1/k)$. In addition, under the Polyak-{\\L}ojasiewicz (PL) condition, we\nobtain linear convergence rates. Finally, our numerical experiments demonstrate\nthe advantages of the proximal variance reduction methods over their gradient\ncounterparts in terms of the stability with respect to the choice of the step\nsize in most cases, especially for difficult problems.\n","authors":["Cheik Traoré","Vassilis Apidopoulos","Saverio Salzo","Silvia Villa"],"pdf_url":"https://arxiv.org/pdf/2308.09310v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03085v1","updated":"2024-08-06T10:25:02Z","published":"2024-08-06T10:25:02Z","title":"Matrix Multiplication on Quantum Computer","summary":"  This paper introduces an innovative and practical approach to universal\nquantum matrix multiplication. We designed optimized quantum adders and\nmultipliers based on Quantum Fourier Transform (QFT), which significantly\nreduced the number of gates used compared to classical adders and multipliers.\nSubsequently, we construct a basic universal quantum matrix multiplication and\nextend it to the Strassen algorithm. We conduct comparative experiments to\nanalyze the performance of the quantum matrix multiplication and evaluate the\nacceleration provided by the optimized quantum adder and multiplier.\nFurthermore, we investigate the advantages and disadvantages of the quantum\nStrassen algorithm compared to basic quantum matrix multiplication.\n","authors":["Jiaqi Yao","Ding Liu"],"pdf_url":"https://arxiv.org/pdf/2408.03085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03084v1","updated":"2024-08-06T10:24:54Z","published":"2024-08-06T10:24:54Z","title":"Research on Autonomous Driving Decision-making Strategies based Deep\n  Reinforcement Learning","summary":"  The behavior decision-making subsystem is a key component of the autonomous\ndriving system, which reflects the decision-making ability of the vehicle and\nthe driver, and is an important symbol of the high-level intelligence of the\nvehicle. However, the existing rule-based decision-making schemes are limited\nby the prior knowledge of designers, and it is difficult to cope with complex\nand changeable traffic scenarios. In this work, an advanced deep reinforcement\nlearning model is adopted, which can autonomously learn and optimize driving\nstrategies in a complex and changeable traffic environment by modeling the\ndriving decision-making process as a reinforcement learning problem.\nSpecifically, we used Deep Q-Network (DQN) and Proximal Policy Optimization\n(PPO) for comparative experiments. DQN guides the agent to choose the best\naction by approximating the state-action value function, while PPO improves the\ndecision-making quality by optimizing the policy function. We also introduce\nimprovements in the design of the reward function to promote the robustness and\nadaptability of the model in real-world driving situations. Experimental\nresults show that the decision-making strategy based on deep reinforcement\nlearning has better performance than the traditional rule-based method in a\nvariety of driving tasks.\n","authors":["Zixiang Wang","Hao Yan","Changsong Wei","Junyu Wang","Shi Bo","Minheng Xiao"],"pdf_url":"https://arxiv.org/pdf/2408.03084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02354v2","updated":"2024-08-06T10:11:28Z","published":"2024-08-05T10:02:29Z","title":"RECE: Reduced Cross-Entropy Loss for Large-Catalogue Sequential\n  Recommenders","summary":"  Scalability is a major challenge in modern recommender systems. In sequential\nrecommendations, full Cross-Entropy (CE) loss achieves state-of-the-art\nrecommendation quality but consumes excessive GPU memory with large item\ncatalogs, limiting its practicality. Using a GPU-efficient locality-sensitive\nhashing-like algorithm for approximating large tensor of logits, this paper\nintroduces a novel RECE (REduced Cross-Entropy) loss. RECE significantly\nreduces memory consumption while allowing one to enjoy the state-of-the-art\nperformance of full CE loss. Experimental results on various datasets show that\nRECE cuts training peak memory usage by up to 12 times compared to existing\nmethods while retaining or exceeding performance metrics of CE loss. The\napproach also opens up new possibilities for large-scale applications in other\ndomains.\n","authors":["Danil Gusak","Gleb Mezentsev","Ivan Oseledets","Evgeny Frolov"],"pdf_url":"https://arxiv.org/pdf/2408.02354v2.pdf","comment":"5 pages, accepted for CIKM'24"},{"id":"http://arxiv.org/abs/2307.15325v2","updated":"2024-08-06T10:09:47Z","published":"2023-07-28T06:03:19Z","title":"Equivariance and partial observations in Koopman operator theory for\n  partial differential equations","summary":"  The Koopman operator has become an essential tool for data-driven analysis,\nprediction and control of complex systems. The main reason is the enormous\npotential of identifying linear function space representations of nonlinear\ndynamics from measurements. This equally applies to ordinary, stochastic, and\npartial differential equations (PDEs). Until now, with a few exceptions only,\nthe PDE case is mostly treated rather superficially, and the specific structure\nof the underlying dynamics is largely ignored. In this paper, we show that\nsymmetries in the system dynamics can be carried over to the Koopman operator,\nwhich allows us to massively increase the model efficacy. Moreover, the\nsituation where we only have access to partial observations (i.e.,\nmeasurements, as is very common for experimental data) has not been treated to\nits full extent, either. Moreover, we address the highly-relevant case where we\ncannot measure the full state, such that alternative approaches such as delay\ncoordinates have to be considered. We derive rigorous statements on the\nrequired number of observables in this situation, based on embedding theory. We\npresent numerical evidence using various numerical examples including the wave\nequation and the Kuramoto-Sivashinsky equation.\n","authors":["Sebastian Peitz","Hans Harder","Feliks Nüske","Friedrich Philipp","Manuel Schaller","Karl Worthmann"],"pdf_url":"https://arxiv.org/pdf/2307.15325v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12465v3","updated":"2024-08-06T09:57:36Z","published":"2024-05-21T02:41:40Z","title":"A finite element-based physics-informed operator learning framework for\n  spatiotemporal partial differential equations on arbitrary domains","summary":"  We propose a novel finite element-based physics-informed operator learning\nframework that allows for predicting spatiotemporal dynamics governed by\npartial differential equations (PDEs). The proposed framework employs a loss\nfunction inspired by the finite element method (FEM) with the implicit Euler\ntime integration scheme. A transient thermal conduction problem is considered\nto benchmark the performance. The proposed operator learning framework takes a\ntemperature field at the current time step as input and predicts a temperature\nfield at the next time step. The Galerkin discretized weak formulation of the\nheat equation is employed to incorporate physics into the loss function, which\nis coined finite operator learning (FOL). Upon training, the networks\nsuccessfully predict the temperature evolution over time for any initial\ntemperature field at high accuracy compared to the FEM solution. The framework\nis also confirmed to be applicable to a heterogeneous thermal conductivity and\narbitrary geometry. The advantages of FOL can be summarized as follows: First,\nthe training is performed in an unsupervised manner, avoiding the need for a\nlarge data set prepared from costly simulations or experiments. Instead, random\ntemperature patterns generated by the Gaussian random process and the Fourier\nseries, combined with constant temperature fields, are used as training data to\ncover possible temperature cases. Second, shape functions and backward\ndifference approximation are exploited for the domain discretization, resulting\nin a purely algebraic equation. This enhances training efficiency, as one\navoids time-consuming automatic differentiation when optimizing weights and\nbiases while accepting possible discretization errors. Finally, thanks to the\ninterpolation power of FEM, any arbitrary geometry can be handled with FOL,\nwhich is crucial to addressing various engineering application scenarios.\n","authors":["Yusuke Yamazaki","Ali Harandi","Mayu Muramatsu","Alexandre Viardin","Markus Apel","Tim Brepols","Stefanie Reese","Shahed Rezaei"],"pdf_url":"https://arxiv.org/pdf/2405.12465v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.10107v4","updated":"2024-08-06T08:39:33Z","published":"2024-01-18T16:18:18Z","title":"Comparison analysis between standard polysomnographic data and\n  in-ear-EEG signals: A preliminary study","summary":"  Study Objectives: Polysomnography (PSG) currently serves as the benchmark for\nevaluating sleep disorders. Its discomfort makes long-term monitoring\nunfeasible, leading to bias in sleep quality assessment. Hence, less invasive,\ncost-effective, and portable alternatives need to be explored. One promising\ncontender is the in-ear-EEG sensor. This study aims to establish a methodology\nto assess the similarity between the single-channel in-ear-EEG and standard PSG\nderivations.\n  Methods: The study involves four-hour signals recorded from ten healthy\nsubjects aged 18 to 60 years. Recordings are analyzed following two\ncomplementary approaches: (i) a hypnogram-based analysis aimed at assessing the\nagreement between PSG and in-ear-EEG-derived hypnograms; and (ii) a\nfeature-based analysis based on time- and frequency- domain feature extraction,\nunsupervised feature selection, and definition of Feature-based Similarity\nIndex via Jensen-Shannon Divergence (JSD-FSI).\n  Results: We find large variability between PSG and in-ear-EEG hypnograms\nscored by the same sleep expert according to Cohen's kappa metric, with\nsignificantly greater agreements for PSG scorers than for in-ear-EEG scorers (p\n< 0.001) based on Fleiss' kappa metric. On average, we demonstrate a high\nsimilarity between PSG and in-ear-EEG signals in terms of JSD-FSI (0.79 +/-\n0.06 -awake, 0.77 +/- 0.07 -NREM, and 0.67 +/- 0.10 -REM) and in line with the\nsimilarity values computed independently on standard PSG-channel-combinations.\n  Conclusions: In-ear-EEG is a valuable solution for home-based sleep\nmonitoring, however further studies with a larger and more heterogeneous\ndataset are needed.\n","authors":["Gianpaolo Palo","Luigi Fiorillo","Giuliana Monachino","Michal Bechny","Michel Walti","Elias Meier","Francesca Pentimalli Biscaretti di Ruffia","Mark Melnykowycz","Athina Tzovara","Valentina Agostini","Francesca Dalia Faraci"],"pdf_url":"https://arxiv.org/pdf/2401.10107v4.pdf","comment":"20 figures, 6 tables"},{"id":"http://arxiv.org/abs/2408.03029v1","updated":"2024-08-06T08:22:16Z","published":"2024-08-06T08:22:16Z","title":"Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning","summary":"  Reward shaping addresses the challenge of sparse rewards in reinforcement\nlearning by constructing denser and more informative reward signals. To achieve\nself-adaptive and highly efficient reward shaping, we propose a novel method\nthat incorporates success rates derived from historical experiences into shaped\nrewards. Our approach utilizes success rates sampled from Beta distributions,\nwhich dynamically evolve from uncertain to reliable values as more data is\ncollected. Initially, the self-adaptive success rates exhibit more randomness\nto encourage exploration. Over time, they become more certain to enhance\nexploitation, thus achieving a better balance between exploration and\nexploitation. We employ Kernel Density Estimation (KDE) combined with Random\nFourier Features (RFF) to derive the Beta distributions, resulting in a\ncomputationally efficient implementation in high-dimensional continuous state\nspaces. This method provides a non-parametric and learning-free approach. The\nproposed method is evaluated on a wide range of continuous control tasks with\nsparse and delayed rewards, demonstrating significant improvements in sample\nefficiency and convergence stability compared to several baselines.\n","authors":["Haozhe Ma","Zhengding Luo","Thanh Vinh Vo","Kuankuan Sima","Tze-Yun Leong"],"pdf_url":"https://arxiv.org/pdf/2408.03029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14244v2","updated":"2024-08-06T08:11:34Z","published":"2024-04-22T14:57:17Z","title":"AI-Generated Faces in the Real World: A Large-Scale Case Study of\n  Twitter Profile Images","summary":"  Recent advances in the field of generative artificial intelligence (AI) have\nblurred the lines between authentic and machine-generated content, making it\nalmost impossible for humans to distinguish between such media. One notable\nconsequence is the use of AI-generated images for fake profiles on social\nmedia. While several types of disinformation campaigns and similar incidents\nhave been reported in the past, a systematic analysis has been lacking. In this\nwork, we conduct the first large-scale investigation of the prevalence of\nAI-generated profile pictures on Twitter. We tackle the challenges of a\nreal-world measurement study by carefully integrating various data sources and\ndesigning a multi-stage detection pipeline. Our analysis of nearly 15 million\nTwitter profile pictures shows that 0.052% were artificially generated,\nconfirming their notable presence on the platform. We comprehensively examine\nthe characteristics of these accounts and their tweet content, and uncover\npatterns of coordinated inauthentic behavior. The results also reveal several\nmotives, including spamming and political amplification campaigns. Our research\nreaffirms the need for effective detection and mitigation strategies to cope\nwith the potential negative effects of generative AI in the future.\n","authors":["Jonas Ricker","Dennis Assenmacher","Thorsten Holz","Asja Fischer","Erwin Quiring"],"pdf_url":"https://arxiv.org/pdf/2404.14244v2.pdf","comment":"Accepted to RAID 2024"},{"id":"http://arxiv.org/abs/2404.03453v3","updated":"2024-08-06T07:53:09Z","published":"2024-04-04T13:57:44Z","title":"Conditioning of Banach Space Valued Gaussian Random Variables: An\n  Approximation Approach Based on Martingales","summary":"  In this paper we investigate the conditional distributions of two Banach\nspace valued, jointly Gaussian random variables. We show that these conditional\ndistributions are again Gaussian and that their means and covariances are\ndetermined by a general finite dimensional approximation scheme based upon a\nmartingale approach. In particular, it turns out that the covariance operators\noccurring in this scheme converge with respect to the nuclear norm and that the\nconditional probabilities converge weakly. Moreover, we discuss in detail, how\nour approximation scheme can be implemented in several classes of important\nBanach spaces such as (reproducing kernel) Hilbert spaces and spaces of\ncontinuous functions. As an example, we then apply our general results to the\ncase of Gaussian processes with continuous paths conditioned to partial but\ninfinite observations of their paths. Here we show that conditioning on\nsufficiently rich, increasing sets of finitely many observations leads to\nconsistent approximations, that is, both the mean and covariance functions\nconverge uniformly and the conditional probabilities converge weakly. Moreover,\nwe discuss how these results improve our understanding of the popular Gaussian\nprocesses for machine learning.\n","authors":["Ingo Steinwart"],"pdf_url":"https://arxiv.org/pdf/2404.03453v3.pdf","comment":"55 pages plus 22 pages of supplemental material"},{"id":"http://arxiv.org/abs/2408.03013v1","updated":"2024-08-06T07:48:51Z","published":"2024-08-06T07:48:51Z","title":"NeurDB: On the Design and Implementation of an AI-powered Autonomous\n  Database","summary":"  Databases are increasingly embracing AI to provide autonomous system\noptimization and intelligent in-database analytics, aiming to relieve end-user\nburdens across various industry sectors. Nonetheless, most existing approaches\nfail to account for the dynamic nature of databases, which renders them\nineffective for real-world applications characterized by evolving data and\nworkloads. This paper introduces NeurDB, an AI-powered autonomous database that\ndeepens the fusion of AI and databases with adaptability to data and workload\ndrift. NeurDB establishes a new in-database AI ecosystem that seamlessly\nintegrates AI workflows within the database. This integration enables efficient\nand effective in-database AI analytics and fast-adaptive learned system\ncomponents. Empirical evaluations demonstrate that NeurDB substantially\noutperforms existing solutions in managing AI analytics tasks, with the\nproposed learned components more effectively handling environmental dynamism\nthan state-of-the-art approaches.\n","authors":["Zhanhao Zhao","Shaofeng Cai","Haotian Gao","Hexiang Pan","Siqi Xiang","Naili Xing","Gang Chen","Beng Chin Ooi","Yanyan Shen","Yuncheng Wu","Meihui Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.03013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16907v2","updated":"2024-08-06T07:24:35Z","published":"2024-02-25T04:24:28Z","title":"Diffusion Posterior Proximal Sampling for Image Restoration","summary":"  Diffusion models have demonstrated remarkable efficacy in generating\nhigh-quality samples. Existing diffusion-based image restoration algorithms\nexploit pre-trained diffusion models to leverage data priors, yet they still\npreserve elements inherited from the unconditional generation paradigm. These\nstrategies initiate the denoising process with pure white noise and incorporate\nrandom noise at each generative step, leading to over-smoothed results. In this\npaper, we present a refined paradigm for diffusion-based image restoration.\nSpecifically, we opt for a sample consistent with the measurement identity at\neach generative step, exploiting the sampling selection as an avenue for output\nstability and enhancement. The number of candidate samples used for selection\nis adaptively determined based on the signal-to-noise ratio of the timestep.\nAdditionally, we start the restoration process with an initialization combined\nwith the measurement signal, providing supplementary information to better\nalign the generative process. Extensive experimental results and analyses\nvalidate that our proposed method significantly enhances image restoration\nperformance while consuming negligible additional computational resources.\n","authors":["Hongjie Wu","Linchao He","Mingqin Zhang","Dongdong Chen","Kunming Luo","Mengting Luo","Ji-Zhe Zhou","Hu Chen","Jiancheng Lv"],"pdf_url":"https://arxiv.org/pdf/2402.16907v2.pdf","comment":"ACM Multimedia 2024 Oral"},{"id":"http://arxiv.org/abs/2408.02998v1","updated":"2024-08-06T07:05:56Z","published":"2024-08-06T07:05:56Z","title":"Federated Learning Architectures: A Performance Evaluation with Crop\n  Yield Prediction Application","summary":"  Federated learning has become an emerging technology for data analysis for\nIoT applications. This paper implements centralized and decentralized federated\nlearning frameworks for crop yield prediction based on Long Short-Term Memory\nNetwork. For centralized federated learning, multiple clients and one server is\nconsidered, where the clients exchange their model updates with the server that\nworks as the aggregator to build the global model. For the decentralized\nframework, a collaborative network is formed among the devices either using\nring topology or using mesh topology. In this network, each device receives\nmodel updates from the neighbour devices, and performs aggregation to build the\nupgraded model. The performance of the centralized and decentralized federated\nlearning frameworks are evaluated in terms of prediction accuracy, precision,\nrecall, F1-Score, and training time. The experimental results present that\n$\\geq$97% and $>$97.5% prediction accuracy are achieved using the centralized\nand decentralized federated learning-based frameworks respectively. The results\nalso show that the using centralized federated learning the response time can\nbe reduced by $\\sim$75% than the cloud-only framework. Finally, the future\nresearch directions of the use of federated learning in crop yield prediction\nare explored in this paper.\n","authors":["Anwesha Mukherjee","Rajkumar Buyya"],"pdf_url":"https://arxiv.org/pdf/2408.02998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02987v1","updated":"2024-08-06T06:42:53Z","published":"2024-08-06T06:42:53Z","title":"A Differential Smoothness-based Compact-Dynamic Graph Convolutional\n  Network for Spatiotemporal Signal Recovery","summary":"  High quality spatiotemporal signal is vitally important for real application\nscenarios like energy management, traffic planning and cyber security. Due to\nthe uncontrollable factors like abrupt sensors breakdown or communication\nfault, the spatiotemporal signal collected by sensors is always incomplete. A\ndynamic graph convolutional network (DGCN) is effective for processing\nspatiotemporal signal recovery. However, it adopts a static GCN and a sequence\nneural network to explore the spatial and temporal patterns, separately. Such a\nseparated two-step processing is loose spatiotemporal, thereby failing to\ncapture the complex inner spatiotemporal correlation. To address this issue,\nthis paper proposes a Compact-Dynamic Graph Convolutional Network (CDGCN) for\nspatiotemporal signal recovery with the following two-fold ideas: a) leveraging\nthe tensor M-product to build a unified tensor graph convolution framework,\nwhich considers both spatial and temporal patterns simultaneously; and b)\nconstructing a differential smoothness-based objective function to reduce the\nnoise interference in spatiotemporal signal, thereby further improve the\nrecovery accuracy. Experiments on real-world spatiotemporal datasets\ndemonstrate that the proposed CDGCN significantly outperforms the\nstate-of-the-art models in terms of recovery accuracy.\n","authors":["Pengcheng Gao","Zicheng Gao","Ye Yuan"],"pdf_url":"https://arxiv.org/pdf/2408.02987v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01829v2","updated":"2024-08-06T06:30:08Z","published":"2024-08-03T17:43:10Z","title":"Neural Network Emulator for Atmospheric Chemical ODE","summary":"  Modeling atmospheric chemistry is complex and computationally intense. Given\nthe recent success of Deep neural networks in digital signal processing, we\npropose a Neural Network Emulator for fast chemical concentration modeling. We\nconsider atmospheric chemistry as a time-dependent Ordinary Differential\nEquation. To extract the hidden correlations between initial states and future\ntime evolution, we propose ChemNNE, an Attention based Neural Network Emulator\n(NNE) that can model the atmospheric chemistry as a neural ODE process. To\nefficiently simulate the chemical changes, we propose the sinusoidal time\nembedding to estimate the oscillating tendency over time. More importantly, we\nuse the Fourier neural operator to model the ODE process for efficient\ncomputation. We also propose three physical-informed losses to supervise the\ntraining optimization. To evaluate our model, we propose a large-scale chemical\ndataset that can be used for neural network training and evaluation. The\nextensive experiments show that our approach achieves state-of-the-art\nperformance in modeling accuracy and computational speed.\n","authors":["Zhi-Song Liu","Petri Clusius","Michael Boy"],"pdf_url":"https://arxiv.org/pdf/2408.01829v2.pdf","comment":"25 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.17963v4","updated":"2024-08-06T06:28:57Z","published":"2023-03-31T11:06:09Z","title":"Learning-Based Optimal Control with Performance Guarantees for Unknown\n  Systems with Latent States","summary":"  As control engineering methods are applied to increasingly complex systems,\ndata-driven approaches for system identification appear as a promising\nalternative to physics-based modeling. While the Bayesian approaches prevalent\nfor safety-critical applications usually rely on the availability of state\nmeasurements, the states of a complex system are often not directly measurable.\nIt may then be necessary to jointly estimate the dynamics and the latent state,\nmaking the quantification of uncertainties and the design of controllers with\nformal performance guarantees considerably more challenging. This paper\nproposes a novel method for the computation of an optimal input trajectory for\nunknown nonlinear systems with latent states based on a combination of particle\nMarkov chain Monte Carlo methods and scenario theory. Probabilistic performance\nguarantees are derived for the resulting input trajectory, and an approach to\nvalidate the performance of arbitrary control laws is presented. The\neffectiveness of the proposed method is demonstrated in a numerical simulation.\n","authors":["Robert Lefringhausen","Supitsana Srithasan","Armin Lederer","Sandra Hirche"],"pdf_url":"https://arxiv.org/pdf/2303.17963v4.pdf","comment":"Accepted version submitted to the 2024 European Control Conference\n  (ECC)"},{"id":"http://arxiv.org/abs/2109.03445v6","updated":"2024-08-06T06:19:46Z","published":"2021-09-08T06:06:28Z","title":"Convergence of Batch Asynchronous Stochastic Approximation With\n  Applications to Reinforcement Learning","summary":"  We begin by briefly surveying some results on the convergence of the\nStochastic Gradient Descent (SGD) Method, proved in a companion paper by the\npresent authors. These results are based on viewing SGD as a version of\nStochastic Approximation (SA). Ever since its introduction in the classic paper\nof Robbins and Monro in 1951, SA has become a standard tool for finding a\nsolution of an equation of the form $f(\\theta) = 0$, when only noisy\nmeasurements of $f(\\cdot)$ are available. In most situations, \\textit{every\ncomponent} of the putative solution $\\theta_t$ is updated at each step $t$. In\nsome applications in Reinforcement Learning (RL), \\textit{only one component}\nof $\\theta_t$ is updated at each $t$. This is known as \\textbf{asynchronous}\nSA. In this paper, we study \\textbf{Block Asynchronous SA (BASA)}, in which, at\neach step $t$, \\textit{some but not necessarily all} components of $\\theta_t$\nare updated. The theory presented here embraces both conventional (synchronous)\nSA as well as asynchronous SA, and all in-between possibilities. We provide\nsufficient conditions for the convergence of BASA, and also prove bounds on the\n\\textit{rate} of convergence of $\\theta_t$ to the solution. For the case of\nconventional SGD, these results reduce to those proved in our companion paper.\nThen we apply these results to the problem of finding a fixed point of a map\nwith only noisy measurements. This problem arises frequently in RL. We prove\nsufficient conditions for convergence as well as estimates for the rate of\nconvergence.\n","authors":["Rajeeva L. Karandikar","M. Vidyasagar"],"pdf_url":"https://arxiv.org/pdf/2109.03445v6.pdf","comment":"34 pages, 1 figure"},{"id":"http://arxiv.org/abs/2408.02971v1","updated":"2024-08-06T06:00:17Z","published":"2024-08-06T06:00:17Z","title":"Wave Interpolation Neural Operator: Interpolated Prediction of Electric\n  Fields Across Untrained Wavelengths","summary":"  Designing photonic structures requires electromagnetic simulations, which\noften require high computational costs. Researchers have developed surrogate\nsolvers for predicting electric fields to alleviate the computational issues.\nHowever, existing surrogate solvers are limited to performing inference at\nfixed simulation conditions and require retraining for different conditions. To\naddress this, we propose Wave Interpolation Neural Operator (WINO), a novel\nsurrogate solver enabling simulation condition interpolation across a\ncontinuous spectrum of broadband wavelengths. WINO introduces the Fourier Group\nConvolution Shuffling operator and a new conditioning method to efficiently\npredict electric fields from both trained and untrained wavelength data,\nachieving significant improvements in parameter efficiency and spectral\ninterpolation performance. Our model demonstrates approximately 100 times\nfaster performance than traditional finite-difference frequency-domain\nsimulations. Moreover, compared to the state-of-the-art model, we achieve a 74%\nreduction in parameters and 80.5% improvements in prediction accuracy for\nuntrained wavelengths, and 13.2% improvements for trained wavelengths.\n","authors":["Joonhyuk Seo","Chanik Kang","Dongjin Seo","Haejun Chung"],"pdf_url":"https://arxiv.org/pdf/2408.02971v1.pdf","comment":"9 pages, 5 figures, 4 tables / Appendix: 4 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2408.02965v1","updated":"2024-08-06T05:21:31Z","published":"2024-08-06T05:21:31Z","title":"Data-Driven Stochastic Closure Modeling via Conditional Diffusion Model\n  and Neural Operator","summary":"  Closure models are widely used in simulating complex multiscale dynamical\nsystems such as turbulence and the earth system, for which direct numerical\nsimulation that resolves all scales is often too expensive. For those systems\nwithout a clear scale separation, deterministic and local closure models often\nlack enough generalization capability, which limits their performance in many\nreal-world applications. In this work, we propose a data-driven modeling\nframework for constructing stochastic and non-local closure models via\nconditional diffusion model and neural operator. Specifically, the Fourier\nneural operator is incorporated into a score-based diffusion model, which\nserves as a data-driven stochastic closure model for complex dynamical systems\ngoverned by partial differential equations (PDEs). We also demonstrate how\naccelerated sampling methods can improve the efficiency of the data-driven\nstochastic closure model. The results show that the proposed methodology\nprovides a systematic approach via generative machine learning techniques to\nconstruct data-driven stochastic closure models for multiscale dynamical\nsystems with continuous spatiotemporal fields.\n","authors":["Xinghao Dong","Chuanqi Chen","Jin-Long Wu"],"pdf_url":"https://arxiv.org/pdf/2408.02965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02961v1","updated":"2024-08-06T05:15:57Z","published":"2024-08-06T05:15:57Z","title":"Synaptic Modulation using Interspike Intervals Increases Energy\n  Efficiency of Spiking Neural Networks","summary":"  Despite basic differences between Spiking Neural Networks (SNN) and\nArtificial Neural Networks (ANN), most research on SNNs involve adapting\nANN-based methods for SNNs. Pruning (dropping connections) and quantization\n(reducing precision) are often used to improve energy efficiency of SNNs. These\nmethods are very effective for ANNs whose energy needs are determined by\nsignals transmitted on synapses. However, the event-driven paradigm in SNNs\nimplies that energy is consumed by spikes. In this paper, we propose a new\nsynapse model whose weights are modulated by Interspike Intervals (ISI) i.e.\ntime difference between two spikes. SNNs composed of this synapse model, termed\nISI Modulated SNNs (IMSNN), can use gradient descent to estimate how the ISI of\na neuron changes after updating its synaptic parameters. A higher ISI implies\nfewer spikes and vice-versa. The learning algorithm for IMSNNs exploits this\ninformation to selectively propagate gradients such that learning is achieved\nby increasing the ISIs resulting in a network that generates fewer spikes. The\nperformance of IMSNNs with dense and convolutional layers have been evaluated\nin terms of classification accuracy and the number of spikes using the MNIST\nand FashionMNIST datasets. The performance comparison with conventional SNNs\nshows that IMSNNs exhibit upto 90% reduction in the number of spikes while\nmaintaining similar classification accuracy.\n","authors":["Dylan Adams","Magda Zajaczkowska","Ashiq Anjum","Andrea Soltoggio","Shirin Dora"],"pdf_url":"https://arxiv.org/pdf/2408.02961v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2407.11652v4","updated":"2024-08-06T05:10:56Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v4.pdf","comment":"I found critical errors in the manuscript affecting its validity. I\n  need to correct these before resubmitting. Major changes to methodology and\n  results are underway, significantly altering the content. I will resubmit the\n  revised version"},{"id":"http://arxiv.org/abs/2408.02950v1","updated":"2024-08-06T04:28:16Z","published":"2024-08-06T04:28:16Z","title":"Kolmogorov-Arnold PointNet: Deep learning for prediction of fluid fields\n  on irregular geometries","summary":"  We present Kolmogorov-Arnold PointNet (KA-PointNet) as a novel supervised\ndeep learning framework for the prediction of incompressible steady-state fluid\nflow fields in irregular domains, where the predicted fields are a function of\nthe geometry of the domains. In KA-PointNet, we implement shared\nKolmogorov-Arnold Networks (KANs) in the segmentation branch of the PointNet\narchitecture. We utilize Jacobi polynomials to construct shared KANs. As a\nbenchmark test case, we consider incompressible laminar steady-state flow over\na cylinder, where the geometry of its cross-section varies over the data set.\nWe investigate the performance of Jacobi polynomials with different degrees as\nwell as special cases of Jacobi polynomials such as Legendre polynomials,\nChebyshev polynomials of the first and second kinds, and Gegenbauer\npolynomials, in terms of the computational cost of training and accuracy of\nprediction of the test set. Additionally, we compare the performance of\nPointNet with shared KANs (i.e., KA-PointNet) and PointNet with shared\nMultilayer Perceptrons (MLPs). It is observed that when the number of trainable\nparameters is approximately equal, PointNet with shared KANs (i.e.,\nKA-PointNet) outperforms PointNet with shared MLPs.\n","authors":["Ali Kashefi"],"pdf_url":"https://arxiv.org/pdf/2408.02950v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02338v3","updated":"2024-08-06T04:15:27Z","published":"2024-02-04T04:21:34Z","title":"NetLLM: Adapting Large Language Models for Networking","summary":"  Many networking tasks now employ deep learning (DL) to solve complex\nprediction and optimization problems. However, current design philosophy of\nDL-based algorithms entails intensive engineering overhead due to the manual\ndesign of deep neural networks (DNNs) for different networking tasks. Besides,\nDNNs tend to achieve poor generalization performance on unseen data\ndistributions/environments.\n  Motivated by the recent success of large language models (LLMs), this work\nstudies the LLM adaptation for networking to explore a more sustainable design\nphilosophy. With the powerful pre-trained knowledge, the LLM is promising to\nserve as the foundation model to achieve \"one model for all tasks\" with even\nbetter performance and stronger generalization. In pursuit of this vision, we\npresent NetLLM, the first framework that provides a coherent design to harness\nthe powerful capabilities of LLMs with low efforts to solve networking\nproblems. Specifically, NetLLM empowers the LLM to effectively process\nmultimodal data in networking and efficiently generate task-specific answers.\nBesides, NetLLM drastically reduces the costs of fine-tuning the LLM to acquire\ndomain knowledge for networking. Across three networking-related use cases -\nviewport prediction, adaptive bitrate streaming and cluster job scheduling, we\nshowcase that the NetLLM-adapted LLM significantly outperforms state-of-the-art\nalgorithms.\n","authors":["Duo Wu","Xianda Wang","Yaqi Qiao","Zhi Wang","Junchen Jiang","Shuguang Cui","Fangxin Wang"],"pdf_url":"https://arxiv.org/pdf/2402.02338v3.pdf","comment":"This paper has been accepted by ACM SIGCOMM 2024. DOI:\n  https://doi.org/10.1145/3651890.3672268"},{"id":"http://arxiv.org/abs/2408.02946v1","updated":"2024-08-06T04:14:29Z","published":"2024-08-06T04:14:29Z","title":"Scaling Laws for Data Poisoning in LLMs","summary":"  Recent work shows that LLMs are vulnerable to data poisoning, in which they\nare trained on partially corrupted or harmful data. Poisoned data is hard to\ndetect, breaks guardrails, and leads to undesirable and harmful behavior. Given\nthe intense efforts by leading labs to train and deploy increasingly larger and\nmore capable LLMs, it is critical to ask if the risk of data poisoning will be\nnaturally mitigated by scale, or if it is an increasing threat. We consider\nthree threat models by which data poisoning can occur: malicious fine-tuning,\nimperfect data curation, and intentional data contamination. Our experiments\nevaluate the effects of data poisoning on 23 frontier LLMs ranging from 1.5-72\nbillion parameters on three datasets which speak to each of our threat models.\nWe find that larger LLMs are increasingly vulnerable, learning harmful behavior\n-- including sleeper agent behavior -- significantly more quickly than smaller\nLLMs with even minimal data poisoning. These results underscore the need for\nrobust safeguards against data poisoning in larger LLMs.\n","authors":["Dillon Bowen","Brendan Murphy","Will Cai","David Khachaturov","Adam Gleave","Kellin Pelrine"],"pdf_url":"https://arxiv.org/pdf/2408.02946v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2408.02336v2","updated":"2024-08-06T04:04:23Z","published":"2024-08-05T09:19:52Z","title":"Infusing Environmental Captions for Long-Form Video Language Grounding","summary":"  In this work, we tackle the problem of long-form video-language grounding\n(VLG). Given a long-form video and a natural language query, a model should\ntemporally localize the precise moment that answers the query. Humans can\neasily solve VLG tasks, even with arbitrarily long videos, by discarding\nirrelevant moments using extensive and robust knowledge gained from experience.\nUnlike humans, existing VLG methods are prone to fall into superficial cues\nlearned from small-scale datasets, even when they are within irrelevant frames.\nTo overcome this challenge, we propose EI-VLG, a VLG method that leverages\nricher textual information provided by a Multi-modal Large Language Model\n(MLLM) as a proxy for human experiences, helping to effectively exclude\nirrelevant frames. We validate the effectiveness of the proposed method via\nextensive experiments on a challenging EgoNLQ benchmark.\n","authors":["Hyogun Lee","Soyeon Hong","Mujeen Sung","Jinwoo Choi"],"pdf_url":"https://arxiv.org/pdf/2408.02336v2.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2312.05356v4","updated":"2024-08-06T03:57:33Z","published":"2023-12-08T20:28:08Z","title":"Neuron Patching: Semantic-based Neuron-level Language Model Repair for\n  Code Generation","summary":"  Large Language Models (LLMs) have already gained widespread adoption in\nsoftware engineering, particularly in code generation tasks. However, updating\nthese models with new knowledge can be prohibitively expensive, yet it is\nessential to maximize their utility, such as implementing a hotfix technique to\naddress urgent or critical LLM errors. In this paper, we propose \\textsc{MENT},\na novel and effective model editing approach to repair LLMs in coding tasks.\n\\textsc{MENT} is effective, efficient, and reliable, capable of correcting a\nneural model by patching just one or two neurons. As pioneering work on\nneuron-level model editing of generative models, we formalize the editing\nprocess and introduce the involved concepts. We also introduce new measures to\nevaluate its generalization ability and establish a benchmark for further\nstudy. Our approach is evaluated on three coding tasks: line-level code\ngeneration, shellcode generation, and intent-to-bash translation. The\nexperimental results demonstrate that the proposed approach significantly\noutperforms the state-of-the-art in both effectiveness and efficiency measures.\nFurthermore, we showcase the applications of \\textsc{MENT} for LLM reasoning in\nsoftware engineering. By editing LLM knowledge, the directly or indirectly\ndependent behaviors of API invocation in the chain-of-thought change\naccordingly. This illustrates the significance of repairing LLMs in the context\nof software engineering.\n","authors":["Jian Gu","Aldeida Aleti","Chunyang Chen","Hongyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.05356v4.pdf","comment":"12 pages, 7 figures, 7 tables, under peer-review"},{"id":"http://arxiv.org/abs/2408.02936v1","updated":"2024-08-06T03:42:38Z","published":"2024-08-06T03:42:38Z","title":"Achieving More with Less: A Tensor-Optimization-Powered Ensemble Method","summary":"  Ensemble learning is a method that leverages weak learners to produce a\nstrong learner. However, obtaining a large number of base learners requires\nsubstantial time and computational resources. Therefore, it is meaningful to\nstudy how to achieve the performance typically obtained with many base learners\nusing only a few. We argue that to achieve this, it is essential to enhance\nboth classification performance and generalization ability during the ensemble\nprocess. To increase model accuracy, each weak base learner needs to be more\nefficiently integrated. It is observed that different base learners exhibit\nvarying levels of accuracy in predicting different classes. To capitalize on\nthis, we introduce confidence tensors $\\tilde{\\mathbf{\\Theta}}$ and\n$\\tilde{\\mathbf{\\Theta}}_{rst}$ signifies that the $t$-th base classifier\nassigns the sample to class $r$ while it actually belongs to class $s$. To the\nbest of our knowledge, this is the first time an evaluation of the performance\nof base classifiers across different classes has been proposed. The proposed\nconfidence tensor compensates for the strengths and weaknesses of each base\nclassifier in different classes, enabling the method to achieve superior\nresults with a smaller number of base learners. To enhance generalization\nperformance, we design a smooth and convex objective function that leverages\nthe concept of margin, making the strong learner more discriminative.\nFurthermore, it is proved that in gradient matrix of the loss function, the sum\nof each column's elements is zero, allowing us to solve a constrained\noptimization problem using gradient-based methods. We then compare our\nalgorithm with random forests of ten times the size and other classical methods\nacross numerous datasets, demonstrating the superiority of our approach.\n","authors":["Jinghui Yuan","Weijin Jiang","Zhe Cao","Fangyuan Xie","Rong Wang","Feiping Nie","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.02936v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02932v1","updated":"2024-08-06T03:34:43Z","published":"2024-08-06T03:34:43Z","title":"Doubly Stochastic Adaptive Neighbors Clustering via the Marcus Mapping","summary":"  Clustering is a fundamental task in machine learning and data science, and\nsimilarity graph-based clustering is an important approach within this domain.\nDoubly stochastic symmetric similarity graphs provide numerous benefits for\nclustering problems and downstream tasks, yet learning such graphs remains a\nsignificant challenge. Marcus theorem states that a strictly positive symmetric\nmatrix can be transformed into a doubly stochastic symmetric matrix by diagonal\nmatrices. However, in clustering, learning sparse matrices is crucial for\ncomputational efficiency. We extend Marcus theorem by proposing the Marcus\nmapping, which indicates that certain sparse matrices can also be transformed\ninto doubly stochastic symmetric matrices via diagonal matrices. Additionally,\nwe introduce rank constraints into the clustering problem and propose the\nDoubly Stochastic Adaptive Neighbors Clustering algorithm based on the Marcus\nMapping (ANCMM). This ensures that the learned graph naturally divides into the\ndesired number of clusters. We validate the effectiveness of our algorithm\nthrough extensive comparisons with state-of-the-art algorithms. Finally, we\nexplore the relationship between the Marcus mapping and optimal transport. We\nprove that the Marcus mapping solves a specific type of optimal transport\nproblem and demonstrate that solving this problem through Marcus mapping is\nmore efficient than directly applying optimal transport methods.\n","authors":["Jinghui Yuan","Chusheng Zeng","Fangyuan Xie","Zhe Cao","Rong Wang","Feiping Nie","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.02932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.10838v7","updated":"2024-08-06T03:34:27Z","published":"2022-01-26T09:44:13Z","title":"Privacy-Preserving Logistic Regression Training with A Faster Gradient\n  Variant","summary":"  Logistic regression training over encrypted data has been an attractive idea\nto security concerns for years. In this paper, we propose a faster gradient\nvariant called $\\texttt{quadratic gradient}$ for privacy-preserving logistic\nregression training. The core of $\\texttt{quadratic gradient}$ can be seen as\nan extension of the simplified fixed Hessian. We enhance Nesterov's accelerated\ngradient (NAG) and Adaptive Gradient Algorithm (Adagrad) respectively with\n$\\texttt{quadratic gradient}$ and evaluate the enhanced algorithms on several\ndatasets. Experiments show that the enhanced methods have a state-of-the-art\nperformance in convergence speed compared to the raw first-order gradient\nmethods. We then adopt the enhanced NAG method to implement homomorphic\nlogistic regression training, obtaining a comparable result by only $3$\niterations. There is a promising chance that $\\texttt{quadratic gradient}$\ncould be used to enhance other first-order gradient methods for general\nnumerical optimization problems.\n","authors":["John Chiang"],"pdf_url":"https://arxiv.org/pdf/2201.10838v7.pdf","comment":"The basic work of this paper, $\\texttt{quadratic gradient}$ and the\n  enhanced full batch NAG, was nearly finished in September 2019. The initial\n  version of this paper was written in April 2020, rejected by ICANN 2020. The\n  enhanced mini-batch NAG was introduced into this paper in September 2020 and\n  later rejected by a special issue on the journal FGCS 2020"},{"id":"http://arxiv.org/abs/2402.12022v2","updated":"2024-08-06T03:34:06Z","published":"2024-02-19T10:31:53Z","title":"Distilling Large Language Models for Text-Attributed Graph Learning","summary":"  Text-Attributed Graphs (TAGs) are graphs of connected textual documents.\nGraph models can efficiently learn TAGs, but their training heavily relies on\nhuman-annotated labels, which are scarce or even unavailable in many\napplications. Large language models (LLMs) have recently demonstrated\nremarkable capabilities in few-shot and zero-shot TAG learning, but they suffer\nfrom scalability, cost, and privacy issues. Therefore, in this work, we focus\non synergizing LLMs and graph models with their complementary strengths by\ndistilling the power of LLMs to a local graph model on TAG learning. To address\nthe inherent gaps between LLMs (generative models for texts) and graph models\n(discriminative models for graphs), we propose first to let LLMs teach an\ninterpreter with rich textual rationale and then let a student model mimic the\ninterpreter's reasoning without LLMs' textual rationale. Extensive experiments\nvalidate the efficacy of our proposed framework.\n","authors":["Bo Pan","Zheng Zhang","Yifei Zhang","Yuntong Hu","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.12022v2.pdf","comment":"CIKM 2024"},{"id":"http://arxiv.org/abs/2404.06675v2","updated":"2024-08-06T03:33:41Z","published":"2024-04-10T01:35:17Z","title":"Toward Cross-Layer Energy Optimizations in AI Systems","summary":"  The \"AI for Science, Energy, and Security\" report from DOE outlines a\nsignificant focus on developing and optimizing artificial intelligence\nworkflows for a foundational impact on a broad range of DOE missions. With the\npervasive usage of artificial intelligence (AI) and machine learning (ML) tools\nand techniques, their energy efficiency is likely to become the gating factor\ntoward adoption. This is because generative AI (GenAI) models are massive\nenergy hogs: for instance, training a 200-billion parameter large language\nmodel (LLM) at Amazon is estimated to have taken 11.9 GWh, which is enough to\npower more than a thousand average U.S. households for a year. Inference\nconsumes even more energy, because a model trained once serve millions. Given\nthis scale, high energy efficiency is key to addressing the power delivery\nproblem of constructing and operating new supercomputers and datacenters\nspecialized for AI workloads. In that regard, we outline software- and\narchitecture-level research challenges and opportunities, setting the stage for\ncreating cross-layer energy optimizations in AI systems.\n","authors":["Jae-Won Chung","Nishil Talati","Mosharaf Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2404.06675v2.pdf","comment":"2024 Energy-Efficient Computing for Science Workshop"},{"id":"http://arxiv.org/abs/2408.02930v1","updated":"2024-08-06T03:26:01Z","published":"2024-08-06T03:26:01Z","title":"The Need for a Big World Simulator: A Scientific Challenge for Continual\n  Learning","summary":"  The \"small agent, big world\" frame offers a conceptual view that motivates\nthe need for continual learning. The idea is that a small agent operating in a\nmuch bigger world cannot store all information that the world has to offer. To\nperform well, the agent must be carefully designed to ingest, retain, and eject\nthe right information. To enable the development of performant continual\nlearning agents, a number of synthetic environments have been proposed.\nHowever, these benchmarks suffer from limitations, including unnatural\ndistribution shifts and a lack of fidelity to the \"small agent, big world\"\nframing. This paper aims to formalize two desiderata for the design of future\nsimulated environments. These two criteria aim to reflect the objectives and\ncomplexity of continual learning in practical settings while enabling rapid\nprototyping of algorithms on a smaller scale.\n","authors":["Saurabh Kumar","Hong Jun Jeon","Alex Lewandowski","Benjamin Van Roy"],"pdf_url":"https://arxiv.org/pdf/2408.02930v1.pdf","comment":"Accepted to the Finding the Frame Workshop at RLC 2024"},{"id":"http://arxiv.org/abs/2408.02927v1","updated":"2024-08-06T03:21:13Z","published":"2024-08-06T03:21:13Z","title":"HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy\n  Protection","summary":"  Data serves as the fundamental foundation for advancing deep learning,\nparticularly tabular data presented in a structured format, which is highly\nconducive to modeling. However, even in the era of LLM, obtaining tabular data\nfrom sensitive domains remains a challenge due to privacy or copyright\nconcerns. Hence, exploring how to effectively use models like LLMs to generate\nrealistic and privacy-preserving synthetic tabular data is urgent. In this\npaper, we take a step forward to explore LLMs for tabular data synthesis and\nprivacy protection, by introducing a new framework HARMONIC for tabular data\ngeneration and evaluation. In the tabular data generation of our framework,\nunlike previous small-scale LLM-based methods that rely on continued\npre-training, we explore the larger-scale LLMs with fine-tuning to generate\ntabular data and enhance privacy. Based on idea of the k-nearest neighbors\nalgorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to\ndiscover inter-row relationships. Then, with fine-tuning, LLMs are trained to\nremember the format and connections of the data rather than the data itself,\nwhich reduces the risk of privacy leakage. In the evaluation part of our\nframework, we develop specific privacy risk metrics DLT for LLM synthetic data\ngeneration, as well as performance evaluation metrics LLE for downstream LLM\ntasks. Our experiments find that this tabular data generation framework\nachieves equivalent performance to existing methods with better privacy, which\nalso demonstrates our evaluation framework for the effectiveness of synthetic\ndata and privacy risks in LLM scenarios.\n","authors":["Yuxin Wang","Duanyu Feng","Yongfu Dai","Zhengyu Chen","Jimin Huang","Sophia Ananiadou","Qianqian Xie","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2408.02927v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00799v2","updated":"2024-08-06T03:03:16Z","published":"2024-07-22T02:27:57Z","title":"Deep Uncertainty-Based Explore for Index Construction and Retrieval in\n  Recommendation System","summary":"  In recommendation systems, the relevance and novelty of the final results are\nselected through a cascade system of Matching -> Ranking -> Strategy. The\nmatching model serves as the starting point of the pipeline and determines the\nupper bound of the subsequent stages. Balancing the relevance and novelty of\nmatching results is a crucial step in the design and optimization of\nrecommendation systems, contributing significantly to improving recommendation\nquality. However, the typical matching algorithms have not simultaneously\naddressed the relevance and novelty perfectly. One main reason is that deep\nmatching algorithms exhibit significant uncertainty when estimating items in\nthe long tail (e.g., due to insufficient training samples) items.The\nuncertainty not only affects the training of the models but also influences the\nconfidence in the index construction and beam search retrieval process of these\nmodels. This paper proposes the UICR (Uncertainty-based explore for Index\nConstruction and Retrieval) algorithm, which introduces the concept of\nuncertainty modeling in the matching stage and achieves multi-task modeling of\nmodel uncertainty and index uncertainty. The final matching results are\nobtained by combining the relevance score and uncertainty score infered by the\nmodel. Experimental results demonstrate that the UICR improves novelty without\nsacrificing relevance on realworld industrial productive environments and\nmultiple open-source datasets. Remarkably, online A/B test results of display\nadvertising in Shopee demonstrates the effectiveness of the proposed algorithm.\n","authors":["Xin Jiang","Kaiqiang Wang","Yinlong Wang","Fengchang Lv","Taiyang Peng","Shuai Yang","Xianteng Wu","Pengye Zhang","Shuo Yuan","Yifan Zeng"],"pdf_url":"https://arxiv.org/pdf/2408.00799v2.pdf","comment":"accepted by cikm2024"},{"id":"http://arxiv.org/abs/2402.13699v4","updated":"2024-08-06T02:32:36Z","published":"2024-02-21T11:00:23Z","title":"Automation of Quantum Dot Measurement Analysis via Explainable Machine\n  Learning","summary":"  The rapid development of quantum dot (QD) devices for quantum computing has\nnecessitated more efficient and automated methods for device characterization\nand tuning. Many of the measurements acquired during the tuning process come in\nthe form of images that need to be properly analyzed to guide the subsequent\ntuning steps. By design, features present in such images capture certain\nbehaviors or states of the measured QD devices. When considered carefully, such\nfeatures can aid the control and calibration of QD devices. An important\nexample of such images are so-called \\textit{triangle plots}, which visually\nrepresent current flow and reveal characteristics important for QD device\ncalibration. While image-based classification tools, such as convolutional\nneural networks (CNNs), can be used to verify whether a given measurement is\n\\textit{good} and thus warrants the initiation of the next phase of tuning,\nthey do not provide any insights into how the device should be adjusted in the\ncase of \\textit{bad} images. This is because CNNs sacrifice prediction and\nmodel intelligibility for high accuracy. To ameliorate this trade-off, a recent\nstudy introduced an image vectorization approach that relies on the Gabor\nwavelet transform [1]. Here we propose an alternative vectorization method that\ninvolves mathematical modeling of synthetic triangles to mimic the experimental\ndata. Using explainable boosting machines, we show that this new method offers\nsuperior explainability of model prediction without sacrificing accuracy. This\nwork demonstrates the feasibility and advantages of applying explainable\nmachine learning techniques to the analysis of quantum dot measurements, paving\nthe way for further advances in automated and transparent QD device tuning.\n","authors":["Daniel Schug","Tyler J. Kovach","M. A. Wolfe","Jared Benson","Sanghyeok Park","J. P. Dodson","J. Corrigan","M. A. Eriksson","Justyna P. Zwolak"],"pdf_url":"https://arxiv.org/pdf/2402.13699v4.pdf","comment":"17 pages, 4 figures, abbreviated version published in Proceedings of\n  the XAI4Sci: Explainable machine learning for sciences workshop at AAAI 2024,\n  (Vancouver, Canada)"},{"id":"http://arxiv.org/abs/2408.02897v1","updated":"2024-08-06T02:06:04Z","published":"2024-08-06T02:06:04Z","title":"A Metric Driven Approach to Mixed Precision Training","summary":"  As deep learning methodologies have developed, it has been generally agreed\nthat increasing neural network size improves model quality. However, this is at\nthe expense of memory and compute requirements, which also need to be\nincreased. Various efficiency techniques have been proposed to rein in hardware\ncosts, one being the use of low precision numerics. Recent accelerators have\nintroduced several different 8-bit data types to help accommodate DNNs in terms\nof numerics. In this paper, we identify a metric driven methodology to aid in\nthe choice of numerics. We demonstrate how such a methodology can help scale\ntraining of a language representation model. The technique can be generalized\nto other model architectures.\n","authors":["Mitchelle Rasquinha","Gil Tabak"],"pdf_url":"https://arxiv.org/pdf/2408.02897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12428v3","updated":"2024-08-06T01:45:44Z","published":"2023-10-19T02:42:20Z","title":"Enhanced Local Explainability and Trust Scores with Random Forest\n  Proximities","summary":"  We initiate a novel approach to explain the predictions and out of sample\nperformance of random forest (RF) regression and classification models by\nexploiting the fact that any RF can be mathematically formulated as an adaptive\nweighted K nearest-neighbors model. Specifically, we employ a recent result\nthat, for both regression and classification tasks, any RF prediction can be\nrewritten exactly as a weighted sum of the training targets, where the weights\nare RF proximities between the corresponding pairs of data points. We show that\nthis linearity facilitates a local notion of explainability of RF predictions\nthat generates attributions for any model prediction across observations in the\ntraining set, and thereby complements established feature-based methods like\nSHAP, which generate attributions for a model prediction across input features.\nWe show how this proximity-based approach to explainability can be used in\nconjunction with SHAP to explain not just the model predictions, but also\nout-of-sample performance, in the sense that proximities furnish a novel means\nof assessing when a given model prediction is more or less likely to be\ncorrect. We demonstrate this approach in the modeling of US corporate bond\nprices and returns in both regression and classification cases.\n","authors":["Joshua Rosaler","Dhruv Desai","Bhaskarjit Sarmah","Dimitrios Vamvourellis","Deran Onay","Dhagash Mehta","Stefano Pasquali"],"pdf_url":"https://arxiv.org/pdf/2310.12428v3.pdf","comment":"5 pages, 6 figures"},{"id":"http://arxiv.org/abs/2405.19779v2","updated":"2024-08-06T01:25:33Z","published":"2024-05-30T07:44:31Z","title":"Automatic Graph Topology-Aware Transformer","summary":"  Existing efforts are dedicated to designing many topologies and graph-aware\nstrategies for the graph Transformer, which greatly improve the model's\nrepresentation capabilities. However, manually determining the suitable\nTransformer architecture for a specific graph dataset or task requires\nextensive expert knowledge and laborious trials. This paper proposes an\nevolutionary graph Transformer architecture search framework (EGTAS) to\nautomate the construction of strong graph Transformers. We build a\ncomprehensive graph Transformer search space with the micro-level and\nmacro-level designs. EGTAS evolves graph Transformer topologies at the macro\nlevel and graph-aware strategies at the micro level. Furthermore, a surrogate\nmodel based on generic architectural coding is proposed to directly predict the\nperformance of graph Transformers, substantially reducing the evaluation cost\nof evolutionary search. We demonstrate the efficacy of EGTAS across a range of\ngraph-level and node-level tasks, encompassing both small-scale and large-scale\ngraph datasets. Experimental results and ablation studies show that EGTAS can\nconstruct high-performance architectures that rival state-of-the-art manual and\nautomated baselines.\n","authors":["Chao Wang","Jiaxuan Zhao","Lingling Li","Licheng Jiao","Fang Liu","Shuyuan Yang"],"pdf_url":"https://arxiv.org/pdf/2405.19779v2.pdf","comment":"This work has been accepted by IEEE Transactions on Neural Networks\n  and Learning Systems. Copyright may be transferred without notice, after\n  which this version may no longer be accessible"},{"id":"http://arxiv.org/abs/2408.02882v1","updated":"2024-08-06T01:20:12Z","published":"2024-08-06T01:20:12Z","title":"Compromising Embodied Agents with Contextual Backdoor Attacks","summary":"  Large language models (LLMs) have transformed the development of embodied\nintelligence. By providing a few contextual demonstrations, developers can\nutilize the extensive internal knowledge of LLMs to effortlessly translate\ncomplex tasks described in abstract language into sequences of code snippets,\nwhich will serve as the execution logic for embodied agents. However, this\npaper uncovers a significant backdoor security threat within this process and\nintroduces a novel method called \\method{}. By poisoning just a few contextual\ndemonstrations, attackers can covertly compromise the contextual environment of\na black-box LLM, prompting it to generate programs with context-dependent\ndefects. These programs appear logically sound but contain defects that can\nactivate and induce unintended behaviors when the operational agent encounters\nspecific triggers in its interactive environment. To compromise the LLM's\ncontextual environment, we employ adversarial in-context generation to optimize\npoisoned demonstrations, where an LLM judge evaluates these poisoned prompts,\nreporting to an additional LLM that iteratively optimizes the demonstration in\na two-player adversarial game using chain-of-thought reasoning. To enable\ncontext-dependent behaviors in downstream agents, we implement a dual-modality\nactivation strategy that controls both the generation and execution of program\ndefects through textual and visual triggers. We expand the scope of our attack\nby developing five program defect modes that compromise key aspects of\nconfidentiality, integrity, and availability in embodied agents. To validate\nthe effectiveness of our approach, we conducted extensive experiments across\nvarious tasks, including robot planning, robot manipulation, and compositional\nvisual reasoning. Additionally, we demonstrate the potential impact of our\napproach by successfully attacking real-world autonomous driving systems.\n","authors":["Aishan Liu","Yuguang Zhou","Xianglong Liu","Tianyuan Zhang","Siyuan Liang","Jiakai Wang","Yanjun Pu","Tianlin Li","Junqi Zhang","Wenbo Zhou","Qing Guo","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2408.02882v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01331v2","updated":"2024-08-06T01:10:34Z","published":"2024-08-02T15:29:39Z","title":"UnifiedNN: Efficient Neural Network Training on the Cloud","summary":"  Nowadays, cloud-based services are widely favored over the traditional\napproach of locally training a Neural Network (NN) model. Oftentimes, a cloud\nservice processes multiple requests from users--thus training multiple NN\nmodels concurrently. However, training NN models concurrently is a challenging\nprocess, which typically requires significant amounts of available computing\nresources and takes a long time to complete. In this paper, we present\nUnifiedNN to effectively train multiple NN models concurrently on the cloud.\nUnifiedNN effectively \"combines\" multiple NN models and features several memory\nand time conservation mechanisms to train multiple NN models simultaneously\nwithout impacting the accuracy of the training process. Specifically, UnifiedNN\nmerges multiple NN models and creates a large singular unified model in order\nto efficiently train all models at once. We have implemented a prototype of\nUnifiedNN in PyTorch and we have compared its performance with relevant\nstate-of-the-art frameworks. Our experimental results demonstrate that\nUnifiedNN can reduce memory consumption by up to 53% and training time by up to\n81% when compared with vanilla PyTorch without impacting the model training and\ntesting accuracy. Finally, our results indicate that UnifiedNN can reduce\nmemory consumption by up to 52% and training time by up to 41% when compared to\nstate-of-the-art frameworks when training multiple models concurrently.\n","authors":["Sifat Ut Taki","Arthi Padmanabhan","Spyridon Mastorakis"],"pdf_url":"https://arxiv.org/pdf/2408.01331v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07143v2","updated":"2024-08-06T00:51:37Z","published":"2023-10-11T02:36:52Z","title":"Imitation Learning from Purified Demonstrations","summary":"  Imitation learning has emerged as a promising approach for addressing\nsequential decision-making problems, with the assumption that expert\ndemonstrations are optimal. However, in real-world scenarios, most\ndemonstrations are often imperfect, leading to challenges in the effectiveness\nof imitation learning. While existing research has focused on optimizing with\nimperfect demonstrations, the training typically requires a certain proportion\nof optimal demonstrations to guarantee performance. To tackle these problems,\nwe propose to purify the potential noises in imperfect demonstrations first,\nand subsequently conduct imitation learning from these purified demonstrations.\nMotivated by the success of diffusion model, we introduce a two-step\npurification via diffusion process. In the first step, we apply a forward\ndiffusion process to smooth potential noises in imperfect demonstrations by\nintroducing additional noise. Subsequently, a reverse generative process is\nutilized to recover the optimal demonstration from the diffused ones. We\nprovide theoretical evidence supporting our approach, demonstrating that the\ndistance between the purified and optimal demonstration can be bounded.\nEmpirical results on MuJoCo and RoboSuite demonstrate the effectiveness of our\nmethod from different aspects.\n","authors":["Yunke Wang","Minjing Dong","Yukun Zhao","Bo Du","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2310.07143v2.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/1912.03573v2","updated":"2024-08-06T00:50:41Z","published":"2019-12-07T23:02:02Z","title":"Deep Variable-Block Chain with Adaptive Variable Selection","summary":"  The architectures of deep neural networks (DNN) rely heavily on the\nunderlying grid structure of variables, for instance, the lattice of pixels in\nan image. For general high dimensional data with variables not associated with\na grid, the multi-layer perceptron and deep belief network are often used.\nHowever, it is frequently observed that those networks do not perform\ncompetitively and they are not helpful for identifying important variables. In\nthis paper, we propose a framework that imposes on blocks of variables a chain\nstructure obtained by step-wise greedy search so that the DNN architecture can\nleverage the constructed grid. We call this new neural network Deep\nVariable-Block Chain (DVC). Because the variable blocks are used for\nclassification in a sequential manner, we further develop the capacity of\nselecting variables adaptively according to a number of regions trained by a\ndecision tree. Our experiments show that DVC outperforms other generic DNNs and\nother strong classifiers. Moreover, DVC can achieve high accuracy at much\nreduced dimensionality and sometimes reveals drastically different sets of\nrelevant variables for different regions.\n","authors":["Lixiang Zhang","Lin Lin","Jia Li"],"pdf_url":"https://arxiv.org/pdf/1912.03573v2.pdf","comment":"24 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.08847v3","updated":"2024-08-06T00:01:01Z","published":"2023-10-13T04:14:51Z","title":"On the Over-Memorization During Natural, Robust and Catastrophic\n  Overfitting","summary":"  Overfitting negatively impacts the generalization ability of deep neural\nnetworks (DNNs) in both natural and adversarial training. Existing methods\nstruggle to consistently address different types of overfitting, typically\ndesigning strategies that focus separately on either natural or adversarial\npatterns. In this work, we adopt a unified perspective by solely focusing on\nnatural patterns to explore different types of overfitting. Specifically, we\nexamine the memorization effect in DNNs and reveal a shared behaviour termed\nover-memorization, which impairs their generalization capacity. This behaviour\nmanifests as DNNs suddenly becoming high-confidence in predicting certain\ntraining patterns and retaining a persistent memory for them. Furthermore, when\nDNNs over-memorize an adversarial pattern, they tend to simultaneously exhibit\nhigh-confidence prediction for the corresponding natural pattern. These\nfindings motivate us to holistically mitigate different types of overfitting by\nhindering the DNNs from over-memorization training patterns. To this end, we\npropose a general framework, Distraction Over-Memorization (DOM), which\nexplicitly prevents over-memorization by either removing or augmenting the\nhigh-confidence natural patterns. Extensive experiments demonstrate the\neffectiveness of our proposed method in mitigating overfitting across various\ntraining paradigms.\n","authors":["Runqi Lin","Chaojian Yu","Bo Han","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2310.08847v3.pdf","comment":null}],"Robotics":[{"id":"http://arxiv.org/abs/2408.03238v1","updated":"2024-08-06T14:50:48Z","published":"2024-08-06T14:50:48Z","title":"LAC-Net: Linear-Fusion Attention-Guided Convolutional Network for\n  Accurate Robotic Grasping Under the Occlusion","summary":"  This paper addresses the challenge of perceiving complete object shapes\nthrough visual perception. While prior studies have demonstrated encouraging\noutcomes in segmenting the visible parts of objects within a scene, amodal\nsegmentation, in particular, has the potential to allow robots to infer the\noccluded parts of objects. To this end, this paper introduces a new framework\nthat explores amodal segmentation for robotic grasping in cluttered scenes,\nthus greatly enhancing robotic grasping abilities. Initially, we use a\nconventional segmentation algorithm to detect the visible segments of the\ntarget object, which provides shape priors for completing the full object mask.\nParticularly, to explore how to utilize semantic features from RGB images and\ngeometric information from depth images, we propose a Linear-fusion\nAttention-guided Convolutional Network (LAC-Net). LAC-Net utilizes the\nlinear-fusion strategy to effectively fuse this cross-modal data, and then uses\nthe prior visible mask as attention map to guide the network to focus on target\nfeature locations for further complete mask recovery. Using the amodal mask of\nthe target object provides advantages in selecting more accurate and robust\ngrasp points compared to relying solely on the visible segments. The results on\ndifferent datasets show that our method achieves state-of-the-art performance.\nFurthermore, the robot experiments validate the feasibility and robustness of\nthis method in the real world. Our code and demonstrations are available on the\nproject page: https://jrryzh.github.io/LAC-Net.\n","authors":["Jinyu Zhang","Yongchong Gu","Jianxiong Gao","Haitao Lin","Qiang Sun","Xinwei Sun","Xiangyang Xue","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2408.03238v1.pdf","comment":"accepted by IROS2024"},{"id":"http://arxiv.org/abs/2408.03208v1","updated":"2024-08-06T14:06:53Z","published":"2024-08-06T14:06:53Z","title":"Personalizing Federated Instrument Segmentation with Visual Trait Priors\n  in Robotic Surgery","summary":"  Personalized federated learning (PFL) for surgical instrument segmentation\n(SIS) is a promising approach. It enables multiple clinical sites to\ncollaboratively train a series of models in privacy, with each model tailored\nto the individual distribution of each site. Existing PFL methods rarely\nconsider the personalization of multi-headed self-attention, and do not account\nfor appearance diversity and instrument shape similarity, both inherent in\nsurgical scenes. We thus propose PFedSIS, a novel PFL method with visual trait\npriors for SIS, incorporating global-personalized disentanglement (GPD),\nappearance-regulation personalized enhancement (APE), and shape-similarity\nglobal enhancement (SGE), to boost SIS performance in each site. GPD represents\nthe first attempt at head-wise assignment for multi-headed self-attention\npersonalization. To preserve the unique appearance representation of each site\nand gradually leverage the inter-site difference, APE introduces appearance\nregulation and provides customized layer-wise aggregation solutions via\nhypernetworks for each site's personalized parameters. The mutual shape\ninformation of instruments is maintained and shared via SGE, which enhances the\ncross-style shape consistency on the image level and computes the\nshape-similarity contribution of each site on the prediction level for updating\nthe global parameters. PFedSIS outperforms state-of-the-art methods with +1.51%\nDice, +2.11% IoU, -2.79 ASSD, -15.55 HD95 performance gains. The corresponding\ncode and models will be released at https://github.com/wzjialang/PFedSIS.\n","authors":["Jialang Xu","Jiacheng Wang","Lequan Yu","Danail Stoyanov","Yueming Jin","Evangelos B. Mazomenos"],"pdf_url":"https://arxiv.org/pdf/2408.03208v1.pdf","comment":"9 pages, 3 figures, under review"},{"id":"http://arxiv.org/abs/2408.03200v1","updated":"2024-08-06T13:58:56Z","published":"2024-08-06T13:58:56Z","title":"Adversarial Safety-Critical Scenario Generation using Naturalistic Human\n  Driving Priors","summary":"  Evaluating the decision-making system is indispensable in developing\nautonomous vehicles, while realistic and challenging safety-critical test\nscenarios play a crucial role. Obtaining these scenarios is non-trivial, thanks\nto the long-tailed distribution, sparsity, and rarity in real-world data sets.\nTo tackle this problem, in this paper, we introduce a natural adversarial\nscenario generation solution using naturalistic human driving priors and\nreinforcement learning techniques. By doing this, we can obtain large-scale\ntest scenarios that are both diverse and realistic. Specifically, we build a\nsimulation environment that mimics natural traffic interaction scenarios.\nInformed by this environment, we implement a two-stage procedure. The first\nstage incorporates conventional rule-based models, e.g., IDM~(Intelligent\nDriver Model) and MOBIL~(Minimizing Overall Braking Induced by Lane changes)\nmodel, to coarsely and discretely capture and calibrate key control parameters\nfrom the real-world dataset. Next, we leverage GAIL~(Generative Adversarial\nImitation Learning) to represent driver behaviors continuously. The derived\nGAIL can be further used to design a PPO~(Proximal Policy Optimization)-based\nactor-critic network framework to fine-tune the reward function, and then\noptimizes our natural adversarial scenario generation solution. Extensive\nexperiments have been conducted in the NGSIM dataset including the trajectory\nof 3,000 vehicles. Essential traffic parameters were measured in comparison\nwith the baseline model, e.g., the collision rate, accelerations, steering, and\nthe number of lane changes. Our findings demonstrate that the proposed model\ncan generate realistic safety-critical test scenarios covering both naturalness\nand adversariality, which can be a cornerstone for the development of\nautonomous vehicles.\n","authors":["Kunkun Hao","Yonggang Luo","Wen Cui","Yuqiao Bai","Jucheng Yang","Songyang Yan","Yuxi Pan","Zijiang Yang"],"pdf_url":"https://arxiv.org/pdf/2408.03200v1.pdf","comment":"Published in IEEE Transactions on Intelligent Vehicles, 2023"},{"id":"http://arxiv.org/abs/2408.03191v1","updated":"2024-08-06T13:43:52Z","published":"2024-08-06T13:43:52Z","title":"Integrated Intention Prediction and Decision-Making with Spectrum\n  Attention Net and Proximal Policy Optimization","summary":"  For autonomous driving in highly dynamic environments, it is anticipated to\npredict the future behaviors of surrounding vehicles (SVs) and make safe and\neffective decisions. However, modeling the inherent coupling effect between the\nprediction and decision-making modules has been a long-standing challenge,\nespecially when there is a need to maintain appropriate computational\nefficiency. To tackle these problems, we propose a novel integrated intention\nprediction and decision-making approach, which explicitly models the coupling\nrelationship and achieves efficient computation. Specifically, a spectrum\nattention net is designed to predict the intentions of SVs by capturing the\ntrends of each frequency component over time and their interrelations. Fast\ncomputation of the intention prediction module is attained as the predicted\nintentions are not decoded to trajectories in the executing process.\nFurthermore, the proximal policy optimization (PPO) algorithm is employed to\naddress the non-stationary problem in the framework through a modest policy\nupdate enabled by a clipping mechanism within its objective function. On the\nbasis of these developments, the intention prediction and decision-making\nmodules are integrated through joint learning. Experiments are conducted in\nrepresentative traffic scenarios, and the results reveal that the proposed\nintegrated framework demonstrates superior performance over several deep\nreinforcement learning (DRL) baselines in terms of success rate, efficiency,\nand safety in driving tasks.\n","authors":["Xiao Zhou","Chengzhen Meng","Wenru Liu","Zengqi Peng","Ming Liu","Jun Ma"],"pdf_url":"https://arxiv.org/pdf/2408.03191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03168v1","updated":"2024-08-06T13:11:36Z","published":"2024-08-06T13:11:36Z","title":"Training on the Fly: On-device Self-supervised Learning aboard\n  Nano-drones within 20 mW","summary":"  Miniaturized cyber-physical systems (CPSes) powered by tiny machine learning\n(TinyML), such as nano-drones, are becoming an increasingly attractive\ntechnology. Their small form factor (i.e., ~10cm diameter) ensures vast\napplicability, ranging from the exploration of narrow disaster scenarios to\nsafe human-robot interaction. Simple electronics make these CPSes inexpensive,\nbut strongly limit the computational, memory, and sensing resources available\non board. In real-world applications, these limitations are further exacerbated\nby domain shift. This fundamental machine learning problem implies that model\nperception performance drops when moving from the training domain to a\ndifferent deployment one. To cope with and mitigate this general problem, we\npresent a novel on-device fine-tuning approach that relies only on the limited\nultra-low power resources available aboard nano-drones. Then, to overcome the\nlack of ground-truth training labels aboard our CPS, we also employ a\nself-supervised method based on ego-motion consistency. Albeit our work builds\non top of a specific real-world vision-based human pose estimation task, it is\nwidely applicable for many embedded TinyML use cases. Our 512-image on-device\ntraining procedure is fully deployed aboard an ultra-low power GWT GAP9\nSystem-on-Chip and requires only 1MB of memory while consuming as low as 19mW\nor running in just 510ms (at 38mW). Finally, we demonstrate the benefits of our\non-device learning approach by field-testing our closed-loop CPS, showing a\nreduction in horizontal position error of up to 26% vs. a non-fine-tuned\nstate-of-the-art baseline. In the most challenging never-seen-before\nenvironment, our on-device learning procedure makes the difference between\nsucceeding or failing the mission.\n","authors":["Elia Cereda","Alessandro Giusti","Daniele Palossi"],"pdf_url":"https://arxiv.org/pdf/2408.03168v1.pdf","comment":"This paper has been accepted for publication in the IEEE Transactions\n  on Computer-Aided Design of Integrated Circuits and Systems. Copyright 2024\n  IEEE"},{"id":"http://arxiv.org/abs/2405.00956v3","updated":"2024-08-06T13:07:37Z","published":"2024-05-02T02:34:19Z","title":"SimEndoGS: Efficient Data-driven Scene Simulation using Robotic Surgery\n  Videos via Physics-embedded 3D Gaussians","summary":"  Surgical scene simulation plays a crucial role in surgical education and\nsimulator-based robot learning. Traditional approaches for creating these\nenvironments with surgical scene involve a labor-intensive process where\ndesigners hand-craft tissues models with textures and geometries for soft body\nsimulations. This manual approach is not only time-consuming but also limited\nin the scalability and realism. In contrast, data-driven simulation offers a\ncompelling alternative. It has the potential to automatically reconstruct 3D\nsurgical scenes from real-world surgical video data, followed by the\napplication of soft body physics. This area, however, is relatively uncharted.\nIn our research, we introduce 3D Gaussian as a learnable representation for\nsurgical scene, which is learned from stereo endoscopic video. To prevent\nover-fitting and ensure the geometrical correctness of these scenes, we\nincorporate depth supervision and anisotropy regularization into the Gaussian\nlearning process. Furthermore, we apply the Material Point Method, which is\nintegrated with physical properties, to the 3D Gaussians to achieve realistic\nscene deformations. Our method was evaluated on our collected in-house and\npublic surgical videos datasets. Results show that it can reconstruct and\nsimulate surgical scenes from endoscopic videos efficiently-taking only a few\nminutes to reconstruct the surgical scene-and produce both visually and\nphysically plausible deformations at a speed approaching real-time. The results\ndemonstrate great potential of our proposed method to enhance the efficiency\nand variety of simulations available for surgical education and robot learning.\n","authors":["Zhenya Yang","Kai Chen","Yonghao Long","Qi Dou"],"pdf_url":"https://arxiv.org/pdf/2405.00956v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03131v1","updated":"2024-08-06T12:16:15Z","published":"2024-08-06T12:16:15Z","title":"Stochastic Trajectory Optimization for Demonstration Imitation","summary":"  Humans often learn new skills by imitating the experts and gradually\ndeveloping their proficiency. In this work, we introduce Stochastic Trajectory\nOptimization for Demonstration Imitation (STODI), a trajectory optimization\nframework for robots to imitate the shape of demonstration trajectories with\nimproved dynamic performance. Consistent with the human learning process,\ndemonstration imitation serves as an initial step, while trajectory\noptimization aims to enhance robot motion performance. By generating random\nnoise and constructing proper cost functions, the STODI effectively explores\nand exploits generated noisy trajectories while preserving the demonstration\nshape characteristics. We employ three metrics to measure the similarity of\ntrajectories in both the time and frequency domains to help with demonstration\nimitation. Theoretical analysis reveals relationships among these metrics,\nemphasizing the benefits of frequency-domain analysis for specific tasks.\nExperiments on a 7-DOF robotic arm in the PyBullet simulator validate the\nefficacy of the STODI framework, showcasing the improved optimization\nperformance and stability compared to previous methods.\n","authors":["Chenlin Ming","Zitong Wang","Boxuan Zhang","Xiaoming Duan","Jianping He"],"pdf_url":"https://arxiv.org/pdf/2408.03131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02558v3","updated":"2024-08-06T11:35:04Z","published":"2023-11-05T03:53:42Z","title":"Multi-Agent 3D Map Reconstruction and Change Detection in Microgravity\n  with Free-Flying Robots","summary":"  Assistive free-flyer robots autonomously caring for future crewed outposts --\nsuch as NASA's Astrobee robots on the International Space Station (ISS) -- must\nbe able to detect day-to-day interior changes to track inventory, detect and\ndiagnose faults, and monitor the outpost status. This work presents a framework\nfor multi-agent cooperative mapping and change detection to enable robotic\nmaintenance of space outposts. One agent is used to reconstruct a 3D model of\nthe environment from sequences of images and corresponding depth information.\nAnother agent is used to periodically scan the environment for inconsistencies\nagainst the 3D model. Change detection is validated after completing the\nsurveys using real image and pose data collected by Astrobee robots in a ground\ntesting environment and from microgravity aboard the ISS. This work outlines\nthe objectives, requirements, and algorithmic modules for the multi-agent\nreconstruction system, including recommendations for its use by assistive\nfree-flyers aboard future microgravity outposts.\n  *Denotes Equal Contribution\n","authors":["Holly Dinkel","Julia Di","Jamie Santos","Keenan Albee","Paulo Borges","Marina Moreira","Oleg Alexandrov","Brian Coltin","Trey Smith"],"pdf_url":"https://arxiv.org/pdf/2311.02558v3.pdf","comment":"11 pages, 8 figures, Manuscript presented at the 74th International\n  Astronautical Congress, IAC 2023, Baku, Azerbaijan, 2 - 6 October 2023. Video\n  presentation: [https://www.youtube.com/watch?v=VfjV-zwFEtU]. Code:\n  [https://github.com/hollydinkel/astrobeecd]"},{"id":"http://arxiv.org/abs/2408.03102v1","updated":"2024-08-06T11:09:38Z","published":"2024-08-06T11:09:38Z","title":"Adaptive-Sliding Mode Trajectory Control of Robot Manipulators with\n  Uncertainties","summary":"  In this paper, we propose and demonstrate an adaptive-sliding mode control\nfor trajectory tracking control of robot manipulators subjected to uncertain\ndynamics, vibration disturbance, and payload variation disturbance. Throughout\nthis work we seek a controller that is, robust to the uncertainty and\ndisturbance, accurate, and implementable. To perform these requirements, we use\na nonlinear Lyapunov-based approach for designing the controller and\nguaranteeing its stability. MATLAB-SIMULINK software is used to validate the\napproach and demonstrate the performance of the controller. Simulation results\nshow that the derived controller is stable, robust to the disturbance and\nuncertainties, accurate, and implementable.\n","authors":["Mustafa M. Mustafa","Carl D. Crane","Ibrahim Hamarash"],"pdf_url":"https://arxiv.org/pdf/2408.03102v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03098v1","updated":"2024-08-06T10:57:42Z","published":"2024-08-06T10:57:42Z","title":"Dedicated Nonlinear Control of Robot Manipulators in the Presence of\n  External Vibration and Uncertain Payload","summary":"  Robot manipulators are often tasked with working in environments with\nvibrations and are subject to load uncertainty. Providing an accurate tracking\ncontrol design with implementable torque input for these robots is a complex\ntopic. This paper presents two approaches to solve this problem. The approaches\nconsider joint space tracking control design in the presence of nonlinear\nuncertain torques caused by external vibration and payload variation. The\nproperties of the uncertain torques are used in both approaches. The first\napproach is based on the boundedness property, while the second approach\nconsiders the differentiability and boundedness together. The controllers\nderived from each approach differ from the perspectives of accuracy, control\neffort, and disturbance properties. A Lyapunov-based analysis is utilized to\nguarantee the stability of the control design in each case. Simulation results\nvalidate the approaches and demonstrate the performance of the controllers. The\nderived controllers show stable results at the cost of the mentioned\nproperties.\n","authors":["Mustafa M. Mustafa","Carl D. Crane","Ibrahim Hamarash"],"pdf_url":"https://arxiv.org/pdf/2408.03098v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03078v1","updated":"2024-08-06T10:13:57Z","published":"2024-08-06T10:13:57Z","title":"BodySLAM: A Generalized Monocular Visual SLAM Framework for Surgical\n  Applications","summary":"  Endoscopic surgery relies on two-dimensional views, posing challenges for\nsurgeons in depth perception and instrument manipulation. While Simultaneous\nLocalization and Mapping (SLAM) has emerged as a promising solution to address\nthese limitations, its implementation in endoscopic procedures presents\nsignificant challenges due to hardware limitations, such as the use of a\nmonocular camera and the absence of odometry sensors. This study presents a\nrobust deep learning-based SLAM approach that combines state-of-the-art and\nnewly developed models. It consists of three main parts: the Monocular Pose\nEstimation Module that introduces a novel unsupervised method based on the\nCycleGAN architecture, the Monocular Depth Estimation Module that leverages the\nnovel Zoe architecture, and the 3D Reconstruction Module which uses information\nfrom the previous models to create a coherent surgical map. The performance of\nthe procedure was rigorously evaluated using three publicly available datasets\n(Hamlyn, EndoSLAM, and SCARED) and benchmarked against two state-of-the-art\nmethods, EndoSFMLearner and EndoDepth. The integration of Zoe in the MDEM\ndemonstrated superior performance compared to state-of-the-art depth estimation\nalgorithms in endoscopy, whereas the novel approach in the MPEM exhibited\ncompetitive performance and the lowest inference time. The results showcase the\nrobustness of our approach in laparoscopy, gastroscopy, and colonoscopy, three\ndifferent scenarios in endoscopic surgery. The proposed SLAM approach has the\npotential to improve the accuracy and efficiency of endoscopic procedures by\nproviding surgeons with enhanced depth perception and 3D reconstruction\ncapabilities.\n","authors":["G. Manni","C. Lauretti","F. Prata","R. Papalia","L. Zollo","P. Soda"],"pdf_url":"https://arxiv.org/pdf/2408.03078v1.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.03063v1","updated":"2024-08-06T09:34:54Z","published":"2024-08-06T09:34:54Z","title":"Social Behavior as a Key to Learning-based Multi-Agent Pathfinding\n  Dilemmas","summary":"  The Multi-agent Path Finding (MAPF) problem involves finding collision-free\npaths for a team of agents in a known, static environment, with important\napplications in warehouse automation, logistics, or last-mile delivery. To meet\nthe needs of these large-scale applications, current learning-based methods\noften deploy the same fully trained, decentralized network to all agents to\nimprove scalability. However, such parameter sharing typically results in\nhomogeneous behaviors among agents, which may prevent agents from breaking ties\naround symmetric conflict (e.g., bottlenecks) and might lead to\nlive-/deadlocks. In this paper, we propose SYLPH, a novel learning-based MAPF\nframework aimed to mitigate the adverse effects of homogeneity by allowing\nagents to learn and dynamically select different social behaviors (akin to\nindividual, dynamic roles), without affecting the scalability offered by\nparameter sharing. Specifically, SYLPH agents learn to select their Social\nValue Orientation (SVO) given the situation at hand, quantifying their own\nlevel of selfishness/altruism, as well as an SVO-conditioned MAPF policy\ndictating their movement actions. To these ends, each agent first determines\nthe most influential other agent in the system by predicting future\nconflicts/interactions with other agents. Each agent selects its own SVO\ntowards that agent, and trains its decentralized MAPF policy to enact this SVO\nuntil another agent becomes more influential. To further allow agents to\nconsider each others' social preferences, each agent gets access to the SVO\nvalue of their neighbors. As a result of this hierarchical decision-making and\nexchange of social preferences, SYLPH endows agents with the ability to reason\nabout the MAPF task through more latent spaces and nuanced contexts, leading to\nvaried responses that can help break ties around symmetric conflicts. [...]\n","authors":["Chengyang He","Tanishq Duhan","Parth Tulsyan","Patrick Kim","Guillaume Sartoretti"],"pdf_url":"https://arxiv.org/pdf/2408.03063v1.pdf","comment":"Submitted to Springer's Artificial Intelligence Journal"},{"id":"http://arxiv.org/abs/2408.03059v1","updated":"2024-08-06T09:21:46Z","published":"2024-08-06T09:21:46Z","title":"Learning to Turn: Diffusion Imitation for Robust Row Turning in\n  Under-Canopy Robots","summary":"  Under-canopy agricultural robots require robust navigation capabilities to\nenable full autonomy but struggle with tight row turning between crop rows due\nto degraded GPS reception, visual aliasing, occlusion, and complex vehicle\ndynamics. We propose an imitation learning approach using diffusion policies to\nlearn row turning behaviors from demonstrations provided by human operators or\nprivileged controllers. Simulation experiments in a corn field environment show\npotential in learning this task with only visual observations and velocity\nstates. However, challenges remain in maintaining control within rows and\nhandling varied initial conditions, highlighting areas for future improvement.\n","authors":["Arun N. Sivakumar","Pranay Thangeda","Yixiao Fang","Mateus V. Gasparino","Jose Cuaran","Melkior Ornik","Girish Chowdhary"],"pdf_url":"https://arxiv.org/pdf/2408.03059v1.pdf","comment":"Accepted as Extended Abstract to the IEEE ICRA@40 2024"},{"id":"http://arxiv.org/abs/2408.03018v1","updated":"2024-08-06T08:01:02Z","published":"2024-08-06T08:01:02Z","title":"Integrating Controllable Motion Skills from Demonstrations","summary":"  The expanding applications of legged robots require their mastery of\nversatile motion skills. Correspondingly, researchers must address the\nchallenge of integrating multiple diverse motion skills into controllers. While\nexisting reinforcement learning (RL)-based approaches have achieved notable\nsuccess in multi-skill integration for legged robots, these methods often\nrequire intricate reward engineering or are restricted to integrating a\npredefined set of motion skills constrained by specific task objectives,\nresulting in limited flexibility. In this work, we introduce a flexible\nmulti-skill integration framework named Controllable Skills Integration (CSI).\nCSI enables the integration of a diverse set of motion skills with varying\nstyles into a single policy without the need for complex reward tuning.\nFurthermore, in a hierarchical control manner, the trained low-level policy can\nbe coupled with a high-level Natural Language Inference (NLI) module to enable\npreliminary language-directed skill control. Our experiments demonstrate that\nCSI can flexibly integrate a diverse array of motion skills more\ncomprehensively and facilitate the transitions between different skills.\nAdditionally, CSI exhibits good scalability as the number of motion skills to\nbe integrated increases significantly.\n","authors":["Honghao Liao","Zhiheng Li","Ziyu Meng","Ran Song","Yibin Li","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.03018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03017v1","updated":"2024-08-06T07:59:01Z","published":"2024-08-06T07:59:01Z","title":"Closed-Loop Magnetic Control of Medical Soft Continuum Robots for\n  Deflection","summary":"  Magnetic soft continuum robots (MSCRs) have emerged as powerful devices in\nendovascular interventions owing to their hyperelastic fibre matrix and\nenhanced magnetic manipulability. Effective closed-loop control of tethered\nmagnetic devices contributes to the achievement of autonomous vascular robotic\nsurgery. In this article, we employ a magnetic actuation system equipped with a\nsingle rotatable permanent magnet to achieve closed-loop deflection control of\nthe MSCR. To this end, we establish a differential kinematic model of MSCRs\nexposed to non-uniform magnetic fields. The relationship between the existence\nand uniqueness of Jacobian and the geometric position between robots is\ndeduced. The accurate control direction induced by Jacobian is demonstrated to\nbe crucial in simulations. Then, the corresponding quasi-static control (QSC)\nframework integrates a linear extended state observer to estimate model\nuncertainties. Finally, the effectiveness of the proposed QSC framework is\nvalidated through comparative trajectory tracking experiments with the PD\ncontroller under external disturbances. The proposed control framework\neffectively prevents the actuator from reaching the joint limit and achieves\nfast and low error-tracking performance without overshooting.\n","authors":["Zhiwei Wu","Siyi Wei","Zhanxin Geng","Jinhui Zhang","Duanduan Chen"],"pdf_url":"https://arxiv.org/pdf/2408.03017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01615v2","updated":"2024-08-06T05:00:47Z","published":"2024-08-03T00:45:01Z","title":"Three-dimensional Morphological Reconstruction of Millimeter-Scale Soft\n  Continuum Robots based on Dual-Stereo-Vision","summary":"  Continuum robots can be miniaturized to just a few millimeters in diameter.\nAmong these, notched tubular continuum robots (NTCR) show great potential in\nmany delicate applications. Existing works in robotic modeling focus on\nkinematics and dynamics but still face challenges in reproducing the robot's\nmorphology -- a significant factor that can expand the research landscape of\ncontinuum robots, especially for those with asymmetric continuum structures.\nThis paper proposes a dual stereo vision-based method for the three-dimensional\nmorphological reconstruction of millimeter-scale NTCRs. The method employs two\noppositely located stationary binocular cameras to capture the point cloud of\nthe NTCR, then utilizes predefined geometry as a reference for the KD tree\nmethod to relocate the capture point clouds, resulting in a morphologically\ncorrect NTCR despite the low-quality raw point cloud collection. The method has\nbeen proved feasible for an NTCR with a 3.5 mm diameter, capturing 14 out of 16\nnotch features, with the measurements generally centered around the standard of\n1.5 mm, demonstrating the capability of revealing morphological details. Our\nproposed method paves the way for 3D morphological reconstruction of\nmillimeter-scale soft robots for further self-modeling study.\n","authors":["Tian-Ao Ren","Wenyan Liu","Tao Zhang","Lei Zhao","Hongliang Ren","Jiewen Lai"],"pdf_url":"https://arxiv.org/pdf/2408.01615v2.pdf","comment":"6 pages, 6 figures, submitted to Robio 2024"},{"id":"http://arxiv.org/abs/2408.02949v1","updated":"2024-08-06T04:25:09Z","published":"2024-08-06T04:25:09Z","title":"Few-shot Scooping Under Domain Shift via Simulated Maximal Deployment\n  Gaps","summary":"  Autonomous lander missions on extraterrestrial bodies need to sample granular\nmaterials while coping with domain shifts, even when sampling strategies are\nextensively tuned on Earth. To tackle this challenge, this paper studies the\nfew-shot scooping problem and proposes a vision-based adaptive scooping\nstrategy that uses the deep kernel Gaussian process method trained with a novel\nmeta-training strategy to learn online from very limited experience on\nout-of-distribution target terrains. Our Deep Kernel Calibration with Maximal\nDeployment Gaps (kCMD) strategy explicitly trains a deep kernel model to adapt\nto large domain shifts by creating simulated maximal deployment gaps from an\noffline training dataset and training models to overcome these deployment gaps\nduring training. Employed in a Bayesian Optimization sequential decision-making\nframework, the proposed method allows the robot to perform high-quality\nscooping actions on out-of-distribution terrains after a few attempts,\nsignificantly outperforming non-adaptive methods proposed in the excavation\nliterature as well as other state-of-the-art meta-learning methods. The\nproposed method also demonstrates zero-shot transfer capability, successfully\nadapting to the NASA OWLAT platform, which serves as a state-of-the-art\nsimulator for potential future planetary missions. These results demonstrate\nthe potential of training deep models with simulated deployment gaps for more\ngeneralizable meta-learning in high-capacity models. Furthermore, they\nhighlight the promise of our method in autonomous lander sampling missions by\nenabling landers to overcome the deployment gap between Earth and\nextraterrestrial bodies.\n","authors":["Yifan Zhu","Pranay Thangeda","Erica L Tevere","Ashish Goel","Erik Kramer","Hari D Nayar","Melkior Ornik","Kris Hauser"],"pdf_url":"https://arxiv.org/pdf/2408.02949v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2303.02893"},{"id":"http://arxiv.org/abs/2404.12794v2","updated":"2024-08-06T03:28:12Z","published":"2024-04-19T11:17:35Z","title":"MambaMOS: LiDAR-based 3D Moving Object Segmentation with Motion-aware\n  State Space Model","summary":"  LiDAR-based Moving Object Segmentation (MOS) aims to locate and segment\nmoving objects in point clouds of the current scan using motion information\nfrom previous scans. Despite the promising results achieved by previous MOS\nmethods, several key issues, such as the weak coupling of temporal and spatial\ninformation, still need further study. In this paper, we propose a novel\nLiDAR-based 3D Moving Object Segmentation with Motion-aware State Space Model,\ntermed MambaMOS. Firstly, we develop a novel embedding module, the Time Clue\nBootstrapping Embedding (TCBE), to enhance the coupling of temporal and spatial\ninformation in point clouds and alleviate the issue of overlooked temporal\nclues. Secondly, we introduce the Motion-aware State Space Model (MSSM) to\nendow the model with the capacity to understand the temporal correlations of\nthe same object across different time steps. Specifically, MSSM emphasizes the\nmotion states of the same object at different time steps through two distinct\ntemporal modeling and correlation steps. We utilize an improved state space\nmodel to represent these motion differences, significantly modeling the motion\nstates. Finally, extensive experiments on the SemanticKITTI-MOS and KITTI-Road\nbenchmarks demonstrate that the proposed MambaMOS achieves state-of-the-art\nperformance. The source code is publicly available at\nhttps://github.com/Terminal-K/MambaMOS.\n","authors":["Kang Zeng","Hao Shi","Jiacheng Lin","Siyu Li","Jintao Cheng","Kaiwei Wang","Zhiyong Li","Kailun Yang"],"pdf_url":"https://arxiv.org/pdf/2404.12794v2.pdf","comment":"Accepted to ACM MM 2024. The source code is publicly available at\n  https://github.com/Terminal-K/MambaMOS"},{"id":"http://arxiv.org/abs/2408.02912v1","updated":"2024-08-06T02:53:55Z","published":"2024-08-06T02:53:55Z","title":"KOI: Accelerating Online Imitation Learning via Hybrid Key-state\n  Guidance","summary":"  Online Imitation Learning methods struggle with the gap between extensive\nonline exploration space and limited expert trajectories, which hinder\nefficient exploration due to inaccurate task-aware reward estimation. Inspired\nby the findings from cognitive neuroscience that task decomposition could\nfacilitate cognitive processing for efficient learning, we hypothesize that an\nagent could estimate precise task-aware imitation rewards for efficient online\nexploration by decomposing the target task into the objectives of \"what to do\"\nand the mechanisms of \"how to do\". In this work, we introduce the hybrid\nKey-state guided Online Imitation (KOI) learning approach, which leverages the\nintegration of semantic and motion key states as guidance for task-aware reward\nestimation. Initially, we utilize the visual-language models to segment the\nexpert trajectory into semantic key states, indicating the objectives of \"what\nto do\". Within the intervals between semantic key states, optical flow is\nemployed to capture motion key states to understand the process of \"how to do\".\nBy integrating a thorough grasp of both semantic and motion key states, we\nrefine the trajectory-matching reward computation, encouraging task-aware\nexploration for efficient online imitation learning. Our experiment results\nprove that our method is more sample efficient in the Meta-World and LIBERO\nenvironments. We also conduct real-world robotic manipulation experiments to\nvalidate the efficacy of our method, demonstrating the practical applicability\nof our KOI method.\n","authors":["Jingxian Lu","Wenke Xia","Dong Wang","Zhigang Wang","Bin Zhao","Di Hu","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.02912v1.pdf","comment":"Submitted to Corl 2024"},{"id":"http://arxiv.org/abs/2303.00952v5","updated":"2024-08-06T02:39:05Z","published":"2023-03-02T04:12:53Z","title":"Towards Activated Muscle Group Estimation in the Wild","summary":"  In this paper, we tackle the new task of video-based Activated Muscle Group\nEstimation (AMGE) aiming at identifying active muscle regions during physical\nactivity in the wild. To this intent, we provide the MuscleMap dataset\nfeaturing >15K video clips with 135 different activities and 20 labeled muscle\ngroups. This dataset opens the vistas to multiple video-based applications in\nsports and rehabilitation medicine under flexible environment constraints. The\nproposed MuscleMap dataset is constructed with YouTube videos, specifically\ntargeting High-Intensity Interval Training (HIIT) physical exercise in the\nwild. To make the AMGE model applicable in real-life situations, it is crucial\nto ensure that the model can generalize well to numerous types of physical\nactivities not present during training and involving new combinations of\nactivated muscles. To achieve this, our benchmark also covers an evaluation\nsetting where the model is exposed to activity types excluded from the training\nset. Our experiments reveal that the generalizability of existing architectures\nadapted for the AMGE task remains a challenge. Therefore, we also propose a new\napproach, TransM3E, which employs a multi-modality feature fusion mechanism\nbetween both the video transformer model and the skeleton-based graph\nconvolution model with novel cross-modal knowledge distillation executed on\nmulti-classification tokens. The proposed method surpasses all popular video\nclassification models when dealing with both, previously seen and new types of\nphysical activities. The database and code can be found at\nhttps://github.com/KPeng9510/MuscleMap.\n","authors":["Kunyu Peng","David Schneider","Alina Roitberg","Kailun Yang","Jiaming Zhang","Chen Deng","Kaiyu Zhang","M. Saquib Sarfraz","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2303.00952v5.pdf","comment":"Accepted to ACM MM 2024. The database and code can be found at\n  https://github.com/KPeng9510/MuscleMap"},{"id":"http://arxiv.org/abs/2403.09975v2","updated":"2024-08-06T00:28:44Z","published":"2024-03-15T02:42:28Z","title":"Skeleton-Based Human Action Recognition with Noisy Labels","summary":"  Understanding human actions from body poses is critical for assistive robots\nsharing space with humans in order to make informed and safe decisions about\nthe next interaction. However, precise temporal localization and annotation of\nactivity sequences is time-consuming and the resulting labels are often noisy.\nIf not effectively addressed, label noise negatively affects the model's\ntraining, resulting in lower recognition quality. Despite its importance,\naddressing label noise for skeleton-based action recognition has been\noverlooked so far. In this study, we bridge this gap by implementing a\nframework that augments well-established skeleton-based human action\nrecognition methods with label-denoising strategies from various research areas\nto serve as the initial benchmark. Observations reveal that these baselines\nyield only marginal performance when dealing with sparse skeleton data.\nConsequently, we introduce a novel methodology, NoiseEraSAR, which integrates\nglobal sample selection, co-teaching, and Cross-Modal Mixture-of-Experts\n(CM-MOE) strategies, aimed at mitigating the adverse impacts of label noise.\nOur proposed approach demonstrates better performance on the established\nbenchmark, setting new state-of-the-art standards. The source code for this\nstudy is accessible at https://github.com/xuyizdby/NoiseEraSAR.\n","authors":["Yi Xu","Kunyu Peng","Di Wen","Ruiping Liu","Junwei Zheng","Yufan Chen","Jiaming Zhang","Alina Roitberg","Kailun Yang","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2403.09975v2.pdf","comment":"Accepted to IROS 2024. The source code for this study is accessible\n  at https://github.com/xuyizdby/NoiseEraSAR"},{"id":"http://arxiv.org/abs/2408.00275v2","updated":"2024-08-06T00:26:18Z","published":"2024-08-01T04:29:34Z","title":"A Reinforcement Learning Based Motion Planner for Quadrotor Autonomous\n  Flight in Dense Environment","summary":"  Quadrotor motion planning is critical for autonomous flight in complex\nenvironments, such as rescue operations. Traditional methods often employ\ntrajectory generation optimization and passive time allocation strategies,\nwhich can limit the exploitation of the quadrotor's dynamic capabilities and\nintroduce delays and inaccuracies. To address these challenges, we propose a\nnovel motion planning framework that integrates visibility path searching and\nreinforcement learning (RL) motion generation. Our method constructs\ncollision-free paths using heuristic search and visibility graphs, which are\nthen refined by an RL policy to generate low-level motion commands. We validate\nour approach in simulated indoor environments, demonstrating better performance\nthan traditional methods in terms of time span.\n","authors":["Zhaohong Liu","Wenxuan Gao","Yinshuai Sun","Peng Dong"],"pdf_url":"https://arxiv.org/pdf/2408.00275v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02650v3","updated":"2024-08-06T00:08:40Z","published":"2023-10-04T08:18:30Z","title":"Active Visual Localization for Multi-Agent Collaboration: A Data-Driven\n  Approach","summary":"  Rather than having each newly deployed robot create its own map of its\nsurroundings, the growing availability of SLAM-enabled devices provides the\noption of simply localizing in a map of another robot or device. In cases such\nas multi-robot or human-robot collaboration, localizing all agents in the same\nmap is even necessary. However, localizing e.g. a ground robot in the map of a\ndrone or head-mounted MR headset presents unique challenges due to viewpoint\nchanges. This work investigates how active visual localization can be used to\novercome such challenges of viewpoint changes. Specifically, we focus on the\nproblem of selecting the optimal viewpoint at a given location. We compare\nexisting approaches in the literature with additional proposed baselines and\npropose a novel data-driven approach. The result demonstrates the superior\nperformance of the data-driven approach when compared to existing methods, both\nin controlled simulation experiments and real-world deployment.\n","authors":["Matthew Hanlon","Boyang Sun","Marc Pollefeys","Hermann Blum"],"pdf_url":"https://arxiv.org/pdf/2310.02650v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00391v3","updated":"2024-08-06T23:58:07Z","published":"2023-12-31T04:14:43Z","title":"SAFE-SIM: Safety-Critical Closed-Loop Traffic Simulation with\n  Diffusion-Controllable Adversaries","summary":"  Evaluating the performance of autonomous vehicle planning algorithms\nnecessitates simulating long-tail safety-critical traffic scenarios. However,\ntraditional methods for generating such scenarios often fall short in terms of\ncontrollability and realism; they also neglect the dynamics of agent\ninteractions. To address these limitations, we introduce SAFE-SIM, a novel\ndiffusion-based controllable closed-loop safety-critical simulation framework.\nOur approach yields two distinct advantages: 1) generating realistic long-tail\nsafety-critical scenarios that closely reflect real-world conditions, and 2)\nproviding controllable adversarial behavior for more comprehensive and\ninteractive evaluations. We develop a novel approach to simulate\nsafety-critical scenarios through an adversarial term in the denoising process\nof diffusion models, which allows an adversarial agent to challenge a planner\nwith plausible maneuvers while all agents in the scene exhibit reactive and\nrealistic behaviors. Furthermore, we propose novel guidance objectives and a\npartial diffusion process that enables users to control key aspects of the\nscenarios, such as the collision type and aggressiveness of the adversarial\nagent, while maintaining the realism of the behavior. We validate our framework\nempirically using the nuScenes and nuPlan datasets across multiple planners,\ndemonstrating improvements in both realism and controllability. These findings\naffirm that diffusion models provide a robust and versatile foundation for\nsafety-critical, interactive traffic simulation, extending their utility across\nthe broader autonomous driving landscape. Project website:\nhttps://safe-sim.github.io/.\n","authors":["Wei-Jer Chang","Francesco Pittaluga","Masayoshi Tomizuka","Wei Zhan","Manmohan Chandraker"],"pdf_url":"https://arxiv.org/pdf/2401.00391v3.pdf","comment":"Accepted by ECCV2024; Project website: https://safe-sim.github.io/"},{"id":"http://arxiv.org/abs/2405.16266v2","updated":"2024-08-06T21:26:31Z","published":"2024-05-25T15:08:36Z","title":"Deep Reinforcement Learning with Enhanced PPO for Safe Mobile Robot\n  Navigation","summary":"  Collision-free motion is essential for mobile robots. Most approaches to\ncollision-free and efficient navigation with wheeled robots require parameter\ntuning by experts to obtain good navigation behavior. This study investigates\nthe application of deep reinforcement learning to train a mobile robot for\nautonomous navigation in a complex environment. The robot utilizes LiDAR sensor\ndata and a deep neural network to generate control signals guiding it toward a\nspecified target while avoiding obstacles. We employ two reinforcement learning\nalgorithms in the Gazebo simulation environment: Deep Deterministic Policy\nGradient and proximal policy optimization. The study introduces an enhanced\nneural network structure in the Proximal Policy Optimization algorithm to boost\nperformance, accompanied by a well-designed reward function to improve\nalgorithm efficacy. Experimental results conducted in both obstacle and\nobstacle-free environments underscore the effectiveness of the proposed\napproach. This research significantly contributes to the advancement of\nautonomous robotics in complex environments through the application of deep\nreinforcement learning.\n","authors":["Hamid Taheri","Seyed Rasoul Hosseini","Mohammad Ali Nekoui"],"pdf_url":"https://arxiv.org/pdf/2405.16266v2.pdf","comment":"This paper is under review by Int. J. of Intelligent Machines and\n  Robotics"},{"id":"http://arxiv.org/abs/2408.03453v1","updated":"2024-08-06T21:18:56Z","published":"2024-08-06T21:18:56Z","title":"An Interactive Augmented Reality Interface for Personalized Proxemics\n  Modeling","summary":"  Understanding and respecting personal space preferences is essential for\nsocially assistive robots designed for older adult users. This work introduces\nand evaluates a novel personalized context-aware method for modeling users'\nproxemics preferences during human-robot interactions. Using an interactive\naugmented reality interface, we collected a set of user-preferred distances\nfrom the robot and employed an active transfer learning approach to fine-tune a\nspecialized deep learning model. We evaluated this approach through two user\nstudies: 1) a convenience population study (N = 24) to validate the efficacy of\nthe active transfer learning approach; and 2) a user study involving older\nadults (N = 15) to assess the system's usability. We compared the data\ncollected with the augmented reality interface and with the physical robot to\nexamine the relationship between proxemics preferences for a virtual robot\nversus a physically embodied robot. We found that fine-tuning significantly\nimproved model performance: on average, the error in testing decreased by\n26.97% after fine-tuning. The system was well-received by older adult\nparticipants, who provided valuable feedback and suggestions for future work.\n","authors":["Massimiliano Nigro","Amy O'Connell","Thomas Groechel","Anna-Maria Velentza","Maja Matarić"],"pdf_url":"https://arxiv.org/pdf/2408.03453v1.pdf","comment":"M. Nigro, A. O'Connell, T. Groechel, A.M. Velentza and M. Matari\\'c,\n  \"An Interactive Augmented Reality Interface for Personalized Proxemics\n  Modeling: Comfort and Human-Robot Interactions,\" in IEEE Robotics &\n  Automation Magazine, doi: 10.1109/MRA.2024.3415108"},{"id":"http://arxiv.org/abs/2403.10795v2","updated":"2024-08-06T21:14:23Z","published":"2024-03-16T03:54:38Z","title":"Can Large Language Models Solve Robot Routing?","summary":"  Routing problems are common in mobile robotics, encompassing tasks such as\ninspection, surveillance, and coverage. Depending on the objective and\nconstraints, these problems often reduce to variants of the Traveling Salesman\nProblem (TSP), with solutions traditionally derived by translating high-level\nobjectives into an optimization formulation and using modern solvers to arrive\nat a solution. Here, we explore the potential of Large Language Models (LLMs)\nto replace the entire pipeline from tasks described in natural language to the\ngeneration of robot routes. We systematically investigate the performance of\nLLMs in robot routing by constructing a dataset with 80 unique robot routing\nproblems across 8 variants in both single and multi-robot settings. We evaluate\nLLMs through three frameworks: single attempt, self-debugging, and\nself-debugging with self-verification and various contexts, including\nmathematical formulations, pseudo-code, and related research papers. Our\nfindings reveal that both self-debugging and self-verification enhance success\nrates without significantly lowering the optimality gap. We observe\ncontext-sensitive behavior - providing mathematical formulations as context\ndecreases the optimality gap but significantly decreases success rates and\nproviding pseudo-code and related research papers as context does not\nconsistently improve success rates or decrease the optimality gap. We identify\nkey challenges and propose future directions to enhance LLM performance in\nsolving robot routing problems. Our source code is available on the project\nwebsite: https://sites.google.com/view/words-to-routes/.\n","authors":["Zhehui Huang","Guangyao Shi","Gaurav S. Sukhatme"],"pdf_url":"https://arxiv.org/pdf/2403.10795v2.pdf","comment":"Submitted to International Symposium of Robotics Research (ISRR 2024)"},{"id":"http://arxiv.org/abs/2408.03445v1","updated":"2024-08-06T20:53:02Z","published":"2024-08-06T20:53:02Z","title":"Spacecraft inertial parameters estimation using time series clustering\n  and reinforcement learning","summary":"  This paper presents a machine learning approach to estimate the inertial\nparameters of a spacecraft in cases when those change during operations, e.g.\nmultiple deployments of payloads, unfolding of appendages and booms, propellant\nconsumption as well as during in-orbit servicing and active debris removal\noperations. The machine learning approach uses time series clustering together\nwith an optimised actuation sequence generated by reinforcement learning to\nfacilitate distinguishing among different inertial parameter sets. The\nperformance of the proposed strategy is assessed against the case of a\nmulti-satellite deployment system showing that the algorithm is resilient\ntowards common disturbances in such kinds of operations.\n","authors":["Konstantinos Platanitis","Miguel Arana-Catania","Leonardo Capicchiano","Saurabh Upadhyay","Leonard Felicetti"],"pdf_url":"https://arxiv.org/pdf/2408.03445v1.pdf","comment":"6 pages, 3 figures, 1 table. To be presented in ESA - AI for Space\n  (SPAICE)"},{"id":"http://arxiv.org/abs/2408.03435v1","updated":"2024-08-06T20:21:53Z","published":"2024-08-06T20:21:53Z","title":"Communication-Aware Consistent Edge Selection for Mobile Users and\n  Autonomous Vehicles","summary":"  Offloading time-sensitive, computationally intensive tasks-such as advanced\nlearning algorithms for autonomous driving-from vehicles to nearby edge\nservers, vehicle-to-infrastructure (V2I) systems, or other collaborating\nvehicles via vehicle-to-vehicle (V2V) communication enhances service\nefficiency. However, whence traversing the path to the destination, the\nvehicle's mobility necessitates frequent handovers among the access points\n(APs) to maintain continuous and uninterrupted wireless connections to maintain\nthe network's Quality of Service (QoS). These frequent handovers subsequently\nlead to task migrations among the edge servers associated with the respective\nAPs. This paper addresses the joint problem of task migration and access-point\nhandover by proposing a deep reinforcement learning framework based on the Deep\nDeterministic Policy Gradient (DDPG) algorithm. A joint allocation method of\ncommunication and computation of APs is proposed to minimize computational\nload, service latency, and interruptions with the overarching goal of\nmaximizing QoS. We implement and evaluate our proposed framework on simulated\nexperiments to achieve smooth and seamless task switching among edge servers,\nultimately reducing latency.\n","authors":["Nazish Tahir","Ramviyas Parasuraman","Haijian Sun"],"pdf_url":"https://arxiv.org/pdf/2408.03435v1.pdf","comment":"Accepted by Vehicular Technology Conference (VTC) Fall 2024"},{"id":"http://arxiv.org/abs/2310.07729v2","updated":"2024-08-06T20:02:24Z","published":"2023-09-30T00:19:05Z","title":"Energy-Aware Routing Algorithm for Mobile Ground-to-Air Charging","summary":"  We investigate the problem of energy-constrained planning for a cooperative\nsystem of an Unmanned Ground Vehicles (UGV) and an Unmanned Aerial Vehicle\n(UAV). In scenarios where the UGV serves as a mobile base to ferry the UAV and\nas a charging station to recharge the UAV, we formulate a novel\nenergy-constrained routing problem. To tackle this problem, we design an\nenergy-aware routing algorithm, aiming to minimize the overall mission duration\nunder the energy limitations of both vehicles. The algorithm first solves a\nTraveling Salesman Problem (TSP) to generate a guided tour. Then, it employs\nthe Monte-Carlo Tree Search (MCTS) algorithm to refine the tour and generate\npaths for the two vehicles. We evaluate the performance of our algorithm\nthrough extensive simulations and a proof-of-concept experiment. The results\nshow that our algorithm consistently achieves near-optimal mission time and\nmaintains fast running time across a wide range of problem instances.\n","authors":["Bill Cai","Fei Lu","Lifeng Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.07729v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03394v1","updated":"2024-08-06T18:41:57Z","published":"2024-08-06T18:41:57Z","title":"Faster Model Predictive Control via Self-Supervised Initialization\n  Learning","summary":"  Optimization for robot control tasks, spanning various methodologies,\nincludes Model Predictive Control (MPC). However, the complexity of the system,\nsuch as non-convex and non-differentiable cost functions and prolonged planning\nhorizons often drastically increases the computation time, limiting MPC's\nreal-world applicability. Prior works in speeding up the optimization have\nlimitations on solving convex problem and generalizing to hold out domains. To\novercome this challenge, we develop a novel framework aiming at expediting\noptimization processes. In our framework, we combine offline self-supervised\nlearning and online fine-tuning through reinforcement learning to improve the\ncontrol performance and reduce optimization time. We demonstrate the\neffectiveness of our method on a novel, challenging Formula-1-track driving\ntask, achieving 3.9\\% higher performance in optimization time and 3.6\\% higher\nperformance in tracking accuracy on challenging holdout tracks.\n","authors":["Zhaoxin Li","Letian Chen","Rohan Paleja","Subramanya Nageshrao","Matthew Gombolay"],"pdf_url":"https://arxiv.org/pdf/2408.03394v1.pdf","comment":null}]},"2024-08-07T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2408.03200v2","updated":"2024-08-07T02:14:19Z","published":"2024-08-06T13:58:56Z","title":"Adversarial Safety-Critical Scenario Generation using Naturalistic Human\n  Driving Priors","summary":"  Evaluating the decision-making system is indispensable in developing\nautonomous vehicles, while realistic and challenging safety-critical test\nscenarios play a crucial role. Obtaining these scenarios is non-trivial, thanks\nto the long-tailed distribution, sparsity, and rarity in real-world data sets.\nTo tackle this problem, in this paper, we introduce a natural adversarial\nscenario generation solution using naturalistic human driving priors and\nreinforcement learning techniques. By doing this, we can obtain large-scale\ntest scenarios that are both diverse and realistic. Specifically, we build a\nsimulation environment that mimics natural traffic interaction scenarios.\nInformed by this environment, we implement a two-stage procedure. The first\nstage incorporates conventional rule-based models, e.g., IDM~(Intelligent\nDriver Model) and MOBIL~(Minimizing Overall Braking Induced by Lane changes)\nmodel, to coarsely and discretely capture and calibrate key control parameters\nfrom the real-world dataset. Next, we leverage GAIL~(Generative Adversarial\nImitation Learning) to represent driver behaviors continuously. The derived\nGAIL can be further used to design a PPO~(Proximal Policy Optimization)-based\nactor-critic network framework to fine-tune the reward function, and then\noptimizes our natural adversarial scenario generation solution. Extensive\nexperiments have been conducted in the NGSIM dataset including the trajectory\nof 3,000 vehicles. Essential traffic parameters were measured in comparison\nwith the baseline model, e.g., the collision rate, accelerations, steering, and\nthe number of lane changes. Our findings demonstrate that the proposed model\ncan generate realistic safety-critical test scenarios covering both naturalness\nand adversariality, which can be a cornerstone for the development of\nautonomous vehicles.\n","authors":["Kunkun Hao","Yonggang Luo","Wen Cui","Yuqiao Bai","Jucheng Yang","Songyang Yan","Yuxi Pan","Zijiang Yang"],"pdf_url":"https://arxiv.org/pdf/2408.03200v2.pdf","comment":"Published in IEEE Transactions on Intelligent Vehicles, 2023"},{"id":"http://arxiv.org/abs/2408.03131v2","updated":"2024-08-07T02:34:32Z","published":"2024-08-06T12:16:15Z","title":"Stochastic Trajectory Optimization for Demonstration Imitation","summary":"  Humans often learn new skills by imitating the experts and gradually\ndeveloping their proficiency. In this work, we introduce Stochastic Trajectory\nOptimization for Demonstration Imitation (STODI), a trajectory optimization\nframework for robots to imitate the shape of demonstration trajectories with\nimproved dynamic performance. Consistent with the human learning process,\ndemonstration imitation serves as an initial step, while trajectory\noptimization aims to enhance robot motion performance. By generating random\nnoise and constructing proper cost functions, the STODI effectively explores\nand exploits generated noisy trajectories while preserving the demonstration\nshape characteristics. We employ three metrics to measure the similarity of\ntrajectories in both the time and frequency domains to help with demonstration\nimitation. Theoretical analysis reveals relationships among these metrics,\nemphasizing the benefits of frequency-domain analysis for specific tasks.\nExperiments on a 7-DOF robotic arm in the PyBullet simulator validate the\nefficacy of the STODI framework, showcasing the improved optimization\nperformance and stability compared to previous methods.\n","authors":["Chenlin Ming","Zitong Wang","Boxuan Zhang","Xiaoming Duan","Jianping He"],"pdf_url":"https://arxiv.org/pdf/2408.03131v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03906v1","updated":"2024-08-07T17:10:55Z","published":"2024-08-07T17:10:55Z","title":"Achieving Human Level Competitive Robot Table Tennis","summary":"  Achieving human-level speed and performance on real world tasks is a north\nstar for the robotics research community. This work takes a step towards that\ngoal and presents the first learned robot agent that reaches amateur\nhuman-level performance in competitive table tennis. Table tennis is a\nphysically demanding sport which requires human players to undergo years of\ntraining to achieve an advanced level of proficiency. In this paper, we\ncontribute (1) a hierarchical and modular policy architecture consisting of (i)\nlow level controllers with their detailed skill descriptors which model the\nagent's capabilities and help to bridge the sim-to-real gap and (ii) a high\nlevel controller that chooses the low level skills, (2) techniques for enabling\nzero-shot sim-to-real including an iterative approach to defining the task\ndistribution that is grounded in the real-world and defines an automatic\ncurriculum, and (3) real time adaptation to unseen opponents. Policy\nperformance was assessed through 29 robot vs. human matches of which the robot\nwon 45% (13/29). All humans were unseen players and their skill level varied\nfrom beginner to tournament level. Whilst the robot lost all matches vs. the\nmost advanced players it won 100% matches vs. beginners and 55% matches vs.\nintermediate players, demonstrating solidly amateur human-level performance.\nVideos of the matches can be viewed at\nhttps://sites.google.com/view/competitive-robot-table-tennis\n","authors":["David B. D'Ambrosio","Saminda Abeyruwan","Laura Graesser","Atil Iscen","Heni Ben Amor","Alex Bewley","Barney J. Reed","Krista Reymann","Leila Takayama","Yuval Tassa","Krzysztof Choromanski","Erwin Coumans","Deepali Jain","Navdeep Jaitly","Natasha Jaques","Satoshi Kataoka","Yuheng Kuang","Nevena Lazic","Reza Mahjourian","Sherry Moore","Kenneth Oslund","Anish Shankar","Vikas Sindhwani","Vincent Vanhoucke","Grace Vesom","Peng Xu","Pannag R. Sanketi"],"pdf_url":"https://arxiv.org/pdf/2408.03906v1.pdf","comment":"v1, 29 pages, 19 main paper, 10 references + appendix"},{"id":"http://arxiv.org/abs/2408.03838v1","updated":"2024-08-07T15:24:25Z","published":"2024-08-07T15:24:25Z","title":"Using a Distance Sensor to Detect Deviations in a Planar Surface","summary":"  We investigate methods for determining if a planar surface contains geometric\ndeviations (e.g., protrusions, objects, divots, or cliffs) using only an\ninstantaneous measurement from a miniature optical time-of-flight sensor. The\nkey to our method is to utilize the entirety of information encoded in raw\ntime-of-flight data captured by off-the-shelf distance sensors. We provide an\nanalysis of the problem in which we identify the key ambiguity between geometry\nand surface photometrics. To overcome this challenging ambiguity, we fit a\nGaussian mixture model to a small dataset of planar surface measurements. This\nmodel implicitly captures the expected geometry and distribution of\nphotometrics of the planar surface and is used to identify measurements that\nare likely to contain deviations. We characterize our method on a variety of\nsurfaces and planar deviations across a range of scenarios. We find that our\nmethod utilizing raw time-of-flight data outperforms baselines which use only\nderived distance estimates. We build an example application in which our method\nenables mobile robot obstacle and cliff avoidance over a wide field-of-view.\n","authors":["Carter Sifferman","William Sun","Mohit Gupta","Michael Gleicher"],"pdf_url":"https://arxiv.org/pdf/2408.03838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03825v1","updated":"2024-08-07T15:01:08Z","published":"2024-08-07T15:01:08Z","title":"Towards Real-Time Gaussian Splatting: Accelerating 3DGS through\n  Photometric SLAM","summary":"  Initial applications of 3D Gaussian Splatting (3DGS) in Visual Simultaneous\nLocalization and Mapping (VSLAM) demonstrate the generation of high-quality\nvolumetric reconstructions from monocular video streams. However, despite these\npromising advancements, current 3DGS integrations have reduced tracking\nperformance and lower operating speeds compared to traditional VSLAM. To\naddress these issues, we propose integrating 3DGS with Direct Sparse Odometry,\na monocular photometric SLAM system. We have done preliminary experiments\nshowing that using Direct Sparse Odometry point cloud outputs, as opposed to\nstandard structure-from-motion methods, significantly shortens the training\ntime needed to achieve high-quality renders. Reducing 3DGS training time\nenables the development of 3DGS-integrated SLAM systems that operate in\nreal-time on mobile hardware. These promising initial findings suggest further\nexploration is warranted in combining traditional VSLAM systems with 3DGS.\n","authors":["Yan Song Hu","Dayou Mao","Yuhao Chen","John Zelek"],"pdf_url":"https://arxiv.org/pdf/2408.03825v1.pdf","comment":"This extended abstract has been submitted to be presented at an IEEE\n  conference. It will be made available online by IEEE but will not be\n  published in IEEE Xplore. Copyright may be transferred without notice, after\n  which this version may no longer be accessible"},{"id":"http://arxiv.org/abs/2408.03807v1","updated":"2024-08-07T14:32:41Z","published":"2024-08-07T14:32:41Z","title":"Navigating the Human Maze: Real-Time Robot Pathfinding with Generative\n  Imitation Learning","summary":"  This paper addresses navigation in crowded environments by integrating\ngoal-conditioned generative models with Sampling-based Model Predictive Control\n(SMPC). We introduce goal-conditioned autoregressive models to generate crowd\nbehaviors, capturing intricate interactions among individuals. The model\nprocesses potential robot trajectory samples and predicts the reactions of\nsurrounding individuals, enabling proactive robotic navigation in complex\nscenarios. Extensive experiments show that this algorithm enables real-time\nnavigation, significantly reducing collision rates and path lengths, and\noutperforming selected baseline methods. The practical effectiveness of this\nalgorithm is validated on an actual robotic platform, demonstrating its\ncapability in dynamic settings.\n","authors":["Martin Moder","Stephen Adhisaputra","Josef Pauli"],"pdf_url":"https://arxiv.org/pdf/2408.03807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09657v3","updated":"2024-08-07T13:44:01Z","published":"2024-04-15T10:45:12Z","title":"Sampling for Model Predictive Trajectory Planning in Autonomous Driving\n  using Normalizing Flows","summary":"  Alongside optimization-based planners, sampling-based approaches are often\nused in trajectory planning for autonomous driving due to their simplicity.\nModel predictive path integral control is a framework that builds upon\noptimization principles while incorporating stochastic sampling of input\ntrajectories. This paper investigates several sampling approaches for\ntrajectory generation. In this context, normalizing flows originating from the\nfield of variational inference are considered for the generation of sampling\ndistributions, as they model transformations of simple to more complex\ndistributions. Accordingly, learning-based normalizing flow models are trained\nfor a more efficient exploration of the input domain for the task at hand. The\ndeveloped algorithm and the proposed sampling distributions are evaluated in\ntwo simulation scenarios.\n","authors":["Georg Rabenstein","Lars Ullrich","Knut Graichen"],"pdf_url":"https://arxiv.org/pdf/2404.09657v3.pdf","comment":"Accepted to be published as part of the 2024 IEEE Intelligent\n  Vehicles Symposium (IV), Jeju Shinhwa World, Jeju Island, Korea, June 2-5,\n  2024"},{"id":"http://arxiv.org/abs/2408.02454v2","updated":"2024-08-07T13:39:27Z","published":"2024-08-05T13:25:27Z","title":"TGS: Trajectory Generation and Selection using Vision Language Models in\n  Mapless Outdoor Environments","summary":"  We present a multi-modal trajectory generation and selection algorithm for\nreal-world mapless outdoor navigation in challenging scenarios with\nunstructured off-road features like buildings, grass, and curbs. Our goal is to\ncompute suitable trajectories that (1) satisfy the environment-specific\ntraversability constraints and (2) generate human-like paths while navigating\nin crosswalks, sidewalks, etc. Our formulation uses a Conditional Variational\nAutoencoder (CVAE) generative model enhanced with traversability constraints to\ngenerate multiple candidate trajectories for global navigation. We use VLMs and\na visual prompting approach with their zero-shot ability of semantic\nunderstanding and logical reasoning to choose the best trajectory given the\ncontextual information about the task. We evaluate our methods in various\noutdoor scenes with wheeled robots and compare the performance with other\nglobal navigation algorithms. In practice, we observe at least 3.35%\nimprovement in traversability and 20.61% improvement in terms of human-like\nnavigation in generated trajectories in challenging outdoor navigation\nscenarios.\n","authors":["Daeun Song","Jing Liang","Xuesu Xiao","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2408.02454v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03768v1","updated":"2024-08-07T13:38:53Z","published":"2024-08-07T13:38:53Z","title":"HDPlanner: Advancing Autonomous Deployments in Unknown Environments\n  through Hierarchical Decision Networks","summary":"  In this paper, we introduce HDPlanner, a deep reinforcement learning (DRL)\nbased framework designed to tackle two core and challenging tasks for mobile\nrobots: autonomous exploration and navigation, where the robot must optimize\nits trajectory adaptively to achieve the task objective through continuous\ninteractions in unknown environments. Specifically, HDPlanner relies on novel\nhierarchical attention networks to empower the robot to reason about its belief\nacross multiple spatial scales and sequence collaborative decisions, where our\nnetworks decompose long-term objectives into short-term informative task\nassignments and informative path plannings. We further propose a contrastive\nlearning-based joint optimization to enhance the robustness of HDPlanner. We\nempirically demonstrate that HDPlanner significantly outperforms\nstate-of-the-art conventional and learning-based baselines on an extensive set\nof simulations, including hundreds of test maps and large-scale, complex Gazebo\nenvironments. Notably, HDPlanner achieves real-time planning with travel\ndistances reduced by up to 35.7% compared to exploration benchmarks and by up\nto 16.5% than navigation benchmarks. Furthermore, we validate our approach on\nhardware, where it generates high-quality, adaptive trajectories in both indoor\nand outdoor environments, highlighting its real-world applicability without\nadditional training.\n","authors":["Jingsong Liang","Yuhong Cao","Yixiao Ma","Hanqi Zhao","Guillaume Sartoretti"],"pdf_url":"https://arxiv.org/pdf/2408.03768v1.pdf","comment":"Submitted to RA-L"},{"id":"http://arxiv.org/abs/2408.01953v2","updated":"2024-08-07T13:24:38Z","published":"2024-08-04T07:59:17Z","title":"EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning","summary":"  Humans perceive and interact with the world with the awareness of\nequivariance, facilitating us in manipulating different objects in diverse\nposes. For robotic manipulation, such equivariance also exists in many\nscenarios. For example, no matter what the pose of a drawer is (translation,\nrotation and tilt), the manipulation strategy is consistent (grasp the handle\nand pull in a line). While traditional models usually do not have the awareness\nof equivariance for robotic manipulation, which might result in more data for\ntraining and poor performance in novel object poses, we propose our EqvAfford\nframework, with novel designs to guarantee the equivariance in point-level\naffordance learning for downstream robotic manipulation, with great performance\nand generalization ability on representative tasks on objects in diverse poses.\n","authors":["Yue Chen","Chenrui Tie","Ruihai Wu","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2408.01953v2.pdf","comment":"Accept to CVPRWorkshop on Equivariant Vision: From Theory to Practice\n  2024"},{"id":"http://arxiv.org/abs/2303.09565v4","updated":"2024-08-07T13:15:50Z","published":"2023-03-17T16:56:48Z","title":"Component reusability evaluation and requirement tracing for agent-based\n  simulation-physical systems","summary":"  In the early stages of product development, evaluating design concepts is\ncrucial due to its impact on quality and cost. However, this process is often\nhindered by vague and uncertain design information. We use the Domain\nSpecification Language (DSL) to improve design analysis and evaluation of\nsystems incorporating simulation and physical parts. '\n  Goal: Our method evaluates the integrity between the simulated and physical\nembodiment of the system. The assessment is done in various scopes, e.g. per\npair of Digital Twins (DT) and its physical counterpart- Physical Twin (PT),\nsystem-wide, or one of many system setups.\n  Method: We propose a DSL based on Systems Modeling Language (SysML). The\nSimulation-Physical Systems Modeling Language (SPSysML) defines the taxonomy of\nCPS consisting of at least a physical or simulated part. Based on SPSysML, we\ndefine quantitative factors and a requirement-based system structuring method,\nwhich enhances requirement analysis and allows DT to perceive exogenous actions\nin the simulated world.\n  Result: SPSysML is used to develop a robotic system for the INCARE project.\nIn subsequent iterations of the system's design process, the\nsimulation-physical integrity of the system is improved, and more system\ncomponents is shared between its simulated and physical embodiments. The\ndesigned system was deployed on the physical robot and two simulators. System\nsetups are based on Robot Operating System (ROS) and ROS2. Therefore, we argue\nthat SPSysML is neither specific for a control system framework nor a robot\nsimulator. SPSysML was used by a third-party developer and was assessed by him\nand other practitioners in a survey.\n  Summary: SPSysML allows the design of systems featuring DTs and evaluation\nfor improved integrity between simulation and physical parts. The\nrequirement-based system structuring enhances the traceability of system\nrequirements allocation.\n","authors":["Wojciech Dudek","Narcis Miguel","Tomasz Winiarski"],"pdf_url":"https://arxiv.org/pdf/2303.09565v4.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2408.03754v1","updated":"2024-08-07T13:09:57Z","published":"2024-08-07T13:09:57Z","title":"A Soft Robotic System Automatically Learns Precise Agile Motions Without\n  Model Information","summary":"  Many application domains, e.g., in medicine and manufacturing, can greatly\nbenefit from pneumatic Soft Robots (SRs). However, the accurate control of SRs\nhas remained a significant challenge to date, mainly due to their nonlinear\ndynamics and viscoelastic material properties. Conventional control design\nmethods often rely on either complex system modeling or time-intensive manual\ntuning, both of which require significant amounts of human expertise and thus\nlimit their practicality. In recent works, the data-driven method, Automatic\nNeural ODE Control (ANODEC) has been successfully used to -- fully\nautomatically and utilizing only input-output data -- design controllers for\nvarious nonlinear systems in silico, and without requiring prior model\nknowledge or extensive manual tuning. In this work, we successfully apply\nANODEC to automatically learn to perform agile, non-repetitive reference\ntracking motion tasks in a real-world SR and within a finite time horizon. To\nthe best of the authors' knowledge, ANODEC achieves, for the first time,\nperformant control of a SR with hysteresis effects from only 30 seconds of\ninput-output data and without any prior model knowledge. We show that for\nmultiple, qualitatively different and even out-of-training-distribution\nreference signals, a single feedback controller designed by ANODEC outperforms\na manually tuned PID baseline consistently. Overall, this contribution not only\nfurther strengthens the validity of ANODEC, but it marks an important step\ntowards more practical, easy-to-use SRs that can automatically learn to perform\nagile motions from minimal experimental interaction time.\n","authors":["Simon Bachhuber","Alexander Pawluchin","Arka Pal","Ivo Boblan","Thomas Seel"],"pdf_url":"https://arxiv.org/pdf/2408.03754v1.pdf","comment":"Submitted to the IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2024)"},{"id":"http://arxiv.org/abs/2408.03723v1","updated":"2024-08-07T12:24:23Z","published":"2024-08-07T12:24:23Z","title":"MS-Mapping: An Uncertainty-Aware Large-Scale Multi-Session LiDAR Mapping\n  System","summary":"  Large-scale multi-session LiDAR mapping is essential for a wide range of\napplications, including surveying, autonomous driving, crowdsourced mapping,\nand multi-agent navigation. However, existing approaches often struggle with\ndata redundancy, robustness, and accuracy in complex environments. To address\nthese challenges, we present MS-Mapping, an novel multi-session LiDAR mapping\nsystem that employs an incremental mapping scheme for robust and accurate map\nassembly in large-scale environments. Our approach introduces three key\ninnovations: 1) A distribution-aware keyframe selection method that captures\nthe subtle contributions of each point cloud frame to the map by analyzing the\nsimilarity of map distributions. This method effectively reduces data\nredundancy and pose graph size, while enhancing graph optimization speed; 2) An\nuncertainty model that automatically performs least-squares adjustments\naccording to the covariance matrix during graph optimization, improving mapping\nprecision, robustness, and flexibility without the need for scene-specific\nparameter tuning. This uncertainty model enables our system to monitor pose\nuncertainty and avoid ill-posed optimizations, thereby increasing adaptability\nto diverse and challenging environments. 3) To ensure fair evaluation, we\nredesign baseline comparisons and the evaluation benchmark. Direct assessment\nof map accuracy demonstrates the superiority of the proposed MS-Mapping\nalgorithm compared to state-of-the-art methods. In addition to employing public\ndatasets such as Urban-Nav, FusionPortable, and Newer College, we conducted\nextensive experiments on such a large \\SI{855}{m}$\\times$\\SI{636}{m} ground\ntruth map, collecting over \\SI{20}{km} of indoor and outdoor data across more\nthan ten sequences...\n","authors":["Xiangcheng Hu","Jin Wu","Jianhao Jiao","Binqian Jiang","Wei Zhang","Wenshuo Wang","Ping Tan"],"pdf_url":"https://arxiv.org/pdf/2408.03723v1.pdf","comment":"18 pages, 22 figures"},{"id":"http://arxiv.org/abs/2408.03722v1","updated":"2024-08-07T12:19:28Z","published":"2024-08-07T12:19:28Z","title":"Improving the Intelligent Driver Model by Incorporating Vehicle\n  Dynamics: Microscopic Calibration and Macroscopic Validation","summary":"  Microscopic traffic simulations are used to evaluate the impact of\ninfrastructure modifications and evolving vehicle technologies, such as\nconnected and automated driving. Simulated vehicles are controlled via\ncar-following, lane-changing and junction models, which are designed to imitate\nhuman driving behavior. However, physics-based car-following models (CFMs)\ncannot fully replicate measured vehicle trajectories. Therefore, we present\nmodel extensions for the Intelligent Driver Model (IDM), of which some are\nalready included in the Extended Intelligent Driver Model (EIDM), to improve\ncalibration and validation results. They consist of equations based on vehicle\ndynamics and drive off procedures. In addition, parameter selection plays a\ndecisive role. Thus, we introduce a framework to calibrate CFMs using drone\ndata captured at a signalized intersection in Stuttgart, Germany. We compare\nthe calibration error of the Krauss Model with the IDM and EIDM. In this setup,\nthe EIDM achieves a 17.78 % lower mean error than the IDM, based on the\ndistance difference between real world and simulated vehicles. Adding vehicle\ndynamics equations to the EIDM further improves the results by an additional\n18.97 %. The calibrated vehicle-driver combinations are then investigated by\nsimulating the traffic in three different scenarios: at the original\nintersection, in a closed loop and in a stop-and-go wave. The data shows that\nthe improved calibration process of individual vehicles, openly available at\nhttps://www.github.com/stepeos/pycarmodel_calibration, also provides more\naccurate macroscopic results.\n","authors":["Dominik Salles","Steve Oswald","Hans-Christian Reuss"],"pdf_url":"https://arxiv.org/pdf/2408.03722v1.pdf","comment":"Accepted to the 27th IEEE International Conference on Intelligent\n  Transportation Systems (IEEE ITSC 2024)"},{"id":"http://arxiv.org/abs/2403.07470v2","updated":"2024-08-07T12:13:30Z","published":"2024-03-12T10:06:17Z","title":"DrPlanner: Diagnosis and Repair of Motion Planners for Automated\n  Vehicles Using Large Language Models","summary":"  Motion planners are essential for the safe operation of automated vehicles\nacross various scenarios. However, no motion planning algorithm has achieved\nperfection in the literature, and improving its performance is often\ntime-consuming and labor-intensive. To tackle the aforementioned issues, we\npresent DrPlanner, the first framework designed to automatically diagnose and\nrepair motion planners using large language models. Initially, we generate a\nstructured description of the planner and its planned trajectories from both\nnatural and programming languages. Leveraging the profound capabilities of\nlarge language models, our framework returns repaired planners with detailed\ndiagnostic descriptions. Furthermore, our framework advances iteratively with\ncontinuous feedback from the evaluation of the repaired outcomes. Our approach\nis validated using both search- and sampling-based motion planners for\nautomated vehicles; experimental results highlight the need for demonstrations\nin the prompt and show the ability of our framework to effectively identify and\nrectify elusive issues.\n","authors":["Yuanfei Lin","Chenran Li","Mingyu Ding","Masayoshi Tomizuka","Wei Zhan","Matthias Althoff"],"pdf_url":"https://arxiv.org/pdf/2403.07470v2.pdf","comment":"@2024 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2408.01334v2","updated":"2024-08-07T12:01:58Z","published":"2024-08-02T15:32:42Z","title":"A Backbone for Long-Horizon Robot Task Understanding","summary":"  End-to-end robot learning, particularly for long-horizon tasks, often results\nin unpredictable outcomes and poor generalization. To address these challenges,\nwe propose a novel Therblig-based Backbone Framework (TBBF) to enhance robot\ntask understanding and transferability. This framework uses therbligs (basic\naction elements) as the backbone to decompose high-level robot tasks into\nelemental robot configurations, which are then integrated with current\nfoundation models to improve task understanding. The approach consists of two\nstages: offline training and online testing. During the offline training stage,\nwe developed the Meta-RGate SynerFusion (MGSF) network for accurate therblig\nsegmentation across various tasks. In the online testing stage, after a\none-shot demonstration of a new task is collected, our MGSF network extracts\nhigh-level knowledge, which is then encoded into the image using Action\nRegistration (ActionREG). Additionally, the Large Language Model\n(LLM)-Alignment Policy for Visual Correction (LAP-VC) is employed to ensure\nprecise action execution, facilitating trajectory transfer in novel robot\nscenarios. Experimental results validate these methods, achieving 94.37% recall\nin therblig segmentation and success rates of 94.4% and 80% in real-world\nonline robot testing for simple and complex scenarios, respectively.\nSupplementary material is available at:\nhttps://sites.google.com/view/therbligsbasedbackbone/home\n","authors":["Xiaoshuai Chen","Wei Chen","Dongmyoung Lee","Yukun Ge","Nicolas Rojas","Petar Kormushev"],"pdf_url":"https://arxiv.org/pdf/2408.01334v2.pdf","comment":"8 pages, 8 figures. This work is intended to be submitted to IEEE\n  Robotics and Automation Letters (RA-L) for possible publication"},{"id":"http://arxiv.org/abs/2408.03696v1","updated":"2024-08-07T11:23:09Z","published":"2024-08-07T11:23:09Z","title":"Bridging the Gap between ROS~2 and Classical Real-Time Scheduling for\n  Periodic Tasks","summary":"  The Robot Operating System 2 (ROS~2) is a widely used middleware that\nprovides software libraries and tools for developing robotic systems. In these\nsystems, tasks are scheduled by ROS~2 executors. Since the scheduling behavior\nof the default ROS~2 executor is inherently different from classical real-time\nscheduling theory, dedicated analyses or alternative executors, requiring\nsubstantial changes to ROS~2, have been required. In 2023, the events executor,\nwhich features an events queue and allows the possibility to make scheduling\ndecisions immediately after a job completes, was introduced into ROS~2. In this\npaper, we show that, with only minor modifications of the events executor, a\nlarge body of research results from classical real-time scheduling theory\nbecomes applicable. Hence, this enables analytical bounds on the worst-case\nresponse time and the end-to-end latency, outperforming bounds for the default\nROS 2 executor in many scenarios. Our solution is easy to integrate into\nexisting ROS 2 systems since it requires only minor backend modifications of\nthe events executor, which is natively included in ROS 2. The evaluation\nresults show that our ROS~2 events executor with minor modifications can have\nsignificant improvement in terms of dropped jobs, worst-case response time,\nend-to-end latency, and performance compared to the default ROS~2 executor.\n","authors":["Harun Teper","Oren Bell","Mario Günzel","Chris Gill","Jian-Jia Chen"],"pdf_url":"https://arxiv.org/pdf/2408.03696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12670v3","updated":"2024-08-07T10:45:03Z","published":"2024-03-19T12:11:57Z","title":"Driving Animatronic Robot Facial Expression From Speech","summary":"  Animatronic robots hold the promise of enabling natural human-robot\ninteraction through lifelike facial expressions. However, generating realistic,\nspeech-synchronized robot expressions poses significant challenges due to the\ncomplexities of facial biomechanics and the need for responsive motion\nsynthesis. This paper introduces a novel, skinning-centric approach to drive\nanimatronic robot facial expressions from speech input. At its core, the\nproposed approach employs linear blend skinning (LBS) as a unifying\nrepresentation, guiding innovations in both embodiment design and motion\nsynthesis. LBS informs the actuation topology, facilitates human expression\nretargeting, and enables efficient speech-driven facial motion generation. This\napproach demonstrates the capability to produce highly realistic facial\nexpressions on an animatronic face in real-time at over 4000 fps on a single\nNvidia RTX 4090, significantly advancing robots' ability to replicate nuanced\nhuman expressions for natural interaction. To foster further research and\ndevelopment in this field, the code has been made publicly available at:\n\\url{https://github.com/library87/OpenRoboExp}.\n","authors":["Boren Li","Hang Li","Hangxin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.12670v3.pdf","comment":"8 pages, 6 figures, accepted to IROS 2024. For associated project\n  page, see https://library87.github.io/animatronic-face-iros24"},{"id":"http://arxiv.org/abs/2408.01126v2","updated":"2024-08-07T10:25:08Z","published":"2024-08-02T09:07:31Z","title":"IG-SLAM: Instant Gaussian SLAM","summary":"  3D Gaussian Splatting has recently shown promising results as an alternative\nscene representation in SLAM systems to neural implicit representations.\nHowever, current methods either lack dense depth maps to supervise the mapping\nprocess or detailed training designs that consider the scale of the\nenvironment. To address these drawbacks, we present IG-SLAM, a dense RGB-only\nSLAM system that employs robust Dense-SLAM methods for tracking and combines\nthem with Gaussian Splatting. A 3D map of the environment is constructed using\naccurate pose and dense depth provided by tracking. Additionally, we utilize\ndepth uncertainty in map optimization to improve 3D reconstruction. Our decay\nstrategy in map optimization enhances convergence and allows the system to run\nat 10 fps in a single process. We demonstrate competitive performance with\nstate-of-the-art RGB-only SLAM systems while achieving faster operation speeds.\nWe present our experiments on the Replica, TUM-RGBD, ScanNet, and EuRoC\ndatasets. The system achieves photo-realistic 3D reconstruction in large-scale\nsequences, particularly in the EuRoC dataset.\n","authors":["F. Aykut Sarikamis","A. Aydin Alatan"],"pdf_url":"https://arxiv.org/pdf/2408.01126v2.pdf","comment":"8 pages, 3 page ref, 5 figures"},{"id":"http://arxiv.org/abs/2407.12941v2","updated":"2024-08-07T08:43:47Z","published":"2024-07-17T18:12:40Z","title":"Robotic Arm Manipulation with Inverse Reinforcement Learning & TD-MPC","summary":"  One unresolved issue is how to scale model-based inverse reinforcement\nlearning (IRL) to actual robotic manipulation tasks with unpredictable\ndynamics. The ability to learn from both visual and proprioceptive examples,\ncreating algorithms that scale to high-dimensional state-spaces, and mastering\nstrong dynamics models are the main obstacles. In this work, we provide a\ngradient-based inverse reinforcement learning framework that learns cost\nfunctions purely from visual human demonstrations. The shown behavior and the\ntrajectory is then optimized using TD visual model predictive control(MPC) and\nthe learned cost functions. We test our system using fundamental object\nmanipulation tasks on hardware.\n","authors":["Md Shoyib Hassan","Sabir Md Sanaullah"],"pdf_url":"https://arxiv.org/pdf/2407.12941v2.pdf","comment":"10 pages, 13 figures"},{"id":"http://arxiv.org/abs/2404.08271v3","updated":"2024-08-07T08:00:43Z","published":"2024-04-12T06:50:32Z","title":"Transfer Learning Study of Motion Transformer-based Trajectory\n  Predictions","summary":"  Trajectory planning in autonomous driving is highly dependent on predicting\nthe emergent behavior of other road users. Learning-based methods are currently\nshowing impressive results in simulation-based challenges, with\ntransformer-based architectures technologically leading the way. Ultimately,\nhowever, predictions are needed in the real world. In addition to the shifts\nfrom simulation to the real world, many vehicle- and country-specific shifts,\ni.e. differences in sensor systems, fusion and perception algorithms as well as\ntraffic rules and laws, are on the agenda. Since models that can cover all\nsystem setups and design domains at once are not yet foreseeable, model\nadaptation plays a central role. Therefore, a simulation-based study on\ntransfer learning techniques is conducted on basis of a transformer-based\nmodel. Furthermore, the study aims to provide insights into possible trade-offs\nbetween computational time and performance to support effective transfers into\nthe real world.\n","authors":["Lars Ullrich","Alex McMaster","Knut Graichen"],"pdf_url":"https://arxiv.org/pdf/2404.08271v3.pdf","comment":"Published in 2024 IEEE Intelligent Vehicles Symposium (IV), Jeju\n  Shinhwa World, Jeju Island, Korea, June 2-5, 2024"},{"id":"http://arxiv.org/abs/2408.03601v1","updated":"2024-08-07T07:41:01Z","published":"2024-08-07T07:41:01Z","title":"DRAMA: An Efficient End-to-end Motion Planner for Autonomous Driving\n  with Mamba","summary":"  Motion planning is a challenging task to generate safe and feasible\ntrajectories in highly dynamic and complex environments, forming a core\ncapability for autonomous vehicles. In this paper, we propose DRAMA, the first\nMamba-based end-to-end motion planner for autonomous vehicles. DRAMA fuses\ncamera, LiDAR Bird's Eye View images in the feature space, as well as ego\nstatus information, to generate a series of future ego trajectories. Unlike\ntraditional transformer-based methods with quadratic attention complexity for\nsequence length, DRAMA is able to achieve a less computationally intensive\nattention complexity, demonstrating potential to deal with increasingly complex\nscenarios. Leveraging our Mamba fusion module, DRAMA efficiently and\neffectively fuses the features of the camera and LiDAR modalities. In addition,\nwe introduce a Mamba-Transformer decoder that enhances the overall planning\nperformance. This module is universally adaptable to any Transformer-based\nmodel, especially for tasks with long sequence inputs. We further introduce a\nnovel feature state dropout which improves the planner's robustness without\nincreasing training and inference times. Extensive experimental results show\nthat DRAMA achieves higher accuracy on the NAVSIM dataset compared to the\nbaseline Transfuser, with fewer parameters and lower computational costs.\n","authors":["Chengran Yuan","Zhanqi Zhang","Jiawei Sun","Shuo Sun","Zefan Huang","Christina Dao Wen Lee","Dongen Li","Yuhang Han","Anthony Wong","Keng Peng Tee","Marcelo H. Ang Jr"],"pdf_url":"https://arxiv.org/pdf/2408.03601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03551v1","updated":"2024-08-07T05:23:52Z","published":"2024-08-07T05:23:52Z","title":"VPOcc: Exploiting Vanishing Point for Monocular 3D Semantic Occupancy\n  Prediction","summary":"  Monocular 3D semantic occupancy prediction is becoming important in robot\nvision due to the compactness of using a single RGB camera. However, existing\nmethods often do not adequately account for camera perspective geometry,\nresulting in information imbalance along the depth range of the image. To\naddress this issue, we propose a vanishing point (VP) guided monocular 3D\nsemantic occupancy prediction framework named VPOcc. Our framework consists of\nthree novel modules utilizing VP. First, in the VPZoomer module, we initially\nutilize VP in feature extraction to achieve information balanced feature\nextraction across the scene by generating a zoom-in image based on VP. Second,\nwe perform perspective geometry-aware feature aggregation by sampling points\ntowards VP using a VP-guided cross-attention (VPCA) module. Finally, we create\nan information-balanced feature volume by effectively fusing original and\nzoom-in voxel feature volumes with a balanced feature volume fusion (BVFV)\nmodule. Experiments demonstrate that our method achieves state-of-the-art\nperformance for both IoU and mIoU on SemanticKITTI and SSCBench-KITTI360. These\nresults are obtained by effectively addressing the information imbalance in\nimages through the utilization of VP. Our code will be available at\nwww.github.com/anonymous.\n","authors":["Junsu Kim","Junhee Lee","Ukcheol Shin","Jean Oh","Kyungdon Joo"],"pdf_url":"https://arxiv.org/pdf/2408.03551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03539v1","updated":"2024-08-07T04:35:38Z","published":"2024-08-07T04:35:38Z","title":"Deep Reinforcement Learning for Robotics: A Survey of Real-World\n  Successes","summary":"  Reinforcement learning (RL), particularly its combination with deep neural\nnetworks referred to as deep RL (DRL), has shown tremendous promise across a\nwide range of applications, suggesting its potential for enabling the\ndevelopment of sophisticated robotic behaviors. Robotics problems, however,\npose fundamental difficulties for the application of RL, stemming from the\ncomplexity and cost of interacting with the physical world. This article\nprovides a modern survey of DRL for robotics, with a particular focus on\nevaluating the real-world successes achieved with DRL in realizing several key\nrobotic competencies. Our analysis aims to identify the key factors underlying\nthose exciting successes, reveal underexplored areas, and provide an overall\ncharacterization of the status of DRL in robotics. We highlight several\nimportant avenues for future work, emphasizing the need for stable and\nsample-efficient real-world RL paradigms, holistic approaches for discovering\nand integrating various competencies to tackle complex long-horizon, open-world\ntasks, and principled development and evaluation procedures. This survey is\ndesigned to offer insights for both RL practitioners and roboticists toward\nharnessing RL's power to create generally capable real-world robotic systems.\n","authors":["Chen Tang","Ben Abbatematteo","Jiaheng Hu","Rohan Chandra","Roberto Martín-Martín","Peter Stone"],"pdf_url":"https://arxiv.org/pdf/2408.03539v1.pdf","comment":"The first three authors contributed equally. Accepted to Annual\n  Review of Control, Robotics, and Autonomous Systems"},{"id":"http://arxiv.org/abs/2408.03525v1","updated":"2024-08-07T03:24:59Z","published":"2024-08-07T03:24:59Z","title":"Hierarchical learning control for autonomous robots inspired by central\n  nervous system","summary":"  Mammals can generate autonomous behaviors in various complex environments\nthrough the coordination and interaction of activities at different levels of\ntheir central nervous system. In this paper, we propose a novel hierarchical\nlearning control framework by mimicking the hierarchical structure of the\ncentral nervous system along with their coordination and interaction behaviors.\nThe framework combines the active and passive control systems to improve both\nthe flexibility and reliability of the control system as well as to achieve\nmore diverse autonomous behaviors of robots. Specifically, the framework has a\nbackbone of independent neural network controllers at different levels and\ntakes a three-level dual descending pathway structure, inspired from the\nfunctionality of the cerebral cortex, cerebellum, and spinal cord. We\ncomprehensively validated the proposed approach through the simulation as well\nas the experiment of a hexapod robot in various complex environments, including\nobstacle crossing and rapid recovery after partial damage. This study reveals\nthe principle that governs the autonomous behavior in the central nervous\nsystem and demonstrates the effectiveness of the hierarchical control approach\nwith the salient features of the hierarchical learning control architecture and\ncombination of active and passive control systems.\n","authors":["Pei Zhang","Zhaobo Hua","Jinliang Ding"],"pdf_url":"https://arxiv.org/pdf/2408.03525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03520v1","updated":"2024-08-07T03:08:57Z","published":"2024-08-07T03:08:57Z","title":"AirSLAM: An Efficient and Illumination-Robust Point-Line Visual SLAM\n  System","summary":"  In this paper, we present an efficient visual SLAM system designed to tackle\nboth short-term and long-term illumination challenges. Our system adopts a\nhybrid approach that combines deep learning techniques for feature detection\nand matching with traditional backend optimization methods. Specifically, we\npropose a unified convolutional neural network (CNN) that simultaneously\nextracts keypoints and structural lines. These features are then associated,\nmatched, triangulated, and optimized in a coupled manner. Additionally, we\nintroduce a lightweight relocalization pipeline that reuses the built map,\nwhere keypoints, lines, and a structure graph are used to match the query frame\nwith the map. To enhance the applicability of the proposed system to real-world\nrobots, we deploy and accelerate the feature detection and matching networks\nusing C++ and NVIDIA TensorRT. Extensive experiments conducted on various\ndatasets demonstrate that our system outperforms other state-of-the-art visual\nSLAM systems in illumination-challenging environments. Efficiency evaluations\nshow that our system can run at a rate of 73Hz on a PC and 40Hz on an embedded\nplatform.\n","authors":["Kuan Xu","Yuefan Hao","Shenghai Yuan","Chen Wang","Lihua Xie"],"pdf_url":"https://arxiv.org/pdf/2408.03520v1.pdf","comment":"19 pages, 14 figures"},{"id":"http://arxiv.org/abs/2408.03516v1","updated":"2024-08-07T02:54:43Z","published":"2024-08-07T02:54:43Z","title":"Leveraging LLMs for Enhanced Open-Vocabulary 3D Scene Understanding in\n  Autonomous Driving","summary":"  This paper introduces a novel method for open-vocabulary 3D scene\nunderstanding in autonomous driving by combining Language Embedded 3D Gaussians\nwith Large Language Models (LLMs) for enhanced inference. We propose utilizing\nLLMs to generate contextually relevant canonical phrases for segmentation and\nscene interpretation. Our method leverages the contextual and semantic\ncapabilities of LLMs to produce a set of canonical phrases, which are then\ncompared with the language features embedded in the 3D Gaussians. This\nLLM-guided approach significantly improves zero-shot scene understanding and\ndetection of objects of interest, even in the most challenging or unfamiliar\nenvironments. Experimental results on the WayveScenes101 dataset demonstrate\nthat our approach surpasses state-of-the-art methods in terms of accuracy and\nflexibility for open-vocabulary object detection and segmentation. This work\nrepresents a significant advancement towards more intelligent, context-aware\nautonomous driving systems, effectively bridging 3D scene representation with\nhigh-level semantic understanding.\n","authors":["Amirhosein Chahe","Lifeng Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.03516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03515v1","updated":"2024-08-07T02:48:22Z","published":"2024-08-07T02:48:22Z","title":"A Study on Prompt Injection Attack Against LLM-Integrated Mobile Robotic\n  Systems","summary":"  The integration of Large Language Models (LLMs) like GPT-4o into robotic\nsystems represents a significant advancement in embodied artificial\nintelligence. These models can process multi-modal prompts, enabling them to\ngenerate more context-aware responses. However, this integration is not without\nchallenges. One of the primary concerns is the potential security risks\nassociated with using LLMs in robotic navigation tasks. These tasks require\nprecise and reliable responses to ensure safe and effective operation.\nMulti-modal prompts, while enhancing the robot's understanding, also introduce\ncomplexities that can be exploited maliciously. For instance, adversarial\ninputs designed to mislead the model can lead to incorrect or dangerous\nnavigational decisions. This study investigates the impact of prompt injections\non mobile robot performance in LLM-integrated systems and explores secure\nprompt strategies to mitigate these risks. Our findings demonstrate a\nsubstantial overall improvement of approximately 30.8% in both attack detection\nand system performance with the implementation of robust defence mechanisms,\nhighlighting their critical role in enhancing security and reliability in\nmission-oriented tasks.\n","authors":["Wenxiao Zhang","Xiangrui Kong","Conan Dewitt","Thomas Braunl","Jin B. Hong"],"pdf_url":"https://arxiv.org/pdf/2408.03515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16087v4","updated":"2024-08-07T02:36:19Z","published":"2024-06-23T12:02:17Z","title":"Imperative Learning: A Self-supervised Neural-Symbolic Learning\n  Framework for Robot Autonomy","summary":"  Data-driven methods such as reinforcement and imitation learning have\nachieved remarkable success in robot autonomy. However, their data-centric\nnature still hinders them from generalizing well to ever-changing environments.\nMoreover, collecting large datasets for robotic tasks is often impractical and\nexpensive. To overcome these challenges, we introduce a new self-supervised\nneural-symbolic (NeSy) computational framework, imperative learning (IL), for\nrobot autonomy, leveraging the generalization abilities of symbolic reasoning.\nThe framework of IL consists of three primary components: a neural module, a\nreasoning engine, and a memory system. We formulate IL as a special bilevel\noptimization (BLO), which enables reciprocal learning over the three modules.\nThis overcomes the label-intensive obstacles associated with data-driven\napproaches and takes advantage of symbolic reasoning concerning logical\nreasoning, physical principles, geometric analysis, etc. We discuss several\noptimization techniques for IL and verify their effectiveness in five distinct\nrobot autonomy tasks including path planning, rule induction, optimal control,\nvisual odometry, and multi-robot routing. Through various experiments, we show\nthat IL can significantly enhance robot autonomy capabilities and we anticipate\nthat it will catalyze further research across diverse domains.\n","authors":["Chen Wang","Kaiyi Ji","Junyi Geng","Zhongqiang Ren","Taimeng Fu","Fan Yang","Yifan Guo","Haonan He","Xiangyu Chen","Zitong Zhan","Qiwei Du","Shaoshu Su","Bowen Li","Yuheng Qiu","Yi Du","Qihang Li","Yifan Yang","Xiao Lin","Zhipeng Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.16087v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.05082v3","updated":"2024-08-07T02:08:27Z","published":"2023-02-10T06:51:00Z","title":"Reinforcement Learning Aided Sequential Optimization for Unsignalized\n  Intersection Management of Robot Traffic","summary":"  We consider the problem of optimal unsignalized intersection management,\nwherein we seek to obtain safe and optimal trajectories, for a set of robots\nthat arrive randomly and continually. This problem involves repeatedly solving\na mixed integer program (with robot acceleration trajectories as decision\nvariables) with different parameters, for which the computation time using a\nnaive optimization algorithm scales exponentially with the number of robots and\nlanes. Hence, such an approach is not suitable for real-time implementation. In\nthis paper, we propose a solution framework that combines learning and\nsequential optimization. In particular, we propose an algorithm for learning a\nshared policy that given the traffic state information, determines the crossing\norder of the robots. Then, we optimize the trajectories of the robots\nsequentially according to that crossing order. This approach inherently\nguarantees safety at all times. We validate the performance of this approach\nusing extensive simulations and compare our approach against $5$ different\nheuristics from the literature in $9$ different simulation settings. Our\napproach, on average, significantly outperforms the heuristics from the\nliterature in various metrics like objective function, weighted average of\ncrossing times and computation time. For example, in some scenarios, we have\nobserved that our approach offers up to $150\\%$ improvement in objective value\nover the first come first serve heuristic. Even on untrained scenarios, our\napproach shows a consistent improvement (in objective value) of more than\n$30\\%$ over all heuristics under consideration. We also show through\nsimulations that the computation time for our approach scales linearly with the\nnumber of robots (assuming all other factors are constant). Learnt policies are\nimplemented on physical robots with slightly modified framework to address\nreal-world challenges.\n","authors":["Nishchal Hoysal G.","Pavankumar Tallapragada"],"pdf_url":"https://arxiv.org/pdf/2302.05082v3.pdf","comment":"19 pages, 35 figures"},{"id":"http://arxiv.org/abs/2408.03503v1","updated":"2024-08-07T02:03:32Z","published":"2024-08-07T02:03:32Z","title":"Opening the Black Box of 3D Reconstruction Error Analysis with VECTOR","summary":"  Reconstruction of 3D scenes from 2D images is a technical challenge that\nimpacts domains from Earth and planetary sciences and space exploration to\naugmented and virtual reality. Typically, reconstruction algorithms first\nidentify common features across images and then minimize reconstruction errors\nafter estimating the shape of the terrain. This bundle adjustment (BA) step\noptimizes around a single, simplifying scalar value that obfuscates many\npossible causes of reconstruction errors (e.g., initial estimate of the\nposition and orientation of the camera, lighting conditions, ease of feature\ndetection in the terrain). Reconstruction errors can lead to inaccurate\nscientific inferences or endanger a spacecraft exploring a remote environment.\nTo address this challenge, we present VECTOR, a visual analysis tool that\nimproves error inspection for stereo reconstruction BA. VECTOR provides\nanalysts with previously unavailable visibility into feature locations, camera\npose, and computed 3D points. VECTOR was developed in partnership with the\nPerseverance Mars Rover and Ingenuity Mars Helicopter terrain reconstruction\nteam at the NASA Jet Propulsion Laboratory. We report on how this tool was used\nto debug and improve terrain reconstruction for the Mars 2020 mission.\n","authors":["Racquel Fygenson","Kazi Jawad","Isabel Li","Francois Ayoub","Robert G. Deen","Scott Davidoff","Dominik Moritz","Mauricio Hess-Flores"],"pdf_url":"https://arxiv.org/pdf/2408.03503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03498v1","updated":"2024-08-07T01:37:34Z","published":"2024-08-07T01:37:34Z","title":"Grasp Failure Constraints for Fast and Reliable Pick-and-Place Using\n  Multi-Suction-Cup Grippers","summary":"  Multi-suction-cup grippers are frequently employed to perform pick-and-place\nrobotic tasks, especially in industrial settings where grasping a wide range of\nlight to heavy objects in limited amounts of time is a common requirement.\nHowever, most existing works focus on using one or two suction cups to grasp\nonly irregularly shaped but light objects. There is a lack of research on\nrobust manipulation of heavy objects using larger arrays of suction cups, which\nintroduces challenges in modeling and predicting grasp failure. This paper\npresents a general approach to modeling grasp strength in multi-suction-cup\ngrippers, introducing new constraints usable for trajectory planning and\noptimization to achieve fast and reliable pick-and-place maneuvers. The primary\nmodeling challenge is the accurate prediction of the distribution of loads at\neach suction cup while grasping objects. To solve for this load distribution,\nwe find minimum spring potential energy configurations through a simple\nquadratic program. This results in a computationally efficient analytical\nsolution that can be integrated to formulate grasp failure constraints in\ntime-optimal trajectory planning. Finally, we present experimental results to\nvalidate the efficiency and accuracy of the proposed model.\n","authors":["Jee-eun Lee","Robert Sun","Andrew Bylard","Luis Sentis"],"pdf_url":"https://arxiv.org/pdf/2408.03498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04119v1","updated":"2024-08-07T23:00:03Z","published":"2024-08-07T23:00:03Z","title":"Active Inference in Contextual Multi-Armed Bandits for Autonomous\n  Robotic Exploration","summary":"  Autonomous selection of optimal options for data collection from multiple\nalternatives is challenging in uncertain environments. When secondary\ninformation about options is accessible, such problems can be framed as\ncontextual multi-armed bandits (CMABs). Neuro-inspired active inference has\ngained interest for its ability to balance exploration and exploitation using\nthe expected free energy objective function. Unlike previous studies that\nshowed the effectiveness of active inference based strategy for CMABs using\nsynthetic data, this study aims to apply active inference to realistic\nscenarios, using a simulated mineralogical survey site selection problem.\nHyperspectral data from AVIRIS-NG at Cuprite, Nevada, serves as contextual\ninformation for predicting outcome probabilities, while geologists' mineral\nlabels represent outcomes. Monte Carlo simulations assess the robustness of\nactive inference against changing expert preferences. Results show that active\ninference requires fewer iterations than standard bandit approaches with\nreal-world noisy and biased data, and performs better when outcome preferences\nvary online by adapting the selection strategy to align with expert shifts.\n","authors":["Shohei Wakayama","Alberto Candela","Paul Hayne","Nisar Ahmed"],"pdf_url":"https://arxiv.org/pdf/2408.04119v1.pdf","comment":"10 pages, 12 figures, submitted to IEEE Transactions on Robotics"},{"id":"http://arxiv.org/abs/2408.04106v1","updated":"2024-08-07T22:01:43Z","published":"2024-08-07T22:01:43Z","title":"Force-Motion Control For A Six Degree-Of-Freedom Robotic Manipulator","summary":"  This paper presents a unified algorithm for motion and force control for a\nsix degree-of-freedom spatial manipulator. The motion-force controller performs\ntrajectory tracking, maneuvering the manipulator's end-effector through desired\nposition, orientations and rates. When contacting an obstacle or target object,\nthe force module of the controller restricts the manipulator movements with a\nnovel force exertion method, which prevents damage to the manipulator, the\nend-effector, and the objects during the contact or collision. The core\nstrategy presented in this paper is to design the linear acceleration for the\nend-effector which ensures both trajectory tracking and restriction of any\ncontact force at the end-effector. The design of the controller is validated\nthrough numerical simulations and digital twin validation.\n","authors":["Sagar Ojha","Karl Leodler","Lou Barbieri","TseHuai Wu"],"pdf_url":"https://arxiv.org/pdf/2408.04106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05582v3","updated":"2024-08-07T21:54:46Z","published":"2024-04-08T14:57:16Z","title":"Learning Prehensile Dexterity by Imitating and Emulating State-only\n  Observations","summary":"  When human acquire physical skills (e.g., tennis) from experts, we tend to\nfirst learn from merely observing the expert. But this is often insufficient.\nWe then engage in practice, where we try to emulate the expert and ensure that\nour actions produce similar effects on our environment. Inspired by this\nobservation, we introduce Combining IMitation and Emulation for Motion\nRefinement (CIMER) -- a two-stage framework to learn dexterous prehensile\nmanipulation skills from state-only observations. CIMER's first stage involves\nimitation: simultaneously encode the complex interdependent motions of the\nrobot hand and the object in a structured dynamical system. This results in a\nreactive motion generation policy that provides a reasonable motion prior, but\nlacks the ability to reason about contact effects due to the lack of action\nlabels. The second stage involves emulation: learn a motion refinement policy\nvia reinforcement that adjusts the robot hand's motion prior such that the\ndesired object motion is reenacted. CIMER is both task-agnostic (no\ntask-specific reward design or shaping) and intervention-free (no additional\nteleoperated or labeled demonstrations). Detailed experiments with prehensile\ndexterity reveal that i) imitation alone is insufficient, but adding emulation\ndrastically improves performance, ii) CIMER outperforms existing methods in\nterms of sample efficiency and the ability to generate realistic and stable\nmotions, iii) CIMER can either zero-shot generalize or learn to adapt to novel\nobjects from the YCB dataset, even outperforming expert policies trained with\naction labels in most cases. Source code and videos are available at\nhttps://sites.google.com/view/cimer-2024/.\n","authors":["Yunhai Han","Zhenyang Chen","Kyle A Williams","Harish Ravichandar"],"pdf_url":"https://arxiv.org/pdf/2404.05582v3.pdf","comment":"Accepted by RA-L"},{"id":"http://arxiv.org/abs/2403.05466v2","updated":"2024-08-07T20:33:27Z","published":"2024-03-08T17:29:51Z","title":"Grasping Trajectory Optimization with Point Clouds","summary":"  We introduce a new trajectory optimization method for robotic grasping based\non a point-cloud representation of robots and task spaces. In our method,\nrobots are represented by 3D points on their link surfaces. The task space of a\nrobot is represented by a point cloud that can be obtained from depth sensors.\nUsing the point-cloud representation, goal reaching in grasping can be\nformulated as point matching, while collision avoidance can be efficiently\nachieved by querying the signed distance values of the robot points in the\nsigned distance field of the scene points. Consequently, a constrained\nnonlinear optimization problem is formulated to solve the joint motion and\ngrasp planning problem. The advantage of our method is that the point-cloud\nrepresentation is general to be used with any robot in any environment. We\ndemonstrate the effectiveness of our method by performing experiments on a\ntabletop scene and a shelf scene for grasping with a Fetch mobile manipulator\nand a Franka Panda arm. The project page is available at\n\\url{https://irvlutd.github.io/GraspTrajOpt}\n","authors":["Yu Xiang","Sai Haneesh Allu","Rohith Peddi","Tyler Summers","Vibhav Gogate"],"pdf_url":"https://arxiv.org/pdf/2403.05466v2.pdf","comment":"Published in IROS 2024"},{"id":"http://arxiv.org/abs/2404.05888v2","updated":"2024-08-07T20:20:46Z","published":"2024-04-08T22:01:28Z","title":"A Realistic Surgical Simulator for Non-Rigid and Contact-Rich\n  Manipulation in Surgeries with the da Vinci Research Kit","summary":"  Realistic real-time surgical simulators play an increasingly important role\nin surgical robotics research, such as surgical robot learning and automation,\nand surgical skills assessment. Although there are a number of existing\nsurgical simulators for research, they generally lack the ability to simulate\nthe diverse types of objects and contact-rich manipulation tasks typically\npresent in surgeries, such as tissue cutting and blood suction. In this work,\nwe introduce CRESSim, a realistic surgical simulator based on PhysX 5 for the\nda Vinci Research Kit (dVRK) that enables simulating various contact-rich\nsurgical tasks involving different surgical instruments, soft tissue, and body\nfluids. The real-world dVRK console and the master tool manipulator (MTM)\nrobots are incorporated into the system to allow for teleoperation through\nvirtual reality (VR). To showcase the advantages and potentials of the\nsimulator, we present three examples of surgical tasks, including tissue\ngrasping and deformation, blood suction, and tissue cutting. These tasks are\nperformed using the simulated surgical instruments, including the large needle\ndriver, suction irrigator, and curved scissor, through VR-based teleoperation.\n","authors":["Yafei Ou","Sadra Zargarzadeh","Paniz Sedighi","Mahdi Tavakoli"],"pdf_url":"https://arxiv.org/pdf/2404.05888v2.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2305.17175v3","updated":"2024-08-07T18:43:49Z","published":"2023-05-26T18:03:42Z","title":"Multi-Stage Monte Carlo Tree Search for Non-Monotone Object\n  Rearrangement Planning in Narrow Confined Environments","summary":"  Non-monotone object rearrangement planning in confined spaces such as\ncabinets and shelves is a widely occurring but challenging problem in robotics.\nBoth the robot motion and the available regions for object relocation are\nhighly constrained because of the limited space. This work proposes a\nMulti-Stage Monte Carlo Tree Search (MS-MCTS) method to solve non-monotone\nobject rearrangement planning problems in confined spaces. Our approach\ndecouples the complex problem into simpler subproblems using an object stage\ntopology. A subgoal-focused tree expansion algorithm that jointly considers the\nhigh-level planning and the low-level robot motion is designed to reduce the\nsearch space and better guide the search process. By fitting the task into the\nMCTS paradigm, our method produces optimistic solutions by balancing\nexploration and exploitation. The experiments demonstrate that our method\noutperforms the existing methods in terms of the planning time, the number of\nsteps, and the total move distance. Moreover, we deploy our MS-MCTS to a\nreal-world robot system and verify its performance in different scenarios.\n","authors":["Hanwen Ren","Ahmed H. Qureshi"],"pdf_url":"https://arxiv.org/pdf/2305.17175v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04026v1","updated":"2024-08-07T18:19:18Z","published":"2024-08-07T18:19:18Z","title":"Multimodal Gender Fairness in Depression Prediction: Insights on Data\n  from the USA & China","summary":"  Social agents and robots are increasingly being used in wellbeing settings.\nHowever, a key challenge is that these agents and robots typically rely on\nmachine learning (ML) algorithms to detect and analyse an individual's mental\nwellbeing. The problem of bias and fairness in ML algorithms is becoming an\nincreasingly greater source of concern. In concurrence, existing literature has\nalso indicated that mental health conditions can manifest differently across\ngenders and cultures. We hypothesise that the representation of features\n(acoustic, textual, and visual) and their inter-modal relations would vary\namong subjects from different cultures and genders, thus impacting the\nperformance and fairness of various ML models. We present the very first\nevaluation of multimodal gender fairness in depression manifestation by\nundertaking a study on two different datasets from the USA and China. We\nundertake thorough statistical and ML experimentation and repeat the\nexperiments for several different algorithms to ensure that the results are not\nalgorithm-dependent. Our findings indicate that though there are differences\nbetween both datasets, it is not conclusive whether this is due to the\ndifference in depression manifestation as hypothesised or other external\nfactors such as differences in data collection methodology. Our findings\nfurther motivate a call for a more consistent and culturally aware data\ncollection process in order to address the problem of ML bias in depression\ndetection and to promote the development of fairer agents and robots for\nwellbeing.\n","authors":["Joseph Cameron","Jiaee Cheong","Micol Spitale","Hatice Gunes"],"pdf_url":"https://arxiv.org/pdf/2408.04026v1.pdf","comment":"9 Pages, 7 Tables. To be published and indexed in the IEEE Xplore\n  Digital Library under the ACII 2024 Workshop Proceedings"},{"id":"http://arxiv.org/abs/2312.02976v2","updated":"2024-08-07T18:11:51Z","published":"2023-12-05T18:59:45Z","title":"SPOC: Imitating Shortest Paths in Simulation Enables Effective\n  Navigation and Manipulation in the Real World","summary":"  Reinforcement learning (RL) with dense rewards and imitation learning (IL)\nwith human-generated trajectories are the most widely used approaches for\ntraining modern embodied agents. RL requires extensive reward shaping and\nauxiliary losses and is often too slow and ineffective for long-horizon tasks.\nWhile IL with human supervision is effective, collecting human trajectories at\nscale is extremely expensive. In this work, we show that imitating\nshortest-path planners in simulation produces agents that, given a language\ninstruction, can proficiently navigate, explore, and manipulate objects in both\nsimulation and in the real world using only RGB sensors (no depth map or GPS\ncoordinates). This surprising result is enabled by our end-to-end,\ntransformer-based, SPOC architecture, powerful visual encoders paired with\nextensive image augmentation, and the dramatic scale and diversity of our\ntraining data: millions of frames of shortest-path-expert trajectories\ncollected inside approximately 200,000 procedurally generated houses containing\n40,000 unique 3D assets. Our models, data, training code, and newly proposed\n10-task benchmarking suite CHORES are available in\nhttps://spoc-robot.github.io.\n","authors":["Kiana Ehsani","Tanmay Gupta","Rose Hendrix","Jordi Salvador","Luca Weihs","Kuo-Hao Zeng","Kunal Pratap Singh","Yejin Kim","Winson Han","Alvaro Herrasti","Ranjay Krishna","Dustin Schwenk","Eli VanderBilt","Aniruddha Kembhavi"],"pdf_url":"https://arxiv.org/pdf/2312.02976v2.pdf","comment":"First six authors contributed equally. Project page:\n  https://spoc-robot.github.io/"}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2408.03281v2","updated":"2024-08-07T01:00:55Z","published":"2024-08-06T16:28:30Z","title":"StructEval: Deepen and Broaden Large Language Model Assessment via\n  Structured Evaluation","summary":"  Evaluation is the baton for the development of large language models. Current\nevaluations typically employ a single-item assessment paradigm for each atomic\ntest objective, which struggles to discern whether a model genuinely possesses\nthe required capabilities or merely memorizes/guesses the answers to specific\nquestions. To this end, we propose a novel evaluation framework referred to as\nStructEval. Starting from an atomic test objective, StructEval deepens and\nbroadens the evaluation by conducting a structured assessment across multiple\ncognitive levels and critical concepts, and therefore offers a comprehensive,\nrobust and consistent evaluation for LLMs. Experiments on three widely-used\nbenchmarks demonstrate that StructEval serves as a reliable tool for resisting\nthe risk of data contamination and reducing the interference of potential\nbiases, thereby providing more reliable and consistent conclusions regarding\nmodel capabilities. Our framework also sheds light on the design of future\nprincipled and trustworthy LLM evaluation protocols.\n","authors":["Boxi Cao","Mengjie Ren","Hongyu Lin","Xianpei Han","Feng Zhang","Junfeng Zhan","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2408.03281v2.pdf","comment":"ACL 2024;Benchmark at https://github.com/c-box/StructEval\n  ;Leaderboard at https://huggingface.co/spaces/Bowieee/StructEval_leaderboard"},{"id":"http://arxiv.org/abs/2408.03200v2","updated":"2024-08-07T02:14:19Z","published":"2024-08-06T13:58:56Z","title":"Adversarial Safety-Critical Scenario Generation using Naturalistic Human\n  Driving Priors","summary":"  Evaluating the decision-making system is indispensable in developing\nautonomous vehicles, while realistic and challenging safety-critical test\nscenarios play a crucial role. Obtaining these scenarios is non-trivial, thanks\nto the long-tailed distribution, sparsity, and rarity in real-world data sets.\nTo tackle this problem, in this paper, we introduce a natural adversarial\nscenario generation solution using naturalistic human driving priors and\nreinforcement learning techniques. By doing this, we can obtain large-scale\ntest scenarios that are both diverse and realistic. Specifically, we build a\nsimulation environment that mimics natural traffic interaction scenarios.\nInformed by this environment, we implement a two-stage procedure. The first\nstage incorporates conventional rule-based models, e.g., IDM~(Intelligent\nDriver Model) and MOBIL~(Minimizing Overall Braking Induced by Lane changes)\nmodel, to coarsely and discretely capture and calibrate key control parameters\nfrom the real-world dataset. Next, we leverage GAIL~(Generative Adversarial\nImitation Learning) to represent driver behaviors continuously. The derived\nGAIL can be further used to design a PPO~(Proximal Policy Optimization)-based\nactor-critic network framework to fine-tune the reward function, and then\noptimizes our natural adversarial scenario generation solution. Extensive\nexperiments have been conducted in the NGSIM dataset including the trajectory\nof 3,000 vehicles. Essential traffic parameters were measured in comparison\nwith the baseline model, e.g., the collision rate, accelerations, steering, and\nthe number of lane changes. Our findings demonstrate that the proposed model\ncan generate realistic safety-critical test scenarios covering both naturalness\nand adversariality, which can be a cornerstone for the development of\nautonomous vehicles.\n","authors":["Kunkun Hao","Yonggang Luo","Wen Cui","Yuqiao Bai","Jucheng Yang","Songyang Yan","Yuxi Pan","Zijiang Yang"],"pdf_url":"https://arxiv.org/pdf/2408.03200v2.pdf","comment":"Published in IEEE Transactions on Intelligent Vehicles, 2023"},{"id":"http://arxiv.org/abs/2408.03936v1","updated":"2024-08-07T17:54:21Z","published":"2024-08-07T17:54:21Z","title":"SLIM-RAFT: A Novel Fine-Tuning Approach to Improve Cross-Linguistic\n  Performance for Mercosur Common Nomenclature","summary":"  Natural language processing (NLP) has seen significant advancements with the\nadvent of large language models (LLMs). However, substantial improvements are\nstill needed for languages other than English, especially for specific domains\nlike the applications of Mercosur Common Nomenclature (NCM), a Brazilian\nHarmonized System (HS). To address this gap, this study uses TeenyTineLLaMA, a\nfoundational Portuguese LLM, as an LLM source to implement the NCM application\nprocessing. Additionally, a simplified Retrieval-Augmented Fine-Tuning (RAFT)\ntechnique, termed SLIM-RAFT, is proposed for task-specific fine-tuning of LLMs.\nThis approach retains the chain-of-thought (CoT) methodology for prompt\ndevelopment in a more concise and streamlined manner, utilizing brief and\nfocused documents for training. The proposed model demonstrates an efficient\nand cost-effective alternative for fine-tuning smaller LLMs, significantly\noutperforming TeenyTineLLaMA and ChatGPT-4 in the same task. Although the\nresearch focuses on NCM applications, the methodology can be easily adapted for\nHS applications worldwide.\n","authors":["Vinícius Di Oliveira","Yuri Façanha Bezerra","Li Weigang","Pedro Carvalho Brom","Victor Rafael R. Celestino"],"pdf_url":"https://arxiv.org/pdf/2408.03936v1.pdf","comment":"13 pages, 1 figure, to be publish in International Conference on Web\n  Information Systems and Technologies - WEBIST 2024 proceedings"},{"id":"http://arxiv.org/abs/2408.03910v1","updated":"2024-08-07T17:13:59Z","published":"2024-08-07T17:13:59Z","title":"CodexGraph: Bridging Large Language Models and Code Repositories via\n  Code Graph Databases","summary":"  Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval\nand MBPP, but struggle with handling entire code repositories. This challenge\nhas prompted research on enhancing LLM-codebase interaction at a repository\nscale. Current solutions rely on similarity-based retrieval or manual tools and\nAPIs, each with notable drawbacks. Similarity-based retrieval often has low\nrecall in complex tasks, while manual tools and APIs are typically\ntask-specific and require expert knowledge, reducing their generalizability\nacross diverse code tasks and real-world applications. To mitigate these\nlimitations, we introduce \\framework, a system that integrates LLM agents with\ngraph database interfaces extracted from code repositories. By leveraging the\nstructural properties of graph databases and the flexibility of the graph query\nlanguage, \\framework enables the LLM agent to construct and execute queries,\nallowing for precise, code structure-aware context retrieval and code\nnavigation. We assess \\framework using three benchmarks: CrossCodeEval,\nSWE-bench, and EvoCodeBench. Additionally, we develop five real-world coding\napplications. With a unified graph database schema, \\framework demonstrates\ncompetitive performance and potential in both academic and real-world\nenvironments, showcasing its versatility and efficacy in software engineering.\nOur application demo:\nhttps://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent.\n","authors":["Xiangyan Liu","Bo Lan","Zhiyuan Hu","Yang Liu","Zhicheng Zhang","Wenmeng Zhou","Fei Wang","Michael Shieh"],"pdf_url":"https://arxiv.org/pdf/2408.03910v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2408.03909v1","updated":"2024-08-07T17:13:46Z","published":"2024-08-07T17:13:46Z","title":"LaFA: Latent Feature Attacks on Non-negative Matrix Factorization","summary":"  As Machine Learning (ML) applications rapidly grow, concerns about\nadversarial attacks compromising their reliability have gained significant\nattention. One unsupervised ML method known for its resilience to such attacks\nis Non-negative Matrix Factorization (NMF), an algorithm that decomposes input\ndata into lower-dimensional latent features. However, the introduction of\npowerful computational tools such as Pytorch enables the computation of\ngradients of the latent features with respect to the original data, raising\nconcerns about NMF's reliability. Interestingly, naively deriving the\nadversarial loss for NMF as in the case of ML would result in the\nreconstruction loss, which can be shown theoretically to be an ineffective\nattacking objective. In this work, we introduce a novel class of attacks in NMF\ntermed Latent Feature Attacks (LaFA), which aim to manipulate the latent\nfeatures produced by the NMF process. Our method utilizes the Feature Error\n(FE) loss directly on the latent features. By employing FE loss, we generate\nperturbations in the original data that significantly affect the extracted\nlatent features, revealing vulnerabilities akin to those found in other ML\ntechniques. To handle large peak-memory overhead from gradient back-propagation\nin FE attacks, we develop a method based on implicit differentiation which\nenables their scaling to larger datasets. We validate NMF vulnerabilities and\nFE attacks effectiveness through extensive experiments on synthetic and\nreal-world data.\n","authors":["Minh Vu","Ben Nebgen","Erik Skau","Geigh Zollicoffer","Juan Castorena","Kim Rasmussen","Boian Alexandrov","Manish Bhattarai"],"pdf_url":"https://arxiv.org/pdf/2408.03909v1.pdf","comment":"LA-UR-24-26951"},{"id":"http://arxiv.org/abs/2408.03907v1","updated":"2024-08-07T17:11:34Z","published":"2024-08-07T17:11:34Z","title":"Decoding Biases: Automated Methods and LLM Judges for Gender Bias\n  Detection in Language Models","summary":"  Large Language Models (LLMs) have excelled at language understanding and\ngenerating human-level text. However, even with supervised training and human\nalignment, these LLMs are susceptible to adversarial attacks where malicious\nusers can prompt the model to generate undesirable text. LLMs also inherently\nencode potential biases that can cause various harmful effects during\ninteractions. Bias evaluation metrics lack standards as well as consensus and\nexisting methods often rely on human-generated templates and annotations which\nare expensive and labor intensive. In this work, we train models to\nautomatically create adversarial prompts to elicit biased responses from target\nLLMs. We present LLM- based bias evaluation metrics and also analyze several\nexisting automatic evaluation methods and metrics. We analyze the various\nnuances of model responses, identify the strengths and weaknesses of model\nfamilies, and assess where evaluation methods fall short. We compare these\nmetrics to human evaluation and validate that the LLM-as-a-Judge metric aligns\nwith human judgement on bias in response generation.\n","authors":["Shachi H Kumar","Saurav Sahay","Sahisnu Mazumder","Eda Okur","Ramesh Manuvinakurike","Nicole Beckage","Hsuan Su","Hung-yi Lee","Lama Nachman"],"pdf_url":"https://arxiv.org/pdf/2408.03907v1.pdf","comment":"6 pages paper content, 17 pages of appendix"},{"id":"http://arxiv.org/abs/2402.14049v2","updated":"2024-08-07T17:09:10Z","published":"2024-02-21T18:25:04Z","title":"Generative Adversarial Models for Extreme Geospatial Downscaling","summary":"  Addressing the challenges of climate change requires accurate and\nhigh-resolution mapping of geospatial data, especially climate and weather\nvariables. However, many existing geospatial datasets, such as the gridded\noutputs of the state-of-the-art numerical climate models (e.g., general\ncirculation models), are only available at very coarse spatial resolutions due\nto the model complexity and extremely high computational demand.\nDeep-learning-based methods, particularly generative adversarial networks\n(GANs) and their variants, have proved effective for refining natural images\nand have shown great promise in improving geospatial datasets. This paper\ndescribes a conditional GAN-based stochastic geospatial downscaling method that\ncan accommodates very high scaling factors. Compared to most existing methods,\nthe method can generate high-resolution accurate climate datasets from very\nlow-resolution inputs. More importantly, the method explicitly considers the\nuncertainty inherent to the downscaling process that tends to be ignored in\nexisting methods. Given an input, the method can produce a multitude of\nplausible high-resolution samples instead of one single deterministic result.\nThese samples allow for an empirical exploration and inferences of model\nuncertainty and robustness. With a case study of gridded climate datasets (wind\nvelocity and solar irradiance), we demonstrate the performances of the\nframework in downscaling tasks with large scaling factors (up to $64\\times$)\nand highlight the advantages of the framework with a comprehensive comparison\nwith commonly used and most recent downscaling methods, including area-to-point\n(ATP) kriging, deep image prior (DIP), enhanced super-resolution generative\nadversarial networks (ESRGAN), physics-informed resolution-enhancing GAN (PhIRE\nGAN), and an efficient diffusion model for remote sensing image\nsuper-resolution (EDiffSR).\n","authors":["Guiye Li","Guofeng Cao"],"pdf_url":"https://arxiv.org/pdf/2402.14049v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03904v1","updated":"2024-08-07T17:08:46Z","published":"2024-08-07T17:08:46Z","title":"Lightweight Video Denoising Using a Classic Bayesian Backbone","summary":"  In recent years, state-of-the-art image and video denoising networks have\nbecome increasingly large, requiring millions of trainable parameters to\nachieve best-in-class performance. Improved denoising quality has come at the\ncost of denoising speed, where modern transformer networks are far slower to\nrun than smaller denoising networks such as FastDVDnet and classic Bayesian\ndenoisers such as the Wiener filter.\n  In this paper, we implement a hybrid Wiener filter which leverages small\nancillary networks to increase the original denoiser performance, while\nretaining fast denoising speeds. These networks are used to refine the Wiener\ncoring estimate, optimise windowing functions and estimate the unknown noise\nprofile. Using these methods, we outperform several popular denoisers and\nremain within 0.2 dB, on average, of the popular VRT transformer. Our method\nwas found to be over x10 faster than the transformer method, with a far lower\nparameter cost.\n","authors":["Clément Bled","François Pitié"],"pdf_url":"https://arxiv.org/pdf/2408.03904v1.pdf","comment":"Paper accepted to ICME 2024"},{"id":"http://arxiv.org/abs/2407.13218v3","updated":"2024-08-07T16:57:06Z","published":"2024-07-18T07:04:33Z","title":"LiNR: Model Based Neural Retrieval on GPUs at LinkedIn","summary":"  This paper introduces LiNR, LinkedIn's large-scale, GPU-based retrieval\nsystem. LiNR supports a billion-sized index on GPU models. We discuss our\nexperiences and challenges in creating scalable, differentiable search indexes\nusing TensorFlow and PyTorch at production scale. In LiNR, both items and model\nweights are integrated into the model binary. Viewing index construction as a\nform of model training, we describe scaling our system for large indexes,\nincorporating full scans and efficient filtering. A key focus is on enabling\nattribute-based pre-filtering for exhaustive GPU searches, addressing the\ncommon challenge of post-filtering in KNN searches that often reduces system\nquality. We further provide multi-embedding retrieval algorithms and strategies\nfor tackling cold start issues in retrieval. Our advancements in supporting\nlarger indexes through quantization are also discussed. We believe LiNR\nrepresents one of the industry's first Live-updated model-based retrieval\nindexes. Applied to out-of-network post recommendations on LinkedIn Feed, LiNR\nhas contributed to a 3% relative increase in professional daily active users.\nWe envisage LiNR as a step towards integrating retrieval and ranking into a\nsingle GPU model, simplifying complex infrastructures and enabling end-to-end\noptimization of the entire differentiable infrastructure through gradient\ndescent.\n","authors":["Fedor Borisyuk","Qingquan Song","Mingzhou Zhou","Ganesh Parameswaran","Madhu Arun","Siva Popuri","Tugrul Bingol","Zhuotao Pei","Kuang-Hsuan Lee","Lu Zheng","Qizhan Shao","Ali Naqvi","Sen Zhou","Aman Gupta"],"pdf_url":"https://arxiv.org/pdf/2407.13218v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03899v1","updated":"2024-08-07T16:55:00Z","published":"2024-08-07T16:55:00Z","title":"Simplifying Scholarly Abstracts for Accessible Digital Libraries","summary":"  Standing at the forefront of knowledge dissemination, digital libraries\ncurate vast collections of scientific literature. However, these scholarly\nwritings are often laden with jargon and tailored for domain experts rather\nthan the general public. As librarians, we strive to offer services to a\ndiverse audience, including those with lower reading levels. To extend our\nservices beyond mere access, we propose fine-tuning a language model to rewrite\nscholarly abstracts into more comprehensible versions, thereby making scholarly\nliterature more accessible when requested. We began by introducing a corpus\nspecifically designed for training models to simplify scholarly abstracts. This\ncorpus consists of over three thousand pairs of abstracts and significance\nstatements from diverse disciplines. We then fine-tuned four language models\nusing this corpus. The outputs from the models were subsequently examined both\nquantitatively for accessibility and semantic coherence, and qualitatively for\nlanguage quality, faithfulness, and completeness. Our findings show that the\nresulting models can improve readability by over three grade levels, while\nmaintaining fidelity to the original content. Although commercial\nstate-of-the-art models still hold an edge, our models are much more compact,\ncan be deployed locally in an affordable manner, and alleviate the privacy\nconcerns associated with using commercial models. We envision this work as a\nstep toward more inclusive and accessible libraries, improving our services for\nyoung readers and those without a college degree.\n","authors":["Haining Wang","Jason Clark"],"pdf_url":"https://arxiv.org/pdf/2408.03899v1.pdf","comment":"Initial submission to JCDL2024"},{"id":"http://arxiv.org/abs/2402.06859v2","updated":"2024-08-07T16:54:23Z","published":"2024-02-10T01:47:10Z","title":"LiRank: Industrial Large Scale Ranking Models at LinkedIn","summary":"  We present LiRank, a large-scale ranking framework at LinkedIn that brings to\nproduction state-of-the-art modeling architectures and optimization methods. We\nunveil several modeling improvements, including Residual DCN, which adds\nattention and residual connections to the famous DCNv2 architecture. We share\ninsights into combining and tuning SOTA architectures to create a unified\nmodel, including Dense Gating, Transformers and Residual DCN. We also propose\nnovel techniques for calibration and describe how we productionalized deep\nlearning based explore/exploit methods. To enable effective, production-grade\nserving of large ranking models, we detail how to train and compress models\nusing quantization and vocabulary compression. We provide details about the\ndeployment setup for large-scale use cases of Feed ranking, Jobs\nRecommendations, and Ads click-through rate (CTR) prediction. We summarize our\nlearnings from various A/B tests by elucidating the most effective technical\napproaches. These ideas have contributed to relative metrics improvements\nacross the board at LinkedIn: +0.5% member sessions in the Feed, +1.76%\nqualified job applications for Jobs search and recommendations, and +4.3% for\nAds CTR. We hope this work can provide practical insights and solutions for\npractitioners interested in leveraging large-scale deep ranking systems.\n","authors":["Fedor Borisyuk","Mingzhou Zhou","Qingquan Song","Siyu Zhu","Birjodh Tiwana","Ganesh Parameswaran","Siddharth Dangi","Lars Hertel","Qiang Xiao","Xiaochen Hou","Yunbo Ouyang","Aman Gupta","Sheallika Singh","Dan Liu","Hailing Cheng","Lei Le","Jonathan Hung","Sathiya Keerthi","Ruoyan Wang","Fengyu Zhang","Mohit Kothari","Chen Zhu","Daqi Sun","Yun Dai","Xun Luan","Sirou Zhu","Zhiwei Wang","Neil Daftary","Qianqi Shen","Chengming Jiang","Haichao Wei","Maneesh Varshney","Amol Ghoting","Souvik Ghosh"],"pdf_url":"https://arxiv.org/pdf/2402.06859v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03892v1","updated":"2024-08-07T16:44:53Z","published":"2024-08-07T16:44:53Z","title":"MORTAR: A Model-based Runtime Action Repair Framework for AI-enabled\n  Cyber-Physical Systems","summary":"  Cyber-Physical Systems (CPSs) are increasingly prevalent across various\nindustrial and daily-life domains, with applications ranging from robotic\noperations to autonomous driving. With recent advancements in artificial\nintelligence (AI), learning-based components, especially AI controllers, have\nbecome essential in enhancing the functionality and efficiency of CPSs.\nHowever, the lack of interpretability in these AI controllers presents\nchallenges to the safety and quality assurance of AI-enabled CPSs (AI-CPSs).\nExisting methods for improving the safety of AI controllers often involve\nneural network repair, which requires retraining with additional adversarial\nexamples or access to detailed internal information of the neural network.\nHence, these approaches have limited applicability for black-box policies,\nwhere only the inputs and outputs are accessible during operation. To overcome\nthis, we propose MORTAR, a runtime action repair framework designed for AI-CPSs\nin this work. MORTAR begins by constructing a prediction model that forecasts\nthe quality of actions proposed by the AI controller. If an unsafe action is\ndetected, MORTAR then initiates a repair process to correct it. The generation\nof repaired actions is achieved through an optimization process guided by the\nsafety estimates from the prediction model. We evaluate the effectiveness of\nMORTAR across various CPS tasks and AI controllers. The results demonstrate\nthat MORTAR can efficiently improve task completion rates of AI controllers\nunder specified safety specifications. Meanwhile, it also maintains minimal\ncomputational overhead, ensuring real-time operation of the AI-CPSs.\n","authors":["Renzhi Wang","Zhehua Zhou","Jiayang Song","Xuan Xie","Xiaofei Xie","Lei Ma"],"pdf_url":"https://arxiv.org/pdf/2408.03892v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03877v1","updated":"2024-08-07T16:27:45Z","published":"2024-08-07T16:27:45Z","title":"Knowledge Probing for Graph Representation Learning","summary":"  Graph learning methods have been extensively applied in diverse application\nareas. However, what kind of inherent graph properties e.g. graph proximity,\ngraph structural information has been encoded into graph representation\nlearning for downstream tasks is still under-explored. In this paper, we\npropose a novel graph probing framework (GraphProbe) to investigate and\ninterpret whether the family of graph learning methods has encoded different\nlevels of knowledge in graph representation learning. Based on the intrinsic\nproperties of graphs, we design three probes to systematically investigate the\ngraph representation learning process from different perspectives, respectively\nthe node-wise level, the path-wise level, and the structural level. We\nconstruct a thorough evaluation benchmark with nine representative graph\nlearning methods from random walk based approaches, basic graph neural networks\nand self-supervised graph methods, and probe them on six benchmark datasets for\nnode classification, link prediction and graph classification. The experimental\nevaluation verify that GraphProbe can estimate the capability of graph\nrepresentation learning. Remaking results have been concluded: GCN and\nWeightedGCN methods are relatively versatile methods achieving better results\nwith respect to different tasks.\n","authors":["Mingyu Zhao","Xingyu Huang","Ziyu Lyu","Yanlin Wang","Lixin Cui","Lu Bai"],"pdf_url":"https://arxiv.org/pdf/2408.03877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03872v1","updated":"2024-08-07T16:22:21Z","published":"2024-08-07T16:22:21Z","title":"Inter-Series Transformer: Attending to Products in Time Series\n  Forecasting","summary":"  Time series forecasting is an important task in many fields ranging from\nsupply chain management to weather forecasting. Recently, Transformer neural\nnetwork architectures have shown promising results in forecasting on common\ntime series benchmark datasets. However, application to supply chain demand\nforecasting, which can have challenging characteristics such as sparsity and\ncross-series effects, has been limited.\n  In this work, we explore the application of Transformer-based models to\nsupply chain demand forecasting. In particular, we develop a new\nTransformer-based forecasting approach using a shared, multi-task per-time\nseries network with an initial component applying attention across time series,\nto capture interactions and help address sparsity. We provide a case study\napplying our approach to successfully improve demand prediction for a medical\ndevice manufacturing company. To further validate our approach, we also apply\nit to public demand forecasting datasets as well and demonstrate competitive to\nsuperior performance compared to a variety of baseline and state-of-the-art\nforecast methods across the private and public datasets.\n","authors":["Rares Cristian","Pavithra Harsha","Clemente Ocejo","Georgia Perakis","Brian Quanz","Ioannis Spantidakis","Hamza Zerhouni"],"pdf_url":"https://arxiv.org/pdf/2408.03872v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03871v1","updated":"2024-08-07T16:21:41Z","published":"2024-08-07T16:21:41Z","title":"BeeManc at the PLABA Track of TAC-2023: Investigating LLMs and\n  Controllable Attributes for Improving Biomedical Text Readability","summary":"  In this system report, we describe the models and methods we used for our\nparticipation in the PLABA2023 task on biomedical abstract simplification, part\nof the TAC 2023 tracks. The system outputs we submitted come from the following\nthree categories: 1) domain fine-tuned T5-like models including Biomedical-T5\nand Lay-SciFive; 2) fine-tuned BARTLarge model with controllable attributes\n(via tokens) BART-w-CTs; 3) ChatGPTprompting. We also present the work we\ncarried out for this task on BioGPT finetuning. In the official automatic\nevaluation using SARI scores, BeeManc ranks 2nd among all teams and our model\nLaySciFive ranks 3rd among all 13 evaluated systems. In the official human\nevaluation, our model BART-w-CTs ranks 2nd on Sentence-Simplicity (score\n92.84), 3rd on Term-Simplicity (score 82.33) among all 7 evaluated systems; It\nalso produced a high score 91.57 on Fluency in comparison to the highest score\n93.53. In the second round of submissions, our team using ChatGPT-prompting\nranks the 2nd in several categories including simplified term accuracy score\n92.26 and completeness score 96.58, and a very similar score on faithfulness\nscore 95.3 to re-evaluated PLABA-base-1 (95.73) via human evaluations. Our\ncodes, fine-tuned models, prompts, and data splits from the system development\nstage will be available at https://github.com/ HECTA-UoM/PLABA-MU\n","authors":["Zihao Li","Samuel Belkadi","Nicolo Micheletti","Lifeng Han","Matthew Shardlow","Goran Nenadic"],"pdf_url":"https://arxiv.org/pdf/2408.03871v1.pdf","comment":"system report for PLABA-2023. arXiv admin note: substantial text\n  overlap with arXiv:2309.13202"},{"id":"http://arxiv.org/abs/2404.14712v3","updated":"2024-08-07T16:19:15Z","published":"2024-04-23T03:39:57Z","title":"ORBIT: Oak Ridge Base Foundation Model for Earth System Predictability","summary":"  Earth system predictability is challenged by the complexity of environmental\ndynamics and the multitude of variables involved. Current AI foundation models,\nalthough advanced by leveraging large and heterogeneous data, are often\nconstrained by their size and data integration, limiting their effectiveness in\naddressing the full range of Earth system prediction challenges. To overcome\nthese limitations, we introduce the Oak Ridge Base Foundation Model for Earth\nSystem Predictability (ORBIT), an advanced vision transformer model that scales\nup to 113 billion parameters using a novel hybrid tensor-data orthogonal\nparallelism technique. As the largest model of its kind, ORBIT surpasses the\ncurrent climate AI foundation model size by a thousandfold. Performance scaling\ntests conducted on the Frontier supercomputer have demonstrated that ORBIT\nachieves 684 petaFLOPS to 1.6 exaFLOPS sustained throughput, with scaling\nefficiency maintained at 41% to 85% across 49,152 AMD GPUs. These breakthroughs\nestablish new advances in AI-driven climate modeling and demonstrate promise to\nsignificantly improve the Earth system predictability.\n","authors":["Xiao Wang","Siyan Liu","Aristeidis Tsaris","Jong-Youl Choi","Ashwin Aji","Ming Fan","Wei Zhang","Junqi Yin","Moetasim Ashfaq","Dan Lu","Prasanna Balaprakash"],"pdf_url":"https://arxiv.org/pdf/2404.14712v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03841v1","updated":"2024-08-07T15:27:22Z","published":"2024-08-07T15:27:22Z","title":"MaxMind: A Memory Loop Network to Enhance Software Productivity based on\n  Large Language Models","summary":"  The application of large language models to facilitate automated software\noperations and tool generation (SOTG), thus augmenting software productivity,\nmirrors the early stages of human evolution when the ability to create and use\ntools accelerated the progress of civilization. These complex tasks require AI\nto continuously summarize and improve. Current research often overlooks the\nimportance of converting real-time task experiences into system memory and\ndifferentiating the value of existing knowledge for future reference. This\npaper addresses these issues by evolving external memory models into\nMemory-Loop Networks for timely memorization and experience referencing. We\nalso enhance a RAG mechanism with knowledge precision segmentation to utilize\nmemory based on value differentiation, and design the MaxMind model for SOTG\naccordingly.To demonstrate our approach, we developed MaxMind4Sheet, an\nelectronic spreadsheet processing system aligned with the MaxMind philosophy.\nComparative experiments with SheetCopilot have demonstrated that the\naccumulation and recycling of task memories lead to a steady enhancement in\ntask success rate, with an improvement rate of approximately 3%-6% per round in\nthis implementation example. Note that as the memories continue to grow, this\ncumulative improvement may be substantial. The inclusion of memory recycling\ncan also boost the system's task execution efficiency by up to 25%, and it can\naddress the retraining issue faced by LLMs when handling specialized tasks\nthrough memories transfer.These suggest that MaxMind has significant potential\nto enhance the capabilities and productivity of LLM systems in SOTG.\n","authors":["Yuchen Dong","XiaoXiang Fang","Yuchen Hu","Renshuang Jiang","Zhe Jiang"],"pdf_url":"https://arxiv.org/pdf/2408.03841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03837v1","updated":"2024-08-07T15:22:44Z","published":"2024-08-07T15:22:44Z","title":"WalledEval: A Comprehensive Safety Evaluation Toolkit for Large Language\n  Models","summary":"  WalledEval is a comprehensive AI safety testing toolkit designed to evaluate\nlarge language models (LLMs). It accommodates a diverse range of models,\nincluding both open-weight and API-based ones, and features over 35 safety\nbenchmarks covering areas such as multilingual safety, exaggerated safety, and\nprompt injections. The framework supports both LLM and judge benchmarking, and\nincorporates custom mutators to test safety against various text-style\nmutations such as future tense and paraphrasing. Additionally, WalledEval\nintroduces WalledGuard, a new, small and performant content moderation tool,\nand SGXSTest, a benchmark for assessing exaggerated safety in cultural\ncontexts. We make WalledEval publicly available at\nhttps://github.com/walledai/walledevalA.\n","authors":["Prannaya Gupta","Le Qi Yau","Hao Han Low","I-Shiang Lee","Hugo Maximus Lim","Yu Xin Teoh","Jia Hng Koh","Dar Win Liew","Rishabh Bhardwaj","Rajat Bhardwaj","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2408.03837v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2408.03834v1","updated":"2024-08-07T15:17:51Z","published":"2024-08-07T15:17:51Z","title":"Target Prompting for Information Extraction with Vision Language Model","summary":"  The recent trend in the Large Vision and Language model has brought a new\nchange in how information extraction systems are built. VLMs have set a new\nbenchmark with their State-of-the-art techniques in understanding documents and\nbuilding question-answering systems across various industries. They are\nsignificantly better at generating text from document images and providing\naccurate answers to questions. However, there are still some challenges in\neffectively utilizing these models to build a precise conversational system.\nGeneral prompting techniques used with large language models are often not\nsuitable for these specially designed vision language models. The output\ngenerated by such generic input prompts is ordinary and may contain information\ngaps when compared with the actual content of the document. To obtain more\naccurate and specific answers, a well-targeted prompt is required by the vision\nlanguage model, along with the document image. In this paper, a technique is\ndiscussed called Target prompting, which focuses on explicitly targeting parts\nof document images and generating related answers from those specific regions\nonly. The paper also covers the evaluation of response for each prompting\ntechnique using different user queries and input prompts.\n","authors":["Dipankar Medhi"],"pdf_url":"https://arxiv.org/pdf/2408.03834v1.pdf","comment":"7 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.10853v2","updated":"2024-08-07T15:12:39Z","published":"2024-07-15T16:04:44Z","title":"An Actionable Framework for Assessing Bias and Fairness in Large\n  Language Model Use Cases","summary":"  Large language models (LLMs) can exhibit bias in a variety of ways. Such\nbiases can create or exacerbate unfair outcomes for certain groups within a\nprotected attribute, including, but not limited to sex, race, sexual\norientation, or age. This paper aims to provide a technical guide for\npractitioners to assess bias and fairness risks in LLM use cases. The main\ncontribution of this work is a decision framework that allows practitioners to\ndetermine which metrics to use for a specific LLM use case. To achieve this,\nthis study categorizes LLM bias and fairness risks, maps those risks to a\ntaxonomy of LLM use cases, and then formally defines various metrics to assess\neach type of risk. As part of this work, several new bias and fairness metrics\nare introduced, including innovative counterfactual metrics as well as metrics\nbased on stereotype classifiers. Instead of focusing solely on the model\nitself, the sensitivity of both prompt-risk and model-risk are taken into\naccount by defining evaluations at the level of an LLM use case, characterized\nby a model and a population of prompts. Furthermore, because all of the\nevaluation metrics are calculated solely using the LLM output, the proposed\nframework is highly practical and easily actionable for practitioners.\n","authors":["Dylan Bouchard"],"pdf_url":"https://arxiv.org/pdf/2407.10853v2.pdf","comment":"Comments: 21 pages, LaTeX; typos corrected, references added"},{"id":"http://arxiv.org/abs/2401.00763v2","updated":"2024-08-07T15:10:15Z","published":"2024-01-01T14:06:55Z","title":"New Job, New Gender? Measuring the Social Bias in Image Generation\n  Models","summary":"  Image generation models can generate or edit images from a given text. Recent\nadvancements in image generation technology, exemplified by DALL-E and\nMidjourney, have been groundbreaking. These advanced models, despite their\nimpressive capabilities, are often trained on massive Internet datasets, making\nthem susceptible to generating content that perpetuates social stereotypes and\nbiases, which can lead to severe consequences. Prior research on assessing bias\nwithin image generation models suffers from several shortcomings, including\nlimited accuracy, reliance on extensive human labor, and lack of comprehensive\nanalysis. In this paper, we propose BiasPainter, a novel evaluation framework\nthat can accurately, automatically and comprehensively trigger social bias in\nimage generation models. BiasPainter uses a diverse range of seed images of\nindividuals and prompts the image generation models to edit these images using\ngender, race, and age-neutral queries. These queries span 62 professions, 39\nactivities, 57 types of objects, and 70 personality traits. The framework then\ncompares the edited images to the original seed images, focusing on the\nsignificant changes related to gender, race, and age. BiasPainter adopts a key\ninsight that these characteristics should not be modified when subjected to\nneutral prompts. Built upon this design, BiasPainter can trigger the social\nbias and evaluate the fairness of image generation models. We use BiasPainter\nto evaluate six widely-used image generation models, such as stable diffusion\nand Midjourney. Experimental results show that BiasPainter can successfully\ntrigger social bias in image generation models. According to our human\nevaluation, BiasPainter can achieve 90.8% accuracy on automatic bias detection,\nwhich is significantly higher than the results reported in previous work.\n","authors":["Wenxuan Wang","Haonan Bai","Jen-tse Huang","Yuxuan Wan","Youliang Yuan","Haoyi Qiu","Nanyun Peng","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2401.00763v2.pdf","comment":"ACM MM 2024 Oral"},{"id":"http://arxiv.org/abs/2408.03827v1","updated":"2024-08-07T15:06:07Z","published":"2024-08-07T15:06:07Z","title":"Automated Code Fix Suggestions for Accessibility Issues in Mobile Apps","summary":"  Accessibility is crucial for inclusive app usability, yet developers often\nstruggle to identify and fix app accessibility issues due to a lack of\nawareness, expertise, and inadequate tools. Current accessibility testing tools\ncan identify accessibility issues but may not always provide guidance on how to\naddress them. We introduce FixAlly, an automated tool designed to suggest\nsource code fixes for accessibility issues detected by automated accessibility\nscanners. FixAlly employs a multi-agent LLM architecture to generate fix\nstrategies, localize issues within the source code, and propose code\nmodification suggestions to fix the accessibility issue. Our empirical study\ndemonstrates FixAlly's capability in suggesting fixes that resolve issues found\nby accessibility scanners -- with an effectiveness of 77% in generating\nplausible fix suggestions -- and our survey of 12 iOS developers finds they\nwould be willing to accept 69.4% of evaluated fix suggestions.\n","authors":["Forough Mehralian","Titus Barik","Jeff Nichols","Amanda Swearngin"],"pdf_url":"https://arxiv.org/pdf/2408.03827v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.02232v2","updated":"2024-08-07T14:52:17Z","published":"2024-08-05T04:53:01Z","title":"SpecRover: Code Intent Extraction via LLMs","summary":"  Autonomous program improvement typically involves automatically producing bug\nfixes and feature additions. Such program improvement can be accomplished by a\ncombination of large language model (LLM) and program analysis capabilities, in\nthe form of an LLM agent. Since program repair or program improvement typically\nrequires a specification of intended behavior - specification inference can be\nuseful for producing high quality program patches. In this work, we examine\nefficient and low-cost workflows for iterative specification inference within\nan LLM agent. Given a GitHub issue to be resolved in a software project, our\ngoal is to conduct iterative code search accompanied by specification inference\n- thereby inferring intent from both the project structure and behavior. The\nintent thus captured is examined by a reviewer agent with the goal of vetting\nthe patches as well as providing a measure of confidence in the vetted patches.\nOur approach SpecRover (AutoCodeRover-v2) is built on the open-source LLM agent\nAutoCodeRover. In an evaluation on the full SWE-Bench consisting of 2294 GitHub\nissues, it shows more than 50% improvement in efficacy over AutoCodeRover.\nCompared to the open-source agents available, our work shows modest cost ($0.65\nper issue) in resolving an average GitHub issue in SWE-Bench lite. The\nproduction of explanation by SpecRover allows for a better \"signal\" to be given\nto the developer, on when the suggested patches can be accepted with\nconfidence. SpecRover also seeks to demonstrate the continued importance of\nspecification inference in automated program repair, even as program repair\ntechnologies enter the LLM era.\n","authors":["Haifeng Ruan","Yuntong Zhang","Abhik Roychoudhury"],"pdf_url":"https://arxiv.org/pdf/2408.02232v2.pdf","comment":"Haifeng Ruan and Yuntong Zhang contributed equally to this work"},{"id":"http://arxiv.org/abs/2408.03811v1","updated":"2024-08-07T14:42:13Z","published":"2024-08-07T14:42:13Z","title":"Generative Language Models with Retrieval Augmented Generation for\n  Automated Short Answer Scoring","summary":"  Automated Short Answer Scoring (ASAS) is a critical component in educational\nassessment. While traditional ASAS systems relied on rule-based algorithms or\ncomplex deep learning methods, recent advancements in Generative Language\nModels (GLMs) offer new opportunities for improvement. This study explores the\napplication of GLMs to ASAS, leveraging their off-the-shelf capabilities and\nperformance in various domains. We propose a novel pipeline that combines\nvector databases, transformer-based encoders, and GLMs to enhance short answer\nscoring accuracy. Our approach stores training responses in a vector database,\nretrieves semantically similar responses during inference, and employs a GLM to\nanalyze these responses and determine appropriate scores. We further optimize\nthe system through fine-tuned retrieval processes and prompt engineering.\nEvaluation on the SemEval 2013 dataset demonstrates a significant improvement\non the SCIENTSBANK 3-way and 2-way tasks compared to existing methods,\nhighlighting the potential of GLMs in advancing ASAS technology.\n","authors":["Zifan Wang","Christopher Ormerod"],"pdf_url":"https://arxiv.org/pdf/2408.03811v1.pdf","comment":"20 pages, 2 figures"},{"id":"http://arxiv.org/abs/2408.03807v1","updated":"2024-08-07T14:32:41Z","published":"2024-08-07T14:32:41Z","title":"Navigating the Human Maze: Real-Time Robot Pathfinding with Generative\n  Imitation Learning","summary":"  This paper addresses navigation in crowded environments by integrating\ngoal-conditioned generative models with Sampling-based Model Predictive Control\n(SMPC). We introduce goal-conditioned autoregressive models to generate crowd\nbehaviors, capturing intricate interactions among individuals. The model\nprocesses potential robot trajectory samples and predicts the reactions of\nsurrounding individuals, enabling proactive robotic navigation in complex\nscenarios. Extensive experiments show that this algorithm enables real-time\nnavigation, significantly reducing collision rates and path lengths, and\noutperforming selected baseline methods. The practical effectiveness of this\nalgorithm is validated on an actual robotic platform, demonstrating its\ncapability in dynamic settings.\n","authors":["Martin Moder","Stephen Adhisaputra","Josef Pauli"],"pdf_url":"https://arxiv.org/pdf/2408.03807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03795v1","updated":"2024-08-07T14:20:09Z","published":"2024-08-07T14:20:09Z","title":"Frank's triangular norms in Piaget's logical proportions","summary":"  Starting from the Boolean notion of logical proportion in Piaget's sense,\nwhich turns out to be equivalent to analogical proportion, this note proposes a\ndefinition of analogical proportion between numerical values based on\ntriangular norms (and dual co-norms). Frank's family of triangular norms is\nparticularly interesting from this perspective. The article concludes with a\ncomparative discussion with another very recent proposal for defining\nanalogical proportions between numerical values based on the family of\ngeneralized means.\n","authors":["Henri Prade","Gilles Richard"],"pdf_url":"https://arxiv.org/pdf/2408.03795v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2407.18581v2","updated":"2024-08-07T14:19:00Z","published":"2024-07-26T08:03:07Z","title":"Dynamic Language Group-Based MoE: Enhancing Code-Switching Speech\n  Recognition with Hierarchical Routing","summary":"  The Mixture of Experts (MoE) approach is well-suited for multilingual and\ncode-switching (CS) tasks due to its multi-expert architecture. This work\nintroduces the DLG-MoE, a Dynamic Language Group-based MoE optimized for\nbilingual and CS scenarios. DLG-MoE operates based on a hierarchical routing\nmechanism. First, the language router explicitly models the language and\ndispatches the representations to the corresponding language expert groups.\nSubsequently, the unsupervised router within each language group implicitly\nmodels attributes beyond language, and coordinates expert routing and\ncollaboration. The model achieves state-of-the-art (SOTA) performance while\nalso having unparalleled flexibility. It supports different top-k inference and\nstreaming capabilities, and can also prune the model parameters to obtain a\nmonolingual sub-model. The Code will be released.\n","authors":["Hukai Huang","Shenghui Lu","Yahui Shan","He Qu","Wenhao Guan","Qingyang Hong","Lin Li"],"pdf_url":"https://arxiv.org/pdf/2407.18581v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11075v2","updated":"2024-08-07T14:05:28Z","published":"2024-07-13T04:29:36Z","title":"A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)","summary":"  Through this comprehensive survey of Kolmogorov-Arnold Networks(KAN), we have\ngained a thorough understanding of its theoretical foundation, architectural\ndesign, application scenarios, and current research progress. KAN, with its\nunique architecture and flexible activation functions, excels in handling\ncomplex data patterns and nonlinear relationships, demonstrating wide-ranging\napplication potential. While challenges remain, KAN is poised to pave the way\nfor innovative solutions in various fields, potentially revolutionizing how we\napproach complex computational problems.\n","authors":["Yuntian Hou","Di Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.11075v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15793v2","updated":"2024-08-07T13:59:46Z","published":"2024-07-22T16:51:28Z","title":"CLIP with Generative Latent Replay: a Strong Baseline for Incremental\n  Learning","summary":"  With the emergence of Transformers and Vision-Language Models (VLMs) such as\nCLIP, large pre-trained models have become a common strategy to enhance\nperformance in Continual Learning scenarios. This led to the development of\nnumerous prompting strategies to effectively fine-tune transformer-based models\nwithout succumbing to catastrophic forgetting. However, these methods struggle\nto specialize the model on domains significantly deviating from the\npre-training and preserving its zero-shot capabilities. In this work, we\npropose Continual Generative training for Incremental prompt-Learning, a novel\napproach to mitigate forgetting while adapting a VLM, which exploits generative\nreplay to align prompts to tasks. We also introduce a new metric to evaluate\nzero-shot capabilities within CL benchmarks. Through extensive experiments on\ndifferent domains, we demonstrate the effectiveness of our framework in\nadapting to new tasks while improving zero-shot capabilities. Further analysis\nreveals that our approach can bridge the gap with joint prompt tuning. The\ncodebase is available at https://github.com/aimagelab/mammoth.\n","authors":["Emanuele Frascaroli","Aniello Panariello","Pietro Buzzega","Lorenzo Bonicelli","Angelo Porrello","Simone Calderara"],"pdf_url":"https://arxiv.org/pdf/2407.15793v2.pdf","comment":"15 pages, 1 figure. Accepted at the The 35th British Machine Vision\n  Conference 2024 (BMVC 2024), Glasgow, UK"},{"id":"http://arxiv.org/abs/2408.03772v1","updated":"2024-08-07T13:48:24Z","published":"2024-08-07T13:48:24Z","title":"Relevance meets Diversity: A User-Centric Framework for Knowledge\n  Exploration through Recommendations","summary":"  Providing recommendations that are both relevant and diverse is a key\nconsideration of modern recommender systems. Optimizing both of these measures\npresents a fundamental trade-off, as higher diversity typically comes at the\ncost of relevance, resulting in lower user engagement. Existing recommendation\nalgorithms try to resolve this trade-off by combining the two measures,\nrelevance and diversity, into one aim and then seeking recommendations that\noptimize the combined objective, for a given number of items to recommend.\nTraditional approaches, however, do not consider the user interaction with the\nrecommended items.\n  In this paper, we put the user at the central stage, and build on the\ninterplay between relevance, diversity, and user behavior. In contrast to\napplications where the goal is solely to maximize engagement, we focus on\nscenarios aiming at maximizing the total amount of knowledge encountered by the\nuser. We use diversity as a surrogate of the amount of knowledge obtained by\nthe user while interacting with the system, and we seek to maximize diversity.\nWe propose a probabilistic user-behavior model in which users keep interacting\nwith the recommender system as long as they receive relevant recommendations,\nbut they may stop if the relevance of the recommended items drops. Thus, for a\nrecommender system to achieve a high-diversity measure, it will need to produce\nrecommendations that are both relevant and diverse.\n  Finally, we propose a novel recommendation strategy that combines relevance\nand diversity by a copula function. We conduct an extensive evaluation of the\nproposed methodology over multiple datasets, and we show that our strategy\noutperforms several state-of-the-art competitors. Our implementation is\npublicly available at https://github.com/EricaCoppolillo/EXPLORE.\n","authors":["Erica Coppolillo","Giuseppe Manco","Aristides Gionis"],"pdf_url":"https://arxiv.org/pdf/2408.03772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21693v2","updated":"2024-08-07T13:42:20Z","published":"2024-07-31T15:38:15Z","title":"TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue\n  System with Transfer Capabilities","summary":"  Task-oriented dialogue (TOD) systems aim to efficiently handle task-oriented\nconversations, including information collection. How to utilize TOD accurately,\nefficiently and effectively for information collection has always been a\ncritical and challenging task. Recent studies have demonstrated that Large\nLanguage Models (LLMs) excel in dialogue, instruction generation, and\nreasoning, and can significantly enhance the performance of TOD through\nfine-tuning. However, current datasets primarily cater to user-led systems and\nare limited to predefined specific scenarios and slots, thereby necessitating\nimprovements in the proactiveness, diversity, and capabilities of TOD. In this\nstudy, we present a detailed multi-domain task-oriented data construction\nprocess for conversations, and a Chinese dialogue dataset generated based on\nthis process, TransferTOD, which authentically simulates human-computer\ndialogues in 30 popular life service scenarios. Leveraging this dataset, we\ntrained a model called TransferTOD-7B using full-parameter fine-tuning,\nshowcasing notable abilities in slot filling and questioning. Our work has\ndemonstrated its strong generalization capabilities in various downstream\nscenarios, significantly enhancing both data utilization efficiency and system\nperformance. The data is released in\nhttps://github.com/KongLongGeFDU/TransferTOD.\n","authors":["Ming Zhang","Caishuang Huang","Yilong Wu","Shichun Liu","Huiyuan Zheng","Yurui Dong","Yujiong Shen","Shihan Dou","Jun Zhao","Junjie Ye","Qi Zhang","Tao Gui","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2407.21693v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18944v3","updated":"2024-08-07T13:37:41Z","published":"2024-06-27T07:14:14Z","title":"Investigating and Defending Shortcut Learning in Personalized Diffusion\n  Models","summary":"  Personalized diffusion models have gained popularity for adapting pre-trained\ntext-to-image models to generate images of specific topics with minimal\ntraining data. However, these models are vulnerable to minor adversarial\nperturbations, leading to degraded performance on corrupted datasets. Such\nvulnerabilities are further exploited to craft protective perturbations on\nsensitive images like portraits that prevent unauthorized generation. In\nresponse, diffusion-based purification methods have been proposed to remove\nthese perturbations and retain generation performance. However, existing works\nturn to over-purifying the images, which causes information loss. In this\npaper, we take a closer look at the fine-tuning process of personalized\ndiffusion models through the lens of shortcut learning. And we propose a\nhypothesis explaining the manipulation mechanisms of existing perturbation\nmethods, demonstrating that perturbed images significantly deviate from their\noriginal prompts in the CLIP-based latent space. This misalignment during\nfine-tuning causes models to associate noisy patterns with identifiers,\nresulting in performance degradation. Based on these insights, we introduce a\nsystematic approach to maintain training performance through purification. Our\nmethod first purifies the images to realign them with their original semantic\nmeanings in latent space. Then, we introduce contrastive learning with negative\ntokens to decouple the learning of clean identities from noisy patterns, which\nshows a strong potential capacity against adaptive perturbation. Our study\nuncovers shortcut learning vulnerabilities in personalized diffusion models and\nprovides a firm evaluation framework for future protective perturbation\nresearch. Code is available at https://github.com/liuyixin-louis/DiffShortcut.\n","authors":["Yixin Liu","Ruoxi Chen","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2406.18944v3.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2309.06045v4","updated":"2024-08-07T13:29:51Z","published":"2023-09-12T08:29:53Z","title":"Improved Monte Carlo tree search formulation with multiple root nodes\n  for discrete sizing optimization of truss structures","summary":"  This paper proposes a novel reinforcement learning (RL) algorithm using\nimproved Monte Carlo tree search (IMCTS) formulation for discrete optimum\ndesign of truss structures. IMCTS with multiple root nodes includes update\nprocess, the best reward, accelerating technique, and terminal condition.\nUpdate process means that once a final solution is found, it is used as the\ninitial solution for next search tree. The best reward is used in the\nbackpropagation step. Accelerating technique is introduced by decreasing the\nwidth of search tree and reducing maximum number of iterations. The agent is\ntrained to minimize the total structural weight under various constraints until\nthe terminal condition is satisfied. Then, optimal solution is the minimum\nvalue of all solutions found by search trees. These numerical examples show\nthat the agent can find optimal solution with low computational cost, stably\nproduces an optimal design, and is suitable for multi-objective structural\noptimization and large-scale structures.\n","authors":["Fu-Yao Ko","Katsuyuki Suzuki","Kazuo Yonekura"],"pdf_url":"https://arxiv.org/pdf/2309.06045v4.pdf","comment":"34 pages, 24 figures, 16 tables"},{"id":"http://arxiv.org/abs/2404.11317v2","updated":"2024-08-07T13:20:30Z","published":"2024-04-17T12:30:54Z","title":"Improving Composed Image Retrieval via Contrastive Learning with Scaling\n  Positives and Negatives","summary":"  The Composed Image Retrieval (CIR) task aims to retrieve target images using\na composed query consisting of a reference image and a modified text. Advanced\nmethods often utilize contrastive learning as the optimization objective, which\nbenefits from adequate positive and negative examples. However, the triplet for\nCIR incurs high manual annotation costs, resulting in limited positive\nexamples. Furthermore, existing methods commonly use in-batch negative\nsampling, which reduces the negative number available for the model. To address\nthe problem of lack of positives, we propose a data generation method by\nleveraging a multi-modal large language model to construct triplets for CIR. To\nintroduce more negatives during fine-tuning, we design a two-stage fine-tuning\nframework for CIR, whose second stage introduces plenty of static\nrepresentations of negatives to optimize the representation space rapidly. The\nabove two improvements can be effectively stacked and designed to be\nplug-and-play, easily applied to existing CIR models without changing their\noriginal architectures. Extensive experiments and ablation analysis demonstrate\nthat our method effectively scales positives and negatives and achieves\nstate-of-the-art results on both FashionIQ and CIRR datasets. In addition, our\nmethod also performs well in zero-shot composed image retrieval, providing a\nnew CIR solution for the low-resources scenario. Our code and data are released\nat https://github.com/BUAADreamer/SPN4CIR.\n","authors":["Zhangchi Feng","Richong Zhang","Zhijie Nie"],"pdf_url":"https://arxiv.org/pdf/2404.11317v2.pdf","comment":"Accepted to ACM MM 2024 Regular Papers"},{"id":"http://arxiv.org/abs/2402.07118v2","updated":"2024-08-07T13:14:00Z","published":"2024-02-11T07:27:01Z","title":"Next-Generation Teleophthalmology: AI-enabled Quality Assessment Aiding\n  Remote Smartphone-based Consultation","summary":"  Blindness and other eye diseases are a global health concern, particularly in\nlow- and middle-income countries like India. In this regard, during the\nCOVID-19 pandemic, teleophthalmology became a lifeline, and the Grabi\nattachment for smartphone-based eye imaging gained in use. However, quality of\nuser-captured image often remained inadequate, requiring clinician vetting and\ndelays. In this backdrop, we propose an AI-based quality assessment system with\ninstant feedback mimicking clinicians' judgments and tested on patient-captured\nimages. Dividing the complex problem hierarchically, here we tackle a\nnontrivial part, and demonstrate a proof of the concept.\n","authors":["Dhruv Srikanth","Jayang Gurung","N Satya Deepika","Vineet Joshi","Lopamudra Giri","Pravin Vaddavalli","Soumya Jana"],"pdf_url":"https://arxiv.org/pdf/2402.07118v2.pdf","comment":"4 pages, Presented at IEEE EMBC 2024"},{"id":"http://arxiv.org/abs/2408.03747v1","updated":"2024-08-07T13:01:10Z","published":"2024-08-07T13:01:10Z","title":"Online Model-based Anomaly Detection in Multivariate Time Series:\n  Taxonomy, Survey, Research Challenges and Future Directions","summary":"  Time-series anomaly detection plays an important role in engineering\nprocesses, like development, manufacturing and other operations involving\ndynamic systems. These processes can greatly benefit from advances in the\nfield, as state-of-the-art approaches may aid in cases involving, for example,\nhighly dimensional data. To provide the reader with understanding of the\nterminology, this survey introduces a novel taxonomy where a distinction\nbetween online and offline, and training and inference is made. Additionally,\nit presents the most popular data sets and evaluation metrics used in the\nliterature, as well as a detailed analysis. Furthermore, this survey provides\nan extensive overview of the state-of-the-art model-based online semi- and\nunsupervised anomaly detection approaches for multivariate time-series data,\ncategorising them into different model families and other properties. The\nbiggest research challenge revolves around benchmarking, as currently there is\nno reliable way to compare different approaches against one another. This\nproblem is two-fold: on the one hand, public data sets suffers from at least\none fundamental flaw, while on the other hand, there is a lack of intuitive and\nrepresentative evaluation metrics in the field. Moreover, the way most\npublications choose a detection threshold disregards real-world conditions,\nwhich hinders the application in the real world. To allow for tangible advances\nin the field, these issues must be addressed in future work.\n","authors":["Lucas Correia","Jan-Christoph Goos","Philipp Klein","Thomas Bäck","Anna V. Kononova"],"pdf_url":"https://arxiv.org/pdf/2408.03747v1.pdf","comment":"Submitted to Engineering Applications of Artificial Intelligence\n  journal"},{"id":"http://arxiv.org/abs/2408.03746v1","updated":"2024-08-07T12:59:58Z","published":"2024-08-07T12:59:58Z","title":"Flexible Bayesian Last Layer Models Using Implicit Priors and Diffusion\n  Posterior Sampling","summary":"  Bayesian Last Layer (BLL) models focus solely on uncertainty in the output\nlayer of neural networks, demonstrating comparable performance to more complex\nBayesian models. However, the use of Gaussian priors for last layer weights in\nBayesian Last Layer (BLL) models limits their expressive capacity when faced\nwith non-Gaussian, outlier-rich, or high-dimensional datasets. To address this\nshortfall, we introduce a novel approach that combines diffusion techniques and\nimplicit priors for variational learning of Bayesian last layer weights. This\nmethod leverages implicit distributions for modeling weight priors in BLL,\ncoupled with diffusion samplers for approximating true posterior predictions,\nthereby establishing a comprehensive Bayesian prior and posterior estimation\nstrategy. By delivering an explicit and computationally efficient variational\nlower bound, our method aims to augment the expressive abilities of BLL models,\nenhancing model accuracy, calibration, and out-of-distribution detection\nproficiency. Through detailed exploration and experimental validation, We\nshowcase the method's potential for improving predictive accuracy and\nuncertainty quantification while ensuring computational efficiency.\n","authors":["Jian Xu","Zhiqi Lin","Shigui Li","Min Chen","Junmei Yang","Delu Zeng","John Paisley"],"pdf_url":"https://arxiv.org/pdf/2408.03746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18381v4","updated":"2024-08-07T12:59:31Z","published":"2023-05-28T06:53:41Z","title":"Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient\n  Dataset Distillation","summary":"  Data-efficient learning has garnered significant attention, especially given\nthe current trend of large multi-modal models. Recently, dataset distillation\nhas become an effective approach by synthesizing data samples that are\nessential for network training. However, it remains to be explored which\nsamples are essential for the dataset distillation process itself. In this\nwork, we study the data efficiency and selection for the dataset distillation\ntask. By re-formulating the dynamics of distillation, we provide insight into\nthe inherent redundancy in the real dataset, both theoretically and\nempirically. We propose to use the empirical loss value as a static data\npruning criterion. To further compensate for the variation of the data value in\ntraining, we find the most contributing samples based on their causal effects\non the distillation. The proposed selection strategy can efficiently exploit\nthe training dataset, outperform the previous SOTA distillation algorithms, and\nconsistently enhance the distillation algorithms, even on much larger-scale and\nmore heterogeneous datasets, e.g., full ImageNet-1K and Kinetics-400. We\nbelieve this paradigm will open up new avenues in the dynamics of distillation\nand pave the way for efficient dataset distillation. Our code is available on\nhttps://github.com/silicx/GoldFromOres-BiLP.\n","authors":["Yue Xu","Yong-Lu Li","Kaitong Cui","Ziyu Wang","Cewu Lu","Yu-Wing Tai","Chi-Keung Tang"],"pdf_url":"https://arxiv.org/pdf/2305.18381v4.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2408.03745v1","updated":"2024-08-07T12:58:39Z","published":"2024-08-07T12:58:39Z","title":"Intuitionistic Fuzzy Cognitive Maps for Interpretable Image\n  Classification","summary":"  The interpretability of machine learning models is critical, as users may be\nreluctant to rely on their inferences. Intuitionistic FCMs (iFCMs) have been\nproposed as an extension of FCMs offering a natural mechanism to assess the\nquality of their output through the estimation of hesitancy, a concept\nresembling to human hesitation in decision making. To address the challenge of\ninterpretable image classification, this paper introduces a novel framework,\nnamed Interpretable Intuitionistic FCM (I2FCM) which is domain-independent,\nsimple to implement, and can be applied on Convolutional Neural Network (CNN)\nmodels, rendering them interpretable. To the best of our knowledge this is the\nfirst time iFCMs are applied for image classification. Further novel\ncontributions include: a feature extraction process focusing on the most\ninformative image regions; a learning algorithm for data-driven determination\nof the intuitionistic fuzzy interconnections of the iFCM; an inherently\ninterpretable classification approach based on image contents. In the context\nof image classification, hesitancy is considered as a degree of inconfidence\nwith which an image is categorized to a class. The constructed iFCM model\ndistinguishes the most representative image semantics and analyses them\nutilizing cause-and-effect relations. The effectiveness of the introduced\nframework is evaluated on publicly available datasets, and the experimental\nresults confirm that it can provide enhanced classification performance, while\nproviding interpretable inferences.\n","authors":["Georgia Sovatzidi","Michael D. Vasilakakis","Dimitris K. Iakovidis"],"pdf_url":"https://arxiv.org/pdf/2408.03745v1.pdf","comment":"This work has been submitted for possible journal publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2307.12754v4","updated":"2024-08-07T12:51:46Z","published":"2023-07-24T12:52:55Z","title":"Nonparametric Linear Feature Learning in Regression Through\n  Regularisation","summary":"  Representation learning plays a crucial role in automated feature selection,\nparticularly in the context of high-dimensional data, where non-parametric\nmethods often struggle. In this study, we focus on supervised learning\nscenarios where the pertinent information resides within a lower-dimensional\nlinear subspace of the data, namely the multi-index model. If this subspace\nwere known, it would greatly enhance prediction, computation, and\ninterpretation. To address this challenge, we propose a novel method for joint\nlinear feature learning and non-parametric function estimation, aimed at more\neffectively leveraging hidden features for learning. Our approach employs\nempirical risk minimisation, augmented with a penalty on function derivatives,\nensuring versatility. Leveraging the orthogonality and rotation invariance\nproperties of Hermite polynomials, we introduce our estimator, named RegFeaL.\nBy using alternative minimisation, we iteratively rotate the data to improve\nalignment with leading directions. We establish that the expected risk of our\nmethod converges in high-probability to the minimal risk under minimal\nassumptions and with explicit rates. Additionally, we provide empirical results\ndemonstrating the performance of RegFeaL in various experiments.\n","authors":["Bertille Follain","Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2307.12754v4.pdf","comment":"45 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.03735v1","updated":"2024-08-07T12:42:09Z","published":"2024-08-07T12:42:09Z","title":"Advancing Multimodal Large Language Models with Quantization-Aware Scale\n  Learning for Efficient Adaptation","summary":"  This paper presents the first study to explore the potential of parameter\nquantization for multimodal large language models to alleviate the significant\nresource constraint encountered during vision-language instruction tuning. We\nintroduce a Quantization-aware Scale LeArning method based on multimodal\nWarmup, termed QSLAW. This method is grounded in two key innovations: (1) The\nlearning of group-wise scale factors for quantized LLM weights to mitigate the\nquantization error arising from activation outliers and achieve more effective\nvision-language instruction tuning; (2) The implementation of a multimodal\nwarmup that progressively integrates linguistic and multimodal training\nsamples, thereby preventing overfitting of the quantized model to multimodal\ndata while ensuring stable adaptation of multimodal large language models to\ndownstream vision-language tasks. Extensive experiments demonstrate that models\nquantized by QSLAW perform on par with, or even surpass, their full-precision\ncounterparts, while facilitating up to 1.4 times reduction in VL tuning time\nand GPU consumption. Our code is released at https://github.com/xjjxmu/QSLAW.\n","authors":["Jingjing Xie","Yuxin Zhang","Mingbao Lin","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2408.03735v1.pdf","comment":"Accepted by ACMMM2024"},{"id":"http://arxiv.org/abs/2408.00655v4","updated":"2024-08-07T12:23:14Z","published":"2024-08-01T15:45:19Z","title":"SentenceVAE: Enable Next-sentence Prediction for Large Language Models\n  with Faster Speed, Higher Accuracy and Longer Context","summary":"  Current large language models (LLMs) primarily utilize next-token prediction\nmethod for inference, which significantly impedes their processing speed. In\nthis paper, we introduce a novel inference methodology termed next-sentence\nprediction, aimed at enhancing the inference efficiency of LLMs. We present\nSentence Variational Autoencoder (SentenceVAE), a tiny model consisting of a\nSentence Encoder and a Sentence Decoder. The Sentence Encoder can effectively\ncondense the information within a sentence into a singular token, while the\nSentence Decoder can reconstruct this compressed token back into sentence. By\nintegrating SentenceVAE into the input and output layers of LLMs, we develop\nSentence-level LLMs (SLLMs) that employ a sentence-by-sentence inference\nmethod. In addition, the SentenceVAE module of SLLMS can maintain the integrity\nof the original semantic content by segmenting the context into sentences,\nthereby improving accuracy while boosting inference speed. Moreover, compared\nto previous LLMs, SLLMs process fewer tokens over equivalent context length,\nsignificantly reducing memory demands for self-attention computation and\nfacilitating the handling of longer context. Extensive experiments on Wanjuan\ndataset have reveal that the proposed method can accelerate inference speed by\n204~365%, reduce perplexity (PPL) to 46~75% of its original metric, and\ndecrease memory overhead by 86~91% for the equivalent context length, compared\nto the token-by-token method.\n","authors":["Hongjun An","Yifan Chen","Zhe Sun","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.00655v4.pdf","comment":"update the article"},{"id":"http://arxiv.org/abs/2408.01334v2","updated":"2024-08-07T12:01:58Z","published":"2024-08-02T15:32:42Z","title":"A Backbone for Long-Horizon Robot Task Understanding","summary":"  End-to-end robot learning, particularly for long-horizon tasks, often results\nin unpredictable outcomes and poor generalization. To address these challenges,\nwe propose a novel Therblig-based Backbone Framework (TBBF) to enhance robot\ntask understanding and transferability. This framework uses therbligs (basic\naction elements) as the backbone to decompose high-level robot tasks into\nelemental robot configurations, which are then integrated with current\nfoundation models to improve task understanding. The approach consists of two\nstages: offline training and online testing. During the offline training stage,\nwe developed the Meta-RGate SynerFusion (MGSF) network for accurate therblig\nsegmentation across various tasks. In the online testing stage, after a\none-shot demonstration of a new task is collected, our MGSF network extracts\nhigh-level knowledge, which is then encoded into the image using Action\nRegistration (ActionREG). Additionally, the Large Language Model\n(LLM)-Alignment Policy for Visual Correction (LAP-VC) is employed to ensure\nprecise action execution, facilitating trajectory transfer in novel robot\nscenarios. Experimental results validate these methods, achieving 94.37% recall\nin therblig segmentation and success rates of 94.4% and 80% in real-world\nonline robot testing for simple and complex scenarios, respectively.\nSupplementary material is available at:\nhttps://sites.google.com/view/therbligsbasedbackbone/home\n","authors":["Xiaoshuai Chen","Wei Chen","Dongmyoung Lee","Yukun Ge","Nicolas Rojas","Petar Kormushev"],"pdf_url":"https://arxiv.org/pdf/2408.01334v2.pdf","comment":"8 pages, 8 figures. This work is intended to be submitted to IEEE\n  Robotics and Automation Letters (RA-L) for possible publication"},{"id":"http://arxiv.org/abs/2408.03706v1","updated":"2024-08-07T11:44:32Z","published":"2024-08-07T11:44:32Z","title":"Local Topology Measures of Contextual Language Model Latent Spaces With\n  Applications to Dialogue Term Extraction","summary":"  A common approach for sequence tagging tasks based on contextual word\nrepresentations is to train a machine learning classifier directly on these\nembedding vectors. This approach has two shortcomings. First, such methods\nconsider single input sequences in isolation and are unable to put an\nindividual embedding vector in relation to vectors outside the current local\ncontext of use. Second, the high performance of these models relies on\nfine-tuning the embedding model in conjunction with the classifier, which may\nnot always be feasible due to the size or inaccessibility of the underlying\nfeature-generation model. It is thus desirable, given a collection of embedding\nvectors of a corpus, i.e., a datastore, to find features of each vector that\ndescribe its relation to other, similar vectors in the datastore. With this in\nmind, we introduce complexity measures of the local topology of the latent\nspace of a contextual language model with respect to a given datastore. The\neffectiveness of our features is demonstrated through their application to\ndialogue term extraction. Our work continues a line of research that explores\nthe manifold hypothesis for word embeddings, demonstrating that local structure\nin the space carved out by word embeddings can be exploited to infer semantic\nproperties.\n","authors":["Benjamin Matthias Ruppik","Michael Heck","Carel van Niekerk","Renato Vukovic","Hsien-chin Lin","Shutong Feng","Marcus Zibrowius","Milica Gašić"],"pdf_url":"https://arxiv.org/pdf/2408.03706v1.pdf","comment":"Accepted as a long paper to SIGDIAL 2024. 9 pages, 2 figures, 3\n  tables"},{"id":"http://arxiv.org/abs/2408.03694v1","updated":"2024-08-07T11:14:18Z","published":"2024-08-07T11:14:18Z","title":"A Blockchain-based Reliable Federated Meta-learning for Metaverse: A\n  Dual Game Framework","summary":"  The metaverse, envisioned as the next digital frontier for avatar-based\nvirtual interaction, involves high-performance models. In this dynamic\nenvironment, users' tasks frequently shift, requiring fast model\npersonalization despite limited data. This evolution consumes extensive\nresources and requires vast data volumes. To address this, meta-learning\nemerges as an invaluable tool for metaverse users, with federated meta-learning\n(FML), offering even more tailored solutions owing to its adaptive\ncapabilities. However, the metaverse is characterized by users heterogeneity\nwith diverse data structures, varied tasks, and uneven sample sizes,\npotentially undermining global training outcomes due to statistical difference.\nGiven this, an urgent need arises for smart coalition formation that accounts\nfor these disparities. This paper introduces a dual game-theoretic framework\nfor metaverse services involving meta-learners as workers to manage FML. A\nblockchain-based cooperative coalition formation game is crafted, grounded on a\nreputation metric, user similarity, and incentives. We also introduce a novel\nreputation system based on users' historical contributions and potential\ncontributions to present tasks, leveraging correlations between past and new\ntasks. Finally, a Stackelberg game-based incentive mechanism is presented to\nattract reliable workers to participate in meta-learning, minimizing users'\nenergy costs, increasing payoffs, boosting FML efficacy, and improving\nmetaverse utility. Results show that our dual game framework outperforms\nbest-effort, random, and non-uniform clustering schemes - improving training\nperformance by up to 10%, cutting completion times by as much as 30%, enhancing\nmetaverse utility by more than 25%, and offering up to 5% boost in training\nefficiency over non-blockchain systems, effectively countering misbehaving\nusers.\n","authors":["Emna Baccour","Aiman Erbad","Amr Mohamed","Mounir Hamdi","Mohsen Guizani"],"pdf_url":"https://arxiv.org/pdf/2408.03694v1.pdf","comment":"Accepted in IEEE Internet of Things Journal"},{"id":"http://arxiv.org/abs/2408.03691v1","updated":"2024-08-07T11:13:19Z","published":"2024-08-07T11:13:19Z","title":"Generative Design of Periodic Orbits in the Restricted Three-Body\n  Problem","summary":"  The Three-Body Problem has fascinated scientists for centuries and it has\nbeen crucial in the design of modern space missions. Recent developments in\nGenerative Artificial Intelligence hold transformative promise for addressing\nthis longstanding problem. This work investigates the use of Variational\nAutoencoder (VAE) and its internal representation to generate periodic orbits.\nWe utilize a comprehensive dataset of periodic orbits in the Circular\nRestricted Three-Body Problem (CR3BP) to train deep-learning architectures that\ncapture key orbital characteristics, and we set up physical evaluation metrics\nfor the generated trajectories. Through this investigation, we seek to enhance\nthe understanding of how Generative AI can improve space mission planning and\nastrodynamics research, leading to novel, data-driven approaches in the field.\n","authors":["Alvaro Francisco Gil","Walther Litteri","Victor Rodriguez-Fernandez","David Camacho","Massimiliano Vasile"],"pdf_url":"https://arxiv.org/pdf/2408.03691v1.pdf","comment":"SPAICE Conference 2024 (7 pages)"},{"id":"http://arxiv.org/abs/2408.01916v2","updated":"2024-08-07T10:37:38Z","published":"2024-08-04T03:32:17Z","title":"MAO: A Framework for Process Model Generation with Multi-Agent\n  Orchestration","summary":"  Process models are frequently used in software engineering to describe\nbusiness requirements, guide software testing and control system improvement.\nHowever, traditional process modeling methods often require the participation\nof numerous experts, which is expensive and time-consuming. Therefore, the\nexploration of a more efficient and cost-effective automated modeling method\nhas emerged as a focal point in current research. This article explores a\nframework for automatically generating process models with multi-agent\norchestration (MAO), aiming to enhance the efficiency of process modeling and\noffer valuable insights for domain experts. Our framework MAO leverages large\nlanguage models as the cornerstone for multi-agent, employing an innovative\nprompt strategy to ensure efficient collaboration among multi-agent.\nSpecifically, 1) generation. The first phase of MAO is to generate a slightly\nrough process model from the text description; 2) refinement. The agents would\ncontinuously refine the initial process model through multiple rounds of\ndialogue; 3) reviewing. Large language models are prone to hallucination\nphenomena among multi-turn dialogues, so the agents need to review and repair\nsemantic hallucinations in process models; 4) testing. The representation of\nprocess models is diverse. Consequently, the agents utilize external tools to\ntest whether the generated process model contains format errors, namely format\nhallucinations, and then adjust the process model to conform to the output\nparadigm. The experiments demonstrate that the process models generated by our\nframework outperform existing methods and surpass manual modeling by 89%, 61%,\n52%, and 75% on four different datasets, respectively.\n","authors":["Leilei Lin","Yumeng Jin","Yingming Zhou","Wenlong Chen","Chen Qian"],"pdf_url":"https://arxiv.org/pdf/2408.01916v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04202v5","updated":"2024-08-07T10:10:34Z","published":"2024-03-07T04:12:24Z","title":"Dynamics of Moral Behavior in Heterogeneous Populations of Learning\n  Agents","summary":"  Growing concerns about safety and alignment of AI systems highlight the\nimportance of embedding moral capabilities in artificial agents: a promising\nsolution is the use of learning from experience, i.e., Reinforcement Learning.\nIn multi-agent (social) environments, complex population-level phenomena may\nemerge from interactions between individual learning agents. Many of the\nexisting studies rely on simulated social dilemma environments to study the\ninteractions of independent learning agents; however, they tend to ignore the\nmoral heterogeneity that is likely to be present in societies of agents in\npractice. For example, at different points in time a single learning agent may\nface opponents who are consequentialist (i.e., focused on maximizing outcomes\nover time), norm-based (i.e., conforming to specific norms), or virtue-based\n(i.e., considering a combination of different virtues). The extent to which\nagents' co-development may be impacted by such moral heterogeneity in\npopulations is not well understood. In this paper, we present a study of the\nlearning dynamics of morally heterogeneous populations interacting in a social\ndilemma setting. Using an Iterated Prisoner's Dilemma environment with a\npartner selection mechanism, we investigate the extent to which the prevalence\nof diverse moral agents in populations affects individual agents' learning\nbehaviors and emergent population-level outcomes. We observe several types of\nnon-trivial interactions between pro-social and anti-social agents, and find\nthat certain types of moral agents are able to steer selfish agents towards\nmore cooperative behavior.\n","authors":["Elizaveta Tennant","Stephen Hailes","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2403.04202v5.pdf","comment":"Accepted at AIES 2024 (7th AAAI/ACM Conference on AI, Ethics, and\n  Society - San Jose, CA, USA)"},{"id":"http://arxiv.org/abs/2408.03648v1","updated":"2024-08-07T09:23:01Z","published":"2024-08-07T09:23:01Z","title":"HiQuE: Hierarchical Question Embedding Network for Multimodal Depression\n  Detection","summary":"  The utilization of automated depression detection significantly enhances\nearly intervention for individuals experiencing depression. Despite numerous\nproposals on automated depression detection using recorded clinical interview\nvideos, limited attention has been paid to considering the hierarchical\nstructure of the interview questions. In clinical interviews for diagnosing\ndepression, clinicians use a structured questionnaire that includes routine\nbaseline questions and follow-up questions to assess the interviewee's\ncondition. This paper introduces HiQuE (Hierarchical Question Embedding\nnetwork), a novel depression detection framework that leverages the\nhierarchical relationship between primary and follow-up questions in clinical\ninterviews. HiQuE can effectively capture the importance of each question in\ndiagnosing depression by learning mutual information across multiple\nmodalities. We conduct extensive experiments on the widely-used clinical\ninterview data, DAIC-WOZ, where our model outperforms other state-of-the-art\nmultimodal depression detection models and emotion recognition models,\nshowcasing its clinical utility in depression detection.\n","authors":["Juho Jung","Chaewon Kang","Jeewoo Yoon","Seungbae Kim","Jinyoung Han"],"pdf_url":"https://arxiv.org/pdf/2408.03648v1.pdf","comment":"11 pages, 6 figures, Proceedings of the 33rd ACM International\n  Conference on Information and Knowledge Management (CIKM '24)"},{"id":"http://arxiv.org/abs/2408.03632v1","updated":"2024-08-07T08:43:58Z","published":"2024-08-07T08:43:58Z","title":"Concept Conductor: Orchestrating Multiple Personalized Concepts in\n  Text-to-Image Synthesis","summary":"  The customization of text-to-image models has seen significant advancements,\nyet generating multiple personalized concepts remains a challenging task.\nCurrent methods struggle with attribute leakage and layout confusion when\nhandling multiple concepts, leading to reduced concept fidelity and semantic\nconsistency. In this work, we introduce a novel training-free framework,\nConcept Conductor, designed to ensure visual fidelity and correct layout in\nmulti-concept customization. Concept Conductor isolates the sampling processes\nof multiple custom models to prevent attribute leakage between different\nconcepts and corrects erroneous layouts through self-attention-based spatial\nguidance. Additionally, we present a concept injection technique that employs\nshape-aware masks to specify the generation area for each concept. This\ntechnique injects the structure and appearance of personalized concepts through\nfeature fusion in the attention layers, ensuring harmony in the final image.\nExtensive qualitative and quantitative experiments demonstrate that Concept\nConductor can consistently generate composite images with accurate layouts\nwhile preserving the visual details of each concept. Compared to existing\nbaselines, Concept Conductor shows significant performance improvements. Our\nmethod supports the combination of any number of concepts and maintains high\nfidelity even when dealing with visually similar concepts. The code and models\nare available at https://github.com/Nihukat/Concept-Conductor.\n","authors":["Zebin Yao","Fangxiang Feng","Ruifan Li","Xiaojie Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03632v1.pdf","comment":"Github Page: https://github.com/Nihukat/Concept-Conductor"},{"id":"http://arxiv.org/abs/2408.03631v1","updated":"2024-08-07T08:43:32Z","published":"2024-08-07T08:43:32Z","title":"Large Language Models for Base Station Siting: Intelligent Deployment\n  based on Prompt or Agent","summary":"  Traditional base station siting (BSS) methods rely heavily on drive testing\nand user feedback, which are laborious and require extensive expertise in\ncommunication, networking, and optimization. As large language models (LLMs)\nand their associated technologies advance, particularly in the realms of prompt\nengineering and agent engineering, network optimization will witness a\nrevolutionary approach. This approach entails the strategic use of well-crafted\nprompts to infuse human experience and knowledge into these sophisticated LLMs,\nand the deployment of autonomous agents as a communication bridge to seamlessly\nconnect the machine language based LLMs with human users using natural\nlanguage. This integration represents the future paradigm of artificial\nintelligence (AI) as a service and AI for more ease. As a preliminary\nexploration, this research first develops a novel LLM-empowered BSS\noptimization framework, and heuristically proposes four different potential\nimplementations: the strategies based on Prompt-optimized LLM (PoL),\nhuman-in-the-Loop LLM (HiLL), LLM-empowered autonomous BSS agent (LaBa), and\nCooperative multiple LLM-based autonomous BSS agents (CLaBa). Through\nevaluation on real-world data, the experiments demonstrate that prompt-assisted\nLLMs and LLM-based agents can generate more efficient, cost-effective, and\nreliable network deployments, noticeably enhancing the efficiency of BSS\noptimization and reducing trivial manual participation.\n","authors":["Yanhu Wang","Muhammad Muzammil Afzal","Zhengyang Li","Jie Zhou","Chenyuan Feng","Shuaishuai Guo","Tony Q. S. Quek"],"pdf_url":"https://arxiv.org/pdf/2408.03631v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03622v1","updated":"2024-08-07T08:31:42Z","published":"2024-08-07T08:31:42Z","title":"Improving the quality of Persian clinical text with a novel spelling\n  correction system","summary":"  Background: The accuracy of spelling in Electronic Health Records (EHRs) is a\ncritical factor for efficient clinical care, research, and ensuring patient\nsafety. The Persian language, with its abundant vocabulary and complex\ncharacteristics, poses unique challenges for real-word error correction. This\nresearch aimed to develop an innovative approach for detecting and correcting\nspelling errors in Persian clinical text.\n  Methods: Our strategy employs a state-of-the-art pre-trained model that has\nbeen meticulously fine-tuned specifically for the task of spelling correction\nin the Persian clinical domain. This model is complemented by an innovative\northographic similarity matching algorithm, PERTO, which uses visual similarity\nof characters for ranking correction candidates.\n  Results: The evaluation of our approach demonstrated its robustness and\nprecision in detecting and rectifying word errors in Persian clinical text. In\nterms of non-word error correction, our model achieved an F1-Score of 90.0%\nwhen the PERTO algorithm was employed. For real-word error detection, our model\ndemonstrated its highest performance, achieving an F1-Score of 90.6%.\nFurthermore, the model reached its highest F1-Score of 91.5% for real-word\nerror correction when the PERTO algorithm was employed.\n  Conclusions: Despite certain limitations, our method represents a substantial\nadvancement in the field of spelling error detection and correction for Persian\nclinical text. By effectively addressing the unique challenges posed by the\nPersian language, our approach paves the way for more accurate and efficient\nclinical documentation, contributing to improved patient care and safety.\nFuture research could explore its use in other areas of the Persian medical\ndomain, enhancing its impact and utility.\n","authors":["Seyed Mohammad Sadegh Dashti","Seyedeh Fatemeh Dashti"],"pdf_url":"https://arxiv.org/pdf/2408.03622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01185v5","updated":"2024-08-07T08:31:05Z","published":"2023-12-02T17:24:17Z","title":"A ripple in time: a discontinuity in American history","summary":"  In this technical note we suggest a novel approach to discover temporal\n(related and unrelated to language dilation) and personality (authorship\nattribution) in historical datasets. We exemplify our approach on the State of\nthe Union speeches given by the past 42 US presidents: this dataset is known\nfor its relatively small amount of data, and high variability of the amount and\nstyle of texts. Nevertheless we manage to achieve about 95\\% accuracy on the\nauthorship attribution task, and pin down the date of writing to a single\npresidential term.\n","authors":["Alexander Kolpakov","Igor Rivin"],"pdf_url":"https://arxiv.org/pdf/2312.01185v5.pdf","comment":"6 pages, 8 figures; GitHub repository\n  (https://github.com/sashakolpakov/ripple_in_time); restructured manuscript"},{"id":"http://arxiv.org/abs/2408.03618v1","updated":"2024-08-07T08:19:44Z","published":"2024-08-07T08:19:44Z","title":"A Logical Fallacy-Informed Framework for Argument Generation","summary":"  Despite the remarkable performance of Large Language Models (LLMs), they\nstill struggle with generating logically sound arguments, resulting in\npotential risks such as spreading misinformation. An important factor\ncontributing to LLMs' suboptimal performance in generating coherent arguments\nis their oversight of logical fallacies. To address this issue, we introduce\nFIPO, a fallacy-informed framework that leverages preference optimization\nmethods to steer LLMs toward logically sound arguments. FIPO includes a\nclassification loss, to capture the fine-grained information on fallacy\ncategories. Our results on argumentation datasets show that our method reduces\nthe fallacy errors by up to 17.5%. Furthermore, our human evaluation results\nindicate that the quality of the generated arguments by our method\nsignificantly outperforms the fine-tuned baselines, as well as prior preference\noptimization methods, such as DPO. These findings highlight the importance of\nensuring models are aware of logical fallacies for effective argument\ngeneration.\n","authors":["Luca Mouchel","Debjit Paul","Shaobo Cui","Robert West","Antoine Bosselut","Boi Faltings"],"pdf_url":"https://arxiv.org/pdf/2408.03618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03617v1","updated":"2024-08-07T08:18:51Z","published":"2024-08-07T08:18:51Z","title":"Is Child-Directed Speech Effective Training Data for Language Models?","summary":"  While high-performing language models are typically trained on hundreds of\nbillions of words, human children become fluent language users with a much\nsmaller amount of data. What are the features of the data they receive, and how\ndo these features support language modeling objectives? To investigate this\nquestion, we train GPT-2 models on 29M words of English-language child-directed\nspeech and a new matched, synthetic dataset (TinyDialogues), comparing to a\nheterogeneous blend of datasets from the BabyLM challenge. We evaluate both the\nsyntactic and semantic knowledge of these models using developmentally-inspired\nevaluations. Through pretraining experiments, we test whether the global\ndevelopmental ordering or the local discourse ordering of children's training\ndata support high performance relative to other datasets. The local properties\nof the data affect model results, but somewhat surprisingly, global properties\ndo not. Further, child language input is not uniquely valuable for training\nlanguage models. These findings support the hypothesis that, rather than\nproceeding from better data, children's learning is instead substantially more\nefficient than current language modeling techniques.\n","authors":["Steven Y. Feng","Noah D. Goodman","Michael C. Frank"],"pdf_url":"https://arxiv.org/pdf/2408.03617v1.pdf","comment":"Preprint. Code and data will be released soon"},{"id":"http://arxiv.org/abs/2408.03615v1","updated":"2024-08-07T08:16:32Z","published":"2024-08-07T08:16:32Z","title":"Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in\n  Long-Horizon Tasks","summary":"  Building a general-purpose agent is a long-standing vision in the field of\nartificial intelligence. Existing agents have made remarkable progress in many\ndomains, yet they still struggle to complete long-horizon tasks in an open\nworld. We attribute this to the lack of necessary world knowledge and\nmultimodal experience that can guide agents through a variety of long-horizon\ntasks. In this paper, we propose a Hybrid Multimodal Memory module to address\nthe above challenges. It 1) transforms knowledge into Hierarchical Directed\nKnowledge Graph that allows agents to explicitly represent and learn world\nknowledge, and 2) summarises historical information into Abstracted Multimodal\nExperience Pool that provide agents with rich references for in-context\nlearning. On top of the Hybrid Multimodal Memory module, a multimodal agent,\nOptimus-1, is constructed with dedicated Knowledge-guided Planner and\nExperience-Driven Reflector, contributing to a better planning and reflection\nin the face of long-horizon tasks in Minecraft. Extensive experimental results\nshow that Optimus-1 significantly outperforms all existing agents on\nchallenging long-horizon task benchmarks, and exhibits near human-level\nperformance on many tasks. In addition, we introduce various Multimodal Large\nLanguage Models (MLLMs) as the backbone of Optimus-1. Experimental results show\nthat Optimus-1 exhibits strong generalization with the help of the Hybrid\nMultimodal Memory module, outperforming the GPT-4V baseline on many tasks.\n","authors":["Zaijing Li","Yuquan Xie","Rui Shao","Gongwei Chen","Dongmei Jiang","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2408.03615v1.pdf","comment":"30 pages, 13 figures"},{"id":"http://arxiv.org/abs/2404.15311v2","updated":"2024-08-07T08:14:56Z","published":"2024-04-02T17:01:51Z","title":"Fusing Pretrained ViTs with TCNet for Enhanced EEG Regression","summary":"  The task of Electroencephalogram (EEG) analysis is paramount to the\ndevelopment of Brain-Computer Interfaces (BCIs). However, to reach the goal of\ndeveloping robust, useful BCIs depends heavily on the speed and the accuracy at\nwhich BCIs can understand neural dynamics. In response to that goal, this paper\ndetails the integration of pre-trained Vision Transformers (ViTs) with Temporal\nConvolutional Networks (TCNet) to enhance the precision of EEG regression. The\ncore of this approach lies in harnessing the sequential data processing\nstrengths of ViTs along with the superior feature extraction capabilities of\nTCNet, to significantly improve EEG analysis accuracy. In addition, we analyze\nthe importance of how to construct optimal patches for the attention mechanism\nto analyze, balancing both speed and accuracy tradeoffs. Our results showcase a\nsubstantial improvement in regression accuracy, as evidenced by the reduction\nof Root Mean Square Error (RMSE) from 55.4 to 51.8 on EEGEyeNet's Absolute\nPosition Task, outperforming existing state-of-the-art models. Without\nsacrificing performance, we increase the speed of this model by an order of\nmagnitude (up to 4.32x faster). This breakthrough not only sets a new benchmark\nin EEG regression analysis but also opens new avenues for future research in\nthe integration of transformer architectures with specialized feature\nextraction methods for diverse EEG datasets.\n","authors":["Eric Modesitt","Haicheng Yin","Williams Huang Wang","Brian Lu"],"pdf_url":"https://arxiv.org/pdf/2404.15311v2.pdf","comment":"Accepted HCI International 2024"},{"id":"http://arxiv.org/abs/2306.17639v2","updated":"2024-08-07T08:10:23Z","published":"2023-06-30T13:26:08Z","title":"Point-Based Value Iteration for POMDPs with Neural Perception Mechanisms","summary":"  The increasing trend to integrate neural networks and conventional software\ncomponents in safety-critical settings calls for methodologies for their formal\nmodelling, verification and correct-by-construction policy synthesis. We\nintroduce neuro-symbolic partially observable Markov decision processes\n(NS-POMDPs), a variant of continuous-state POMDPs with discrete observations\nand actions, in which the agent perceives a continuous-state environment using\na neural {\\revise perception mechanism} and makes decisions symbolically. The\nperception mechanism classifies inputs such as images and sensor values into\nsymbolic percepts, which are used in decision making.\n  We study the problem of optimising discounted cumulative rewards for\nNS-POMDPs. Working directly with the continuous state space, we exploit the\nunderlying structure of the model and the neural perception mechanism to\npropose a novel piecewise linear and convex representation (P-PWLC) in terms of\npolyhedra covering the state space and value vectors, and extend Bellman\nbackups to this representation. We prove the convexity and continuity of value\nfunctions and present two value iteration algorithms that ensure finite\nrepresentability. The first is a classical (exact) value iteration algorithm\nextending the $\\alpha$-functions of Porta {\\em et al} (2006) to the P-PWLC\nrepresentation for continuous-state spaces. The second is a point-based\n(approximate) method called NS-HSVI, which uses the P-PWLC representation and\nbelief-value induced functions to approximate value functions from below and\nabove for two types of beliefs, particle-based and region-based. Using a\nprototype implementation, we show the practical applicability of our approach\non two case studies that employ (trained) ReLU neural networks as perception\nfunctions, by synthesising (approximately) optimal strategies.\n","authors":["Rui Yan","Gabriel Santos","Gethin Norman","David Parker","Marta Kwiatkowska"],"pdf_url":"https://arxiv.org/pdf/2306.17639v2.pdf","comment":"65 pages, 14 figures"},{"id":"http://arxiv.org/abs/2401.06836v3","updated":"2024-08-07T08:09:53Z","published":"2024-01-12T16:42:10Z","title":"Enhancing Emotional Generation Capability of Large Language Models via\n  Emotional Chain-of-Thought","summary":"  Large Language Models (LLMs) have shown remarkable performance in various\nemotion recognition tasks, thereby piquing the research community's curiosity\nfor exploring their potential in emotional intelligence. However, several\nissues in the field of emotional generation tasks remain unresolved, including\nhuman preference alignment and emotional generation assessment. In this paper,\nwe propose the Emotional Chain-of-Thought (ECoT), a plug-and-play prompting\nmethod that enhances the performance of LLMs on various emotional generation\ntasks by aligning with human emotional intelligence guidelines. To assess the\nreliability of ECoT, we propose an automated model-based evaluation method\ncalled Emotional Generation Score (EGS). EGS incorporates Goleman's Emotional\nIntelligence Theory as a consensus of human experts, providing a new\nperspective on the evaluation of emotional generation tasks. Extensive\nexperimental results demonstrate the effectiveness of ECoT and EGS. Further, we\ndiscuss the promise of LLMs in the field of emotional intelligence and present\nkey insights into the LLMs with the ECoT in emotional generation tasks.\n","authors":["Zaijing Li","Gongwei Chen","Rui Shao","Yuquan Xie","Dongmei Jiang","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2401.06836v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02835v2","updated":"2024-08-07T08:09:02Z","published":"2024-08-05T21:12:12Z","title":"Training a multilayer dynamical spintronic network with standard machine\n  learning tools to perform time series classification","summary":"  The ability to process time-series at low energy cost is critical for many\napplications. Recurrent neural network, which can perform such tasks, are\ncomputationally expensive when implementing in software on conventional\ncomputers. Here we propose to implement a recurrent neural network in hardware\nusing spintronic oscillators as dynamical neurons. Using numerical simulations,\nwe build a multi-layer network and demonstrate that we can use backpropagation\nthrough time (BPTT) and standard machine learning tools to train this network.\nLeveraging the transient dynamics of the spintronic oscillators, we solve the\nsequential digits classification task with $89.83\\pm2.91~\\%$ accuracy, as good\nas the equivalent software network. We devise guidelines on how to choose the\ntime constant of the oscillators as well as hyper-parameters of the network to\nadapt to different input time scales.\n","authors":["Erwan Plouet","Dédalo Sanz-Hernández","Aymeric Vecchiola","Julie Grollier","Frank Mizrahi"],"pdf_url":"https://arxiv.org/pdf/2408.02835v2.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.18990v2","updated":"2024-08-07T07:46:39Z","published":"2024-07-25T12:07:55Z","title":"Stay Tuned: An Empirical Study of the Impact of Hyperparameters on LLM\n  Tuning in Real-World Applications","summary":"  Fine-tuning Large Language Models (LLMs) is an effective method to enhance\ntheir performance on downstream tasks. However, choosing the appropriate\nsetting of tuning hyperparameters (HPs) is a labor-intensive and\ncomputationally expensive process. Here, we provide recommended HP\nconfigurations for practical use-cases that represent a better starting point\nfor practitioners, when considering two SOTA LLMs and two commonly used tuning\nmethods. We describe Coverage-based Search (CBS), a process for ranking HP\nconfigurations based on an offline extensive grid search, such that the top\nranked configurations collectively provide a practical robust recommendation\nfor a wide range of datasets and domains. We focus our experiments on\nLlama-3-8B and Mistral-7B, as well as full fine-tuning and LoRa, conducting a\ntotal of > 10,000 tuning experiments. Our results suggest that, in general,\nLlama-3-8B and LoRA should be preferred, when possible. Moreover, we show that\nfor both models and tuning methods, exploring only a few HP configurations, as\nrecommended by our analysis, can provide excellent results in practice, making\nthis work a valuable resource for practitioners.\n","authors":["Alon Halfon","Shai Gretz","Ofir Arviv","Artem Spector","Orith Toledo-Ronen","Yoav Katz","Liat Ein-Dor","Michal Shmueli-Scheuer","Noam Slonim"],"pdf_url":"https://arxiv.org/pdf/2407.18990v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03603v1","updated":"2024-08-07T07:46:08Z","published":"2024-08-07T07:46:08Z","title":"EnJa: Ensemble Jailbreak on Large Language Models","summary":"  As Large Language Models (LLMs) are increasingly being deployed in\nsafety-critical applications, their vulnerability to potential jailbreaks --\nmalicious prompts that can disable the safety mechanism of LLMs -- has\nattracted growing research attention. While alignment methods have been\nproposed to protect LLMs from jailbreaks, many have found that aligned LLMs can\nstill be jailbroken by carefully crafted malicious prompts, producing content\nthat violates policy regulations. Existing jailbreak attacks on LLMs can be\ncategorized into prompt-level methods which make up stories/logic to circumvent\nsafety alignment and token-level attack methods which leverage gradient methods\nto find adversarial tokens. In this work, we introduce the concept of Ensemble\nJailbreak and explore methods that can integrate prompt-level and token-level\njailbreak into a more powerful hybrid jailbreak attack. Specifically, we\npropose a novel EnJa attack to hide harmful instructions using prompt-level\njailbreak, boost the attack success rate using a gradient-based attack, and\nconnect the two types of jailbreak attacks via a template-based connector. We\nevaluate the effectiveness of EnJa on several aligned models and show that it\nachieves a state-of-the-art attack success rate with fewer queries and is much\nstronger than any individual jailbreak.\n","authors":["Jiahao Zhang","Zilong Wang","Ruofan Wang","Xingjun Ma","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2408.03603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03599v1","updated":"2024-08-07T07:36:49Z","published":"2024-08-07T07:36:49Z","title":"Activations Through Extensions: A Framework To Boost Performance Of\n  Neural Networks","summary":"  Activation functions are non-linearities in neural networks that allow them\nto learn complex mapping between inputs and outputs. Typical choices for\nactivation functions are ReLU, Tanh, Sigmoid etc., where the choice generally\ndepends on the application domain. In this work, we propose a\nframework/strategy that unifies several works on activation functions and\ntheoretically explains the performance benefits of these works. We also propose\nnovel techniques that originate from the framework and allow us to obtain\n``extensions'' (i.e. special generalizations of a given neural network) of\nneural networks through operations on activation functions. We theoretically\nand empirically show that ``extensions'' of neural networks have performance\nbenefits compared to vanilla neural networks with insignificant space and time\ncomplexity costs on standard test functions. We also show the benefits of\nneural network ``extensions'' in the time-series domain on real-world datasets.\n","authors":["Chandramouli Kamanchi","Sumatra Mukherjee","Kameshwaran Sampath","Pankaj Dayama","Arindam Jati","Vijay Ekambaram","Dzung Phan"],"pdf_url":"https://arxiv.org/pdf/2408.03599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17640v2","updated":"2024-08-07T07:29:39Z","published":"2024-05-27T20:24:03Z","title":"Probabilistically Plausible Counterfactual Explanations with Normalizing\n  Flows","summary":"  We present PPCEF, a novel method for generating probabilistically plausible\ncounterfactual explanations (CFs). PPCEF advances beyond existing methods by\ncombining a probabilistic formulation that leverages the data distribution with\nthe optimization of plausibility within a unified framework. Compared to\nreference approaches, our method enforces plausibility by directly optimizing\nthe explicit density function without assuming a particular family of\nparametrized distributions. This ensures CFs are not only valid (i.e., achieve\nclass change) but also align with the underlying data's probability density.\nFor that purpose, our approach leverages normalizing flows as powerful density\nestimators to capture the complex high-dimensional data distribution.\nFurthermore, we introduce a novel loss that balances the trade-off between\nachieving class change and maintaining closeness to the original instance while\nalso incorporating a probabilistic plausibility term. PPCEF's unconstrained\nformulation allows for efficient gradient-based optimization with batch\nprocessing, leading to orders of magnitude faster computation compared to prior\nmethods. Moreover, the unconstrained formulation of PPCEF allows for the\nseamless integration of future constraints tailored to specific counterfactual\nproperties. Finally, extensive evaluations demonstrate PPCEF's superiority in\ngenerating high-quality, probabilistically plausible counterfactual\nexplanations in high-dimensional tabular settings. This makes PPCEF a powerful\ntool for not only interpreting complex machine learning models but also for\nimproving fairness, accountability, and trust in AI systems.\n","authors":["Patryk Wielopolski","Oleksii Furman","Jerzy Stefanowski","Maciej Zięba"],"pdf_url":"https://arxiv.org/pdf/2405.17640v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03591v1","updated":"2024-08-07T07:09:14Z","published":"2024-08-07T07:09:14Z","title":"Focal Depth Estimation: A Calibration-Free, Subject- and Daytime\n  Invariant Approach","summary":"  In an era where personalized technology is increasingly intertwined with\ndaily life, traditional eye-tracking systems and autofocal glasses face a\nsignificant challenge: the need for frequent, user-specific calibration, which\nimpedes their practicality. This study introduces a groundbreaking\ncalibration-free method for estimating focal depth, leveraging machine learning\ntechniques to analyze eye movement features within short sequences. Our\napproach, distinguished by its innovative use of LSTM networks and\ndomain-specific feature engineering, achieves a mean absolute error (MAE) of\nless than 10 cm, setting a new focal depth estimation accuracy standard. This\nadvancement promises to enhance the usability of autofocal glasses and pave the\nway for their seamless integration into extended reality environments, marking\na significant leap forward in personalized visual technology.\n","authors":["Benedikt W. Hosp","Björn Severitt","Rajat Agarwala","Evgenia Rusak","Yannick Sauer","Siegfried Wahl"],"pdf_url":"https://arxiv.org/pdf/2408.03591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03588v1","updated":"2024-08-07T07:04:29Z","published":"2024-08-07T07:04:29Z","title":"Facing the Music: Tackling Singing Voice Separation in Cinematic Audio\n  Source Separation","summary":"  Cinematic audio source separation (CASS) is a fairly new subtask of audio\nsource separation. A typical setup of CASS is a three-stem problem, with the\naim of separating the mixture into the dialogue stem (DX), music stem (MX), and\neffects stem (FX). In practice, however, several edge cases exist as some sound\nsources do not fit neatly in either of these three stems, necessitating the use\nof additional auxiliary stems in production. One very common edge case is the\nsinging voice in film audio, which may belong in either the DX or MX, depending\nheavily on the cinematic context. In this work, we demonstrate a very\nstraightforward extension of the dedicated-decoder Bandit and query-based\nsingle-decoder Banquet models to a four-stem problem, treating non-musical\ndialogue, instrumental music, singing voice, and effects as separate stems.\nInterestingly, the query-based Banquet model outperformed the dedicated-decoder\nBandit model. We hypothesized that this is due to a better feature alignment at\nthe bottleneck as enforced by the band-agnostic FiLM layer. Dataset and model\nimplementation will be made available at\nhttps://github.com/kwatcharasupat/source-separation-landing.\n","authors":["Karn N. Watcharasupat","Chih-Wei Wu","Iroro Orife"],"pdf_url":"https://arxiv.org/pdf/2408.03588v1.pdf","comment":"Submitted to the Late-Breaking Demo Session of the 25th International\n  Society for Music Information Retrieval (ISMIR) Conference, 2024"},{"id":"http://arxiv.org/abs/2408.03585v1","updated":"2024-08-07T06:44:47Z","published":"2024-08-07T06:44:47Z","title":"Hierarchical Neural Constructive Solver for Real-world TSP Scenarios","summary":"  Existing neural constructive solvers for routing problems have predominantly\nemployed transformer architectures, conceptualizing the route construction as a\nset-to-sequence learning task. However, their efficacy has primarily been\ndemonstrated on entirely random problem instances that inadequately capture\nreal-world scenarios. In this paper, we introduce realistic Traveling Salesman\nProblem (TSP) scenarios relevant to industrial settings and derive the\nfollowing insights: (1) The optimal next node (or city) to visit often lies\nwithin proximity to the current node, suggesting the potential benefits of\nbiasing choices based on current locations. (2) Effectively solving the TSP\nrequires robust tracking of unvisited nodes and warrants succinct grouping\nstrategies. Building upon these insights, we propose integrating a learnable\nchoice layer inspired by Hypernetworks to prioritize choices based on the\ncurrent location, and a learnable approximate clustering algorithm inspired by\nthe Expectation-Maximization algorithm to facilitate grouping the unvisited\ncities. Together, these two contributions form a hierarchical approach towards\nsolving the realistic TSP by considering both immediate local neighbourhoods\nand learning an intermediate set of node representations. Our hierarchical\napproach yields superior performance compared to both classical and recent\ntransformer models, showcasing the efficacy of the key designs.\n","authors":["Yong Liang Goh","Zhiguang Cao","Yining Ma","Yanfei Dong","Mohammed Haroon Dupty","Wee Sun Lee"],"pdf_url":"https://arxiv.org/pdf/2408.03585v1.pdf","comment":"Accepted to KDD 2024"},{"id":"http://arxiv.org/abs/2408.03573v1","updated":"2024-08-07T06:17:48Z","published":"2024-08-07T06:17:48Z","title":"Active Testing of Large Language Model via Multi-Stage Sampling","summary":"  Performance evaluation plays a crucial role in the development life cycle of\nlarge language models (LLMs). It estimates the model's capability, elucidates\nbehavior characteristics, and facilitates the identification of potential\nissues and limitations, thereby guiding further improvement. Given that LLMs'\ndiverse task-handling abilities stem from large volumes of training data, a\ncomprehensive evaluation also necessitates abundant, well-annotated, and\nrepresentative test data to assess LLM performance across various downstream\ntasks. However, the demand for high-quality test data often entails substantial\ntime, computational resources, and manual efforts, sometimes causing the\nevaluation to be inefficient or impractical. To address these challenges,\nresearchers propose active testing, which estimates the overall performance by\nselecting a subset of test data. Nevertheless, the existing active testing\nmethods tend to be inefficient, even inapplicable, given the unique new\nchallenges of LLMs (e.g., diverse task types, increased model complexity, and\nunavailability of training data). To mitigate such limitations and expedite the\ndevelopment cycle of LLMs, in this work, we introduce AcTracer, an active\ntesting framework tailored for LLMs that strategically selects a small subset\nof test data to achieve a nearly optimal performance estimation for LLMs.\nAcTracer utilizes both internal and external information from LLMs to guide the\ntest sampling process, reducing variance through a multi-stage pool-based\nactive selection. Our experiment results demonstrate that AcTracer achieves\nstate-of-the-art performance compared to existing methods across various tasks,\nwith up to 38.83% improvement over previous SOTA.\n","authors":["Yuheng Huang","Jiayang Song","Qiang Hu","Felix Juefei-Xu","Lei Ma"],"pdf_url":"https://arxiv.org/pdf/2408.03573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03572v1","updated":"2024-08-07T06:16:17Z","published":"2024-08-07T06:16:17Z","title":"2D-OOB: Attributing Data Contribution through Joint Valuation Framework","summary":"  Data valuation has emerged as a powerful framework to quantify the\ncontribution of each datum to the training of a particular machine learning\nmodel. However, it is crucial to recognize that the quality of various cells\nwithin a single data point can vary greatly in practice. For example, even in\nthe case of an abnormal data point, not all cells are necessarily noisy. The\nsingle scalar valuation assigned by existing methods blurs the distinction\nbetween noisy and clean cells of a data point, thereby compromising the\ninterpretability of the valuation. In this paper, we propose 2D-OOB, an\nout-of-bag estimation framework for jointly determining helpful (or\ndetrimental) samples, as well as the particular cells that drive them. Our\ncomprehensive experiments demonstrate that 2D-OOB achieves state-of-the-art\nperformance across multiple use cases, while being exponentially faster. 2D-OOB\nexcels in detecting and rectifying fine-grained outliers at the cell level, as\nwell as localizing backdoor triggers in data poisoning attacks.\n","authors":["Yifan Sun","Jingyan Shen","Yongchan Kwon"],"pdf_url":"https://arxiv.org/pdf/2408.03572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11755v3","updated":"2024-08-07T06:05:42Z","published":"2024-03-18T13:03:24Z","title":"Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs","summary":"  Prompt ensembling of Large Language Model (LLM) generated category-specific\nprompts has emerged as an effective method to enhance zero-shot recognition\nability of Vision-Language Models (VLMs). To obtain these category-specific\nprompts, the present methods rely on hand-crafting the prompts to the LLMs for\ngenerating VLM prompts for the downstream tasks. However, this requires\nmanually composing these task-specific prompts and still, they might not cover\nthe diverse set of visual concepts and task-specific styles associated with the\ncategories of interest. To effectively take humans out of the loop and\ncompletely automate the prompt generation process for zero-shot recognition, we\npropose Meta-Prompting for Visual Recognition (MPVR). Taking as input only\nminimal information about the target task, in the form of its short natural\nlanguage description, and a list of associated class labels, MPVR automatically\nproduces a diverse set of category-specific prompts resulting in a strong\nzero-shot classifier. MPVR generalizes effectively across various popular\nzero-shot image recognition benchmarks belonging to widely different domains\nwhen tested with multiple LLMs and VLMs. For example, MPVR obtains a zero-shot\nrecognition improvement over CLIP by up to 19.8% and 18.2% (5.0% and 4.5% on\naverage over 20 datasets) leveraging GPT and Mixtral LLMs, respectively\n","authors":["M. Jehanzeb Mirza","Leonid Karlinsky","Wei Lin","Sivan Doveh","Jakub Micorek","Mateusz Kozinski","Hilde Kuehne","Horst Possegger"],"pdf_url":"https://arxiv.org/pdf/2403.11755v3.pdf","comment":"ECCV Camera Ready. Code & Data:\n  https://jmiemirza.github.io/Meta-Prompting/"},{"id":"http://arxiv.org/abs/2408.02085v3","updated":"2024-08-07T06:04:31Z","published":"2024-08-04T16:50:07Z","title":"Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data\n  Assessment and Selection for Instruction Tuning of Language Models","summary":"  Instruction tuning plays a critical role in aligning large language models\n(LLMs) with human preference. Despite the vast amount of open instruction\ndatasets, naively training a LLM on all existing instructions may not be\noptimal and practical. To pinpoint the most beneficial datapoints, data\nassessment and selection methods have been proposed in the fields of natural\nlanguage processing (NLP) and deep learning. However, under the context of\ninstruction tuning, there still exists a gap in knowledge on what kind of data\nevaluation metrics can be employed and how they can be integrated into the\nselection mechanism. To bridge this gap, we present a comprehensive review on\nexisting literature of data assessment and selection especially for instruction\ntuning of LLMs. We systematically categorize all applicable methods into\nquality-based, diversity-based, and importance-based ones where a unified,\nfine-grained taxonomy is structured. For each category, representative methods\nare elaborated to describe the landscape of relevant research. In addition,\ncomparison between latest methods is conducted on their officially reported\nresults to provide in-depth discussions on their limitations. Finally, we\nsummarize the open challenges and propose the promosing avenues for future\nstudies. All related contents are available at\nhttps://github.com/yuleiqin/fantastic-data-engineering.\n","authors":["Yulei Qin","Yuncheng Yang","Pengcheng Guo","Gang Li","Hang Shao","Yuchen Shi","Zihan Xu","Yun Gu","Ke Li","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2408.02085v3.pdf","comment":"review, survey, 28 pages, 2 figures, 4 tables"},{"id":"http://arxiv.org/abs/2408.03029v2","updated":"2024-08-07T05:59:46Z","published":"2024-08-06T08:22:16Z","title":"Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning","summary":"  Reward shaping addresses the challenge of sparse rewards in reinforcement\nlearning by constructing denser and more informative reward signals. To achieve\nself-adaptive and highly efficient reward shaping, we propose a novel method\nthat incorporates success rates derived from historical experiences into shaped\nrewards. Our approach utilizes success rates sampled from Beta distributions,\nwhich dynamically evolve from uncertain to reliable values as more data is\ncollected. Initially, the self-adaptive success rates exhibit more randomness\nto encourage exploration. Over time, they become more certain to enhance\nexploitation, thus achieving a better balance between exploration and\nexploitation. We employ Kernel Density Estimation (KDE) combined with Random\nFourier Features (RFF) to derive the Beta distributions, resulting in a\ncomputationally efficient implementation in high-dimensional continuous state\nspaces. This method provides a non-parametric and learning-free approach. The\nproposed method is evaluated on a wide range of continuous control tasks with\nsparse and delayed rewards, demonstrating significant improvements in sample\nefficiency and convergence stability compared to relevant baselines.\n","authors":["Haozhe Ma","Zhengding Luo","Thanh Vinh Vo","Kuankuan Sima","Tze-Yun Leong"],"pdf_url":"https://arxiv.org/pdf/2408.03029v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03562v1","updated":"2024-08-07T05:52:00Z","published":"2024-08-07T05:52:00Z","title":"A Comparison of LLM Finetuning Methods & Evaluation Metrics with Travel\n  Chatbot Use Case","summary":"  This research compares large language model (LLM) fine-tuning methods,\nincluding Quantized Low Rank Adapter (QLoRA), Retrieval Augmented fine-tuning\n(RAFT), and Reinforcement Learning from Human Feedback (RLHF), and additionally\ncompared LLM evaluation methods including End to End (E2E) benchmark method of\n\"Golden Answers\", traditional natural language processing (NLP) metrics, RAG\nAssessment (Ragas), OpenAI GPT-4 evaluation metrics, and human evaluation,\nusing the travel chatbot use case. The travel dataset was sourced from the the\nReddit API by requesting posts from travel-related subreddits to get\ntravel-related conversation prompts and personalized travel experiences, and\naugmented for each fine-tuning method. We used two pretrained LLMs utilized for\nfine-tuning research: LLaMa 2 7B, and Mistral 7B. QLoRA and RAFT are applied to\nthe two pretrained models. The inferences from these models are extensively\nevaluated against the aforementioned metrics. The best model according to human\nevaluation and some GPT-4 metrics was Mistral RAFT, so this underwent a\nReinforcement Learning from Human Feedback (RLHF) training pipeline, and\nultimately was evaluated as the best model. Our main findings are that: 1)\nquantitative and Ragas metrics do not align with human evaluation, 2) Open AI\nGPT-4 evaluation most aligns with human evaluation, 3) it is essential to keep\nhumans in the loop for evaluation because, 4) traditional NLP metrics\ninsufficient, 5) Mistral generally outperformed LLaMa, 6) RAFT outperforms\nQLoRA, but still needs postprocessing, 7) RLHF improves model performance\nsignificantly. Next steps include improving data quality, increasing data\nquantity, exploring RAG methods, and focusing data collection on a specific\ncity, which would improve data quality by narrowing the focus, while creating a\nuseful product.\n","authors":["Sonia Meyer","Shreya Singh","Bertha Tam","Christopher Ton","Angel Ren"],"pdf_url":"https://arxiv.org/pdf/2408.03562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03561v1","updated":"2024-08-07T05:50:17Z","published":"2024-08-07T05:50:17Z","title":"MPC-Minimized Secure LLM Inference","summary":"  Many inference services based on large language models (LLMs) pose a privacy\nconcern, either revealing user prompts to the service or the proprietary\nweights to the user. Secure inference offers a solution to this problem through\nsecure multi-party computation (MPC), however, it is still impractical for\nmodern LLM workload due to the large overhead imposed by MPC. To address this\noverhead, we propose Marill, a framework that adapts LLM fine-tuning to\nminimize MPC usage during secure inference. Marill introduces high-level\narchitectural changes during fine-tuning that significantly reduce the number\nof expensive operations needed within MPC during inference, by removing some\nand relocating others outside MPC without compromising security. As a result,\nMarill-generated models are more efficient across all secure inference\nprotocols and our approach complements MPC-friendly approximations for such\noperations. Compared to standard fine-tuning, Marill results in 3.6-11.3x\nbetter runtime and 2.4-6.9x better communication during secure inference across\nvarious MPC settings, while typically preserving over 90% performance across\ndownstream tasks.\n","authors":["Deevashwer Rathee","Dacheng Li","Ion Stoica","Hao Zhang","Raluca Popa"],"pdf_url":"https://arxiv.org/pdf/2408.03561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03558v1","updated":"2024-08-07T05:47:06Z","published":"2024-08-07T05:47:06Z","title":"D2Styler: Advancing Arbitrary Style Transfer with Discrete Diffusion\n  Methods","summary":"  In image processing, one of the most challenging tasks is to render an\nimage's semantic meaning using a variety of artistic approaches. Existing\ntechniques for arbitrary style transfer (AST) frequently experience\nmode-collapse, over-stylization, or under-stylization due to a disparity\nbetween the style and content images. We propose a novel framework called\nD$^2$Styler (Discrete Diffusion Styler) that leverages the discrete\nrepresentational capability of VQ-GANs and the advantages of discrete\ndiffusion, including stable training and avoidance of mode collapse. Our method\nuses Adaptive Instance Normalization (AdaIN) features as a context guide for\nthe reverse diffusion process. This makes it easy to move features from the\nstyle image to the content image without bias. The proposed method\nsubstantially enhances the visual quality of style-transferred images, allowing\nthe combination of content and style in a visually appealing manner. We take\nstyle images from the WikiArt dataset and content images from the COCO dataset.\nExperimental results demonstrate that D$^2$Styler produces high-quality\nstyle-transferred images and outperforms twelve existing methods on nearly all\nthe metrics. The qualitative results and ablation studies provide further\ninsights into the efficacy of our technique. The code is available at\nhttps://github.com/Onkarsus13/D2Styler.\n","authors":["Onkar Susladkar","Gayatri Deshmukh","Sparsh Mittal","Parth Shastri"],"pdf_url":"https://arxiv.org/pdf/2408.03558v1.pdf","comment":"Paper accepted at 27th International Conference on Pattern\n  Recognition (ICPR), 2024"},{"id":"http://arxiv.org/abs/2106.11760v5","updated":"2024-08-07T05:28:30Z","published":"2021-06-19T06:25:10Z","title":"Fingerprinting Image-to-Image Generative Adversarial Networks","summary":"  Generative Adversarial Networks (GANs) have been widely used in various\napplication scenarios. Since the production of a commercial GAN requires\nsubstantial computational and human resources, the copyright protection of GANs\nis urgently needed. This paper presents a novel fingerprinting scheme for the\nIntellectual Property (IP) protection of image-to-image GANs based on a trusted\nthird party. We break through the stealthiness and robustness bottlenecks\nsuffered by previous fingerprinting methods for classification models being\nnaively transferred to GANs. Specifically, we innovatively construct a\ncomposite deep learning model from the target GAN and a classifier. Then we\ngenerate fingerprint samples from this composite model, and embed them in the\nclassifier for effective ownership verification. This scheme inspires some\nconcrete methodologies to practically protect the modern image-to-image\ntranslation GANs. Theoretical analysis proves that these methods can satisfy\ndifferent security requirements necessary for IP protection. We also conduct\nextensive experiments to show that our solutions outperform existing\nstrategies.\n","authors":["Guanlin Li","Guowen Xu","Han Qiu","Shangwei Guo","Run Wang","Jiwei Li","Tianwei Zhang","Rongxing Lu"],"pdf_url":"https://arxiv.org/pdf/2106.11760v5.pdf","comment":"Accepted by EuroS&P 2024"},{"id":"http://arxiv.org/abs/2407.11652v5","updated":"2024-08-07T05:14:24Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v5.pdf","comment":"I found critical errors in the manuscript affecting its validity. I\n  need to correct these before resubmitting. Major changes to methodology and\n  results are underway, significantly altering the content. I will resubmit the\n  revised version"},{"id":"http://arxiv.org/abs/2408.03544v1","updated":"2024-08-07T04:49:38Z","published":"2024-08-07T04:49:38Z","title":"Unlocking the Non-Native Language Context Limitation: Native Language\n  Prompting Facilitates Knowledge Elicitation","summary":"  Multilingual large language models (MLLMs) struggle to answer questions posed\nin non-dominant languages, even though they have already acquired the relevant\nknowledge from their dominant language corpus. In contrast, human multilinguals\ncan overcome this issue by invoking the relatively rich knowledge acquired from\nnative language texts through Positive Native Language Transfer (PNLT).\nInspired by this, we analogize the dominant language of MLLMs to the native\nlanguage of human multilinguals, and propose Native Language Prompting (NatLan)\nto simulate the PNLT observed in human multilinguals. It explicitly creates\nnative language contexts for MLLMs to facilitate the elicitation of the rich\nnative language knowledge during question-answering, unlocking the limitations\nimposed by non-native language contexts on the effective application of\nknowledge. By employing multi-MLLM collaboration, NatLan reduces the workload\non each MLLM in simulating PNLT and refines semantic transfer. On the C-Eval\nbenchmark, NatLan provides up to a 10.1% average accuracy improvement and up to\na 5.0% increase in the hard-level subset across five MLLMs, surpassing all\ntop-notch related methods. Our code is available at\nhttps://github.com/AnonyNLP/NatLan.\n","authors":["Baixuan Li","Yunlong Fan","Zhiqiang Gao"],"pdf_url":"https://arxiv.org/pdf/2408.03544v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03542v1","updated":"2024-08-07T04:42:10Z","published":"2024-08-07T04:42:10Z","title":"Automatic identification of the area covered by acorn trees in the\n  dehesa (pastureland) Extremadura of Spain","summary":"  The acorn is the fruit of the oak and is an important crop in the Spanish\ndehesa extreme\\~na, especially for the value it provides in the Iberian pig\nfood to obtain the \"acorn\" certification. For this reason, we want to maximise\nthe production of Iberian pigs with the appropriate weight. Hence the need to\nknow the area covered by the crowns of the acorn trees, to determine the\ncovered wooded area (CWA, from the Spanish Superficie Arbolada Cubierta SAC)\nand thereby estimate the number of Iberian pigs that can be released per\nhectare, as indicated by the royal decree 4/2014. In this work, we propose the\nautomatic estimation of the CWA, through aerial digital images (orthophotos) of\nthe pastureland of Extremadura, and with this, to offer the possibility of\ndetermining the number of Iberian pigs to be released in a specific plot of\nland. Among the main issues for automatic detection are, first, the correct\nidentification of acorn trees, secondly, correctly discriminating the shades of\nthe acorn trees and, finally, detect the arbuscles (young acorn trees not yet\nproductive, or shrubs that are not oaks). These difficulties represent a real\nchallenge, both for the automatic segmentation process and for manual\nsegmentation. In this work, the proposed method for automatic segmentation is\nbased on the clustering algorithm proposed by Gustafson-Kessel (GK) but the\nmodified version of Babuska (GK-B) and on the use of real orthophotos. The\nobtained results are promising both in their comparison with the real images\nand when compared with the images segmented by hand. The whole set of\northophotos used in this work correspond to an approximate area of 142\nhectares, and the results are of great interest to producers of certified\n\"acorn\" pork.\n","authors":["Ojeda-Magaña Benjamin","Ruelas Ruben","Quintanilla-Dominguez Joel","Gomez-Barba Leopoldo","Lopez de Herrera Juan","Robledo-Hernandez Jose","Tarquis Ana"],"pdf_url":"https://arxiv.org/pdf/2408.03542v1.pdf","comment":"22 pages, 15 Figures, 2 Tables"},{"id":"http://arxiv.org/abs/2408.03541v1","updated":"2024-08-07T04:38:38Z","published":"2024-08-07T04:38:38Z","title":"EXAONE 3.0 7.8B Instruction Tuned Language Model","summary":"  We introduce EXAONE 3.0 instruction-tuned language model, the first open\nmodel in the family of Large Language Models (LLMs) developed by LG AI\nResearch. Among different model sizes, we publicly release the 7.8B\ninstruction-tuned model to promote open research and innovations. Through\nextensive evaluations across a wide range of public and in-house benchmarks,\nEXAONE 3.0 demonstrates highly competitive real-world performance with\ninstruction-following capability against other state-of-the-art open models of\nsimilar size. Our comparative analysis shows that EXAONE 3.0 excels\nparticularly in Korean, while achieving compelling performance across general\ntasks and complex reasoning. With its strong real-world effectiveness and\nbilingual proficiency, we hope that EXAONE keeps contributing to advancements\nin Expert AI. Our EXAONE 3.0 instruction-tuned model is available at\nhttps://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\n","authors":["LG AI Research","Soyoung An","Kyunghoon Bae","Eunbi Choi","Stanley Jungkyu Choi","Yemuk Choi","Seokhee Hong","Yeonjung Hong","Junwon Hwang","Hyojin Jeon","Gerrard Jeongwon Jo","Hyunjik Jo","Jiyeon Jung","Yountae Jung","Euisoon Kim","Hyosang Kim","Joonkee Kim","Seonghwan Kim","Soyeon Kim","Sunkyoung Kim","Yireun Kim","Youchul Kim","Edward Hwayoung Lee","Haeju Lee","Honglak Lee","Jinsik Lee","Kyungmin Lee","Moontae Lee","Seungjun Lee","Woohyung Lim","Sangha Park","Sooyoun Park","Yongmin Park","Boseong Seo","Sihoon Yang","Heuiyeen Yeen","Kyungjae Yoo","Hyeongu Yun"],"pdf_url":"https://arxiv.org/pdf/2408.03541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21320v2","updated":"2024-08-07T04:34:11Z","published":"2024-07-31T04:01:08Z","title":"MetaOpenFOAM: an LLM-based multi-agent framework for CFD","summary":"  Remarkable progress has been made in automated problem solving through\nsocieties of agents based on large language models (LLMs). Computational fluid\ndynamics (CFD), as a complex problem, presents unique challenges in automated\nsimulations that require sophisticated solutions. MetaOpenFOAM, as a novel\nmulti-agent collaborations framework, aims to complete CFD simulation tasks\nwith only natural language as input. These simulation tasks include mesh\npre-processing, simulation and so on. MetaOpenFOAM harnesses the power of\nMetaGPT's assembly line paradigm, which assigns diverse roles to various\nagents, efficiently breaking down complex CFD tasks into manageable subtasks.\nLangchain further complements MetaOpenFOAM by integrating Retrieval-Augmented\nGeneration (RAG) technology, which enhances the framework's ability by\nintegrating a searchable database of OpenFOAM tutorials for LLMs. Tests on a\nbenchmark for natural language-based CFD solver, consisting of eight CFD\nsimulation tasks, have shown that MetaOpenFOAM achieved a high pass rate per\ntest (85%), with each test case costing only $0.22 on average. The eight CFD\nsimulation tasks encompass a range of multidimensional flow problems, covering\ncompressible and incompressible flows with different physical processes. This\ndemonstrates the capability to automate CFD simulations using only natural\nlanguage input, iteratively correcting errors to achieve the desired\nsimulations. An ablation study was conducted to verify the necessity of each\ncomponent in the multi-agent system and the RAG technology. A sensitivity study\non the randomness of LLM showed that LLM with low randomness can obtain more\nstable and accurate results. Additionally, MetaOpenFOAM owns the ability to\nidentify and modify key parameters in user requirements, and excels in\ncorrecting bugs when failure match occur,which demonstrates the generalization\nof MetaOpenFOAM.\n","authors":["Yuxuan Chen","Xu Zhu","Hua Zhou","Zhuyin Ren"],"pdf_url":"https://arxiv.org/pdf/2407.21320v2.pdf","comment":"31 pages,11 figures, 11 tables"},{"id":"http://arxiv.org/abs/2408.03533v1","updated":"2024-08-07T04:20:28Z","published":"2024-08-07T04:20:28Z","title":"Lifelong Personalized Low-Rank Adaptation of Large Language Models for\n  Recommendation","summary":"  We primarily focus on the field of large language models (LLMs) for\nrecommendation, which has been actively explored recently and poses a\nsignificant challenge in effectively enhancing recommender systems with logical\nreasoning abilities and open-world knowledge. Current mainstream efforts mainly\ncenter around injecting personalized information from recommendation models\ninto LLMs by customizing input templates or aligning representations between\nsemantic and recommendation spaces at the prediction layer. However, they face\nthree significant limitations: (1) LoRA is mostly used as a core component in\nexisting works, but personalization is not well established in LoRA parameters\nas the LoRA matrix shared by every user may not cater to different users'\ncharacteristics, leading to suboptimal performance. (2) Although lifelong\npersonalized behavior sequences are ideal for personalization, their use raises\neffectiveness and efficiency issues since LLMs require escalating training and\ninference time to extend text lengths. (3) Existing approaches aren't scalable\nfor large datasets due to training efficiency constraints. Thus, LLMs only see\na small fraction of the datasets (e.g., less than 10%) instead of the whole\ndatasets, limiting their exposure to the full training space. To address these\nproblems, we propose RecLoRA. This model incorporates a Personalized LoRA\nmodule that maintains independent LoRAs for different users and a Long-Short\nModality Retriever that retrieves different history lengths for different\nmodalities, significantly improving performance while adding minimal time cost.\nFurthermore, we design a Few2Many Learning Strategy, using a conventional\nrecommendation model as a lens to magnify small training spaces to full spaces.\nExtensive experiments on public datasets demonstrate the efficacy of our\nRecLoRA compared to existing baseline models.\n","authors":["Jiachen Zhu","Jianghao Lin","Xinyi Dai","Bo Chen","Rong Shan","Jieming Zhu","Ruiming Tang","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.03533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03528v1","updated":"2024-08-07T03:48:07Z","published":"2024-08-07T03:48:07Z","title":"Exploring the extent of similarities in software failures across\n  industries using LLMs","summary":"  The rapid evolution of software development necessitates enhanced safety\nmeasures. Extracting information about software failures from companies is\nbecoming increasingly more available through news articles.\n  This research utilizes the Failure Analysis Investigation with LLMs (FAIL)\nmodel to extract industry-specific information. Although the FAIL model's\ndatabase is rich in information, it could benefit from further categorization\nand industry-specific insights to further assist software engineers.\n  In previous work news articles were collected from reputable sources and\ncategorized by incidents inside a database. Prompt engineering and Large\nLanguage Models (LLMs) were then applied to extract relevant information\nregarding the software failure. This research extends these methods by\ncategorizing articles into specific domains and types of software failures. The\nresults are visually represented through graphs.\n  The analysis shows that throughout the database some software failures occur\nsignificantly more often in specific industries. This categorization provides a\nvaluable resource for software engineers and companies to identify and address\ncommon failures.\n  This research highlights the synergy between software engineering and Large\nLanguage Models (LLMs) to automate and enhance the analysis of software\nfailures. By transforming data from the database into an industry specific\nmodel, we provide a valuable resource that can be used to identify common\nvulnerabilities, predict potential risks, and implement proactive measures for\npreventing software failures. Leveraging the power of the current FAIL database\nand data visualization, we aim to provide an avenue for safer and more secure\nsoftware in the future.\n","authors":["Martin Detloff"],"pdf_url":"https://arxiv.org/pdf/2408.03528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03525v1","updated":"2024-08-07T03:24:59Z","published":"2024-08-07T03:24:59Z","title":"Hierarchical learning control for autonomous robots inspired by central\n  nervous system","summary":"  Mammals can generate autonomous behaviors in various complex environments\nthrough the coordination and interaction of activities at different levels of\ntheir central nervous system. In this paper, we propose a novel hierarchical\nlearning control framework by mimicking the hierarchical structure of the\ncentral nervous system along with their coordination and interaction behaviors.\nThe framework combines the active and passive control systems to improve both\nthe flexibility and reliability of the control system as well as to achieve\nmore diverse autonomous behaviors of robots. Specifically, the framework has a\nbackbone of independent neural network controllers at different levels and\ntakes a three-level dual descending pathway structure, inspired from the\nfunctionality of the cerebral cortex, cerebellum, and spinal cord. We\ncomprehensively validated the proposed approach through the simulation as well\nas the experiment of a hexapod robot in various complex environments, including\nobstacle crossing and rapid recovery after partial damage. This study reveals\nthe principle that governs the autonomous behavior in the central nervous\nsystem and demonstrates the effectiveness of the hierarchical control approach\nwith the salient features of the hierarchical learning control architecture and\ncombination of active and passive control systems.\n","authors":["Pei Zhang","Zhaobo Hua","Jinliang Ding"],"pdf_url":"https://arxiv.org/pdf/2408.03525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03519v1","updated":"2024-08-07T03:06:57Z","published":"2024-08-07T03:06:57Z","title":"RepoMasterEval: Evaluating Code Completion via Real-World Repositories","summary":"  With the growing reliance on automated code completion tools in software\ndevelopment, the need for robust evaluation benchmarks has become critical.\nHowever, existing benchmarks focus more on code generation tasks in function\nand class level and provide rich text description to prompt the model. By\ncontrast, such descriptive prompt is commonly unavailable in real development\nand code completion can occur in wider range of situations such as in the\nmiddle of a function or a code block. These limitations makes the evaluation\npoorly align with the practical scenarios of code completion tools. In this\npaper, we propose RepoMasterEval, a novel benchmark for evaluating code\ncompletion models constructed from real-world Python and TypeScript\nrepositories. Each benchmark datum is generated by masking a code snippet\n(ground truth) from one source code file with existing test suites. To improve\ntest accuracy of model generated code, we employ mutation testing to measure\nthe effectiveness of the test cases and we manually crafted new test cases for\nthose test suites with low mutation score. Our empirical evaluation on 6\nstate-of-the-art models shows that test argumentation is critical in improving\nthe accuracy of the benchmark and RepoMasterEval is able to report difference\nin model performance in real-world scenarios. The deployment of RepoMasterEval\nin a collaborated company for one month also revealed that the benchmark is\nuseful to give accurate feedback during model training and the score is in high\ncorrelation with the model's performance in practice. Based on our findings, we\ncall for the software engineering community to build more LLM benchmarks\ntailored for code generation tools taking the practical and complex development\nenvironment into consideration.\n","authors":["Qinyun Wu","Chao Peng","Pengfei Gao","Ruida Hu","Haoyu Gan","Bo Jiang","Jinhe Tang","Zhiwen Deng","Zhanming Guan","Cuiyun Gao","Xia Liu","Ping Yang"],"pdf_url":"https://arxiv.org/pdf/2408.03519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03515v1","updated":"2024-08-07T02:48:22Z","published":"2024-08-07T02:48:22Z","title":"A Study on Prompt Injection Attack Against LLM-Integrated Mobile Robotic\n  Systems","summary":"  The integration of Large Language Models (LLMs) like GPT-4o into robotic\nsystems represents a significant advancement in embodied artificial\nintelligence. These models can process multi-modal prompts, enabling them to\ngenerate more context-aware responses. However, this integration is not without\nchallenges. One of the primary concerns is the potential security risks\nassociated with using LLMs in robotic navigation tasks. These tasks require\nprecise and reliable responses to ensure safe and effective operation.\nMulti-modal prompts, while enhancing the robot's understanding, also introduce\ncomplexities that can be exploited maliciously. For instance, adversarial\ninputs designed to mislead the model can lead to incorrect or dangerous\nnavigational decisions. This study investigates the impact of prompt injections\non mobile robot performance in LLM-integrated systems and explores secure\nprompt strategies to mitigate these risks. Our findings demonstrate a\nsubstantial overall improvement of approximately 30.8% in both attack detection\nand system performance with the implementation of robust defence mechanisms,\nhighlighting their critical role in enhancing security and reliability in\nmission-oriented tasks.\n","authors":["Wenxiao Zhang","Xiangrui Kong","Conan Dewitt","Thomas Braunl","Jin B. Hong"],"pdf_url":"https://arxiv.org/pdf/2408.03515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.06345v3","updated":"2024-08-07T02:46:46Z","published":"2023-04-13T08:52:34Z","title":"ASR: Attention-alike Structural Re-parameterization","summary":"  The structural re-parameterization (SRP) technique is a novel deep learning\ntechnique that achieves interconversion between different network architectures\nthrough equivalent parameter transformations. This technique enables the\nmitigation of the extra costs for performance improvement during training, such\nas parameter size and inference time, through these transformations during\ninference, and therefore SRP has great potential for industrial and practical\napplications. The existing SRP methods have successfully considered many\ncommonly used architectures, such as normalizations, pooling methods, and\nmulti-branch convolution. However, the widely used attention modules which\ndrastically slow inference speed cannot be directly implemented by SRP due to\nthese modules usually act on the backbone network in a multiplicative manner\nand the modules' output is input-dependent during inference, which limits the\napplication scenarios of SRP. In this paper, we conduct extensive experiments\nfrom a statistical perspective and discover an interesting phenomenon Stripe\nObservation, which reveals that channel attention values quickly approach some\nconstant vectors during training. This observation inspires us to propose a\nsimple-yet-effective attention-alike structural re-parameterization (ASR) that\nallows us to achieve SRP for a given network while enjoying the effectiveness\nof the attention mechanism. Extensive experiments conducted on several standard\nbenchmarks demonstrate the effectiveness of ASR in generally improving the\nperformance of existing backbone networks, attention modules, and SRP methods\nwithout any elaborated model crafting. We also analyze the limitations and\nprovide experimental and theoretical evidence for the strong robustness of the\nproposed ASR.\n","authors":["Shanshan Zhong","Zhongzhan Huang","Wushao Wen","Jinghui Qin","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2304.06345v3.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2406.16087v4","updated":"2024-08-07T02:36:19Z","published":"2024-06-23T12:02:17Z","title":"Imperative Learning: A Self-supervised Neural-Symbolic Learning\n  Framework for Robot Autonomy","summary":"  Data-driven methods such as reinforcement and imitation learning have\nachieved remarkable success in robot autonomy. However, their data-centric\nnature still hinders them from generalizing well to ever-changing environments.\nMoreover, collecting large datasets for robotic tasks is often impractical and\nexpensive. To overcome these challenges, we introduce a new self-supervised\nneural-symbolic (NeSy) computational framework, imperative learning (IL), for\nrobot autonomy, leveraging the generalization abilities of symbolic reasoning.\nThe framework of IL consists of three primary components: a neural module, a\nreasoning engine, and a memory system. We formulate IL as a special bilevel\noptimization (BLO), which enables reciprocal learning over the three modules.\nThis overcomes the label-intensive obstacles associated with data-driven\napproaches and takes advantage of symbolic reasoning concerning logical\nreasoning, physical principles, geometric analysis, etc. We discuss several\noptimization techniques for IL and verify their effectiveness in five distinct\nrobot autonomy tasks including path planning, rule induction, optimal control,\nvisual odometry, and multi-robot routing. Through various experiments, we show\nthat IL can significantly enhance robot autonomy capabilities and we anticipate\nthat it will catalyze further research across diverse domains.\n","authors":["Chen Wang","Kaiyi Ji","Junyi Geng","Zhongqiang Ren","Taimeng Fu","Fan Yang","Yifan Guo","Haonan He","Xiangyu Chen","Zitong Zhan","Qiwei Du","Shaoshu Su","Bowen Li","Yuheng Qiu","Yi Du","Qihang Li","Yifan Yang","Xiao Lin","Zhipeng Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.16087v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03505v1","updated":"2024-08-07T02:08:29Z","published":"2024-08-07T02:08:29Z","title":"Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble\n  Exploitation","summary":"  Multimodal large language models (MLLMs) have extended the success of large\nlanguage models (LLMs) to multiple data types, such as image, text and audio,\nachieving significant performance in various domains, including multimodal\ntranslation, visual question answering and content generation. Nonetheless,\nexisting systems are inefficient to train MLLMs due to substantial GPU bubbles\ncaused by the heterogeneous modality models and complex data dependencies in 3D\nparallelism. This paper proposes Optimus, a distributed MLLM training system\nthat reduces end-to-end MLLM training time. Optimus is based on our principled\nanalysis that scheduling the encoder computation within the LLM bubbles can\nreduce bubbles in MLLM training. To make scheduling encoder computation\npossible for all GPUs, Optimus searches the separate parallel plans for encoder\nand LLM, and adopts a bubble scheduling algorithm to enable exploiting LLM\nbubbles without breaking the original data dependencies in the MLLM model\narchitecture. We further decompose encoder layer computation into a series of\nkernels, and analyze the common bubble pattern of 3D parallelism to carefully\noptimize the sub-millisecond bubble scheduling, minimizing the overall training\ntime. Our experiments in a production cluster show that Optimus accelerates\nMLLM training by 20.5%-21.3% with ViT-22B and GPT-175B model over 3072 GPUs\ncompared to baselines.\n","authors":["Weiqi Feng","Yangrui Chen","Shaoyu Wang","Yanghua Peng","Haibin Lin","Minlan Yu"],"pdf_url":"https://arxiv.org/pdf/2408.03505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03497v1","updated":"2024-08-07T01:37:10Z","published":"2024-08-07T01:37:10Z","title":"Advanced User Credit Risk Prediction Model using LightGBM, XGBoost and\n  Tabnet with SMOTEENN","summary":"  Bank credit risk is a significant challenge in modern financial transactions,\nand the ability to identify qualified credit card holders among a large number\nof applicants is crucial for the profitability of a bank'sbank's credit card\nbusiness. In the past, screening applicants'applicants' conditions often\nrequired a significant amount of manual labor, which was time-consuming and\nlabor-intensive. Although the accuracy and reliability of previously used ML\nmodels have been continuously improving, the pursuit of more reliable and\npowerful AI intelligent models is undoubtedly the unremitting pursuit by major\nbanks in the financial industry. In this study, we used a dataset of over\n40,000 records provided by a commercial bank as the research object. We\ncompared various dimensionality reduction techniques such as PCA and T-SNE for\npreprocessing high-dimensional datasets and performed in-depth adaptation and\ntuning of distributed models such as LightGBM and XGBoost, as well as deep\nmodels like Tabnet. After a series of research and processing, we obtained\nexcellent research results by combining SMOTEENN with these techniques. The\nexperiments demonstrated that LightGBM combined with PCA and SMOTEENN\ntechniques can assist banks in accurately predicting potential high-quality\ncustomers, showing relatively outstanding performance compared to other models.\n","authors":["Chang Yu","Yixin Jin","Qianwen Xing","Ye Zhang","Shaobo Guo","Shuchen Meng"],"pdf_url":"https://arxiv.org/pdf/2408.03497v1.pdf","comment":"8 pagess on IEEE ICPICS"},{"id":"http://arxiv.org/abs/2407.08516v4","updated":"2024-08-07T01:37:03Z","published":"2024-07-11T14:00:53Z","title":"Converging Paradigms: The Synergy of Symbolic and Connectionist AI in\n  LLM-Empowered Autonomous Agents","summary":"  This article explores the convergence of connectionist and symbolic\nartificial intelligence (AI), from historical debates to contemporary\nadvancements. Traditionally considered distinct paradigms, connectionist AI\nfocuses on neural networks, while symbolic AI emphasizes symbolic\nrepresentation and logic. Recent advancements in large language models (LLMs),\nexemplified by ChatGPT and GPT-4, highlight the potential of connectionist\narchitectures in handling human language as a form of symbols. The study argues\nthat LLM-empowered Autonomous Agents (LAAs) embody this paradigm convergence.\nBy utilizing LLMs for text-based knowledge modeling and representation, LAAs\nintegrate neuro-symbolic AI principles, showcasing enhanced reasoning and\ndecision-making capabilities. Comparing LAAs with Knowledge Graphs within the\nneuro-symbolic AI theme highlights the unique strengths of LAAs in mimicking\nhuman-like reasoning processes, scaling effectively with large datasets, and\nleveraging in-context samples without explicit re-training. The research\nunderscores promising avenues in neuro-vector-symbolic integration,\ninstructional encoding, and implicit reasoning, aimed at further enhancing LAA\ncapabilities. By exploring the progression of neuro-symbolic AI and proposing\nfuture research trajectories, this work advances the understanding and\ndevelopment of AI technologies.\n","authors":["Haoyi Xiong","Zhiyuan Wang","Xuhong Li","Jiang Bian","Zeke Xie","Shahid Mumtaz","Laura E. Barnes"],"pdf_url":"https://arxiv.org/pdf/2407.08516v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15068v4","updated":"2024-08-07T01:04:29Z","published":"2023-08-29T07:00:35Z","title":"A Comprehensive Augmentation Framework for Anomaly Detection","summary":"  Data augmentation methods are commonly integrated into the training of\nanomaly detection models. Previous approaches have primarily focused on\nreplicating real-world anomalies or enhancing diversity, without considering\nthat the standard of anomaly varies across different classes, potentially\nleading to a biased training distribution.This paper analyzes crucial traits of\nsimulated anomalies that contribute to the training of reconstructive networks\nand condenses them into several methods, thus creating a comprehensive\nframework by selectively utilizing appropriate combinations.Furthermore, we\nintegrate this framework with a reconstruction-based approach and concurrently\npropose a split training strategy that alleviates the issue of overfitting\nwhile avoiding introducing interference to the reconstruction process. The\nevaluations conducted on the MVTec anomaly detection dataset demonstrate that\nour method outperforms the previous state-of-the-art approach, particularly in\nterms of object classes. To evaluate generalizability, we generate a simulated\ndataset comprising anomalies with diverse characteristics since the original\ntest samples only include specific types of anomalies and may lead to biased\nevaluations. Experimental results demonstrate that our approach exhibits\npromising potential for generalizing effectively to various unforeseen\nanomalies encountered in real-world scenarios.\n","authors":["Jiang Lin","Yaping Yan"],"pdf_url":"https://arxiv.org/pdf/2308.15068v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03492v1","updated":"2024-08-07T01:03:56Z","published":"2024-08-07T01:03:56Z","title":"Automated Theorem Provers Help Improve Large Language Model Reasoning","summary":"  In this paper we demonstrate how logic programming systems and Automated\nfirst-order logic Theorem Provers (ATPs) can improve the accuracy of Large\nLanguage Models (LLMs) for logical reasoning tasks where the baseline\nperformance is given by direct LLM solutions. We first evaluate LLM reasoning\non steamroller problems using the PRONTOQA benchmark. We show how accuracy can\nbe improved with a neuro-symbolic architecture where the LLM acts solely as a\nfront-end for translating a given problem into a formal logic language and an\nautomated reasoning engine is called for solving it. However, this approach\ncritically hinges on the correctness of the LLM translation. To assess this\ntranslation correctness, we secondly define a framework of syntactic and\nsemantic error categories. We implemented the framework and used it to identify\nerrors that LLMs make in the benchmark domain. Based on these findings, we\nthirdly extended our method with capabilities for automatically correcting\nsyntactic and semantic errors. For semantic error correction we integrate\nfirst-order logic ATPs, which is our main and novel contribution. We\ndemonstrate that this approach reduces semantic errors significantly and\nfurther increases the accurracy of LLM logical reasoning.\n","authors":["Lachlan McGinness","Peter Baumgartner"],"pdf_url":"https://arxiv.org/pdf/2408.03492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00114v2","updated":"2024-08-07T00:52:07Z","published":"2024-07-31T18:47:11Z","title":"Inductive or Deductive? Rethinking the Fundamental Reasoning Abilities\n  of LLMs","summary":"  Reasoning encompasses two typical types: deductive reasoning and inductive\nreasoning. Despite extensive research into the reasoning capabilities of Large\nLanguage Models (LLMs), most studies have failed to rigorously differentiate\nbetween inductive and deductive reasoning, leading to a blending of the two.\nThis raises an essential question: In LLM reasoning, which poses a greater\nchallenge - deductive or inductive reasoning? While the deductive reasoning\ncapabilities of LLMs, (i.e. their capacity to follow instructions in reasoning\ntasks), have received considerable attention, their abilities in true inductive\nreasoning remain largely unexplored. To investigate into the true inductive\nreasoning capabilities of LLMs, we propose a novel framework, SolverLearner.\nThis framework enables LLMs to learn the underlying function (i.e., $y =\nf_w(x)$), that maps input data points $(x)$ to their corresponding output\nvalues $(y)$, using only in-context examples. By focusing on inductive\nreasoning and separating it from LLM-based deductive reasoning, we can isolate\nand investigate inductive reasoning of LLMs in its pure form via SolverLearner.\nOur observations reveal that LLMs demonstrate remarkable inductive reasoning\ncapabilities through SolverLearner, achieving near-perfect performance with ACC\nof 1 in most cases. Surprisingly, despite their strong inductive reasoning\nabilities, LLMs tend to relatively lack deductive reasoning capabilities,\nparticularly in tasks involving ``counterfactual'' reasoning.\n","authors":["Kewei Cheng","Jingfeng Yang","Haoming Jiang","Zhengyang Wang","Binxuan Huang","Ruirui Li","Shiyang Li","Zheng Li","Yifan Gao","Xian Li","Bing Yin","Yizhou Sun"],"pdf_url":"https://arxiv.org/pdf/2408.00114v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03489v1","updated":"2024-08-07T00:48:49Z","published":"2024-08-07T00:48:49Z","title":"Harnessing the Power of LLMs in Source Code Vulnerability Detection","summary":"  Software vulnerabilities, caused by unintentional flaws in source code, are a\nprimary root cause of cyberattacks. Static analysis of source code has been\nwidely used to detect these unintentional defects introduced by software\ndevelopers. Large Language Models (LLMs) have demonstrated human-like\nconversational abilities due to their capacity to capture complex patterns in\nsequential data, such as natural languages. In this paper, we harness LLMs'\ncapabilities to analyze source code and detect known vulnerabilities. To ensure\nthe proposed vulnerability detection method is universal across multiple\nprogramming languages, we convert source code to LLVM IR and train LLMs on\nthese intermediate representations. We conduct extensive experiments on various\nLLM architectures and compare their accuracy. Our comprehensive experiments on\nreal-world and synthetic codes from NVD and SARD demonstrate high accuracy in\nidentifying source code vulnerabilities.\n","authors":["Andrew A Mahyari"],"pdf_url":"https://arxiv.org/pdf/2408.03489v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2408.03281v2","updated":"2024-08-07T01:00:55Z","published":"2024-08-06T16:28:30Z","title":"StructEval: Deepen and Broaden Large Language Model Assessment via\n  Structured Evaluation","summary":"  Evaluation is the baton for the development of large language models. Current\nevaluations typically employ a single-item assessment paradigm for each atomic\ntest objective, which struggles to discern whether a model genuinely possesses\nthe required capabilities or merely memorizes/guesses the answers to specific\nquestions. To this end, we propose a novel evaluation framework referred to as\nStructEval. Starting from an atomic test objective, StructEval deepens and\nbroadens the evaluation by conducting a structured assessment across multiple\ncognitive levels and critical concepts, and therefore offers a comprehensive,\nrobust and consistent evaluation for LLMs. Experiments on three widely-used\nbenchmarks demonstrate that StructEval serves as a reliable tool for resisting\nthe risk of data contamination and reducing the interference of potential\nbiases, thereby providing more reliable and consistent conclusions regarding\nmodel capabilities. Our framework also sheds light on the design of future\nprincipled and trustworthy LLM evaluation protocols.\n","authors":["Boxi Cao","Mengjie Ren","Hongyu Lin","Xianpei Han","Feng Zhang","Junfeng Zhan","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2408.03281v2.pdf","comment":"ACL 2024;Benchmark at https://github.com/c-box/StructEval\n  ;Leaderboard at https://huggingface.co/spaces/Bowieee/StructEval_leaderboard"},{"id":"http://arxiv.org/abs/2408.03936v1","updated":"2024-08-07T17:54:21Z","published":"2024-08-07T17:54:21Z","title":"SLIM-RAFT: A Novel Fine-Tuning Approach to Improve Cross-Linguistic\n  Performance for Mercosur Common Nomenclature","summary":"  Natural language processing (NLP) has seen significant advancements with the\nadvent of large language models (LLMs). However, substantial improvements are\nstill needed for languages other than English, especially for specific domains\nlike the applications of Mercosur Common Nomenclature (NCM), a Brazilian\nHarmonized System (HS). To address this gap, this study uses TeenyTineLLaMA, a\nfoundational Portuguese LLM, as an LLM source to implement the NCM application\nprocessing. Additionally, a simplified Retrieval-Augmented Fine-Tuning (RAFT)\ntechnique, termed SLIM-RAFT, is proposed for task-specific fine-tuning of LLMs.\nThis approach retains the chain-of-thought (CoT) methodology for prompt\ndevelopment in a more concise and streamlined manner, utilizing brief and\nfocused documents for training. The proposed model demonstrates an efficient\nand cost-effective alternative for fine-tuning smaller LLMs, significantly\noutperforming TeenyTineLLaMA and ChatGPT-4 in the same task. Although the\nresearch focuses on NCM applications, the methodology can be easily adapted for\nHS applications worldwide.\n","authors":["Vinícius Di Oliveira","Yuri Façanha Bezerra","Li Weigang","Pedro Carvalho Brom","Victor Rafael R. Celestino"],"pdf_url":"https://arxiv.org/pdf/2408.03936v1.pdf","comment":"13 pages, 1 figure, to be publish in International Conference on Web\n  Information Systems and Technologies - WEBIST 2024 proceedings"},{"id":"http://arxiv.org/abs/2310.12294v3","updated":"2024-08-07T17:46:32Z","published":"2023-10-18T19:55:11Z","title":"Open-Set Multivariate Time-Series Anomaly Detection","summary":"  Numerous methods for time-series anomaly detection (TSAD) have emerged in\nrecent years, most of which are unsupervised and assume that only normal\nsamples are available during the training phase, due to the challenge of\nobtaining abnormal data in real-world scenarios. Still, limited samples of\nabnormal data are often available, albeit they are far from representative of\nall possible anomalies. Supervised methods can be utilized to classify normal\nand seen anomalies, but they tend to overfit to the seen anomalies present\nduring training, hence, they fail to generalize to unseen anomalies. We propose\nthe first algorithm to address the open-set TSAD problem, called Multivariate\nOpen-Set Time-Series Anomaly Detector (MOSAD), that leverages only a few shots\nof labeled anomalies during the training phase in order to achieve superior\nanomaly detection performance compared to both supervised and unsupervised TSAD\nalgorithms. MOSAD is a novel multi-head TSAD framework with a shared\nrepresentation space and specialized heads, including the Generative head, the\nDiscriminative head, and the Anomaly-Aware Contrastive head. The latter\nproduces a superior representation space for anomaly detection compared to\nconventional supervised contrastive learning. Extensive experiments on three\nreal-world datasets establish MOSAD as a new state-of-the-art in the TSAD\nfield.\n","authors":["Thomas Lai","Thi Kieu Khanh Ho","Narges Armanfard"],"pdf_url":"https://arxiv.org/pdf/2310.12294v3.pdf","comment":"Accepted to ECAI-2024"},{"id":"http://arxiv.org/abs/2408.03915v1","updated":"2024-08-07T17:20:52Z","published":"2024-08-07T17:20:52Z","title":"Hard to Explain: On the Computational Hardness of In-Distribution Model\n  Interpretation","summary":"  The ability to interpret Machine Learning (ML) models is becoming\nincreasingly essential. However, despite significant progress in the field,\nthere remains a lack of rigorous characterization regarding the innate\ninterpretability of different models. In an attempt to bridge this gap, recent\nwork has demonstrated that it is possible to formally assess interpretability\nby studying the computational complexity of explaining the decisions of various\nmodels. In this setting, if explanations for a particular model can be obtained\nefficiently, the model is considered interpretable (since it can be explained\n``easily''). However, if generating explanations over an ML model is\ncomputationally intractable, it is considered uninterpretable. Prior research\nidentified two key factors that influence the complexity of interpreting an ML\nmodel: (i) the type of the model (e.g., neural networks, decision trees, etc.);\nand (ii) the form of explanation (e.g., contrastive explanations, Shapley\nvalues, etc.). In this work, we claim that a third, important factor must also\nbe considered for this analysis -- the underlying distribution over which the\nexplanation is obtained. Considering the underlying distribution is key in\navoiding explanations that are socially misaligned, i.e., convey information\nthat is biased and unhelpful to users. We demonstrate the significant influence\nof the underlying distribution on the resulting overall interpretation\ncomplexity, in two settings: (i) prediction models paired with an external\nout-of-distribution (OOD) detector; and (ii) prediction models designed to\ninherently generate socially aligned explanations. Our findings prove that the\nexpressiveness of the distribution can significantly influence the overall\ncomplexity of interpretation, and identify essential prerequisites that a model\nmust possess to generate socially aligned explanations.\n","authors":["Guy Amir","Shahaf Bassan","Guy Katz"],"pdf_url":"https://arxiv.org/pdf/2408.03915v1.pdf","comment":"To appear in ECAI 2024"},{"id":"http://arxiv.org/abs/2408.03913v1","updated":"2024-08-07T17:19:15Z","published":"2024-08-07T17:19:15Z","title":"AdapMTL: Adaptive Pruning Framework for Multitask Learning Model","summary":"  In the domain of multimedia and multimodal processing, the efficient handling\nof diverse data streams such as images, video, and sensor data is paramount.\nModel compression and multitask learning (MTL) are crucial in this field,\noffering the potential to address the resource-intensive demands of processing\nand interpreting multiple forms of media simultaneously. However, effectively\ncompressing a multitask model presents significant challenges due to the\ncomplexities of balancing sparsity allocation and accuracy performance across\nmultiple tasks. To tackle these challenges, we propose AdapMTL, an adaptive\npruning framework for MTL models. AdapMTL leverages multiple learnable soft\nthresholds independently assigned to the shared backbone and the task-specific\nheads to capture the nuances in different components' sensitivity to pruning.\nDuring training, it co-optimizes the soft thresholds and MTL model weights to\nautomatically determine the suitable sparsity level at each component to\nachieve both high task accuracy and high overall sparsity. It further\nincorporates an adaptive weighting mechanism that dynamically adjusts the\nimportance of task-specific losses based on each task's robustness to pruning.\nWe demonstrate the effectiveness of AdapMTL through comprehensive experiments\non popular multitask datasets, namely NYU-v2 and Tiny-Taskonomy, with different\narchitectures, showcasing superior performance compared to state-of-the-art\npruning methods.\n","authors":["Mingcan Xiang","Steven Jiaxun Tang","Qizheng Yang","Hui Guan","Tongping Liu"],"pdf_url":"https://arxiv.org/pdf/2408.03913v1.pdf","comment":"13 pages, 9 figures, Published at ACM Multimedia (ACM MM) 2024"},{"id":"http://arxiv.org/abs/2408.03909v1","updated":"2024-08-07T17:13:46Z","published":"2024-08-07T17:13:46Z","title":"LaFA: Latent Feature Attacks on Non-negative Matrix Factorization","summary":"  As Machine Learning (ML) applications rapidly grow, concerns about\nadversarial attacks compromising their reliability have gained significant\nattention. One unsupervised ML method known for its resilience to such attacks\nis Non-negative Matrix Factorization (NMF), an algorithm that decomposes input\ndata into lower-dimensional latent features. However, the introduction of\npowerful computational tools such as Pytorch enables the computation of\ngradients of the latent features with respect to the original data, raising\nconcerns about NMF's reliability. Interestingly, naively deriving the\nadversarial loss for NMF as in the case of ML would result in the\nreconstruction loss, which can be shown theoretically to be an ineffective\nattacking objective. In this work, we introduce a novel class of attacks in NMF\ntermed Latent Feature Attacks (LaFA), which aim to manipulate the latent\nfeatures produced by the NMF process. Our method utilizes the Feature Error\n(FE) loss directly on the latent features. By employing FE loss, we generate\nperturbations in the original data that significantly affect the extracted\nlatent features, revealing vulnerabilities akin to those found in other ML\ntechniques. To handle large peak-memory overhead from gradient back-propagation\nin FE attacks, we develop a method based on implicit differentiation which\nenables their scaling to larger datasets. We validate NMF vulnerabilities and\nFE attacks effectiveness through extensive experiments on synthetic and\nreal-world data.\n","authors":["Minh Vu","Ben Nebgen","Erik Skau","Geigh Zollicoffer","Juan Castorena","Kim Rasmussen","Boian Alexandrov","Manish Bhattarai"],"pdf_url":"https://arxiv.org/pdf/2408.03909v1.pdf","comment":"LA-UR-24-26951"},{"id":"http://arxiv.org/abs/2402.14049v2","updated":"2024-08-07T17:09:10Z","published":"2024-02-21T18:25:04Z","title":"Generative Adversarial Models for Extreme Geospatial Downscaling","summary":"  Addressing the challenges of climate change requires accurate and\nhigh-resolution mapping of geospatial data, especially climate and weather\nvariables. However, many existing geospatial datasets, such as the gridded\noutputs of the state-of-the-art numerical climate models (e.g., general\ncirculation models), are only available at very coarse spatial resolutions due\nto the model complexity and extremely high computational demand.\nDeep-learning-based methods, particularly generative adversarial networks\n(GANs) and their variants, have proved effective for refining natural images\nand have shown great promise in improving geospatial datasets. This paper\ndescribes a conditional GAN-based stochastic geospatial downscaling method that\ncan accommodates very high scaling factors. Compared to most existing methods,\nthe method can generate high-resolution accurate climate datasets from very\nlow-resolution inputs. More importantly, the method explicitly considers the\nuncertainty inherent to the downscaling process that tends to be ignored in\nexisting methods. Given an input, the method can produce a multitude of\nplausible high-resolution samples instead of one single deterministic result.\nThese samples allow for an empirical exploration and inferences of model\nuncertainty and robustness. With a case study of gridded climate datasets (wind\nvelocity and solar irradiance), we demonstrate the performances of the\nframework in downscaling tasks with large scaling factors (up to $64\\times$)\nand highlight the advantages of the framework with a comprehensive comparison\nwith commonly used and most recent downscaling methods, including area-to-point\n(ATP) kriging, deep image prior (DIP), enhanced super-resolution generative\nadversarial networks (ESRGAN), physics-informed resolution-enhancing GAN (PhIRE\nGAN), and an efficient diffusion model for remote sensing image\nsuper-resolution (EDiffSR).\n","authors":["Guiye Li","Guofeng Cao"],"pdf_url":"https://arxiv.org/pdf/2402.14049v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13218v3","updated":"2024-08-07T16:57:06Z","published":"2024-07-18T07:04:33Z","title":"LiNR: Model Based Neural Retrieval on GPUs at LinkedIn","summary":"  This paper introduces LiNR, LinkedIn's large-scale, GPU-based retrieval\nsystem. LiNR supports a billion-sized index on GPU models. We discuss our\nexperiences and challenges in creating scalable, differentiable search indexes\nusing TensorFlow and PyTorch at production scale. In LiNR, both items and model\nweights are integrated into the model binary. Viewing index construction as a\nform of model training, we describe scaling our system for large indexes,\nincorporating full scans and efficient filtering. A key focus is on enabling\nattribute-based pre-filtering for exhaustive GPU searches, addressing the\ncommon challenge of post-filtering in KNN searches that often reduces system\nquality. We further provide multi-embedding retrieval algorithms and strategies\nfor tackling cold start issues in retrieval. Our advancements in supporting\nlarger indexes through quantization are also discussed. We believe LiNR\nrepresents one of the industry's first Live-updated model-based retrieval\nindexes. Applied to out-of-network post recommendations on LinkedIn Feed, LiNR\nhas contributed to a 3% relative increase in professional daily active users.\nWe envisage LiNR as a step towards integrating retrieval and ranking into a\nsingle GPU model, simplifying complex infrastructures and enabling end-to-end\noptimization of the entire differentiable infrastructure through gradient\ndescent.\n","authors":["Fedor Borisyuk","Qingquan Song","Mingzhou Zhou","Ganesh Parameswaran","Madhu Arun","Siva Popuri","Tugrul Bingol","Zhuotao Pei","Kuang-Hsuan Lee","Lu Zheng","Qizhan Shao","Ali Naqvi","Sen Zhou","Aman Gupta"],"pdf_url":"https://arxiv.org/pdf/2407.13218v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.04262v3","updated":"2024-08-07T16:54:40Z","published":"2023-02-08T18:55:49Z","title":"Algorithmic Collective Action in Machine Learning","summary":"  We initiate a principled study of algorithmic collective action on digital\nplatforms that deploy machine learning algorithms. We propose a simple\ntheoretical model of a collective interacting with a firm's learning algorithm.\nThe collective pools the data of participating individuals and executes an\nalgorithmic strategy by instructing participants how to modify their own data\nto achieve a collective goal. We investigate the consequences of this model in\nthree fundamental learning-theoretic settings: the case of a nonparametric\noptimal learning algorithm, a parametric risk minimizer, and gradient-based\noptimization. In each setting, we come up with coordinated algorithmic\nstrategies and characterize natural success criteria as a function of the\ncollective's size. Complementing our theory, we conduct systematic experiments\non a skill classification task involving tens of thousands of resumes from a\ngig platform for freelancers. Through more than two thousand model training\nruns of a BERT-like language model, we see a striking correspondence emerge\nbetween our empirical observations and the predictions made by our theory.\nTaken together, our theory and experiments broadly support the conclusion that\nalgorithmic collectives of exceedingly small fractional size can exert\nsignificant control over a platform's learning algorithm.\n","authors":["Moritz Hardt","Eric Mazumdar","Celestine Mendler-Dünner","Tijana Zrnic"],"pdf_url":"https://arxiv.org/pdf/2302.04262v3.pdf","comment":"Published at ICML 2023; Revision corrects epsilon-dependence in the\n  analysis"},{"id":"http://arxiv.org/abs/2402.06859v2","updated":"2024-08-07T16:54:23Z","published":"2024-02-10T01:47:10Z","title":"LiRank: Industrial Large Scale Ranking Models at LinkedIn","summary":"  We present LiRank, a large-scale ranking framework at LinkedIn that brings to\nproduction state-of-the-art modeling architectures and optimization methods. We\nunveil several modeling improvements, including Residual DCN, which adds\nattention and residual connections to the famous DCNv2 architecture. We share\ninsights into combining and tuning SOTA architectures to create a unified\nmodel, including Dense Gating, Transformers and Residual DCN. We also propose\nnovel techniques for calibration and describe how we productionalized deep\nlearning based explore/exploit methods. To enable effective, production-grade\nserving of large ranking models, we detail how to train and compress models\nusing quantization and vocabulary compression. We provide details about the\ndeployment setup for large-scale use cases of Feed ranking, Jobs\nRecommendations, and Ads click-through rate (CTR) prediction. We summarize our\nlearnings from various A/B tests by elucidating the most effective technical\napproaches. These ideas have contributed to relative metrics improvements\nacross the board at LinkedIn: +0.5% member sessions in the Feed, +1.76%\nqualified job applications for Jobs search and recommendations, and +4.3% for\nAds CTR. We hope this work can provide practical insights and solutions for\npractitioners interested in leveraging large-scale deep ranking systems.\n","authors":["Fedor Borisyuk","Mingzhou Zhou","Qingquan Song","Siyu Zhu","Birjodh Tiwana","Ganesh Parameswaran","Siddharth Dangi","Lars Hertel","Qiang Xiao","Xiaochen Hou","Yunbo Ouyang","Aman Gupta","Sheallika Singh","Dan Liu","Hailing Cheng","Lei Le","Jonathan Hung","Sathiya Keerthi","Ruoyan Wang","Fengyu Zhang","Mohit Kothari","Chen Zhu","Daqi Sun","Yun Dai","Xun Luan","Sirou Zhu","Zhiwei Wang","Neil Daftary","Qianqi Shen","Chengming Jiang","Haichao Wei","Maneesh Varshney","Amol Ghoting","Souvik Ghosh"],"pdf_url":"https://arxiv.org/pdf/2402.06859v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03877v1","updated":"2024-08-07T16:27:45Z","published":"2024-08-07T16:27:45Z","title":"Knowledge Probing for Graph Representation Learning","summary":"  Graph learning methods have been extensively applied in diverse application\nareas. However, what kind of inherent graph properties e.g. graph proximity,\ngraph structural information has been encoded into graph representation\nlearning for downstream tasks is still under-explored. In this paper, we\npropose a novel graph probing framework (GraphProbe) to investigate and\ninterpret whether the family of graph learning methods has encoded different\nlevels of knowledge in graph representation learning. Based on the intrinsic\nproperties of graphs, we design three probes to systematically investigate the\ngraph representation learning process from different perspectives, respectively\nthe node-wise level, the path-wise level, and the structural level. We\nconstruct a thorough evaluation benchmark with nine representative graph\nlearning methods from random walk based approaches, basic graph neural networks\nand self-supervised graph methods, and probe them on six benchmark datasets for\nnode classification, link prediction and graph classification. The experimental\nevaluation verify that GraphProbe can estimate the capability of graph\nrepresentation learning. Remaking results have been concluded: GCN and\nWeightedGCN methods are relatively versatile methods achieving better results\nwith respect to different tasks.\n","authors":["Mingyu Zhao","Xingyu Huang","Ziyu Lyu","Yanlin Wang","Lixin Cui","Lu Bai"],"pdf_url":"https://arxiv.org/pdf/2408.03877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03872v1","updated":"2024-08-07T16:22:21Z","published":"2024-08-07T16:22:21Z","title":"Inter-Series Transformer: Attending to Products in Time Series\n  Forecasting","summary":"  Time series forecasting is an important task in many fields ranging from\nsupply chain management to weather forecasting. Recently, Transformer neural\nnetwork architectures have shown promising results in forecasting on common\ntime series benchmark datasets. However, application to supply chain demand\nforecasting, which can have challenging characteristics such as sparsity and\ncross-series effects, has been limited.\n  In this work, we explore the application of Transformer-based models to\nsupply chain demand forecasting. In particular, we develop a new\nTransformer-based forecasting approach using a shared, multi-task per-time\nseries network with an initial component applying attention across time series,\nto capture interactions and help address sparsity. We provide a case study\napplying our approach to successfully improve demand prediction for a medical\ndevice manufacturing company. To further validate our approach, we also apply\nit to public demand forecasting datasets as well and demonstrate competitive to\nsuperior performance compared to a variety of baseline and state-of-the-art\nforecast methods across the private and public datasets.\n","authors":["Rares Cristian","Pavithra Harsha","Clemente Ocejo","Georgia Perakis","Brian Quanz","Ioannis Spantidakis","Hamza Zerhouni"],"pdf_url":"https://arxiv.org/pdf/2408.03872v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08973v2","updated":"2024-08-07T16:21:25Z","published":"2024-04-13T11:40:05Z","title":"PraFFL: A Preference-Aware Scheme in Fair Federated Learning","summary":"  Fairness in federated learning has emerged as a critical concern, aiming to\ndevelop an unbiased model for any special group (e.g., male or female) of\nsensitive features. However, there is a trade-off between model performance and\nfairness, i.e., improving model fairness will decrease model performance.\nExisting approaches have characterized such a trade-off by introducing\nhyperparameters to quantify client's preferences for model fairness and model\nperformance. Nevertheless, these approaches are limited to scenarios where each\nclient has only a single pre-defined preference, and fail to work in practical\nsystems where each client generally have multiple preferences. The key\nchallenge is to design a method that allows the model to adapt to diverse\npreferences of each client in real time. To this end, we propose a\nPreference-aware scheme in Fair Federated Learning paradigm (called PraFFL) to\ngenerate preference-wise model in real time. PraFFL can adaptively adjust the\nmodel based on each client's preferences to meet their needs. We theoretically\nprove that PraFFL can offer the optimal model tailored to an arbitrary\npreference of each client, and show its linear convergence. Experimental\nresults show that our proposed PraFFL outperforms five fair federated learning\nalgorithms in terms of the model's capability of adapting to clients' different\npreferences.\n","authors":["Rongguang Ye","Wei-Bin Kou","Ming Tang"],"pdf_url":"https://arxiv.org/pdf/2404.08973v2.pdf","comment":"14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.02426v2","updated":"2024-08-07T16:20:56Z","published":"2024-03-04T19:18:53Z","title":"Digital Twins and Civil Engineering Phases: Reorienting Adoption\n  Strategies","summary":"  Digital twin (DT) technology has received immense attention over the years\ndue to the promises it presents to various stakeholders in science and\nengineering. As a result, different thematic areas of DT have been explored.\nThis is no different in specific fields such as manufacturing, automation, oil\nand gas, and civil engineering, leading to fragmented approaches for\nfield-specific applications. The civil engineering industry is further\ndisadvantaged in this regard as it relies on external techniques by other\nengineering fields for its DT adoption. A rising consequence of these\nextensions is a concentrated application of DT to the operations and\nmaintenance phase. On another spectrum, Building Information Modeling (BIM) is\npervasively utilized in the planning/design phase, and the transient nature of\nthe construction phase remains a challenge for its DT adoption. In this paper,\nwe present a phase-based development of DT in the Architecture, Engineering,\nand Construction industry. We commence by presenting succinct expositions on DT\nas a concept and as a service, and establish a five-level scale system.\nFurthermore, we present separately a systematic literature review of the\nconventional techniques employed at each civil engineering phase. In this\nregard, we identified enabling technologies such as computer vision for\nextended sensing and the Internet of Things for reliable integration.\nUltimately, we attempt to reveal DT as an important tool across the entire life\ncycle of civil engineering projects, and nudge researchers to think more\nholistically in their quest for the integration of DT for civil engineering\napplications.\n","authors":["Taiwo A. Adebiyi","Nafeezat A. Ajenifuja","Ruda Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.02426v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03865v1","updated":"2024-08-07T16:13:43Z","published":"2024-08-07T16:13:43Z","title":"PackMamba: Efficient Processing of Variable-Length Sequences in Mamba\n  training","summary":"  With the evolution of large language models, traditional Transformer models\nbecome computationally demanding for lengthy sequences due to the quadratic\ngrowth in computation with respect to the sequence length. Mamba, emerging as a\ngroundbreaking architecture in the field of generative AI, demonstrates\nremarkable proficiency in handling elongated sequences with reduced\ncomputational and memory complexity. Nevertheless, the existing training\nframework of Mamba presents inefficiency with variable-length sequence inputs.\nEither single-sequence training results in low GPU utilization, or batched\nprocessing of variable-length sequences to a maximum length incurs considerable\nmemory and computational overhead. To address this problem, we analyze the\nperformance of bottleneck operators in Mamba under diverse tensor shapes and\nproposed PackMamba, a high-throughput Mamba that efficiently handles\nvariable-length sequences. Diving deep into state-space models (SSMs), we\nmodify the parallel operators to avoid passing information between individual\nsequences while maintaining high performance. Experimental results on an NVIDIA\nA100 GPU demonstrate throughput exceeding the baseline single-sequence\nprocessing scheme: 3.06x speedup on the 1.4B model and 2.62x on the 2.8B model.\n","authors":["Haoran Xu","Ziqian Liu","Rong Fu","Zhongling Su","Zerui Wang","Zheng Cai","Zhilin Pei","Xingcheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.03865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15636v3","updated":"2024-08-07T16:07:05Z","published":"2024-05-24T15:22:58Z","title":"Visualize and Paint GAN Activations","summary":"  We investigate how generated structures of GANs correlate with their\nactivations in hidden layers, with the purpose of better understanding the\ninner workings of those models and being able to paint structures with\nunconditionally trained GANs. This gives us more control over the generated\nimages, allowing to generate them from a semantic segmentation map while not\nrequiring such a segmentation in the training data. To this end we introduce\nthe concept of tileable features, allowing us to identify activations that work\nwell for painting.\n","authors":["Rudolf Herdt","Peter Maass"],"pdf_url":"https://arxiv.org/pdf/2405.15636v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05941v3","updated":"2024-08-07T15:58:17Z","published":"2023-11-10T08:54:51Z","title":"Out-of-Distribution-Aware Electric Vehicle Charging","summary":"  We tackle the challenge of learning to charge Electric Vehicles (EVs) with\nOut-of-Distribution (OOD) data. Traditional scheduling algorithms typically\nfail to balance near-optimal average performance with worst-case guarantees,\nparticularly with OOD data. Model Predictive Control (MPC) is often too\nconservative and data-independent, whereas Reinforcement Learning (RL) tends to\nbe overly aggressive and fully trusts the data, hindering their ability to\nconsistently achieve the best-of-both-worlds. To bridge this gap, we introduce\na novel OOD-aware scheduling algorithm, denoted OOD-Charging. This algorithm\nemploys a dynamic \"awareness radius\", which updates in real-time based on the\nTemporal Difference (TD)-error that reflects the severity of OOD. The\nOOD-Charging algorithm allows for a more effective balance between consistency\nand robustness in EV charging schedules, thereby significantly enhancing\nadaptability and efficiency in real-world charging environments. Our results\ndemonstrate that this approach improves the scheduling reward reliably under\nreal OOD scenarios with remarkable shifts of EV charging behaviors caused by\nCOVID-19 in the Caltech ACN-Data.\n","authors":["Tongxin Li","Chenxi Sun"],"pdf_url":"https://arxiv.org/pdf/2311.05941v3.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.03849v1","updated":"2024-08-07T15:46:45Z","published":"2024-08-07T15:46:45Z","title":"Hate Speech Detection and Classification in Amharic Text with Deep\n  Learning","summary":"  Hate speech is a growing problem on social media. It can seriously impact\nsociety, especially in countries like Ethiopia, where it can trigger conflicts\namong diverse ethnic and religious groups. While hate speech detection in\nresource rich languages are progressing, for low resource languages such as\nAmharic are lacking. To address this gap, we develop Amharic hate speech data\nand SBi-LSTM deep learning model that can detect and classify text into four\ncategories of hate speech: racial, religious, gender, and non-hate speech. We\nhave annotated 5k Amharic social media post and comment data into four\ncategories. The data is annotated using a custom annotation tool by a total of\n100 native Amharic speakers. The model achieves a 94.8 F1-score performance.\nFuture improvements will include expanding the dataset and develop state-of-the\nart models.\n  Keywords: Amharic hate speech detection, classification, Amharic dataset,\nDeep Learning, SBi-LSTM\n","authors":["Samuel Minale Gashe","Seid Muhie Yimam","Yaregal Assabie"],"pdf_url":"https://arxiv.org/pdf/2408.03849v1.pdf","comment":"Dataset: https://data.mendeley.com/datasets/p74pfhz3yx/1"},{"id":"http://arxiv.org/abs/2306.07350v3","updated":"2024-08-07T15:36:15Z","published":"2023-06-12T18:16:33Z","title":"G-invariant diffusion maps","summary":"  The diffusion maps embedding of data lying on a manifold has shown success in\ntasks such as dimensionality reduction, clustering, and data visualization. In\nthis work, we consider embedding data sets that were sampled from a manifold\nwhich is closed under the action of a continuous matrix group. An example of\nsuch a data set is images whose planar rotations are arbitrary. The G-invariant\ngraph Laplacian, introduced in Part I of this work, admits eigenfunctions in\nthe form of tensor products between the elements of the irreducible unitary\nrepresentations of the group and eigenvectors of certain matrices. We employ\nthese eigenfunctions to derive diffusion maps that intrinsically account for\nthe group action on the data. In particular, we construct both equivariant and\ninvariant embeddings, which can be used to cluster and align the data points.\nWe demonstrate the utility of our construction in the problem of random\ncomputerized tomography.\n","authors":["Eitan Rosen","Xiuyuan Cheng","Yoel Shkolnisky"],"pdf_url":"https://arxiv.org/pdf/2306.07350v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03842v1","updated":"2024-08-07T15:35:25Z","published":"2024-08-07T15:35:25Z","title":"Bi-Level Spatial and Channel-aware Transformer for Learned Image\n  Compression","summary":"  Recent advancements in learned image compression (LIC) methods have\ndemonstrated superior performance over traditional hand-crafted codecs. These\nlearning-based methods often employ convolutional neural networks (CNNs) or\nTransformer-based architectures. However, these nonlinear approaches frequently\noverlook the frequency characteristics of images, which limits their\ncompression efficiency. To address this issue, we propose a novel\nTransformer-based image compression method that enhances the transformation\nstage by considering frequency components within the feature map. Our method\nintegrates a novel Hybrid Spatial-Channel Attention Transformer Block (HSCATB),\nwhere a spatial-based branch independently handles high and low frequencies at\nthe attention layer, and a Channel-aware Self-Attention (CaSA) module captures\ninformation across channels, significantly improving compression performance.\nAdditionally, we introduce a Mixed Local-Global Feed Forward Network (MLGFFN)\nwithin the Transformer block to enhance the extraction of diverse and rich\ninformation, which is crucial for effective compression. These innovations\ncollectively improve the transformation's ability to project data into a more\ndecorrelated latent space, thereby boosting overall compression efficiency.\nExperimental results demonstrate that our framework surpasses state-of-the-art\nLIC methods in rate-distortion performance.\n","authors":["Hamidreza Soltani","Erfan Ghasemi"],"pdf_url":"https://arxiv.org/pdf/2408.03842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19588v2","updated":"2024-08-07T15:11:01Z","published":"2024-03-28T17:12:39Z","title":"DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs","summary":"  This paper revives Densely Connected Convolutional Networks (DenseNets) and\nreveals the underrated effectiveness over predominant ResNet-style\narchitectures. We believe DenseNets' potential was overlooked due to untouched\ntraining methods and traditional design elements not fully revealing their\ncapabilities. Our pilot study shows dense connections through concatenation are\nstrong, demonstrating that DenseNets can be revitalized to compete with modern\narchitectures. We methodically refine suboptimal components - architectural\nadjustments, block redesign, and improved training recipes towards widening\nDenseNets and boosting memory efficiency while keeping concatenation shortcuts.\nOur models, employing simple architectural elements, ultimately surpass Swin\nTransformer, ConvNeXt, and DeiT-III - key architectures in the residual\nlearning lineage. Furthermore, our models exhibit near state-of-the-art\nperformance on ImageNet-1K, competing with the very recent models and\ndownstream tasks, ADE20k semantic segmentation, and COCO object\ndetection/instance segmentation. Finally, we provide empirical analyses that\nuncover the merits of the concatenation over additive shortcuts, steering a\nrenewed preference towards DenseNet-style designs. Our code is available at\nhttps://github.com/naver-ai/rdnet.\n","authors":["Donghyun Kim","Byeongho Heo","Dongyoon Han"],"pdf_url":"https://arxiv.org/pdf/2403.19588v2.pdf","comment":"ECCV 2024. Code at https://github.com/naver-ai/rdnet"},{"id":"http://arxiv.org/abs/2301.04660v2","updated":"2024-08-07T15:07:41Z","published":"2023-01-11T19:00:00Z","title":"Anomalies, Representations, and Self-Supervision","summary":"  We develop a self-supervised method for density-based anomaly detection using\ncontrastive learning, and test it using event-level anomaly data from CMS\nADC2021. The AnomalyCLR technique is data-driven and uses augmentations of the\nbackground data to mimic non-Standard-Model events in a model-agnostic way. It\nuses a permutation-invariant Transformer Encoder architecture to map the\nobjects measured in a collider event to the representation space, where the\ndata augmentations define a representation space which is sensitive to\npotential anomalous features. An AutoEncoder trained on background\nrepresentations then computes anomaly scores for a variety of signals in the\nrepresentation space. With AnomalyCLR we find significant improvements on\nperformance metrics for all signals when compared to the raw data baseline.\n","authors":["Barry M. Dillon","Luigi Favaro","Friedrich Feiden","Tanmoy Modak","Tilman Plehn"],"pdf_url":"https://arxiv.org/pdf/2301.04660v2.pdf","comment":"19 pages, 3 figures, journal version"},{"id":"http://arxiv.org/abs/2408.03819v1","updated":"2024-08-07T14:55:04Z","published":"2024-08-07T14:55:04Z","title":"Leveraging Variation Theory in Counterfactual Data Augmentation for\n  Optimized Active Learning","summary":"  Active Learning (AL) allows models to learn interactively from user feedback.\nThis paper introduces a counterfactual data augmentation approach to AL,\nparticularly addressing the selection of datapoints for user querying, a\npivotal concern in enhancing data efficiency. Our approach is inspired by\nVariation Theory, a theory of human concept learning that emphasizes the\nessential features of a concept by focusing on what stays the same and what\nchanges. Instead of just querying with existing datapoints, our approach\nsynthesizes artificial datapoints that highlight potential key similarities and\ndifferences among labels using a neuro-symbolic pipeline combining large\nlanguage models (LLMs) and rule-based models. Through an experiment in the\nexample domain of text classification, we show that our approach achieves\nsignificantly higher performance when there are fewer annotated data. As the\nannotated training data gets larger the impact of the generated data starts to\ndiminish showing its capability to address the cold start problem in AL. This\nresearch sheds light on integrating theories of human learning into the\noptimization of AL.\n","authors":["Simret Araya Gebreegziabher","Kuangshi Ai","Zheng Zhang","Elena L. Glassman","Toby Jia-Jun Li"],"pdf_url":"https://arxiv.org/pdf/2408.03819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03816v1","updated":"2024-08-07T14:52:06Z","published":"2024-08-07T14:52:06Z","title":"Early Prediction of Causes (not Effects) in Healthcare by Long-Term\n  Clinical Time Series Forecasting","summary":"  Machine learning for early syndrome diagnosis aims to solve the intricate\ntask of predicting a ground truth label that most often is the outcome (effect)\nof a medical consensus definition applied to observed clinical measurements\n(causes), given clinical measurements observed several hours before. Instead of\nfocusing on the prediction of the future effect, we propose to directly predict\nthe causes via time series forecasting (TSF) of clinical variables and\ndetermine the effect by applying the gold standard consensus definition to the\nforecasted values. This method has the invaluable advantage of being\nstraightforwardly interpretable to clinical practitioners, and because model\ntraining does not rely on a particular label anymore, the forecasted data can\nbe used to predict any consensus-based label. We exemplify our method by means\nof long-term TSF with Transformer models, with a focus on accurate prediction\nof sparse clinical variables involved in the SOFA-based Sepsis-3 definition and\nthe new Simplified Acute Physiology Score (SAPS-II) definition. Our experiments\nare conducted on two datasets and show that contrary to recent proposals which\nadvocate set function encoders for time series and direct multi-step decoders,\nbest results are achieved by a combination of standard dense encoders with\niterative multi-step decoders. The key for success of iterative multi-step\ndecoding can be attributed to its ability to capture cross-variate dependencies\nand to a student forcing training strategy that teaches the model to rely on\nits own previous time step predictions for the next time step prediction.\n","authors":["Michael Staniek","Marius Fracarolli","Michael Hagmann","Stefan Riezler"],"pdf_url":"https://arxiv.org/pdf/2408.03816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03806v1","updated":"2024-08-07T14:32:36Z","published":"2024-08-07T14:32:36Z","title":"Trustworthy Image Semantic Communication with GenAI: Explainablity,\n  Controllability, and Efficiency","summary":"  Image semantic communication (ISC) has garnered significant attention for its\npotential to achieve high efficiency in visual content transmission. However,\nexisting ISC systems based on joint source-channel coding face challenges in\ninterpretability, operability, and compatibility. To address these limitations,\nwe propose a novel trustworthy ISC framework. This approach leverages text\nextraction and segmentation mapping techniques to convert images into\nexplainable semantics, while employing Generative Artificial Intelligence\n(GenAI) for multiple downstream inference tasks. We also introduce a multi-rate\nISC transmission protocol that dynamically adapts to both the received\nexplainable semantic content and specific task requirements at the receiver.\nSimulation results demonstrate that our framework achieves explainable\nlearning, decoupled training, and compatible transmission in various\napplication scenarios. Finally, some intriguing research directions and\napplication scenarios are identified.\n","authors":["Xijun Wang","Dongshan Ye","Chenyuan Feng","Howard H. Yang","Xiang Chen","Tony Q. S. Quek"],"pdf_url":"https://arxiv.org/pdf/2408.03806v1.pdf","comment":"8 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2407.11075v2","updated":"2024-08-07T14:05:28Z","published":"2024-07-13T04:29:36Z","title":"A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)","summary":"  Through this comprehensive survey of Kolmogorov-Arnold Networks(KAN), we have\ngained a thorough understanding of its theoretical foundation, architectural\ndesign, application scenarios, and current research progress. KAN, with its\nunique architecture and flexible activation functions, excels in handling\ncomplex data patterns and nonlinear relationships, demonstrating wide-ranging\napplication potential. While challenges remain, KAN is poised to pave the way\nfor innovative solutions in various fields, potentially revolutionizing how we\napproach complex computational problems.\n","authors":["Yuntian Hou","Di Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.11075v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15793v2","updated":"2024-08-07T13:59:46Z","published":"2024-07-22T16:51:28Z","title":"CLIP with Generative Latent Replay: a Strong Baseline for Incremental\n  Learning","summary":"  With the emergence of Transformers and Vision-Language Models (VLMs) such as\nCLIP, large pre-trained models have become a common strategy to enhance\nperformance in Continual Learning scenarios. This led to the development of\nnumerous prompting strategies to effectively fine-tune transformer-based models\nwithout succumbing to catastrophic forgetting. However, these methods struggle\nto specialize the model on domains significantly deviating from the\npre-training and preserving its zero-shot capabilities. In this work, we\npropose Continual Generative training for Incremental prompt-Learning, a novel\napproach to mitigate forgetting while adapting a VLM, which exploits generative\nreplay to align prompts to tasks. We also introduce a new metric to evaluate\nzero-shot capabilities within CL benchmarks. Through extensive experiments on\ndifferent domains, we demonstrate the effectiveness of our framework in\nadapting to new tasks while improving zero-shot capabilities. Further analysis\nreveals that our approach can bridge the gap with joint prompt tuning. The\ncodebase is available at https://github.com/aimagelab/mammoth.\n","authors":["Emanuele Frascaroli","Aniello Panariello","Pietro Buzzega","Lorenzo Bonicelli","Angelo Porrello","Simone Calderara"],"pdf_url":"https://arxiv.org/pdf/2407.15793v2.pdf","comment":"15 pages, 1 figure. Accepted at the The 35th British Machine Vision\n  Conference 2024 (BMVC 2024), Glasgow, UK"},{"id":"http://arxiv.org/abs/2404.09657v3","updated":"2024-08-07T13:44:01Z","published":"2024-04-15T10:45:12Z","title":"Sampling for Model Predictive Trajectory Planning in Autonomous Driving\n  using Normalizing Flows","summary":"  Alongside optimization-based planners, sampling-based approaches are often\nused in trajectory planning for autonomous driving due to their simplicity.\nModel predictive path integral control is a framework that builds upon\noptimization principles while incorporating stochastic sampling of input\ntrajectories. This paper investigates several sampling approaches for\ntrajectory generation. In this context, normalizing flows originating from the\nfield of variational inference are considered for the generation of sampling\ndistributions, as they model transformations of simple to more complex\ndistributions. Accordingly, learning-based normalizing flow models are trained\nfor a more efficient exploration of the input domain for the task at hand. The\ndeveloped algorithm and the proposed sampling distributions are evaluated in\ntwo simulation scenarios.\n","authors":["Georg Rabenstein","Lars Ullrich","Knut Graichen"],"pdf_url":"https://arxiv.org/pdf/2404.09657v3.pdf","comment":"Accepted to be published as part of the 2024 IEEE Intelligent\n  Vehicles Symposium (IV), Jeju Shinhwa World, Jeju Island, Korea, June 2-5,\n  2024"},{"id":"http://arxiv.org/abs/2408.03765v1","updated":"2024-08-07T13:36:03Z","published":"2024-08-07T13:36:03Z","title":"Reliable Node Similarity Matrix Guided Contrastive Graph Clustering","summary":"  Graph clustering, which involves the partitioning of nodes within a graph\ninto disjoint clusters, holds significant importance for numerous subsequent\napplications. Recently, contrastive learning, known for utilizing supervisory\ninformation, has demonstrated encouraging results in deep graph clustering.\nThis methodology facilitates the learning of favorable node representations for\nclustering by attracting positively correlated node pairs and distancing\nnegatively correlated pairs within the representation space. Nevertheless, a\nsignificant limitation of existing methods is their inadequacy in thoroughly\nexploring node-wise similarity. For instance, some hypothesize that the node\nsimilarity matrix within the representation space is identical, ignoring the\ninherent semantic relationships among nodes. Given the fundamental role of\ninstance similarity in clustering, our research investigates contrastive graph\nclustering from the perspective of the node similarity matrix. We argue that an\nideal node similarity matrix within the representation space should accurately\nreflect the inherent semantic relationships among nodes, ensuring the\npreservation of semantic similarities in the learned representations. In\nresponse to this, we introduce a new framework, Reliable Node Similarity Matrix\nGuided Contrastive Graph Clustering (NS4GC), which estimates an approximately\nideal node similarity matrix within the representation space to guide\nrepresentation learning. Our method introduces node-neighbor alignment and\nsemantic-aware sparsification, ensuring the node similarity matrix is both\naccurate and efficiently sparse. Comprehensive experiments conducted on $8$\nreal-world datasets affirm the efficacy of learning the node similarity matrix\nand the superior performance of NS4GC.\n","authors":["Yunhui Liu","Xinyi Gao","Tieke He","Tao Zheng","Jianhua Zhao","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2408.03765v1.pdf","comment":"Accepted by IEEE Transactions on Knowledge and Data Engineering\n  (TKDE)"},{"id":"http://arxiv.org/abs/2408.01953v2","updated":"2024-08-07T13:24:38Z","published":"2024-08-04T07:59:17Z","title":"EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning","summary":"  Humans perceive and interact with the world with the awareness of\nequivariance, facilitating us in manipulating different objects in diverse\nposes. For robotic manipulation, such equivariance also exists in many\nscenarios. For example, no matter what the pose of a drawer is (translation,\nrotation and tilt), the manipulation strategy is consistent (grasp the handle\nand pull in a line). While traditional models usually do not have the awareness\nof equivariance for robotic manipulation, which might result in more data for\ntraining and poor performance in novel object poses, we propose our EqvAfford\nframework, with novel designs to guarantee the equivariance in point-level\naffordance learning for downstream robotic manipulation, with great performance\nand generalization ability on representative tasks on objects in diverse poses.\n","authors":["Yue Chen","Chenrui Tie","Ruihai Wu","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2408.01953v2.pdf","comment":"Accept to CVPRWorkshop on Equivariant Vision: From Theory to Practice\n  2024"},{"id":"http://arxiv.org/abs/2207.03927v2","updated":"2024-08-07T13:15:55Z","published":"2022-07-08T14:27:52Z","title":"BAST: Binaural Audio Spectrogram Transformer for Binaural Sound\n  Localization","summary":"  Accurate sound localization in a reverberation environment is essential for\nhuman auditory perception. Recently, Convolutional Neural Networks (CNNs) have\nbeen utilized to model the binaural human auditory pathway. However, CNN shows\nbarriers in capturing the global acoustic features. To address this issue, we\npropose a novel end-to-end Binaural Audio Spectrogram Transformer (BAST) model\nto predict the sound azimuth in both anechoic and reverberation environments.\nTwo modes of implementation, i.e. BAST-SP and BAST-NSP corresponding to BAST\nmodel with shared and non-shared parameters respectively, are explored. Our\nmodel with subtraction interaural integration and hybrid loss achieves an\nangular distance of 1.29 degrees and a Mean Square Error of 1e-3 at all\nazimuths, significantly surpassing CNN based model. The exploratory analysis of\nthe BAST's performance on the left-right hemifields and anechoic and\nreverberation environments shows its generalization ability as well as the\nfeasibility of binaural Transformers in sound localization. Furthermore, the\nanalysis of the attention maps is provided to give additional insights on the\ninterpretation of the localization process in a natural reverberant\nenvironment.\n","authors":["Sheng Kuang","Jie Shi","Kiki van der Heijden","Siamak Mehrkanoon"],"pdf_url":"https://arxiv.org/pdf/2207.03927v2.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2402.07118v2","updated":"2024-08-07T13:14:00Z","published":"2024-02-11T07:27:01Z","title":"Next-Generation Teleophthalmology: AI-enabled Quality Assessment Aiding\n  Remote Smartphone-based Consultation","summary":"  Blindness and other eye diseases are a global health concern, particularly in\nlow- and middle-income countries like India. In this regard, during the\nCOVID-19 pandemic, teleophthalmology became a lifeline, and the Grabi\nattachment for smartphone-based eye imaging gained in use. However, quality of\nuser-captured image often remained inadequate, requiring clinician vetting and\ndelays. In this backdrop, we propose an AI-based quality assessment system with\ninstant feedback mimicking clinicians' judgments and tested on patient-captured\nimages. Dividing the complex problem hierarchically, here we tackle a\nnontrivial part, and demonstrate a proof of the concept.\n","authors":["Dhruv Srikanth","Jayang Gurung","N Satya Deepika","Vineet Joshi","Lopamudra Giri","Pravin Vaddavalli","Soumya Jana"],"pdf_url":"https://arxiv.org/pdf/2402.07118v2.pdf","comment":"4 pages, Presented at IEEE EMBC 2024"},{"id":"http://arxiv.org/abs/2404.15213v2","updated":"2024-08-07T13:11:14Z","published":"2024-03-28T10:15:10Z","title":"Automatic Classification of Subjective Time Perception Using Multi-modal\n  Physiological Data of Air Traffic Controllers","summary":"  In high-pressure environments where human individuals must simultaneously\nmonitor multiple entities, communicate effectively, and maintain intense focus,\nthe perception of time becomes a critical factor influencing performance and\nwell-being. One indicator of well-being can be the person's subjective time\nperception. In our project $ChronoPilot$, we aim to develop a device that\nmodulates human subjective time perception. In this study, we present a method\nto automatically assess the subjective time perception of air traffic\ncontrollers, a group often faced with demanding conditions, using their\nphysiological data and eleven state-of-the-art machine learning classifiers.\nThe physiological data consist of photoplethysmogram, electrodermal activity,\nand temperature data. We find that the support vector classifier works best\nwith an accuracy of 79 % and electrodermal activity provides the most\ndescriptive biomarker. These findings are an important step towards closing the\nfeedback loop of our $ChronoPilot$-device to automatically modulate the user's\nsubjective time perception. This technological advancement may promise\nimprovements in task management, stress reduction, and overall productivity in\nhigh-stakes professions.\n","authors":["Till Aust","Eirini Balta","Argiro Vatakis","Heiko Hamann"],"pdf_url":"https://arxiv.org/pdf/2404.15213v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2408.03747v1","updated":"2024-08-07T13:01:10Z","published":"2024-08-07T13:01:10Z","title":"Online Model-based Anomaly Detection in Multivariate Time Series:\n  Taxonomy, Survey, Research Challenges and Future Directions","summary":"  Time-series anomaly detection plays an important role in engineering\nprocesses, like development, manufacturing and other operations involving\ndynamic systems. These processes can greatly benefit from advances in the\nfield, as state-of-the-art approaches may aid in cases involving, for example,\nhighly dimensional data. To provide the reader with understanding of the\nterminology, this survey introduces a novel taxonomy where a distinction\nbetween online and offline, and training and inference is made. Additionally,\nit presents the most popular data sets and evaluation metrics used in the\nliterature, as well as a detailed analysis. Furthermore, this survey provides\nan extensive overview of the state-of-the-art model-based online semi- and\nunsupervised anomaly detection approaches for multivariate time-series data,\ncategorising them into different model families and other properties. The\nbiggest research challenge revolves around benchmarking, as currently there is\nno reliable way to compare different approaches against one another. This\nproblem is two-fold: on the one hand, public data sets suffers from at least\none fundamental flaw, while on the other hand, there is a lack of intuitive and\nrepresentative evaluation metrics in the field. Moreover, the way most\npublications choose a detection threshold disregards real-world conditions,\nwhich hinders the application in the real world. To allow for tangible advances\nin the field, these issues must be addressed in future work.\n","authors":["Lucas Correia","Jan-Christoph Goos","Philipp Klein","Thomas Bäck","Anna V. Kononova"],"pdf_url":"https://arxiv.org/pdf/2408.03747v1.pdf","comment":"Submitted to Engineering Applications of Artificial Intelligence\n  journal"},{"id":"http://arxiv.org/abs/2408.03746v1","updated":"2024-08-07T12:59:58Z","published":"2024-08-07T12:59:58Z","title":"Flexible Bayesian Last Layer Models Using Implicit Priors and Diffusion\n  Posterior Sampling","summary":"  Bayesian Last Layer (BLL) models focus solely on uncertainty in the output\nlayer of neural networks, demonstrating comparable performance to more complex\nBayesian models. However, the use of Gaussian priors for last layer weights in\nBayesian Last Layer (BLL) models limits their expressive capacity when faced\nwith non-Gaussian, outlier-rich, or high-dimensional datasets. To address this\nshortfall, we introduce a novel approach that combines diffusion techniques and\nimplicit priors for variational learning of Bayesian last layer weights. This\nmethod leverages implicit distributions for modeling weight priors in BLL,\ncoupled with diffusion samplers for approximating true posterior predictions,\nthereby establishing a comprehensive Bayesian prior and posterior estimation\nstrategy. By delivering an explicit and computationally efficient variational\nlower bound, our method aims to augment the expressive abilities of BLL models,\nenhancing model accuracy, calibration, and out-of-distribution detection\nproficiency. Through detailed exploration and experimental validation, We\nshowcase the method's potential for improving predictive accuracy and\nuncertainty quantification while ensuring computational efficiency.\n","authors":["Jian Xu","Zhiqi Lin","Shigui Li","Min Chen","Junmei Yang","Delu Zeng","John Paisley"],"pdf_url":"https://arxiv.org/pdf/2408.03746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18381v4","updated":"2024-08-07T12:59:31Z","published":"2023-05-28T06:53:41Z","title":"Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient\n  Dataset Distillation","summary":"  Data-efficient learning has garnered significant attention, especially given\nthe current trend of large multi-modal models. Recently, dataset distillation\nhas become an effective approach by synthesizing data samples that are\nessential for network training. However, it remains to be explored which\nsamples are essential for the dataset distillation process itself. In this\nwork, we study the data efficiency and selection for the dataset distillation\ntask. By re-formulating the dynamics of distillation, we provide insight into\nthe inherent redundancy in the real dataset, both theoretically and\nempirically. We propose to use the empirical loss value as a static data\npruning criterion. To further compensate for the variation of the data value in\ntraining, we find the most contributing samples based on their causal effects\non the distillation. The proposed selection strategy can efficiently exploit\nthe training dataset, outperform the previous SOTA distillation algorithms, and\nconsistently enhance the distillation algorithms, even on much larger-scale and\nmore heterogeneous datasets, e.g., full ImageNet-1K and Kinetics-400. We\nbelieve this paradigm will open up new avenues in the dynamics of distillation\nand pave the way for efficient dataset distillation. Our code is available on\nhttps://github.com/silicx/GoldFromOres-BiLP.\n","authors":["Yue Xu","Yong-Lu Li","Kaitong Cui","Ziyu Wang","Cewu Lu","Yu-Wing Tai","Chi-Keung Tang"],"pdf_url":"https://arxiv.org/pdf/2305.18381v4.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2307.12754v4","updated":"2024-08-07T12:51:46Z","published":"2023-07-24T12:52:55Z","title":"Nonparametric Linear Feature Learning in Regression Through\n  Regularisation","summary":"  Representation learning plays a crucial role in automated feature selection,\nparticularly in the context of high-dimensional data, where non-parametric\nmethods often struggle. In this study, we focus on supervised learning\nscenarios where the pertinent information resides within a lower-dimensional\nlinear subspace of the data, namely the multi-index model. If this subspace\nwere known, it would greatly enhance prediction, computation, and\ninterpretation. To address this challenge, we propose a novel method for joint\nlinear feature learning and non-parametric function estimation, aimed at more\neffectively leveraging hidden features for learning. Our approach employs\nempirical risk minimisation, augmented with a penalty on function derivatives,\nensuring versatility. Leveraging the orthogonality and rotation invariance\nproperties of Hermite polynomials, we introduce our estimator, named RegFeaL.\nBy using alternative minimisation, we iteratively rotate the data to improve\nalignment with leading directions. We establish that the expected risk of our\nmethod converges in high-probability to the minimal risk under minimal\nassumptions and with explicit rates. Additionally, we provide empirical results\ndemonstrating the performance of RegFeaL in various experiments.\n","authors":["Bertille Follain","Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2307.12754v4.pdf","comment":"45 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.03735v1","updated":"2024-08-07T12:42:09Z","published":"2024-08-07T12:42:09Z","title":"Advancing Multimodal Large Language Models with Quantization-Aware Scale\n  Learning for Efficient Adaptation","summary":"  This paper presents the first study to explore the potential of parameter\nquantization for multimodal large language models to alleviate the significant\nresource constraint encountered during vision-language instruction tuning. We\nintroduce a Quantization-aware Scale LeArning method based on multimodal\nWarmup, termed QSLAW. This method is grounded in two key innovations: (1) The\nlearning of group-wise scale factors for quantized LLM weights to mitigate the\nquantization error arising from activation outliers and achieve more effective\nvision-language instruction tuning; (2) The implementation of a multimodal\nwarmup that progressively integrates linguistic and multimodal training\nsamples, thereby preventing overfitting of the quantized model to multimodal\ndata while ensuring stable adaptation of multimodal large language models to\ndownstream vision-language tasks. Extensive experiments demonstrate that models\nquantized by QSLAW perform on par with, or even surpass, their full-precision\ncounterparts, while facilitating up to 1.4 times reduction in VL tuning time\nand GPU consumption. Our code is released at https://github.com/xjjxmu/QSLAW.\n","authors":["Jingjing Xie","Yuxin Zhang","Mingbao Lin","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2408.03735v1.pdf","comment":"Accepted by ACMMM2024"},{"id":"http://arxiv.org/abs/2408.03733v1","updated":"2024-08-07T12:41:56Z","published":"2024-08-07T12:41:56Z","title":"Bayes-optimal learning of an extensive-width neural network from\n  quadratically many samples","summary":"  We consider the problem of learning a target function corresponding to a\nsingle hidden layer neural network, with a quadratic activation function after\nthe first layer, and random weights. We consider the asymptotic limit where the\ninput dimension and the network width are proportionally large. Recent work\n[Cui & al '23] established that linear regression provides Bayes-optimal test\nerror to learn such a function when the number of available samples is only\nlinear in the dimension. That work stressed the open challenge of theoretically\nanalyzing the optimal test error in the more interesting regime where the\nnumber of samples is quadratic in the dimension. In this paper, we solve this\nchallenge for quadratic activations and derive a closed-form expression for the\nBayes-optimal test error. We also provide an algorithm, that we call GAMP-RIE,\nwhich combines approximate message passing with rotationally invariant matrix\ndenoising, and that asymptotically achieves the optimal performance.\nTechnically, our result is enabled by establishing a link with recent works on\noptimal denoising of extensive-rank matrices and on the ellipsoid fitting\nproblem. We further show empirically that, in the absence of noise,\nrandomly-initialized gradient descent seems to sample the space of weights,\nleading to zero training loss, and averaging over initialization leads to a\ntest error equal to the Bayes-optimal one.\n","authors":["Antoine Maillard","Emanuele Troiani","Simon Martin","Florent Krzakala","Lenka Zdeborová"],"pdf_url":"https://arxiv.org/pdf/2408.03733v1.pdf","comment":"47 pages"},{"id":"http://arxiv.org/abs/2408.03732v1","updated":"2024-08-07T12:38:23Z","published":"2024-08-07T12:38:23Z","title":"Question Rephrasing for Quantifying Uncertainty in Large Language\n  Models: Applications in Molecular Chemistry Tasks","summary":"  Uncertainty quantification enables users to assess the reliability of\nresponses generated by large language models (LLMs). We present a novel\nQuestion Rephrasing technique to evaluate the input uncertainty of LLMs, which\nrefers to the uncertainty arising from equivalent variations of the inputs\nprovided to LLMs. This technique is integrated with sampling methods that\nmeasure the output uncertainty of LLMs, thereby offering a more comprehensive\nuncertainty assessment. We validated our approach on property prediction and\nreaction prediction for molecular chemistry tasks.\n","authors":["Zizhang Chen","Pengyu Hong","Sandeep Madireddy"],"pdf_url":"https://arxiv.org/pdf/2408.03732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03728v1","updated":"2024-08-07T12:33:46Z","published":"2024-08-07T12:33:46Z","title":"A Convex-optimization-based Layer-wise Post-training Pruner for Large\n  Language Models","summary":"  Pruning is a critical strategy for compressing trained large language models\n(LLMs), aiming at substantial memory conservation and computational\nacceleration without compromising performance. However, existing pruning\nmethods often necessitate inefficient retraining for billion-scale LLMs or rely\non heuristic methods such as the optimal brain surgeon framework, which degrade\nperformance. In this paper, we introduce FISTAPruner, the first post-training\npruner based on convex optimization models and algorithms. Specifically, we\npropose a convex optimization model incorporating $\\ell_1$ norm to induce\nsparsity and utilize the FISTA solver for optimization. FISTAPruner\nincorporates an intra-layer cumulative error correction mechanism and supports\nparallel pruning. We comprehensively evaluate FISTAPruner on models such as\nOPT, LLaMA, LLaMA-2, and LLaMA-3 with 125M to 70B parameters under unstructured\nand 2:4 semi-structured sparsity, demonstrating superior performance over\nexisting state-of-the-art methods across various language benchmarks.\n","authors":["Pengxiang Zhao","Hanyu Hu","Ping Li","Yi Zheng","Zhefeng Wang","Xiaoming Yuan"],"pdf_url":"https://arxiv.org/pdf/2408.03728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03706v1","updated":"2024-08-07T11:44:32Z","published":"2024-08-07T11:44:32Z","title":"Local Topology Measures of Contextual Language Model Latent Spaces With\n  Applications to Dialogue Term Extraction","summary":"  A common approach for sequence tagging tasks based on contextual word\nrepresentations is to train a machine learning classifier directly on these\nembedding vectors. This approach has two shortcomings. First, such methods\nconsider single input sequences in isolation and are unable to put an\nindividual embedding vector in relation to vectors outside the current local\ncontext of use. Second, the high performance of these models relies on\nfine-tuning the embedding model in conjunction with the classifier, which may\nnot always be feasible due to the size or inaccessibility of the underlying\nfeature-generation model. It is thus desirable, given a collection of embedding\nvectors of a corpus, i.e., a datastore, to find features of each vector that\ndescribe its relation to other, similar vectors in the datastore. With this in\nmind, we introduce complexity measures of the local topology of the latent\nspace of a contextual language model with respect to a given datastore. The\neffectiveness of our features is demonstrated through their application to\ndialogue term extraction. Our work continues a line of research that explores\nthe manifold hypothesis for word embeddings, demonstrating that local structure\nin the space carved out by word embeddings can be exploited to infer semantic\nproperties.\n","authors":["Benjamin Matthias Ruppik","Michael Heck","Carel van Niekerk","Renato Vukovic","Hsien-chin Lin","Shutong Feng","Marcus Zibrowius","Milica Gašić"],"pdf_url":"https://arxiv.org/pdf/2408.03706v1.pdf","comment":"Accepted as a long paper to SIGDIAL 2024. 9 pages, 2 figures, 3\n  tables"},{"id":"http://arxiv.org/abs/2312.10271v2","updated":"2024-08-07T11:32:19Z","published":"2023-12-16T00:23:21Z","title":"Robustness of Deep Learning for Accelerated MRI: Benefits of Diverse\n  Training Data","summary":"  Deep learning based methods for image reconstruction are state-of-the-art for\na variety of imaging tasks. However, neural networks often perform worse if the\ntraining data differs significantly from the data they are applied to. For\nexample, a model trained for accelerated magnetic resonance imaging (MRI) on\none scanner performs worse on another scanner. In this work, we investigate the\nimpact of the training data on a model's performance and robustness for\naccelerated MRI. We find that models trained on the combination of various data\ndistributions, such as those obtained from different MRI scanners and\nanatomies, exhibit robustness equal or superior to models trained on the best\nsingle distribution for a specific target distribution. Thus training on such\ndiverse data tends to improve robustness. Furthermore, training on such a\ndiverse dataset does not compromise in-distribution performance, i.e., a model\ntrained on diverse data yields in-distribution performance at least as good as\nmodels trained on the more narrow individual distributions. Our results suggest\nthat training a model for imaging on a variety of distributions tends to yield\na more effective and robust model than maintaining separate models for\nindividual distributions.\n","authors":["Kang Lin","Reinhard Heckel"],"pdf_url":"https://arxiv.org/pdf/2312.10271v2.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2408.03694v1","updated":"2024-08-07T11:14:18Z","published":"2024-08-07T11:14:18Z","title":"A Blockchain-based Reliable Federated Meta-learning for Metaverse: A\n  Dual Game Framework","summary":"  The metaverse, envisioned as the next digital frontier for avatar-based\nvirtual interaction, involves high-performance models. In this dynamic\nenvironment, users' tasks frequently shift, requiring fast model\npersonalization despite limited data. This evolution consumes extensive\nresources and requires vast data volumes. To address this, meta-learning\nemerges as an invaluable tool for metaverse users, with federated meta-learning\n(FML), offering even more tailored solutions owing to its adaptive\ncapabilities. However, the metaverse is characterized by users heterogeneity\nwith diverse data structures, varied tasks, and uneven sample sizes,\npotentially undermining global training outcomes due to statistical difference.\nGiven this, an urgent need arises for smart coalition formation that accounts\nfor these disparities. This paper introduces a dual game-theoretic framework\nfor metaverse services involving meta-learners as workers to manage FML. A\nblockchain-based cooperative coalition formation game is crafted, grounded on a\nreputation metric, user similarity, and incentives. We also introduce a novel\nreputation system based on users' historical contributions and potential\ncontributions to present tasks, leveraging correlations between past and new\ntasks. Finally, a Stackelberg game-based incentive mechanism is presented to\nattract reliable workers to participate in meta-learning, minimizing users'\nenergy costs, increasing payoffs, boosting FML efficacy, and improving\nmetaverse utility. Results show that our dual game framework outperforms\nbest-effort, random, and non-uniform clustering schemes - improving training\nperformance by up to 10%, cutting completion times by as much as 30%, enhancing\nmetaverse utility by more than 25%, and offering up to 5% boost in training\nefficiency over non-blockchain systems, effectively countering misbehaving\nusers.\n","authors":["Emna Baccour","Aiman Erbad","Amr Mohamed","Mounir Hamdi","Mohsen Guizani"],"pdf_url":"https://arxiv.org/pdf/2408.03694v1.pdf","comment":"Accepted in IEEE Internet of Things Journal"},{"id":"http://arxiv.org/abs/2408.03691v1","updated":"2024-08-07T11:13:19Z","published":"2024-08-07T11:13:19Z","title":"Generative Design of Periodic Orbits in the Restricted Three-Body\n  Problem","summary":"  The Three-Body Problem has fascinated scientists for centuries and it has\nbeen crucial in the design of modern space missions. Recent developments in\nGenerative Artificial Intelligence hold transformative promise for addressing\nthis longstanding problem. This work investigates the use of Variational\nAutoencoder (VAE) and its internal representation to generate periodic orbits.\nWe utilize a comprehensive dataset of periodic orbits in the Circular\nRestricted Three-Body Problem (CR3BP) to train deep-learning architectures that\ncapture key orbital characteristics, and we set up physical evaluation metrics\nfor the generated trajectories. Through this investigation, we seek to enhance\nthe understanding of how Generative AI can improve space mission planning and\nastrodynamics research, leading to novel, data-driven approaches in the field.\n","authors":["Alvaro Francisco Gil","Walther Litteri","Victor Rodriguez-Fernandez","David Camacho","Massimiliano Vasile"],"pdf_url":"https://arxiv.org/pdf/2408.03691v1.pdf","comment":"SPAICE Conference 2024 (7 pages)"},{"id":"http://arxiv.org/abs/2408.03685v1","updated":"2024-08-07T10:53:07Z","published":"2024-08-07T10:53:07Z","title":"RL-ADN: A High-Performance Deep Reinforcement Learning Environment for\n  Optimal Energy Storage Systems Dispatch in Active Distribution Networks","summary":"  Deep Reinforcement Learning (DRL) presents a promising avenue for optimizing\nEnergy Storage Systems (ESSs) dispatch in distribution networks. This paper\nintroduces RL-ADN, an innovative open-source library specifically designed for\nsolving the optimal ESSs dispatch in active distribution networks. RL-ADN\noffers unparalleled flexibility in modeling distribution networks, and ESSs,\naccommodating a wide range of research goals. A standout feature of RL-ADN is\nits data augmentation module, based on Gaussian Mixture Model and Copula (GMC)\nfunctions, which elevates the performance ceiling of DRL agents. Additionally,\nRL-ADN incorporates the Laurent power flow solver, significantly reducing the\ncomputational burden of power flow calculations during training without\nsacrificing accuracy. The effectiveness of RL-ADN is demonstrated using in\ndifferent sizes of distribution networks, showing marked performance\nimprovements in the adaptability of DRL algorithms for ESS dispatch tasks. This\nenhancement is particularly beneficial from the increased diversity of training\nscenarios. Furthermore, RL-ADN achieves a tenfold increase in computational\nefficiency during training, making it highly suitable for large-scale network\napplications. The library sets a new benchmark in DRL-based ESSs dispatch in\ndistribution networks and it is poised to advance DRL applications in\ndistribution network operations significantly. RL-ADN is available at:\nhttps://github.com/ShengrenHou/RL-ADN.\n","authors":["Shengren Hou","Shuyi Gao","Weijie Xia","Edgar Mauricio Salazar Duque","Peter Palensky","Pedro P. Vergara"],"pdf_url":"https://arxiv.org/pdf/2408.03685v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03669v1","updated":"2024-08-07T10:24:59Z","published":"2024-08-07T10:24:59Z","title":"Beyond Over-smoothing: Uncovering the Trainability Challenges in Deep\n  Graph Neural Networks","summary":"  The drastic performance degradation of Graph Neural Networks (GNNs) as the\ndepth of the graph propagation layers exceeds 8-10 is widely attributed to a\nphenomenon of Over-smoothing. Although recent research suggests that\nOver-smoothing may not be the dominant reason for such a performance\ndegradation, they have not provided rigorous analysis from a theoretical view,\nwhich warrants further investigation. In this paper, we systematically analyze\nthe real dominant problem in deep GNNs and identify the issues that these GNNs\ntowards addressing Over-smoothing essentially work on via empirical experiments\nand theoretical gradient analysis. We theoretically prove that the difficult\ntraining problem of deep MLPs is actually the main challenge, and various\nexisting methods that supposedly tackle Over-smoothing actually improve the\ntrainability of MLPs, which is the main reason for their performance gains. Our\nfurther investigation into trainability issues reveals that properly\nconstrained smaller upper bounds of gradient flow notably enhance the\ntrainability of GNNs. Experimental results on diverse datasets demonstrate\nconsistency between our theoretical findings and empirical evidence. Our\nanalysis provides new insights in constructing deep graph models.\n","authors":["Jie Peng","Runlin Lei","Zhewei Wei"],"pdf_url":"https://arxiv.org/pdf/2408.03669v1.pdf","comment":"CIKM2024"},{"id":"http://arxiv.org/abs/2403.04202v5","updated":"2024-08-07T10:10:34Z","published":"2024-03-07T04:12:24Z","title":"Dynamics of Moral Behavior in Heterogeneous Populations of Learning\n  Agents","summary":"  Growing concerns about safety and alignment of AI systems highlight the\nimportance of embedding moral capabilities in artificial agents: a promising\nsolution is the use of learning from experience, i.e., Reinforcement Learning.\nIn multi-agent (social) environments, complex population-level phenomena may\nemerge from interactions between individual learning agents. Many of the\nexisting studies rely on simulated social dilemma environments to study the\ninteractions of independent learning agents; however, they tend to ignore the\nmoral heterogeneity that is likely to be present in societies of agents in\npractice. For example, at different points in time a single learning agent may\nface opponents who are consequentialist (i.e., focused on maximizing outcomes\nover time), norm-based (i.e., conforming to specific norms), or virtue-based\n(i.e., considering a combination of different virtues). The extent to which\nagents' co-development may be impacted by such moral heterogeneity in\npopulations is not well understood. In this paper, we present a study of the\nlearning dynamics of morally heterogeneous populations interacting in a social\ndilemma setting. Using an Iterated Prisoner's Dilemma environment with a\npartner selection mechanism, we investigate the extent to which the prevalence\nof diverse moral agents in populations affects individual agents' learning\nbehaviors and emergent population-level outcomes. We observe several types of\nnon-trivial interactions between pro-social and anti-social agents, and find\nthat certain types of moral agents are able to steer selfish agents towards\nmore cooperative behavior.\n","authors":["Elizaveta Tennant","Stephen Hailes","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2403.04202v5.pdf","comment":"Accepted at AIES 2024 (7th AAAI/ACM Conference on AI, Ethics, and\n  Society - San Jose, CA, USA)"},{"id":"http://arxiv.org/abs/2408.03664v1","updated":"2024-08-07T10:06:04Z","published":"2024-08-07T10:06:04Z","title":"AI-Driven approach for sustainable extraction of earth's subsurface\n  renewable energy while minimizing seismic activity","summary":"  Deep Geothermal Energy, Carbon Capture and Storage, and Hydrogen Storage hold\nconsiderable promise for meeting the energy sector's large-scale requirements\nand reducing CO$_2$ emissions. However, the injection of fluids into the\nEarth's crust, essential for these activities, can induce or trigger\nearthquakes. In this paper, we highlight a new approach based on Reinforcement\nLearning for the control of human-induced seismicity in the highly complex\nenvironment of an underground reservoir. This complex system poses significant\nchallenges in the control design due to parameter uncertainties and unmodeled\ndynamics. We show that the reinforcement learning algorithm can interact\nefficiently with a robust controller, by choosing the controller parameters in\nreal-time, reducing human-induced seismicity and allowing the consideration of\nfurther production objectives, \\textit{e.g.}, minimal control power.\nSimulations are presented for a simplified underground reservoir under various\nenergy demand scenarios, demonstrating the reliability and effectiveness of the\nproposed control-reinforcement learning approach.\n","authors":["Diego Gutierrez-Oribio","Alexandros Stathas","Ioannis Stefanou"],"pdf_url":"https://arxiv.org/pdf/2408.03664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18613v2","updated":"2024-08-07T09:46:49Z","published":"2024-03-27T14:28:44Z","title":"Scalable Lipschitz Estimation for CNNs","summary":"  Estimating the Lipschitz constant of deep neural networks is of growing\ninterest as it is useful for informing on generalisability and adversarial\nrobustness. Convolutional neural networks (CNNs) in particular, underpin much\nof the recent success in computer vision related applications. However,\nalthough existing methods for estimating the Lipschitz constant can be tight,\nthey have limited scalability when applied to CNNs. To tackle this, we propose\na novel method to accelerate Lipschitz constant estimation for CNNs. The core\nidea is to divide a large convolutional block via a joint layer and width-wise\npartition, into a collection of smaller blocks. We prove an upper-bound on the\nLipschitz constant of the larger block in terms of the Lipschitz constants of\nthe smaller blocks. Through varying the partition factor, the resulting method\ncan be adjusted to prioritise either accuracy or scalability and permits\nparallelisation. We demonstrate an enhanced scalability and comparable accuracy\nto existing baselines through a range of experiments.\n","authors":["Yusuf Sulehman","Tingting Mu"],"pdf_url":"https://arxiv.org/pdf/2403.18613v2.pdf","comment":"An inconsistency between the input of the flattened convolutional\n  block and the flattened, partitioned input impacts the validity of the\n  proposed Lipschitz bound"},{"id":"http://arxiv.org/abs/2408.03655v1","updated":"2024-08-07T09:45:24Z","published":"2024-08-07T09:45:24Z","title":"Consumer Transactions Simulation through Generative Adversarial Networks","summary":"  In the rapidly evolving domain of large-scale retail data systems,\nenvisioning and simulating future consumer transactions has become a crucial\narea of interest. It offers significant potential to fortify demand forecasting\nand fine-tune inventory management. This paper presents an innovative\napplication of Generative Adversarial Networks (GANs) to generate synthetic\nretail transaction data, specifically focusing on a novel system architecture\nthat combines consumer behavior modeling with stock-keeping unit (SKU)\navailability constraints to address real-world assortment optimization\nchallenges. We diverge from conventional methodologies by integrating SKU data\ninto our GAN architecture and using more sophisticated embedding methods (e.g.,\nhyper-graphs). This design choice enables our system to generate not only\nsimulated consumer purchase behaviors but also reflects the dynamic interplay\nbetween consumer behavior and SKU availability -- an aspect often overlooked,\namong others, because of data scarcity in legacy retail simulation models. Our\nGAN model generates transactions under stock constraints, pioneering a\nresourceful experimental system with practical implications for real-world\nretail operation and strategy. Preliminary results demonstrate enhanced realism\nin simulated transactions measured by comparing generated items with real ones\nusing methods employed earlier in related studies. This underscores the\npotential for more accurate predictive modeling.\n","authors":["Sergiy Tkachuk","Szymon Łukasik","Anna Wróblewska"],"pdf_url":"https://arxiv.org/pdf/2408.03655v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2408.03652v1","updated":"2024-08-07T09:34:55Z","published":"2024-08-07T09:34:55Z","title":"mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search","summary":"  Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.\n","authors":["Ahmed Abdou","Tasneem Mohsen"],"pdf_url":"https://arxiv.org/pdf/2408.03652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.03703v2","updated":"2024-08-07T09:18:40Z","published":"2022-08-07T12:02:48Z","title":"Granger Causality using Neural Networks","summary":"  Dependence between nodes in a network is an important concept that pervades\nmany areas including finance, politics, sociology, genomics and the brain\nsciences. One way to characterize dependence between components of a\nmultivariate time series data is via Granger Causality (GC). Standard\ntraditional approaches to GC estimation / inference commonly assume linear\ndynamics, however such simplification does not hold in many real-world\napplications where signals are inherently non-linear. In such cases, imposing\nlinear models such as vector autoregressive (VAR) models can lead to\nmis-characterization of true Granger Causal interactions. To overcome this\nlimitation, Tank et al (IEEE Transactions on Pattern Analysis and Machine\nLearning, 2022) proposed a solution that uses neural networks with sparse\nregularization penalties. The regularization encourages learnable weights to be\nsparse, which enables inference on GC. This paper overcomes the limitations of\ncurrent methods by leveraging advances in machine learning and deep learning\nwhich have been demonstrated to learn hidden patterns in the data. We propose\nnovel classes of models that can handle underlying non-linearity in a\ncomputationally efficient manner, simultaneously providing GC and lag order\nselection. Firstly, we present the Learned Kernel VAR (LeKVAR) model that\nlearns kernel parameterized by a shared neural net followed by penalization on\nlearnable weights to discover GC structure. Secondly, we show one can directly\ndecouple lags and individual time series importance via decoupled penalties.\nThis is important as we want to select the lag order during the process of GC\nestimation. This decoupling acts as a filtering and can be extended to any DL\nmodel including Multi-Layer Perceptrons (MLP), Recurrent Neural Networks (RNN),\nLong Short Term Memory Networks (LSTM), Transformers etc, for simultaneous GC\nestimation and lag selection.\n","authors":["Malik Shahid Sultan","Samuel Horvath","Hernando Ombao"],"pdf_url":"https://arxiv.org/pdf/2208.03703v2.pdf","comment":"To be Submitted to a Journal work Presented at JSM. arXiv admin note:\n  text overlap with arXiv:1802.05842 by other authors"},{"id":"http://arxiv.org/abs/2407.19265v2","updated":"2024-08-07T09:16:12Z","published":"2024-07-27T14:16:25Z","title":"Towards Robust Few-shot Class Incremental Learning in Audio\n  Classification using Contrastive Representation","summary":"  In machine learning applications, gradual data ingress is common, especially\nin audio processing where incremental learning is vital for real-time\nanalytics. Few-shot class-incremental learning addresses challenges arising\nfrom limited incoming data. Existing methods often integrate additional\ntrainable components or rely on a fixed embedding extractor post-training on\nbase sessions to mitigate concerns related to catastrophic forgetting and the\ndangers of model overfitting. However, using cross-entropy loss alone during\nbase session training is suboptimal for audio data. To address this, we propose\nincorporating supervised contrastive learning to refine the representation\nspace, enhancing discriminative power and leading to better generalization\nsince it facilitates seamless integration of incremental classes, upon arrival.\nExperimental results on NSynth and LibriSpeech datasets with 100 classes, as\nwell as ESC dataset with 50 and 10 classes, demonstrate state-of-the-art\nperformance.\n","authors":["Riyansha Singh","Parinita Nema","Vinod K Kurmi"],"pdf_url":"https://arxiv.org/pdf/2407.19265v2.pdf","comment":"INTERSPEECH 2024 accepted"},{"id":"http://arxiv.org/abs/2408.03287v2","updated":"2024-08-07T09:07:01Z","published":"2024-08-06T16:35:25Z","title":"Malicious Internet Entity Detection Using Local Graph Inference","summary":"  Detection of malicious behavior in a large network is a challenging problem\nfor machine learning in computer security, since it requires a model with high\nexpressive power and scalable inference. Existing solutions struggle to achieve\nthis feat -- current cybersec-tailored approaches are still limited in\nexpressivity, and methods successful in other domains do not scale well for\nlarge volumes of data, rendering frequent retraining impossible. This work\nproposes a new perspective for learning from graph data that is modeling\nnetwork entity interactions as a large heterogeneous graph. High expressivity\nof the method is achieved with neural network architecture HMILnet that\nnaturally models this type of data and provides theoretical guarantees. The\nscalability is achieved by pursuing local graph inference, i.e., classifying\nindividual vertices and their neighborhood as independent samples. Our\nexperiments exhibit improvement over the state-of-the-art Probabilistic Threat\nPropagation (PTP) algorithm, show a further threefold accuracy improvement when\nadditional data is used, which is not possible with the PTP algorithm, and\ndemonstrate the generalization capabilities of the method to new, previously\nunseen entities.\n","authors":["Simon Mandlik","Tomas Pevny","Vaclav Smidl","Lukas Bajer"],"pdf_url":"https://arxiv.org/pdf/2408.03287v2.pdf","comment":"A preprint. Full publication:\n  https://ieeexplore.ieee.org/document/10418120"},{"id":"http://arxiv.org/abs/2403.14973v2","updated":"2024-08-07T08:53:25Z","published":"2024-03-22T06:04:11Z","title":"Pose-Aware Self-Supervised Learning with Viewpoint Trajectory\n  Regularization","summary":"  Learning visual features from unlabeled images has proven successful for\nsemantic categorization, often by mapping different $views$ of the same object\nto the same feature to achieve recognition invariance. However, visual\nrecognition involves not only identifying $what$ an object is but also\nunderstanding $how$ it is presented. For example, seeing a car from the side\nversus head-on is crucial for deciding whether to stay put or jump out of the\nway. While unsupervised feature learning for downstream viewpoint reasoning is\nimportant, it remains under-explored, partly due to the lack of a standardized\nevaluation method and benchmarks.\n  We introduce a new dataset of adjacent image triplets obtained from a\nviewpoint trajectory, without any semantic or pose labels. We benchmark both\nsemantic classification and pose estimation accuracies on the same visual\nfeature. Additionally, we propose a viewpoint trajectory regularization loss\nfor learning features from unlabeled image triplets. Our experiments\ndemonstrate that this approach helps develop a visual representation that\nencodes object identity and organizes objects by their poses, retaining\nsemantic classification accuracy while achieving emergent global pose awareness\nand better generalization to novel objects. Our dataset and code are available\nat http://pwang.pw/trajSSL/.\n","authors":["Jiayun Wang","Yubei Chen","Stella X. Yu"],"pdf_url":"https://arxiv.org/pdf/2403.14973v2.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2408.03636v1","updated":"2024-08-07T08:51:10Z","published":"2024-08-07T08:51:10Z","title":"Time is Not Enough: Time-Frequency based Explanation for Time-Series\n  Black-Box Models","summary":"  Despite the massive attention given to time-series explanations due to their\nextensive applications, a notable limitation in existing approaches is their\nprimary reliance on the time-domain. This overlooks the inherent characteristic\nof time-series data containing both time and frequency features. In this work,\nwe present Spectral eXplanation (SpectralX), an XAI framework that provides\ntime-frequency explanations for time-series black-box classifiers. This easily\nadaptable framework enables users to \"plug-in\" various perturbation-based XAI\nmethods for any pre-trained time-series classification models to assess their\nimpact on the explanation quality without having to modify the framework\narchitecture. Additionally, we introduce Feature Importance Approximations\n(FIA), a new perturbation-based XAI method. These methods consist of feature\ninsertion, deletion, and combination techniques to enhance computational\nefficiency and class-specific explanations in time-series classification tasks.\nWe conduct extensive experiments in the generated synthetic dataset and various\nUCR Time-Series datasets to first compare the explanation performance of FIA\nand other existing perturbation-based XAI methods in both time-domain and\ntime-frequency domain, and then show the superiority of our FIA in the\ntime-frequency domain with the SpectralX framework. Finally, we conduct a user\nstudy to confirm the practicality of our FIA in SpectralX framework for\nclass-specific time-frequency based time-series explanations. The source code\nis available in https://github.com/gustmd0121/Time_is_not_Enough\n","authors":["Hyunseung Chung","Sumin Jo","Yeonsu Kwon","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2408.03636v1.pdf","comment":"Accepted to CIKM 2024 (10 pages, 4 figures, 6 tables)"},{"id":"http://arxiv.org/abs/2408.03626v1","updated":"2024-08-07T08:37:23Z","published":"2024-08-07T08:37:23Z","title":"On the choice of the non-trainable internal weights in random feature\n  maps","summary":"  The computationally cheap machine learning architecture of random feature\nmaps can be viewed as a single-layer feedforward network in which the weights\nof the hidden layer are random but fixed and only the outer weights are learned\nvia linear regression. The internal weights are typically chosen from a\nprescribed distribution. The choice of the internal weights significantly\nimpacts the accuracy of random feature maps. We address here the task of how to\nbest select the internal weights. In particular, we consider the forecasting\nproblem whereby random feature maps are used to learn a one-step propagator map\nfor a dynamical system. We provide a computationally cheap hit-and-run\nalgorithm to select good internal weights which lead to good forecasting skill.\nWe show that the number of good features is the main factor controlling the\nforecasting skill of random feature maps and acts as an effective feature\ndimension. Lastly, we compare random feature maps with single-layer feedforward\nneural networks in which the internal weights are now learned using gradient\ndescent. We find that random feature maps have superior forecasting\ncapabilities whilst having several orders of magnitude lower computational\ncost.\n","authors":["Pinak Mandal","Georg A. Gottwald"],"pdf_url":"https://arxiv.org/pdf/2408.03626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01185v5","updated":"2024-08-07T08:31:05Z","published":"2023-12-02T17:24:17Z","title":"A ripple in time: a discontinuity in American history","summary":"  In this technical note we suggest a novel approach to discover temporal\n(related and unrelated to language dilation) and personality (authorship\nattribution) in historical datasets. We exemplify our approach on the State of\nthe Union speeches given by the past 42 US presidents: this dataset is known\nfor its relatively small amount of data, and high variability of the amount and\nstyle of texts. Nevertheless we manage to achieve about 95\\% accuracy on the\nauthorship attribution task, and pin down the date of writing to a single\npresidential term.\n","authors":["Alexander Kolpakov","Igor Rivin"],"pdf_url":"https://arxiv.org/pdf/2312.01185v5.pdf","comment":"6 pages, 8 figures; GitHub repository\n  (https://github.com/sashakolpakov/ripple_in_time); restructured manuscript"},{"id":"http://arxiv.org/abs/2408.03619v1","updated":"2024-08-07T08:23:42Z","published":"2024-08-07T08:23:42Z","title":"Making Robust Generalizers Less Rigid with Soft Ascent-Descent","summary":"  While the traditional formulation of machine learning tasks is in terms of\nperformance on average, in practice we are often interested in how well a\ntrained model performs on rare or difficult data points at test time. To\nachieve more robust and balanced generalization, methods applying\nsharpness-aware minimization to a subset of worst-case examples have proven\nsuccessful for image classification tasks, but only using deep neural networks\nin a scenario where the most difficult points are also the least common. In\nthis work, we show how such a strategy can dramatically break down under more\ndiverse models, and as a more robust alternative, instead of typical sharpness\nwe propose and evaluate a training criterion which penalizes poor loss\nconcentration, which can be easily combined with loss transformations such as\nCVaR or DRO that control tail emphasis.\n","authors":["Matthew J. Holland","Toma Hamada"],"pdf_url":"https://arxiv.org/pdf/2408.03619v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14573v2","updated":"2024-08-07T08:22:35Z","published":"2024-07-21T06:27:45Z","title":"Trading Devil Final: Backdoor attack via Stock market and Bayesian\n  Optimization","summary":"  Since the advent of generative artificial intelligence, every company and\nresearcher has been rushing to develop their own generative models, whether\ncommercial or not. Given the large number of users of these powerful new tools,\nthere is currently no intrinsically verifiable way to explain from the ground\nup what happens when LLMs (large language models) learn. For example, those\nbased on automatic speech recognition systems, which have to rely on huge and\nastronomical amounts of data collected from all over the web to produce fast\nand efficient results, In this article, we develop a backdoor attack called\nMarketBackFinal 2.0, based on acoustic data poisoning, MarketBackFinal 2.0 is\nmainly based on modern stock market models. In order to show the possible\nvulnerabilities of speech-based transformers that may rely on LLMs.\n","authors":["Orson Mengara"],"pdf_url":"https://arxiv.org/pdf/2407.14573v2.pdf","comment":"END :jumps-Diffusion and stock market: Better quantify uncertainty in\n  financial simulations"},{"id":"http://arxiv.org/abs/2304.09914v4","updated":"2024-08-07T08:20:43Z","published":"2023-04-19T18:32:49Z","title":"The Face of Populism: Examining Differences in Facial Emotional\n  Expressions of Political Leaders Using Machine Learning","summary":"  Populist rhetoric employed on online media is characterized as deeply\nimpassioned and often imbued with strong emotions. The aim of this paper is to\nempirically investigate the differences in affective nonverbal communication of\npolitical leaders. We use a deep-learning approach to process a sample of 220\nYouTube videos of political leaders from 15 different countries, analyze their\nfacial expressions of emotion and then examine differences in average emotion\nscores representing the relative presence of 6 emotional states (anger,\ndisgust, fear, happiness, sadness, and surprise) and a neutral expression for\neach frame of the YouTube video. Based on a sample of manually coded images, we\nfind that this deep-learning approach has 53-60\\% agreement with human labels.\nWe observe statistically significant differences in the average score of\nnegative emotions between groups of leaders with varying degrees of populist\nrhetoric.\n","authors":["Sara Major","Aleksandar Tomašević"],"pdf_url":"https://arxiv.org/pdf/2304.09914v4.pdf","comment":"Version 4.0: Annotation study added, supplementary information\n  extended"},{"id":"http://arxiv.org/abs/2408.03618v1","updated":"2024-08-07T08:19:44Z","published":"2024-08-07T08:19:44Z","title":"A Logical Fallacy-Informed Framework for Argument Generation","summary":"  Despite the remarkable performance of Large Language Models (LLMs), they\nstill struggle with generating logically sound arguments, resulting in\npotential risks such as spreading misinformation. An important factor\ncontributing to LLMs' suboptimal performance in generating coherent arguments\nis their oversight of logical fallacies. To address this issue, we introduce\nFIPO, a fallacy-informed framework that leverages preference optimization\nmethods to steer LLMs toward logically sound arguments. FIPO includes a\nclassification loss, to capture the fine-grained information on fallacy\ncategories. Our results on argumentation datasets show that our method reduces\nthe fallacy errors by up to 17.5%. Furthermore, our human evaluation results\nindicate that the quality of the generated arguments by our method\nsignificantly outperforms the fine-tuned baselines, as well as prior preference\noptimization methods, such as DPO. These findings highlight the importance of\nensuring models are aware of logical fallacies for effective argument\ngeneration.\n","authors":["Luca Mouchel","Debjit Paul","Shaobo Cui","Robert West","Antoine Bosselut","Boi Faltings"],"pdf_url":"https://arxiv.org/pdf/2408.03618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03617v1","updated":"2024-08-07T08:18:51Z","published":"2024-08-07T08:18:51Z","title":"Is Child-Directed Speech Effective Training Data for Language Models?","summary":"  While high-performing language models are typically trained on hundreds of\nbillions of words, human children become fluent language users with a much\nsmaller amount of data. What are the features of the data they receive, and how\ndo these features support language modeling objectives? To investigate this\nquestion, we train GPT-2 models on 29M words of English-language child-directed\nspeech and a new matched, synthetic dataset (TinyDialogues), comparing to a\nheterogeneous blend of datasets from the BabyLM challenge. We evaluate both the\nsyntactic and semantic knowledge of these models using developmentally-inspired\nevaluations. Through pretraining experiments, we test whether the global\ndevelopmental ordering or the local discourse ordering of children's training\ndata support high performance relative to other datasets. The local properties\nof the data affect model results, but somewhat surprisingly, global properties\ndo not. Further, child language input is not uniquely valuable for training\nlanguage models. These findings support the hypothesis that, rather than\nproceeding from better data, children's learning is instead substantially more\nefficient than current language modeling techniques.\n","authors":["Steven Y. Feng","Noah D. Goodman","Michael C. Frank"],"pdf_url":"https://arxiv.org/pdf/2408.03617v1.pdf","comment":"Preprint. Code and data will be released soon"},{"id":"http://arxiv.org/abs/2404.15311v2","updated":"2024-08-07T08:14:56Z","published":"2024-04-02T17:01:51Z","title":"Fusing Pretrained ViTs with TCNet for Enhanced EEG Regression","summary":"  The task of Electroencephalogram (EEG) analysis is paramount to the\ndevelopment of Brain-Computer Interfaces (BCIs). However, to reach the goal of\ndeveloping robust, useful BCIs depends heavily on the speed and the accuracy at\nwhich BCIs can understand neural dynamics. In response to that goal, this paper\ndetails the integration of pre-trained Vision Transformers (ViTs) with Temporal\nConvolutional Networks (TCNet) to enhance the precision of EEG regression. The\ncore of this approach lies in harnessing the sequential data processing\nstrengths of ViTs along with the superior feature extraction capabilities of\nTCNet, to significantly improve EEG analysis accuracy. In addition, we analyze\nthe importance of how to construct optimal patches for the attention mechanism\nto analyze, balancing both speed and accuracy tradeoffs. Our results showcase a\nsubstantial improvement in regression accuracy, as evidenced by the reduction\nof Root Mean Square Error (RMSE) from 55.4 to 51.8 on EEGEyeNet's Absolute\nPosition Task, outperforming existing state-of-the-art models. Without\nsacrificing performance, we increase the speed of this model by an order of\nmagnitude (up to 4.32x faster). This breakthrough not only sets a new benchmark\nin EEG regression analysis but also opens new avenues for future research in\nthe integration of transformer architectures with specialized feature\nextraction methods for diverse EEG datasets.\n","authors":["Eric Modesitt","Haicheng Yin","Williams Huang Wang","Brian Lu"],"pdf_url":"https://arxiv.org/pdf/2404.15311v2.pdf","comment":"Accepted HCI International 2024"},{"id":"http://arxiv.org/abs/2408.02835v2","updated":"2024-08-07T08:09:02Z","published":"2024-08-05T21:12:12Z","title":"Training a multilayer dynamical spintronic network with standard machine\n  learning tools to perform time series classification","summary":"  The ability to process time-series at low energy cost is critical for many\napplications. Recurrent neural network, which can perform such tasks, are\ncomputationally expensive when implementing in software on conventional\ncomputers. Here we propose to implement a recurrent neural network in hardware\nusing spintronic oscillators as dynamical neurons. Using numerical simulations,\nwe build a multi-layer network and demonstrate that we can use backpropagation\nthrough time (BPTT) and standard machine learning tools to train this network.\nLeveraging the transient dynamics of the spintronic oscillators, we solve the\nsequential digits classification task with $89.83\\pm2.91~\\%$ accuracy, as good\nas the equivalent software network. We devise guidelines on how to choose the\ntime constant of the oscillators as well as hyper-parameters of the network to\nadapt to different input time scales.\n","authors":["Erwan Plouet","Dédalo Sanz-Hernández","Aymeric Vecchiola","Julie Grollier","Frank Mizrahi"],"pdf_url":"https://arxiv.org/pdf/2408.02835v2.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.03612v1","updated":"2024-08-07T08:08:08Z","published":"2024-08-07T08:08:08Z","title":"JARViS: Detecting Actions in Video Using Unified Actor-Scene Context\n  Relation Modeling","summary":"  Video action detection (VAD) is a formidable vision task that involves the\nlocalization and classification of actions within the spatial and temporal\ndimensions of a video clip. Among the myriad VAD architectures, two-stage VAD\nmethods utilize a pre-trained person detector to extract the region of interest\nfeatures, subsequently employing these features for action detection. However,\nthe performance of two-stage VAD methods has been limited as they depend solely\non localized actor features to infer action semantics. In this study, we\npropose a new two-stage VAD framework called Joint Actor-scene context Relation\nmodeling based on Visual Semantics (JARViS), which effectively consolidates\ncross-modal action semantics distributed globally across spatial and temporal\ndimensions using Transformer attention. JARViS employs a person detector to\nproduce densely sampled actor features from a keyframe. Concurrently, it uses a\nvideo backbone to create spatio-temporal scene features from a video clip.\nFinally, the fine-grained interactions between actors and scenes are modeled\nthrough a Unified Action-Scene Context Transformer to directly output the final\nset of actions in parallel. Our experimental results demonstrate that JARViS\noutperforms existing methods by significant margins and achieves\nstate-of-the-art performance on three popular VAD datasets, including AVA,\nUCF101-24, and JHMDB51-21.\n","authors":["Seok Hwan Lee","Taein Son","Soo Won Seo","Jisong Kim","Jun Won Choi"],"pdf_url":"https://arxiv.org/pdf/2408.03612v1.pdf","comment":"31 pages, 10 figures"},{"id":"http://arxiv.org/abs/2404.08271v3","updated":"2024-08-07T08:00:43Z","published":"2024-04-12T06:50:32Z","title":"Transfer Learning Study of Motion Transformer-based Trajectory\n  Predictions","summary":"  Trajectory planning in autonomous driving is highly dependent on predicting\nthe emergent behavior of other road users. Learning-based methods are currently\nshowing impressive results in simulation-based challenges, with\ntransformer-based architectures technologically leading the way. Ultimately,\nhowever, predictions are needed in the real world. In addition to the shifts\nfrom simulation to the real world, many vehicle- and country-specific shifts,\ni.e. differences in sensor systems, fusion and perception algorithms as well as\ntraffic rules and laws, are on the agenda. Since models that can cover all\nsystem setups and design domains at once are not yet foreseeable, model\nadaptation plays a central role. Therefore, a simulation-based study on\ntransfer learning techniques is conducted on basis of a transformer-based\nmodel. Furthermore, the study aims to provide insights into possible trade-offs\nbetween computational time and performance to support effective transfers into\nthe real world.\n","authors":["Lars Ullrich","Alex McMaster","Knut Graichen"],"pdf_url":"https://arxiv.org/pdf/2404.08271v3.pdf","comment":"Published in 2024 IEEE Intelligent Vehicles Symposium (IV), Jeju\n  Shinhwa World, Jeju Island, Korea, June 2-5, 2024"},{"id":"http://arxiv.org/abs/2408.03608v1","updated":"2024-08-07T07:54:19Z","published":"2024-08-07T07:54:19Z","title":"InPer: Whole-Process Domain Generalization via Causal Intervention and\n  Perturbation","summary":"  Despite the considerable advancements achieved by deep neural networks, their\nperformance tends to degenerate when the test environment diverges from the\ntraining ones. Domain generalization (DG) solves this issue by learning\nrepresentations independent of domain-related information, thus facilitating\nextrapolation to unseen environments. Existing approaches typically focus on\nformulating tailored training objectives to extract shared features from the\nsource data. However, the disjointed training and testing procedures may\ncompromise robustness, particularly in the face of unforeseen variations during\ndeployment. In this paper, we propose a novel and holistic framework based on\ncausality, named InPer, designed to enhance model generalization by\nincorporating causal intervention during training and causal perturbation\nduring testing. Specifically, during the training phase, we employ\nentropy-based causal intervention (EnIn) to refine the selection of causal\nvariables. To identify samples with anti-interference causal variables from the\ntarget domain, we propose a novel metric, homeostatic score, through causal\nperturbation (HoPer) to construct a prototype classifier in test time.\nExperimental results across multiple cross-domain tasks confirm the efficacy of\nInPer.\n","authors":["Luyao Tang","Yuxuan Yuan","Chaoqi Chen","Xinghao Ding","Yue Huang"],"pdf_url":"https://arxiv.org/pdf/2408.03608v1.pdf","comment":"Accepted by BMVC2024"},{"id":"http://arxiv.org/abs/2407.18990v2","updated":"2024-08-07T07:46:39Z","published":"2024-07-25T12:07:55Z","title":"Stay Tuned: An Empirical Study of the Impact of Hyperparameters on LLM\n  Tuning in Real-World Applications","summary":"  Fine-tuning Large Language Models (LLMs) is an effective method to enhance\ntheir performance on downstream tasks. However, choosing the appropriate\nsetting of tuning hyperparameters (HPs) is a labor-intensive and\ncomputationally expensive process. Here, we provide recommended HP\nconfigurations for practical use-cases that represent a better starting point\nfor practitioners, when considering two SOTA LLMs and two commonly used tuning\nmethods. We describe Coverage-based Search (CBS), a process for ranking HP\nconfigurations based on an offline extensive grid search, such that the top\nranked configurations collectively provide a practical robust recommendation\nfor a wide range of datasets and domains. We focus our experiments on\nLlama-3-8B and Mistral-7B, as well as full fine-tuning and LoRa, conducting a\ntotal of > 10,000 tuning experiments. Our results suggest that, in general,\nLlama-3-8B and LoRA should be preferred, when possible. Moreover, we show that\nfor both models and tuning methods, exploring only a few HP configurations, as\nrecommended by our analysis, can provide excellent results in practice, making\nthis work a valuable resource for practitioners.\n","authors":["Alon Halfon","Shai Gretz","Ofir Arviv","Artem Spector","Orith Toledo-Ronen","Yoav Katz","Liat Ein-Dor","Michal Shmueli-Scheuer","Noam Slonim"],"pdf_url":"https://arxiv.org/pdf/2407.18990v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03603v1","updated":"2024-08-07T07:46:08Z","published":"2024-08-07T07:46:08Z","title":"EnJa: Ensemble Jailbreak on Large Language Models","summary":"  As Large Language Models (LLMs) are increasingly being deployed in\nsafety-critical applications, their vulnerability to potential jailbreaks --\nmalicious prompts that can disable the safety mechanism of LLMs -- has\nattracted growing research attention. While alignment methods have been\nproposed to protect LLMs from jailbreaks, many have found that aligned LLMs can\nstill be jailbroken by carefully crafted malicious prompts, producing content\nthat violates policy regulations. Existing jailbreak attacks on LLMs can be\ncategorized into prompt-level methods which make up stories/logic to circumvent\nsafety alignment and token-level attack methods which leverage gradient methods\nto find adversarial tokens. In this work, we introduce the concept of Ensemble\nJailbreak and explore methods that can integrate prompt-level and token-level\njailbreak into a more powerful hybrid jailbreak attack. Specifically, we\npropose a novel EnJa attack to hide harmful instructions using prompt-level\njailbreak, boost the attack success rate using a gradient-based attack, and\nconnect the two types of jailbreak attacks via a template-based connector. We\nevaluate the effectiveness of EnJa on several aligned models and show that it\nachieves a state-of-the-art attack success rate with fewer queries and is much\nstronger than any individual jailbreak.\n","authors":["Jiahao Zhang","Zilong Wang","Ruofan Wang","Xingjun Ma","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2408.03603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03599v1","updated":"2024-08-07T07:36:49Z","published":"2024-08-07T07:36:49Z","title":"Activations Through Extensions: A Framework To Boost Performance Of\n  Neural Networks","summary":"  Activation functions are non-linearities in neural networks that allow them\nto learn complex mapping between inputs and outputs. Typical choices for\nactivation functions are ReLU, Tanh, Sigmoid etc., where the choice generally\ndepends on the application domain. In this work, we propose a\nframework/strategy that unifies several works on activation functions and\ntheoretically explains the performance benefits of these works. We also propose\nnovel techniques that originate from the framework and allow us to obtain\n``extensions'' (i.e. special generalizations of a given neural network) of\nneural networks through operations on activation functions. We theoretically\nand empirically show that ``extensions'' of neural networks have performance\nbenefits compared to vanilla neural networks with insignificant space and time\ncomplexity costs on standard test functions. We also show the benefits of\nneural network ``extensions'' in the time-series domain on real-world datasets.\n","authors":["Chandramouli Kamanchi","Sumatra Mukherjee","Kameshwaran Sampath","Pankaj Dayama","Arindam Jati","Vijay Ekambaram","Dzung Phan"],"pdf_url":"https://arxiv.org/pdf/2408.03599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17640v2","updated":"2024-08-07T07:29:39Z","published":"2024-05-27T20:24:03Z","title":"Probabilistically Plausible Counterfactual Explanations with Normalizing\n  Flows","summary":"  We present PPCEF, a novel method for generating probabilistically plausible\ncounterfactual explanations (CFs). PPCEF advances beyond existing methods by\ncombining a probabilistic formulation that leverages the data distribution with\nthe optimization of plausibility within a unified framework. Compared to\nreference approaches, our method enforces plausibility by directly optimizing\nthe explicit density function without assuming a particular family of\nparametrized distributions. This ensures CFs are not only valid (i.e., achieve\nclass change) but also align with the underlying data's probability density.\nFor that purpose, our approach leverages normalizing flows as powerful density\nestimators to capture the complex high-dimensional data distribution.\nFurthermore, we introduce a novel loss that balances the trade-off between\nachieving class change and maintaining closeness to the original instance while\nalso incorporating a probabilistic plausibility term. PPCEF's unconstrained\nformulation allows for efficient gradient-based optimization with batch\nprocessing, leading to orders of magnitude faster computation compared to prior\nmethods. Moreover, the unconstrained formulation of PPCEF allows for the\nseamless integration of future constraints tailored to specific counterfactual\nproperties. Finally, extensive evaluations demonstrate PPCEF's superiority in\ngenerating high-quality, probabilistically plausible counterfactual\nexplanations in high-dimensional tabular settings. This makes PPCEF a powerful\ntool for not only interpreting complex machine learning models but also for\nimproving fairness, accountability, and trust in AI systems.\n","authors":["Patryk Wielopolski","Oleksii Furman","Jerzy Stefanowski","Maciej Zięba"],"pdf_url":"https://arxiv.org/pdf/2405.17640v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09032v2","updated":"2024-08-07T07:20:23Z","published":"2024-03-14T01:51:35Z","title":"CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language\n  Models to Coding Preferences","summary":"  Evaluating the alignment of large language models (LLMs) with user-defined\ncoding preferences is a challenging endeavour that requires a deep assessment\nof LLMs' outputs. Existing methods and benchmarks rely primarily on automated\nmetrics and static analysis tools, which often fail to capture the nuances of\nuser instructions and LLM outputs. To address this gap, we propose using the\nLLM-as-a-Judge methodology to evaluate the alignment of LLMs with coding\npreferences. Based on this approach, we present CodeUltraFeedback, a\ncomprehensive dataset designed to facilitate the evaluation and improvement of\nLLM alignment. CodeUltraFeedback consists of 10,000 coding instructions, each\nannotated with four responses generated from a diverse pool of 14 LLMs. These\nresponses are ranked based on five distinct coding preferences using GPT-3.5 as\na judge, providing both numerical scores and detailed textual feedback. Our\nanalysis of CodeUltraFeedback reveals that responses from GPT-3.5 and GPT-4 are\ngenerally preferred over those from open-weight LLMs, highlighting significant\ndifferences in alignment between closed and open-weight models. In turn, we\nexplore the usage of CodeUltraFeedback as feedback data to fine-tune and align\nCodeLlama-7B-Instruct using supervised fine-tuning (SFT) and reinforcement\nlearning from AI feedback (RLAIF) with direct preference optimization (DPO).\nThe resulting aligned CodeLlama-7B-Instruct model outperforms larger LLMs in\nterms of alignment with coding preferences and shows improved functional\ncorrectness on the HumanEval+ benchmark compared to the original instruct\nmodel. Therefore, our contributions bridge the gap in preference tuning of LLMs\nfor code and set the stage for further advancements in model alignment and\nRLAIF in automated software engineering.\n","authors":["Martin Weyssow","Aton Kamanda","Houari Sahraoui"],"pdf_url":"https://arxiv.org/pdf/2403.09032v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03591v1","updated":"2024-08-07T07:09:14Z","published":"2024-08-07T07:09:14Z","title":"Focal Depth Estimation: A Calibration-Free, Subject- and Daytime\n  Invariant Approach","summary":"  In an era where personalized technology is increasingly intertwined with\ndaily life, traditional eye-tracking systems and autofocal glasses face a\nsignificant challenge: the need for frequent, user-specific calibration, which\nimpedes their practicality. This study introduces a groundbreaking\ncalibration-free method for estimating focal depth, leveraging machine learning\ntechniques to analyze eye movement features within short sequences. Our\napproach, distinguished by its innovative use of LSTM networks and\ndomain-specific feature engineering, achieves a mean absolute error (MAE) of\nless than 10 cm, setting a new focal depth estimation accuracy standard. This\nadvancement promises to enhance the usability of autofocal glasses and pave the\nway for their seamless integration into extended reality environments, marking\na significant leap forward in personalized visual technology.\n","authors":["Benedikt W. Hosp","Björn Severitt","Rajat Agarwala","Evgenia Rusak","Yannick Sauer","Siegfried Wahl"],"pdf_url":"https://arxiv.org/pdf/2408.03591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03590v1","updated":"2024-08-07T07:09:06Z","published":"2024-08-07T07:09:06Z","title":"Sensitivity analysis using the Metamodel of Optimal Prognosis","summary":"  In real case applications within the virtual prototyping process, it is not\nalways possible to reduce the complexity of the physical models and to obtain\nnumerical models which can be solved quickly. Usually, every single numerical\nsimulation takes hours or even days. Although the progresses in numerical\nmethods and high performance computing, in such cases, it is not possible to\nexplore various model configurations, hence efficient surrogate models are\nrequired. Generally the available meta-model techniques show several advantages\nand disadvantages depending on the investigated problem. In this paper we\npresent an automatic approach for the selection of the optimal suitable\nmeta-model for the actual problem. Together with an automatic reduction of the\nvariable space using advanced filter techniques an efficient approximation is\nenabled also for high dimensional problems. This filter techniques enable a\nreduction of the high dimensional variable space to a much smaller subspace\nwhere meta-model-based sensitivity analyses are carried out to assess the\ninfluence of important variables and to identify the optimal subspace with\ncorresponding surrogate model which enables the most accurate probabilistic\nanalysis. For this purpose we investigate variance-based and moment-free\nsensitivity measures in combination with advanced meta-models as moving least\nsquares and kriging.\n","authors":["Thomas Most","Johannes Will"],"pdf_url":"https://arxiv.org/pdf/2408.03590v1.pdf","comment":"presented at 8th Optimization and Stochastic Days, Weimar, Germany,\n  24-25 November, 2011"},{"id":"http://arxiv.org/abs/2408.03588v1","updated":"2024-08-07T07:04:29Z","published":"2024-08-07T07:04:29Z","title":"Facing the Music: Tackling Singing Voice Separation in Cinematic Audio\n  Source Separation","summary":"  Cinematic audio source separation (CASS) is a fairly new subtask of audio\nsource separation. A typical setup of CASS is a three-stem problem, with the\naim of separating the mixture into the dialogue stem (DX), music stem (MX), and\neffects stem (FX). In practice, however, several edge cases exist as some sound\nsources do not fit neatly in either of these three stems, necessitating the use\nof additional auxiliary stems in production. One very common edge case is the\nsinging voice in film audio, which may belong in either the DX or MX, depending\nheavily on the cinematic context. In this work, we demonstrate a very\nstraightforward extension of the dedicated-decoder Bandit and query-based\nsingle-decoder Banquet models to a four-stem problem, treating non-musical\ndialogue, instrumental music, singing voice, and effects as separate stems.\nInterestingly, the query-based Banquet model outperformed the dedicated-decoder\nBandit model. We hypothesized that this is due to a better feature alignment at\nthe bottleneck as enforced by the band-agnostic FiLM layer. Dataset and model\nimplementation will be made available at\nhttps://github.com/kwatcharasupat/source-separation-landing.\n","authors":["Karn N. Watcharasupat","Chih-Wei Wu","Iroro Orife"],"pdf_url":"https://arxiv.org/pdf/2408.03588v1.pdf","comment":"Submitted to the Late-Breaking Demo Session of the 25th International\n  Society for Music Information Retrieval (ISMIR) Conference, 2024"},{"id":"http://arxiv.org/abs/2405.15081v3","updated":"2024-08-07T07:03:11Z","published":"2024-05-23T22:07:54Z","title":"Distributed Harmonization: Federated Clustered Batch Effect Adjustment\n  and Generalization","summary":"  Independent and identically distributed (i.i.d.) data is essential to many\ndata analysis and modeling techniques. In the medical domain, collecting data\nfrom multiple sites or institutions is a common strategy that guarantees\nsufficient clinical diversity, determined by the decentralized nature of\nmedical data. However, data from various sites are easily biased by the local\nenvironment or facilities, thereby violating the i.i.d. rule. A common strategy\nis to harmonize the site bias while retaining important biological information.\nThe ComBat is among the most popular harmonization approaches and has recently\nbeen extended to handle distributed sites. However, when faced with situations\ninvolving newly joined sites in training or evaluating data from unknown/unseen\nsites, ComBat lacks compatibility and requires retraining with data from all\nthe sites. The retraining leads to significant computational and logistic\noverhead that is usually prohibitive. In this work, we develop a novel Cluster\nComBat harmonization algorithm, which leverages cluster patterns of the data in\ndifferent sites and greatly advances the usability of ComBat harmonization. We\nuse extensive simulation and real medical imaging data from ADNI to demonstrate\nthe superiority of the proposed approach. Our codes are provided in\nhttps://github.com/illidanlab/distributed-cluster-harmonization.\n","authors":["Bao Hoang","Yijiang Pang","Siqi Liang","Liang Zhan","Paul Thompson","Jiayu Zhou"],"pdf_url":"https://arxiv.org/pdf/2405.15081v3.pdf","comment":"11 pages, 7 figures, accepted to KDD2024-ADS"},{"id":"http://arxiv.org/abs/2407.21282v2","updated":"2024-08-07T07:00:13Z","published":"2024-07-31T02:12:05Z","title":"FedBChain: A Blockchain-enabled Federated Learning Framework for\n  Improving DeepConvLSTM with Comparative Strategy Insights","summary":"  Recent research in the field of Human Activity Recognition has shown that an\nimprovement in prediction performance can be achieved by reducing the number of\nLSTM layers. However, this kind of enhancement is only significant on\nmonolithic architectures, and when it runs on large-scale distributed training,\ndata security and privacy issues will be reconsidered, and its prediction\nperformance is unknown. In this paper, we introduce a novel framework:\nFedBChain, which integrates the federated learning paradigm based on a modified\nDeepConvLSTM architecture with a single LSTM layer. This framework performs\ncomparative tests of prediction performance on three different real-world\ndatasets based on three different hidden layer units (128, 256, and 512)\ncombined with five different federated learning strategies, respectively. The\nresults show that our architecture has significant improvements in Precision,\nRecall and F1-score compared to the centralized training approach on all\ndatasets with all hidden layer units for all strategies: FedAvg strategy\nimproves on average by 4.54%, FedProx improves on average by 4.57%,\nFedTrimmedAvg improves on average by 4.35%, Krum improves by 4.18% on average,\nand FedAvgM improves by 4.46% on average. Based on our results, it can be seen\nthat FedBChain not only improves in performance, but also guarantees the\nsecurity and privacy of user data compared to centralized training methods\nduring the training process. The code for our experiments is publicly available\n(https://github.com/Glen909/FedBChain).\n","authors":["Gaoxuan Li","Chern Hong Lim","Qiyao Ma","Xinyu Tang","Hwa Hui Tew","Fan Ding","Xuewen Luo"],"pdf_url":"https://arxiv.org/pdf/2407.21282v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03585v1","updated":"2024-08-07T06:44:47Z","published":"2024-08-07T06:44:47Z","title":"Hierarchical Neural Constructive Solver for Real-world TSP Scenarios","summary":"  Existing neural constructive solvers for routing problems have predominantly\nemployed transformer architectures, conceptualizing the route construction as a\nset-to-sequence learning task. However, their efficacy has primarily been\ndemonstrated on entirely random problem instances that inadequately capture\nreal-world scenarios. In this paper, we introduce realistic Traveling Salesman\nProblem (TSP) scenarios relevant to industrial settings and derive the\nfollowing insights: (1) The optimal next node (or city) to visit often lies\nwithin proximity to the current node, suggesting the potential benefits of\nbiasing choices based on current locations. (2) Effectively solving the TSP\nrequires robust tracking of unvisited nodes and warrants succinct grouping\nstrategies. Building upon these insights, we propose integrating a learnable\nchoice layer inspired by Hypernetworks to prioritize choices based on the\ncurrent location, and a learnable approximate clustering algorithm inspired by\nthe Expectation-Maximization algorithm to facilitate grouping the unvisited\ncities. Together, these two contributions form a hierarchical approach towards\nsolving the realistic TSP by considering both immediate local neighbourhoods\nand learning an intermediate set of node representations. Our hierarchical\napproach yields superior performance compared to both classical and recent\ntransformer models, showcasing the efficacy of the key designs.\n","authors":["Yong Liang Goh","Zhiguang Cao","Yining Ma","Yanfei Dong","Mohammed Haroon Dupty","Wee Sun Lee"],"pdf_url":"https://arxiv.org/pdf/2408.03585v1.pdf","comment":"Accepted to KDD 2024"},{"id":"http://arxiv.org/abs/2408.03574v1","updated":"2024-08-07T06:26:04Z","published":"2024-08-07T06:26:04Z","title":"Teach CLIP to Develop a Number Sense for Ordinal Regression","summary":"  Ordinal regression is a fundamental problem within the field of computer\nvision, with customised well-trained models on specific tasks. While\npre-trained vision-language models (VLMs) have exhibited impressive performance\non various vision tasks, their potential for ordinal regression has received\nless exploration. In this study, we first investigate CLIP's potential for\nordinal regression, from which we expect the model could generalise to\ndifferent ordinal regression tasks and scenarios. Unfortunately, vanilla CLIP\nfails on this task, since current VLMs have a well-documented limitation of\nencapsulating compositional concepts such as number sense. We propose a simple\nyet effective method called NumCLIP to improve the quantitative understanding\nof VLMs. We disassemble the exact image to number-specific text matching\nproblem into coarse classification and fine prediction stages. We discretize\nand phrase each numerical bin with common language concept to better leverage\nthe available pre-trained alignment in CLIP. To consider the inherent\ncontinuous property of ordinal regression, we propose a novel fine-grained\ncross-modal ranking-based regularisation loss specifically designed to keep\nboth semantic and ordinal alignment in CLIP's feature space. Experimental\nresults on three general ordinal regression tasks demonstrate the effectiveness\nof NumCLIP, with 10% and 3.83% accuracy improvement on historical image dating\nand image aesthetics assessment task, respectively. Code is publicly available\nat https://github.com/xmed-lab/NumCLIP.\n","authors":["Yao Du","Qiang Zhai","Weihang Dai","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2408.03574v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2408.03572v1","updated":"2024-08-07T06:16:17Z","published":"2024-08-07T06:16:17Z","title":"2D-OOB: Attributing Data Contribution through Joint Valuation Framework","summary":"  Data valuation has emerged as a powerful framework to quantify the\ncontribution of each datum to the training of a particular machine learning\nmodel. However, it is crucial to recognize that the quality of various cells\nwithin a single data point can vary greatly in practice. For example, even in\nthe case of an abnormal data point, not all cells are necessarily noisy. The\nsingle scalar valuation assigned by existing methods blurs the distinction\nbetween noisy and clean cells of a data point, thereby compromising the\ninterpretability of the valuation. In this paper, we propose 2D-OOB, an\nout-of-bag estimation framework for jointly determining helpful (or\ndetrimental) samples, as well as the particular cells that drive them. Our\ncomprehensive experiments demonstrate that 2D-OOB achieves state-of-the-art\nperformance across multiple use cases, while being exponentially faster. 2D-OOB\nexcels in detecting and rectifying fine-grained outliers at the cell level, as\nwell as localizing backdoor triggers in data poisoning attacks.\n","authors":["Yifan Sun","Jingyan Shen","Yongchan Kwon"],"pdf_url":"https://arxiv.org/pdf/2408.03572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03569v1","updated":"2024-08-07T06:11:37Z","published":"2024-08-07T06:11:37Z","title":"Maximum a Posteriori Estimation for Linear Structural Dynamics Models\n  Using Bayesian Optimization with Rational Polynomial Chaos Expansions","summary":"  Bayesian analysis enables combining prior knowledge with measurement data to\nlearn model parameters. Commonly, one resorts to computing the maximum a\nposteriori (MAP) estimate, when only a point estimate of the parameters is of\ninterest. We apply MAP estimation in the context of structural dynamic models,\nwhere the system response can be described by the frequency response function.\nTo alleviate high computational demands from repeated expensive model calls, we\nutilize a rational polynomial chaos expansion (RPCE) surrogate model that\nexpresses the system frequency response as a rational of two polynomials with\ncomplex coefficients. We propose an extension to an existing sparse Bayesian\nlearning approach for RPCE based on Laplace's approximation for the posterior\ndistribution of the denominator coefficients. Furthermore, we introduce a\nBayesian optimization approach, which allows to adaptively enrich the\nexperimental design throughout the optimization process of MAP estimation.\nThereby, we utilize the expected improvement acquisition function as a means to\nidentify sample points in the input space that are possibly associated with\nlarge objective function values. The acquisition function is estimated through\nMonte Carlo sampling based on the posterior distribution of the expansion\ncoefficients identified in the sparse Bayesian learning process. By combining\nthe sparsity-inducing learning procedure with the sequential experimental\ndesign, we effectively reduce the number of model evaluations in the MAP\nestimation problem. We demonstrate the applicability of the presented methods\non the parameter updating problem of an algebraic two-degree-of-freedom system\nand the finite element model of a cross-laminated timber plate.\n","authors":["Felix Schneider","Iason Papaioannou","Bruno Sudret","Gerhard Müller"],"pdf_url":"https://arxiv.org/pdf/2408.03569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03568v1","updated":"2024-08-07T06:11:25Z","published":"2024-08-07T06:11:25Z","title":"A comparative study of generative adversarial networks for image\n  recognition algorithms based on deep learning and traditional methods","summary":"  In this paper, an image recognition algorithm based on the combination of\ndeep learning and generative adversarial network (GAN) is studied, and compared\nwith traditional image recognition methods. The purpose of this study is to\nevaluate the advantages and application prospects of deep learning technology,\nespecially GAN, in the field of image recognition. Firstly, this paper reviews\nthe basic principles and techniques of traditional image recognition methods,\nincluding the classical algorithms based on feature extraction such as SIFT,\nHOG and their combination with support vector machine (SVM), random forest, and\nother classifiers. Then, the working principle, network structure, and unique\nadvantages of GAN in image generation and recognition are introduced. In order\nto verify the effectiveness of GAN in image recognition, a series of\nexperiments are designed and carried out using multiple public image data sets\nfor training and testing. The experimental results show that compared with\ntraditional methods, GAN has excellent performance in processing complex\nimages, recognition accuracy, and anti-noise ability. Specifically, Gans are\nbetter able to capture high-dimensional features and details of images,\nsignificantly improving recognition performance. In addition, Gans shows unique\nadvantages in dealing with image noise, partial missing information, and\ngenerating high-quality images.\n","authors":["Yihao Zhong","Yijing Wei","Yingbin Liang","Xiqing Liu","Rongwei Ji","Yiru Cang"],"pdf_url":"https://arxiv.org/pdf/2408.03568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11755v3","updated":"2024-08-07T06:05:42Z","published":"2024-03-18T13:03:24Z","title":"Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs","summary":"  Prompt ensembling of Large Language Model (LLM) generated category-specific\nprompts has emerged as an effective method to enhance zero-shot recognition\nability of Vision-Language Models (VLMs). To obtain these category-specific\nprompts, the present methods rely on hand-crafting the prompts to the LLMs for\ngenerating VLM prompts for the downstream tasks. However, this requires\nmanually composing these task-specific prompts and still, they might not cover\nthe diverse set of visual concepts and task-specific styles associated with the\ncategories of interest. To effectively take humans out of the loop and\ncompletely automate the prompt generation process for zero-shot recognition, we\npropose Meta-Prompting for Visual Recognition (MPVR). Taking as input only\nminimal information about the target task, in the form of its short natural\nlanguage description, and a list of associated class labels, MPVR automatically\nproduces a diverse set of category-specific prompts resulting in a strong\nzero-shot classifier. MPVR generalizes effectively across various popular\nzero-shot image recognition benchmarks belonging to widely different domains\nwhen tested with multiple LLMs and VLMs. For example, MPVR obtains a zero-shot\nrecognition improvement over CLIP by up to 19.8% and 18.2% (5.0% and 4.5% on\naverage over 20 datasets) leveraging GPT and Mixtral LLMs, respectively\n","authors":["M. Jehanzeb Mirza","Leonid Karlinsky","Wei Lin","Sivan Doveh","Jakub Micorek","Mateusz Kozinski","Hilde Kuehne","Horst Possegger"],"pdf_url":"https://arxiv.org/pdf/2403.11755v3.pdf","comment":"ECCV Camera Ready. Code & Data:\n  https://jmiemirza.github.io/Meta-Prompting/"},{"id":"http://arxiv.org/abs/2408.03029v2","updated":"2024-08-07T05:59:46Z","published":"2024-08-06T08:22:16Z","title":"Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning","summary":"  Reward shaping addresses the challenge of sparse rewards in reinforcement\nlearning by constructing denser and more informative reward signals. To achieve\nself-adaptive and highly efficient reward shaping, we propose a novel method\nthat incorporates success rates derived from historical experiences into shaped\nrewards. Our approach utilizes success rates sampled from Beta distributions,\nwhich dynamically evolve from uncertain to reliable values as more data is\ncollected. Initially, the self-adaptive success rates exhibit more randomness\nto encourage exploration. Over time, they become more certain to enhance\nexploitation, thus achieving a better balance between exploration and\nexploitation. We employ Kernel Density Estimation (KDE) combined with Random\nFourier Features (RFF) to derive the Beta distributions, resulting in a\ncomputationally efficient implementation in high-dimensional continuous state\nspaces. This method provides a non-parametric and learning-free approach. The\nproposed method is evaluated on a wide range of continuous control tasks with\nsparse and delayed rewards, demonstrating significant improvements in sample\nefficiency and convergence stability compared to relevant baselines.\n","authors":["Haozhe Ma","Zhengding Luo","Thanh Vinh Vo","Kuankuan Sima","Tze-Yun Leong"],"pdf_url":"https://arxiv.org/pdf/2408.03029v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03561v1","updated":"2024-08-07T05:50:17Z","published":"2024-08-07T05:50:17Z","title":"MPC-Minimized Secure LLM Inference","summary":"  Many inference services based on large language models (LLMs) pose a privacy\nconcern, either revealing user prompts to the service or the proprietary\nweights to the user. Secure inference offers a solution to this problem through\nsecure multi-party computation (MPC), however, it is still impractical for\nmodern LLM workload due to the large overhead imposed by MPC. To address this\noverhead, we propose Marill, a framework that adapts LLM fine-tuning to\nminimize MPC usage during secure inference. Marill introduces high-level\narchitectural changes during fine-tuning that significantly reduce the number\nof expensive operations needed within MPC during inference, by removing some\nand relocating others outside MPC without compromising security. As a result,\nMarill-generated models are more efficient across all secure inference\nprotocols and our approach complements MPC-friendly approximations for such\noperations. Compared to standard fine-tuning, Marill results in 3.6-11.3x\nbetter runtime and 2.4-6.9x better communication during secure inference across\nvarious MPC settings, while typically preserving over 90% performance across\ndownstream tasks.\n","authors":["Deevashwer Rathee","Dacheng Li","Ion Stoica","Hao Zhang","Raluca Popa"],"pdf_url":"https://arxiv.org/pdf/2408.03561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03560v1","updated":"2024-08-07T05:48:05Z","published":"2024-08-07T05:48:05Z","title":"In2Core: Leveraging Influence Functions for Coreset Selection in\n  Instruction Finetuning of Large Language Models","summary":"  Despite advancements, fine-tuning Large Language Models (LLMs) remains costly\ndue to the extensive parameter count and substantial data requirements for\nmodel generalization. Accessibility to computing resources remains a barrier\nfor the open-source community. To address this challenge, we propose the\nIn2Core algorithm, which selects a coreset by analyzing the correlation between\ntraining and evaluation samples with a trained model. Notably, we assess the\nmodel's internal gradients to estimate this relationship, aiming to rank the\ncontribution of each training point. To enhance efficiency, we propose an\noptimization to compute influence functions with a reduced number of layers\nwhile achieving similar accuracy. By applying our algorithm to instruction\nfine-tuning data of LLMs, we can achieve similar performance with just 50% of\nthe training data. Meantime, using influence functions to analyze model\ncoverage to certain testing samples could provide a reliable and interpretable\nsignal on the training set's coverage of those test points.\n","authors":["Ayrton San Joaquin","Bin Wang","Zhengyuan Liu","Nicholas Asher","Brian Lim","Philippe Muller","Nancy Chen"],"pdf_url":"https://arxiv.org/pdf/2408.03560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03554v1","updated":"2024-08-07T05:30:10Z","published":"2024-08-07T05:30:10Z","title":"Empirical Analysis of Large Vision-Language Models against Goal\n  Hijacking via Visual Prompt Injection","summary":"  We explore visual prompt injection (VPI) that maliciously exploits the\nability of large vision-language models (LVLMs) to follow instructions drawn\nonto the input image. We propose a new VPI method, \"goal hijacking via visual\nprompt injection\" (GHVPI), that swaps the execution task of LVLMs from an\noriginal task to an alternative task designated by an attacker. The\nquantitative analysis indicates that GPT-4V is vulnerable to the GHVPI and\ndemonstrates a notable attack success rate of 15.8%, which is an unignorable\nsecurity risk. Our analysis also shows that successful GHVPI requires high\ncharacter recognition capability and instruction-following ability in LVLMs.\n","authors":["Subaru Kimura","Ryota Tanaka","Shumpei Miyawaki","Jun Suzuki","Keisuke Sakaguchi"],"pdf_url":"https://arxiv.org/pdf/2408.03554v1.pdf","comment":"8 pages, 6 figures, Accepted to NAACL 2024 SRW"},{"id":"http://arxiv.org/abs/2106.11760v5","updated":"2024-08-07T05:28:30Z","published":"2021-06-19T06:25:10Z","title":"Fingerprinting Image-to-Image Generative Adversarial Networks","summary":"  Generative Adversarial Networks (GANs) have been widely used in various\napplication scenarios. Since the production of a commercial GAN requires\nsubstantial computational and human resources, the copyright protection of GANs\nis urgently needed. This paper presents a novel fingerprinting scheme for the\nIntellectual Property (IP) protection of image-to-image GANs based on a trusted\nthird party. We break through the stealthiness and robustness bottlenecks\nsuffered by previous fingerprinting methods for classification models being\nnaively transferred to GANs. Specifically, we innovatively construct a\ncomposite deep learning model from the target GAN and a classifier. Then we\ngenerate fingerprint samples from this composite model, and embed them in the\nclassifier for effective ownership verification. This scheme inspires some\nconcrete methodologies to practically protect the modern image-to-image\ntranslation GANs. Theoretical analysis proves that these methods can satisfy\ndifferent security requirements necessary for IP protection. We also conduct\nextensive experiments to show that our solutions outperform existing\nstrategies.\n","authors":["Guanlin Li","Guowen Xu","Han Qiu","Shangwei Guo","Run Wang","Jiwei Li","Tianwei Zhang","Rongxing Lu"],"pdf_url":"https://arxiv.org/pdf/2106.11760v5.pdf","comment":"Accepted by EuroS&P 2024"},{"id":"http://arxiv.org/abs/2302.01089v4","updated":"2024-08-07T05:22:57Z","published":"2023-02-02T13:22:18Z","title":"Curriculum Learning for ab initio Deep Learned Refractive Optics","summary":"  Deep optical optimization has recently emerged as a new paradigm for\ndesigning computational imaging systems using only the output image as the\nobjective. However, it has been limited to either simple optical systems\nconsisting of a single element such as a diffractive optical element (DOE) or\nmetalens, or the fine-tuning of compound lenses from good initial designs. Here\nwe present a DeepLens design method based on curriculum learning, which is able\nto learn optical designs of compound lenses ab initio from randomly initialized\nsurfaces without human intervention, therefore overcoming the need for a good\ninitial design. We demonstrate the effectiveness of our approach by fully\nautomatically designing both classical imaging lenses and a large field-of-view\nextended depth-of-field computational lens in a cellphone-style form factor,\nwith highly aspheric surfaces and a short back focal length.\n","authors":["Xinge Yang","Qiang Fu","Wolfgang Heidrich"],"pdf_url":"https://arxiv.org/pdf/2302.01089v4.pdf","comment":"Automatically design computational lenses from scratch with\n  differentiable ray tracing"},{"id":"http://arxiv.org/abs/2408.02050v2","updated":"2024-08-07T05:15:53Z","published":"2024-08-04T14:57:44Z","title":"Recovering the state and dynamics of autonomous system with partial\n  states solution using neural networks","summary":"  In this paper we explore the performance of deep hidden physics model (M.\nRaissi 2018) for autonomous systems. These systems are described by set of\nordinary differential equations which do not explicitly depend on time. Such\nsystems can be found in nature and have applications in modeling chemical\nconcentrations, population dynamics, n-body problems in physics etc. In this\nwork we consider dynamics of states, which explain how the states will evolve\nare unknown to us. We approximate state and dynamics both using neural\nnetworks. We have considered examples of 2D linear/nonlinear and Lorenz\nsystems. We observe that even without knowing all the states information, we\ncan estimate dynamics of certain states whose state information are known.\n","authors":["Vijay Kag"],"pdf_url":"https://arxiv.org/pdf/2408.02050v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11652v5","updated":"2024-08-07T05:14:24Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v5.pdf","comment":"I found critical errors in the manuscript affecting its validity. I\n  need to correct these before resubmitting. Major changes to methodology and\n  results are underway, significantly altering the content. I will resubmit the\n  revised version"},{"id":"http://arxiv.org/abs/2304.06237v3","updated":"2024-08-07T04:59:21Z","published":"2023-04-13T03:20:45Z","title":"Deep learning based ECG segmentation for delineation of diverse\n  arrhythmias","summary":"  Accurate delineation of key waveforms in an ECG is a critical step in\nextracting relevant features to support the diagnosis and treatment of heart\nconditions. Although deep learning based methods using segmentation models to\nlocate P, QRS, and T waves have shown promising results, their ability to\nhandle arrhythmias has not been studied in any detail. In this paper we\ninvestigate the effect of arrhythmias on delineation quality and develop\nstrategies to improve performance in such cases. We introduce a U-Net-like\nsegmentation model for ECG delineation with a particular focus on diverse\narrhythmias. This is followed by a post-processing algorithm which removes\nnoise and automatically determines the boundaries of P, QRS, and T waves. Our\nmodel has been trained on a diverse dataset and evaluated against the LUDB and\nQTDB datasets to show strong performance, with F1-scores exceeding 99% for QRS\nand T waves, and over 97% for P waves in the LUDB dataset. Furthermore, we\nassess various models across a wide array of arrhythmias and observe that\nmodels with a strong performance on standard benchmarks may still perform\npoorly on arrhythmias that are underrepresented in these benchmarks, such as\ntachycardias. We propose solutions to address this discrepancy.\n","authors":["Chankyu Joung","Mijin Kim","Taejin Paik","Seong-Ho Kong","Seung-Young Oh","Won Kyeong Jeon","Jae-hu Jeon","Joong-Sik Hong","Wan-Joong Kim","Woong Kook","Myung-Jin Cha","Otto van Koert"],"pdf_url":"https://arxiv.org/pdf/2304.06237v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03539v1","updated":"2024-08-07T04:35:38Z","published":"2024-08-07T04:35:38Z","title":"Deep Reinforcement Learning for Robotics: A Survey of Real-World\n  Successes","summary":"  Reinforcement learning (RL), particularly its combination with deep neural\nnetworks referred to as deep RL (DRL), has shown tremendous promise across a\nwide range of applications, suggesting its potential for enabling the\ndevelopment of sophisticated robotic behaviors. Robotics problems, however,\npose fundamental difficulties for the application of RL, stemming from the\ncomplexity and cost of interacting with the physical world. This article\nprovides a modern survey of DRL for robotics, with a particular focus on\nevaluating the real-world successes achieved with DRL in realizing several key\nrobotic competencies. Our analysis aims to identify the key factors underlying\nthose exciting successes, reveal underexplored areas, and provide an overall\ncharacterization of the status of DRL in robotics. We highlight several\nimportant avenues for future work, emphasizing the need for stable and\nsample-efficient real-world RL paradigms, holistic approaches for discovering\nand integrating various competencies to tackle complex long-horizon, open-world\ntasks, and principled development and evaluation procedures. This survey is\ndesigned to offer insights for both RL practitioners and roboticists toward\nharnessing RL's power to create generally capable real-world robotic systems.\n","authors":["Chen Tang","Ben Abbatematteo","Jiaheng Hu","Rohan Chandra","Roberto Martín-Martín","Peter Stone"],"pdf_url":"https://arxiv.org/pdf/2408.03539v1.pdf","comment":"The first three authors contributed equally. Accepted to Annual\n  Review of Control, Robotics, and Autonomous Systems"},{"id":"http://arxiv.org/abs/2408.03526v1","updated":"2024-08-07T03:37:25Z","published":"2024-08-07T03:37:25Z","title":"Minimum Enclosing Ball Synthetic Minority Oversampling Technique from a\n  Geometric Perspective","summary":"  Class imbalance refers to the significant difference in the number of samples\nfrom different classes within a dataset, making it challenging to identify\nminority class samples correctly. This issue is prevalent in real-world\nclassification tasks, such as software defect prediction, medical diagnosis,\nand fraud detection. The synthetic minority oversampling technique (SMOTE) is\nwidely used to address class imbalance issue, which is based on interpolation\nbetween randomly selected minority class samples and their neighbors. However,\ntraditional SMOTE and most of its variants only interpolate between existing\nsamples, which may be affected by noise samples in some cases and synthesize\nsamples that lack diversity. To overcome these shortcomings, this paper\nproposes the Minimum Enclosing Ball SMOTE (MEB-SMOTE) method from a geometry\nperspective. Specifically, MEB is innovatively introduced into the oversampling\nmethod to construct a representative point. Then, high-quality samples are\nsynthesized by interpolation between this representative point and the existing\nsamples. The rationale behind constructing a representative point is discussed,\ndemonstrating that the center of MEB is more suitable as the representative\npoint. To exhibit the superiority of MEB-SMOTE, experiments are conducted on 15\nreal-world imbalanced datasets. The results indicate that MEB-SMOTE can\neffectively improve the classification performance on imbalanced datasets.\n","authors":["Yi-Yang Shangguan","Shi-Shun Chen","Xiao-Yang Li"],"pdf_url":"https://arxiv.org/pdf/2408.03526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.04360v2","updated":"2024-08-07T03:36:51Z","published":"2024-04-05T19:14:14Z","title":"Prompt Public Large Language Models to Synthesize Data for Private\n  On-device Applications","summary":"  Pre-training on public data is an effective method to improve the performance\nfor federated learning (FL) with differential privacy (DP). This paper\ninvestigates how large language models (LLMs) trained on public data can\nimprove the quality of pre-training data for the on-device language models\ntrained with DP and FL. We carefully design LLM prompts to filter and transform\nexisting public data, and generate new data to resemble the real user data\ndistribution. The model pre-trained on our synthetic dataset achieves relative\nimprovement of 19.0% and 22.8% in next word prediction accuracy compared to the\nbaseline model pre-trained on a standard public dataset, when evaluated over\nthe real user data in Gboard (Google Keyboard, a production mobile keyboard\napplication). Furthermore, our method achieves evaluation accuracy better than\nor comparable to the baseline during the DP FL fine-tuning over millions of\nmobile devices, and our final model outperforms the baseline in production A/B\ntesting. Our experiments demonstrate the strengths of LLMs in synthesizing data\nclose to the private distribution even without accessing the private data, and\nalso suggest future research directions to further reduce the distribution gap.\n","authors":["Shanshan Wu","Zheng Xu","Yanxiang Zhang","Yuanbo Zhang","Daniel Ramage"],"pdf_url":"https://arxiv.org/pdf/2404.04360v2.pdf","comment":"COLM 2024"},{"id":"http://arxiv.org/abs/2305.18403v5","updated":"2024-08-07T03:30:30Z","published":"2023-05-28T15:15:48Z","title":"LoRAPrune: Structured Pruning Meets Low-Rank Parameter-Efficient\n  Fine-Tuning","summary":"  Large Language Models (LLMs), such as LLaMA and T5, have shown exceptional\nperformance across various tasks through fine-tuning. Although low-rank\nadaption (LoRA) has emerged to cheaply fine-tune these LLMs on downstream\ntasks, their deployment is still hindered by the vast model scale and\ncomputational costs. Post-training model pruning offers a way to compress LLMs.\nHowever, the current pruning methods designed for LLMs are not compatible with\nLoRA. This is due to their utilization of unstructured pruning on LLMs,\nimpeding the merging of LoRA weights, or their dependence on the gradients of\npre-trained weights to guide pruning, which can impose significant memory\noverhead. To this end, we propose LoRAPrune, a new framework that delivers an\naccurate structured pruned model in a highly memory-efficient manner.\nSpecifically, we first design a LoRA-guided pruning criterion, which uses the\nweights and gradients of LoRA, rather than the gradients of pre-trained weights\nfor importance estimation. We subsequently integrate this criterion into an\niterative pruning process, effectively removing redundant channels and heads.\nExtensive experimental results demonstrate the superior performance of our\nLoRAPrune over existing approaches on the LLaMA series models. At a 50\\%\ncompression rate, LoRAPrune demonstrates superior performance over LLM-Pruner,\nachieving a reduction in perplexity by 4.81 on WikiText2 and 3.46 on PTB, while\nalso decreasing memory usage by 52.6%. Besides, LoRAPrune also matches\nsemi-structural pruning across multiple LLMs, proving its wide applicability.\nThe code is available at https://github.com/aim-uofa/LoRAPrune.\n","authors":["Mingyang Zhang","Hao Chen","Chunhua Shen","Zhen Yang","Linlin Ou","Xinyi Yu","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2305.18403v5.pdf","comment":"accepted by acl 2024 findings"},{"id":"http://arxiv.org/abs/2408.03516v1","updated":"2024-08-07T02:54:43Z","published":"2024-08-07T02:54:43Z","title":"Leveraging LLMs for Enhanced Open-Vocabulary 3D Scene Understanding in\n  Autonomous Driving","summary":"  This paper introduces a novel method for open-vocabulary 3D scene\nunderstanding in autonomous driving by combining Language Embedded 3D Gaussians\nwith Large Language Models (LLMs) for enhanced inference. We propose utilizing\nLLMs to generate contextually relevant canonical phrases for segmentation and\nscene interpretation. Our method leverages the contextual and semantic\ncapabilities of LLMs to produce a set of canonical phrases, which are then\ncompared with the language features embedded in the 3D Gaussians. This\nLLM-guided approach significantly improves zero-shot scene understanding and\ndetection of objects of interest, even in the most challenging or unfamiliar\nenvironments. Experimental results on the WayveScenes101 dataset demonstrate\nthat our approach surpasses state-of-the-art methods in terms of accuracy and\nflexibility for open-vocabulary object detection and segmentation. This work\nrepresents a significant advancement towards more intelligent, context-aware\nautonomous driving systems, effectively bridging 3D scene representation with\nhigh-level semantic understanding.\n","authors":["Amirhosein Chahe","Lifeng Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.03516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16087v4","updated":"2024-08-07T02:36:19Z","published":"2024-06-23T12:02:17Z","title":"Imperative Learning: A Self-supervised Neural-Symbolic Learning\n  Framework for Robot Autonomy","summary":"  Data-driven methods such as reinforcement and imitation learning have\nachieved remarkable success in robot autonomy. However, their data-centric\nnature still hinders them from generalizing well to ever-changing environments.\nMoreover, collecting large datasets for robotic tasks is often impractical and\nexpensive. To overcome these challenges, we introduce a new self-supervised\nneural-symbolic (NeSy) computational framework, imperative learning (IL), for\nrobot autonomy, leveraging the generalization abilities of symbolic reasoning.\nThe framework of IL consists of three primary components: a neural module, a\nreasoning engine, and a memory system. We formulate IL as a special bilevel\noptimization (BLO), which enables reciprocal learning over the three modules.\nThis overcomes the label-intensive obstacles associated with data-driven\napproaches and takes advantage of symbolic reasoning concerning logical\nreasoning, physical principles, geometric analysis, etc. We discuss several\noptimization techniques for IL and verify their effectiveness in five distinct\nrobot autonomy tasks including path planning, rule induction, optimal control,\nvisual odometry, and multi-robot routing. Through various experiments, we show\nthat IL can significantly enhance robot autonomy capabilities and we anticipate\nthat it will catalyze further research across diverse domains.\n","authors":["Chen Wang","Kaiyi Ji","Junyi Geng","Zhongqiang Ren","Taimeng Fu","Fan Yang","Yifan Guo","Haonan He","Xiangyu Chen","Zitong Zhan","Qiwei Du","Shaoshu Su","Bowen Li","Yuheng Qiu","Yi Du","Qihang Li","Yifan Yang","Xiao Lin","Zhipeng Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.16087v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03508v1","updated":"2024-08-07T02:19:17Z","published":"2024-08-07T02:19:17Z","title":"Unsupervised, Self-driving Multi-Step Growth of InAs/GaAs Quantum Dots\n  Heterostructures Guided by Machine Learning","summary":"  The semiconductor industry has prioritized automating repetitive tasks by\nclosed-loop, autonomous experimentation which enables accelerated optimization\nof complex multi-step processes. The emergence of machine learning (ML) has\nushered in automated process with minimal human intervention. In this work, we\ndevelop SemiEpi, a self-driving automation platform capable of executing\nmolecular beam epitaxy (MBE) growth with multi-steps, continuous in-situ\nmonitoring, and on-the-fly feedback control. By integrating standard hardware,\nhomemade software, curve fitting, and multiple ML models, SemiEpi operates\nautonomously, eliminating the need for extensive expertise in MBE processes to\nachieve optimal outcomes. The platform actively learns from previous\nexperimental results, identifying favorable conditions and proposing new\nexperiments to achieve the desired results. We standardize and optimize growth\nfor InAs/GaAs quantum dots (QDs) heterostructures to showcase the power of\nML-guided multi-step growth. A temperature calibration was implemented to get\nthe initial growth condition, and fine control of the process was executed\nusing ML. Leveraging RHEED movies acquired during the growth, SemiEpi\nsuccessfully identified and optimized a novel route for multi-step\nheterostructure growth. This work demonstrates the capabilities of closed-loop,\nML-guided systems in addressing challenges in multi-step growth for any device.\nOur method is critical to achieve repeatable materials growth using\ncommercially scalable tools. Our strategy facilitates the development of a\nhardware-independent process and enhancing process repeatability and stability,\neven without exhaustive knowledge of growth parameters.\n","authors":["Chao Shen","Wenkang Zhan","Hongyu Sun","Kaiyao Xin","Bo Xu","Zhanguo Wang","Chao Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.03508v1.pdf","comment":"5 figures"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2408.03940v1","updated":"2024-08-07T17:59:40Z","published":"2024-08-07T17:59:40Z","title":"How Well Can Vision Language Models See Image Details?","summary":"  Large Language Model-based Vision-Language Models (LLM-based VLMs) have\ndemonstrated impressive results in various vision-language understanding tasks.\nHowever, how well these VLMs can see image detail beyond the semantic level\nremains unclear. In our study, we introduce a pixel value prediction task (PVP)\nto explore \"How Well Can Vision Language Models See Image Details?\" and to\nassist VLMs in perceiving more details. Typically, these models comprise a\nfrozen CLIP visual encoder, a large language model, and a connecting module.\nAfter fine-tuning VLMs on the PVP task, we find: 1) existing VLMs struggle to\npredict precise pixel values by only fine-tuning the connection module and LLM;\nand 2) prediction precision is significantly improved when the vision encoder\nis also adapted. Additionally, our research reveals that incorporating pixel\nvalue prediction as one of the VLM pre-training tasks and vision encoder\nadaptation markedly boosts VLM performance on downstream image-language\nunderstanding tasks requiring detailed image perception, such as referring\nimage segmentation (with an average +10.19 cIoU improvement) and video game\ndecision making (with average score improvements of +80.34 and +70.54 on two\ngames, respectively).\n","authors":["Chenhui Gou","Abdulwahab Felemban","Faizan Farooq Khan","Deyao Zhu","Jianfei Cai","Hamid Rezatofighi","Mohamed Elhoseiny"],"pdf_url":"https://arxiv.org/pdf/2408.03940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19674v3","updated":"2024-08-07T17:45:05Z","published":"2024-07-29T03:30:09Z","title":"Advancing Prompt Learning through an External Layer","summary":"  Prompt learning represents a promising method for adapting pre-trained\nvision-language models (VLMs) to various downstream tasks by learning a set of\ntext embeddings. One challenge inherent to these methods is the poor\ngeneralization performance due to the invalidity of the learned text embeddings\nfor unseen tasks. A straightforward approach to bridge this gap is to freeze\nthe text embeddings in prompts, which results in a lack of capacity to adapt\nVLMs for downstream tasks. To address this dilemma, we propose a paradigm\ncalled EnPrompt with a novel External Layer (EnLa). Specifically, we propose a\ntextual external layer and learnable visual embeddings for adapting VLMs to\ndownstream tasks. The learnable external layer is built upon valid embeddings\nof pre-trained CLIP. This design considers the balance of learning capabilities\nbetween the two branches. To align the textual and visual features, we propose\na novel two-pronged approach: i) we introduce the optimal transport as the\ndiscrepancy metric to align the vision and text modalities, and ii) we\nintroduce a novel strengthening feature to enhance the interaction between\nthese two modalities. Four representative experiments (i.e., base-to-novel\ngeneralization, few-shot learning, cross-dataset generalization, domain shifts\ngeneralization) across 15 datasets demonstrate that our method outperforms the\nexisting prompt learning method.\n","authors":["Fangming Cui","Xun Yang","Chao Wu","Liang Xiao","Xinmei Tian"],"pdf_url":"https://arxiv.org/pdf/2407.19674v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03923v1","updated":"2024-08-07T17:30:59Z","published":"2024-08-07T17:30:59Z","title":"Fast Sprite Decomposition from Animated Graphics","summary":"  This paper presents an approach to decomposing animated graphics into\nsprites, a set of basic elements or layers. Our approach builds on the\noptimization of sprite parameters to fit the raster video. For efficiency, we\nassume static textures for sprites to reduce the search space while preventing\nartifacts using a texture prior model. To further speed up the optimization, we\nintroduce the initialization of the sprite parameters utilizing a pre-trained\nvideo object segmentation model and user input of single frame annotations. For\nour study, we construct the Crello Animation dataset from an online design\nservice and define quantitative metrics to measure the quality of the extracted\nsprites. Experiments show that our method significantly outperforms baselines\nfor similar decomposition tasks in terms of the quality/efficiency tradeoff.\n","authors":["Tomoyuki Suzuki","Kotaro Kikuchi","Kota Yamaguchi"],"pdf_url":"https://arxiv.org/pdf/2408.03923v1.pdf","comment":"To be published ECCV 2024, project page:\n  https://cyberagentailab.github.io/sprite-decompose/"},{"id":"http://arxiv.org/abs/2405.19450v2","updated":"2024-08-07T17:30:16Z","published":"2024-05-29T18:58:59Z","title":"FourierMamba: Fourier Learning Integration with State Space Models for\n  Image Deraining","summary":"  Image deraining aims to remove rain streaks from rainy images and restore\nclear backgrounds. Currently, some research that employs the Fourier transform\nhas proved to be effective for image deraining, due to it acting as an\neffective frequency prior for capturing rain streaks. However, despite there\nexists dependency of low frequency and high frequency in images, these\nFourier-based methods rarely exploit the correlation of different frequencies\nfor conjuncting their learning procedures, limiting the full utilization of\nfrequency information for image deraining. Alternatively, the recently emerged\nMamba technique depicts its effectiveness and efficiency for modeling\ncorrelation in various domains (e.g., spatial, temporal), and we argue that\nintroducing Mamba into its unexplored Fourier spaces to correlate different\nfrequencies would help improve image deraining. This motivates us to propose a\nnew framework termed FourierMamba, which performs image deraining with Mamba in\nthe Fourier space. Owning to the unique arrangement of frequency orders in\nFourier space, the core of FourierMamba lies in the scanning encoding of\ndifferent frequencies, where the low-high frequency order formats exhibit\ndifferently in the spatial dimension (unarranged in axis) and channel dimension\n(arranged in axis). Therefore, we design FourierMamba that correlates Fourier\nspace information in the spatial and channel dimensions with distinct designs.\nSpecifically, in the spatial dimension Fourier space, we introduce the zigzag\ncoding to scan the frequencies to rearrange the orders from low to high\nfrequencies, thereby orderly correlating the connections between frequencies;\nin the channel dimension Fourier space with arranged orders of frequencies in\naxis, we can directly use Mamba to perform frequency correlation and improve\nthe channel information representation.\n","authors":["Dong Li","Yidi Liu","Xueyang Fu","Senyan Xu","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2405.19450v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03922v1","updated":"2024-08-07T17:29:19Z","published":"2024-08-07T17:29:19Z","title":"FMiFood: Multi-modal Contrastive Learning for Food Image Classification","summary":"  Food image classification is the fundamental step in image-based dietary\nassessment, which aims to estimate participants' nutrient intake from eating\noccasion images. A common challenge of food images is the intra-class diversity\nand inter-class similarity, which can significantly hinder classification\nperformance. To address this issue, we introduce a novel multi-modal\ncontrastive learning framework called FMiFood, which learns more discriminative\nfeatures by integrating additional contextual information, such as food\ncategory text descriptions, to enhance classification accuracy. Specifically,\nwe propose a flexible matching technique that improves the similarity matching\nbetween text and image embeddings to focus on multiple key information.\nFurthermore, we incorporate the classification objectives into the framework\nand explore the use of GPT-4 to enrich the text descriptions and provide more\ndetailed context. Our method demonstrates improved performance on both the\nUPMC-101 and VFN datasets compared to existing methods.\n","authors":["Xinyue Pan","Jiangpeng He","Fengqing Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.03922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03913v1","updated":"2024-08-07T17:19:15Z","published":"2024-08-07T17:19:15Z","title":"AdapMTL: Adaptive Pruning Framework for Multitask Learning Model","summary":"  In the domain of multimedia and multimodal processing, the efficient handling\nof diverse data streams such as images, video, and sensor data is paramount.\nModel compression and multitask learning (MTL) are crucial in this field,\noffering the potential to address the resource-intensive demands of processing\nand interpreting multiple forms of media simultaneously. However, effectively\ncompressing a multitask model presents significant challenges due to the\ncomplexities of balancing sparsity allocation and accuracy performance across\nmultiple tasks. To tackle these challenges, we propose AdapMTL, an adaptive\npruning framework for MTL models. AdapMTL leverages multiple learnable soft\nthresholds independently assigned to the shared backbone and the task-specific\nheads to capture the nuances in different components' sensitivity to pruning.\nDuring training, it co-optimizes the soft thresholds and MTL model weights to\nautomatically determine the suitable sparsity level at each component to\nachieve both high task accuracy and high overall sparsity. It further\nincorporates an adaptive weighting mechanism that dynamically adjusts the\nimportance of task-specific losses based on each task's robustness to pruning.\nWe demonstrate the effectiveness of AdapMTL through comprehensive experiments\non popular multitask datasets, namely NYU-v2 and Tiny-Taskonomy, with different\narchitectures, showcasing superior performance compared to state-of-the-art\npruning methods.\n","authors":["Mingcan Xiang","Steven Jiaxun Tang","Qizheng Yang","Hui Guan","Tongping Liu"],"pdf_url":"https://arxiv.org/pdf/2408.03913v1.pdf","comment":"13 pages, 9 figures, Published at ACM Multimedia (ACM MM) 2024"},{"id":"http://arxiv.org/abs/2408.03904v1","updated":"2024-08-07T17:08:46Z","published":"2024-08-07T17:08:46Z","title":"Lightweight Video Denoising Using a Classic Bayesian Backbone","summary":"  In recent years, state-of-the-art image and video denoising networks have\nbecome increasingly large, requiring millions of trainable parameters to\nachieve best-in-class performance. Improved denoising quality has come at the\ncost of denoising speed, where modern transformer networks are far slower to\nrun than smaller denoising networks such as FastDVDnet and classic Bayesian\ndenoisers such as the Wiener filter.\n  In this paper, we implement a hybrid Wiener filter which leverages small\nancillary networks to increase the original denoiser performance, while\nretaining fast denoising speeds. These networks are used to refine the Wiener\ncoring estimate, optimise windowing functions and estimate the unknown noise\nprofile. Using these methods, we outperform several popular denoisers and\nremain within 0.2 dB, on average, of the popular VRT transformer. Our method\nwas found to be over x10 faster than the transformer method, with a far lower\nparameter cost.\n","authors":["Clément Bled","François Pitié"],"pdf_url":"https://arxiv.org/pdf/2408.03904v1.pdf","comment":"Paper accepted to ICME 2024"},{"id":"http://arxiv.org/abs/2407.14153v2","updated":"2024-08-07T17:04:53Z","published":"2024-07-19T09:32:30Z","title":"ESP-MedSAM: Efficient Self-Prompting SAM for Universal Image\n  Segmentation","summary":"  The Segment Anything Model (SAM) has demonstrated outstanding adaptation to\nmedical image segmentation but still faces three major challenges. Firstly, the\nhuge computational costs of SAM limit its real-world applicability. Secondly,\nSAM depends on manual annotations (e.g., points, boxes) as prompts, which are\nlaborious and impractical in clinical scenarios. Thirdly, SAM handles all\nsegmentation targets equally, which is suboptimal for diverse medical\nmodalities with inherent heterogeneity. To address these issues, we propose an\nEfficient Self-Prompting SAM for universal medical image segmentation, named\nESP-MedSAM. We devise a Multi-Modal Decoupled Knowledge Distillation (MMDKD)\nstrategy to distil common image knowledge and domain-specific medical knowledge\nfrom the foundation model to train a lightweight image encoder and a modality\ncontroller. Further, they combine with the additionally introduced Self-Patch\nPrompt Generator (SPPG) and Query-Decoupled Modality Decoder (QDMD) to\nconstruct ESP-MedSAM. Specifically, SPPG aims to generate a set of patch\nprompts automatically and QDMD leverages a one-to-one strategy to provide an\nindependent decoding channel for every modality. Extensive experiments indicate\nthat ESP-MedSAM outperforms state-of-the-arts in diverse medical imaging\nsegmentation takes, displaying superior zero-shot learning and modality\ntransfer ability. Especially, our framework uses only 31.4% parameters compared\nto SAM-Base.\n","authors":["Qing Xu","Jiaxuan Li","Xiangjian He","Ziyu Liu","Zhen Chen","Wenting Duan","Chenxin Li","Maggie M. He","Fiseha B. Tesema","Wooi P. Cheah","Yi Wang","Rong Qu","Jonathan M. Garibaldi"],"pdf_url":"https://arxiv.org/pdf/2407.14153v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15098v3","updated":"2024-08-07T17:03:30Z","published":"2024-03-22T10:36:50Z","title":"UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction","summary":"  Vehicle trajectory prediction has increasingly relied on data-driven\nsolutions, but their ability to scale to different data domains and the impact\nof larger dataset sizes on their generalization remain under-explored. While\nthese questions can be studied by employing multiple datasets, it is\nchallenging due to several discrepancies, e.g., in data formats, map\nresolution, and semantic annotation types. To address these challenges, we\nintroduce UniTraj, a comprehensive framework that unifies various datasets,\nmodels, and evaluation criteria, presenting new opportunities for the vehicle\ntrajectory prediction field. In particular, using UniTraj, we conduct extensive\nexperiments and find that model performance significantly drops when\ntransferred to other datasets. However, enlarging data size and diversity can\nsubstantially improve performance, leading to a new state-of-the-art result for\nthe nuScenes dataset. We provide insights into dataset characteristics to\nexplain these findings. The code can be found here:\nhttps://github.com/vita-epfl/UniTraj\n","authors":["Lan Feng","Mohammadhossein Bahari","Kaouther Messaoud Ben Amor","Éloi Zablocki","Matthieu Cord","Alexandre Alahi"],"pdf_url":"https://arxiv.org/pdf/2403.15098v3.pdf","comment":"Accepted in ECCV 2024"},{"id":"http://arxiv.org/abs/2403.11956v5","updated":"2024-08-07T17:02:00Z","published":"2024-03-18T16:52:49Z","title":"Subjective-Aligned Dataset and Metric for Text-to-Video Quality\n  Assessment","summary":"  With the rapid development of generative models, Artificial\nIntelligence-Generated Contents (AIGC) have exponentially increased in daily\nlives. Among them, Text-to-Video (T2V) generation has received widespread\nattention. Though many T2V models have been released for generating high\nperceptual quality videos, there is still lack of a method to evaluate the\nquality of these videos quantitatively. To solve this issue, we establish the\nlargest-scale Text-to-Video Quality Assessment DataBase (T2VQA-DB) to date. The\ndataset is composed of 10,000 videos generated by 9 different T2V models. We\nalso conduct a subjective study to obtain each video's corresponding mean\nopinion score. Based on T2VQA-DB, we propose a novel transformer-based model\nfor subjective-aligned Text-to-Video Quality Assessment (T2VQA). The model\nextracts features from text-video alignment and video fidelity perspectives,\nthen it leverages the ability of a large language model to give the prediction\nscore. Experimental results show that T2VQA outperforms existing T2V metrics\nand SOTA video quality assessment models. Quantitative analysis indicates that\nT2VQA is capable of giving subjective-align predictions, validating its\neffectiveness. The dataset and code will be released at\nhttps://github.com/QMME/T2VQA.\n","authors":["Tengchuan Kou","Xiaohong Liu","Zicheng Zhang","Chunyi Li","Haoning Wu","Xiongkuo Min","Guangtao Zhai","Ning Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11956v5.pdf","comment":"Accepted by ACMMM 24"},{"id":"http://arxiv.org/abs/2408.03888v1","updated":"2024-08-07T16:39:16Z","published":"2024-08-07T16:39:16Z","title":"Dual-Modeling Decouple Distillation for Unsupervised Anomaly Detection","summary":"  Knowledge distillation based on student-teacher network is one of the\nmainstream solution paradigms for the challenging unsupervised Anomaly\nDetection task, utilizing the difference in representation capabilities of the\nteacher and student networks to implement anomaly localization. However,\nover-generalization of the student network to the teacher network may lead to\nnegligible differences in representation capabilities of anomaly, thus\naffecting the detection effectiveness. Existing methods address the possible\nover-generalization by using differentiated students and teachers from the\nstructural perspective or explicitly expanding distilled information from the\ncontent perspective, which inevitably result in an increased likelihood of\nunderfitting of the student network and poor anomaly detection capabilities in\nanomaly center or edge. In this paper, we propose Dual-Modeling Decouple\nDistillation (DMDD) for the unsupervised anomaly detection. In DMDD, a Decouple\nStudent-Teacher Network is proposed to decouple the initial student features\ninto normality and abnormality features. We further introduce Dual-Modeling\nDistillation based on normal-anomaly image pairs, fitting normality features of\nanomalous image and the teacher features of the corresponding normal image,\nwidening the distance between abnormality features and the teacher features in\nanomalous regions. Synthesizing these two distillation ideas, we achieve\nanomaly detection which focuses on both edge and center of anomaly. Finally, a\nMulti-perception Segmentation Network is proposed to achieve focused anomaly\nmap fusion based on multiple attention. Experimental results on MVTec AD show\nthat DMDD surpasses SOTA localization performance of previous knowledge\ndistillation-based methods, reaching 98.85% on pixel-level AUC and 96.13% on\nPRO.\n","authors":["Xinyue Liu","Jianyuan Wang","Biao Leng","Shuo Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.03888v1.pdf","comment":"10 pages, 8 figures, Accepted to ACM MM '24"},{"id":"http://arxiv.org/abs/2408.03885v1","updated":"2024-08-07T16:34:32Z","published":"2024-08-07T16:34:32Z","title":"Global-Local Progressive Integration Network for Blind Image Quality\n  Assessment","summary":"  Vision transformers (ViTs) excel in computer vision for modeling long-term\ndependencies, yet face two key challenges for image quality assessment (IQA):\ndiscarding fine details during patch embedding, and requiring extensive\ntraining data due to lack of inductive biases. In this study, we propose a\nGlobal-Local progressive INTegration network for IQA, called GlintIQA, to\naddress these issues through three key components: 1) Hybrid feature extraction\ncombines ViT-based global feature extractor (VGFE) and convolutional neural\nnetworks (CNNs)-based local feature extractor (CLFE) to capture global\ncoarse-grained features and local fine-grained features, respectively. The\nincorporation of CNNs mitigates the patch-level information loss and inductive\nbias constraints inherent to ViT architectures. 2) Progressive feature\nintegration leverages diverse kernel sizes in embedding to spatially align\ncoarse- and fine-grained features, and progressively aggregate these features\nby interactively stacking channel-wise attention and spatial enhancement\nmodules to build effective quality-aware representations. 3) Content\nsimilarity-based labeling approach is proposed that automatically assigns\nquality labels to images with diverse content based on subjective quality\nscores. This addresses the scarcity of labeled training data in synthetic\ndatasets and bolsters model generalization. The experimental results\ndemonstrate the efficacy of our approach, yielding 5.04% average SROCC gains on\ncross-authentic dataset evaluations. Moreover, our model and its counterpart\npre-trained on the proposed dataset respectively exhibited 5.40% and 13.23%\nimprovements on across-synthetic datasets evaluation. The codes and proposed\ndataset will be released at https://github.com/XiaoqiWang/GlintIQA.\n","authors":["Xiaoqi Wang","Yun Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.03885v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00920v3","updated":"2024-08-07T16:30:56Z","published":"2023-10-02T06:17:24Z","title":"Every Dataset Counts: Scaling up Monocular 3D Object Detection with\n  Joint Datasets Training","summary":"  Monocular 3D object detection plays a crucial role in autonomous driving.\nHowever, existing monocular 3D detection algorithms depend on 3D labels derived\nfrom LiDAR measurements, which are costly to acquire for new datasets and\nchallenging to deploy in novel environments. Specifically, this study\ninvestigates the pipeline for training a monocular 3D object detection model on\na diverse collection of 3D and 2D datasets. The proposed framework comprises\nthree components: (1) a robust monocular 3D model capable of functioning across\nvarious camera settings, (2) a selective-training strategy to accommodate\ndatasets with differing class annotations, and (3) a pseudo 3D training\napproach using 2D labels to enhance detection performance in scenes containing\nonly 2D labels. With this framework, we could train models on a joint set of\nvarious open 3D/2D datasets to obtain models with significantly stronger\ngeneralization capability and enhanced performance on new dataset with only 2D\nlabels. We conduct extensive experiments on\nKITTI/nuScenes/ONCE/Cityscapes/BDD100K datasets to demonstrate the scaling\nability of the proposed method.\n","authors":["Fulong Ma","Xiaoyang Yan","Guoyang Zhao","Xiaojie Xu","Yuxuan Liu","Ming Liu"],"pdf_url":"https://arxiv.org/pdf/2310.00920v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03867v1","updated":"2024-08-07T16:16:31Z","published":"2024-08-07T16:16:31Z","title":"Surgformer: Surgical Transformer with Hierarchical Temporal Attention\n  for Surgical Phase Recognition","summary":"  Existing state-of-the-art methods for surgical phase recognition either rely\non the extraction of spatial-temporal features at a short-range temporal\nresolution or adopt the sequential extraction of the spatial and temporal\nfeatures across the entire temporal resolution. However, these methods have\nlimitations in modeling spatial-temporal dependency and addressing\nspatial-temporal redundancy: 1) These methods fail to effectively model\nspatial-temporal dependency, due to the lack of long-range information or joint\nspatial-temporal modeling. 2) These methods utilize dense spatial features\nacross the entire temporal resolution, resulting in significant\nspatial-temporal redundancy. In this paper, we propose the Surgical Transformer\n(Surgformer) to address the issues of spatial-temporal modeling and redundancy\nin an end-to-end manner, which employs divided spatial-temporal attention and\ntakes a limited set of sparse frames as input. Moreover, we propose a novel\nHierarchical Temporal Attention (HTA) to capture both global and local\ninformation within varied temporal resolutions from a target frame-centric\nperspective. Distinct from conventional temporal attention that primarily\nemphasizes dense long-range similarity, HTA not only captures long-term\ninformation but also considers local latent consistency among informative\nframes. HTA then employs pyramid feature aggregation to effectively utilize\ntemporal information across diverse temporal resolutions, thereby enhancing the\noverall temporal representation. Extensive experiments on two challenging\nbenchmark datasets verify that our proposed Surgformer performs favorably\nagainst the state-of-the-art methods. The code is released at\nhttps://github.com/isyangshu/Surgformer.\n","authors":["Shu Yang","Luyang Luo","Qiong Wang","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2408.03867v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15636v3","updated":"2024-08-07T16:07:05Z","published":"2024-05-24T15:22:58Z","title":"Visualize and Paint GAN Activations","summary":"  We investigate how generated structures of GANs correlate with their\nactivations in hidden layers, with the purpose of better understanding the\ninner workings of those models and being able to paint structures with\nunconditionally trained GANs. This gives us more control over the generated\nimages, allowing to generate them from a semantic segmentation map while not\nrequiring such a segmentation in the training data. To this end we introduce\nthe concept of tileable features, allowing us to identify activations that work\nwell for painting.\n","authors":["Rudolf Herdt","Peter Maass"],"pdf_url":"https://arxiv.org/pdf/2405.15636v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03842v1","updated":"2024-08-07T15:35:25Z","published":"2024-08-07T15:35:25Z","title":"Bi-Level Spatial and Channel-aware Transformer for Learned Image\n  Compression","summary":"  Recent advancements in learned image compression (LIC) methods have\ndemonstrated superior performance over traditional hand-crafted codecs. These\nlearning-based methods often employ convolutional neural networks (CNNs) or\nTransformer-based architectures. However, these nonlinear approaches frequently\noverlook the frequency characteristics of images, which limits their\ncompression efficiency. To address this issue, we propose a novel\nTransformer-based image compression method that enhances the transformation\nstage by considering frequency components within the feature map. Our method\nintegrates a novel Hybrid Spatial-Channel Attention Transformer Block (HSCATB),\nwhere a spatial-based branch independently handles high and low frequencies at\nthe attention layer, and a Channel-aware Self-Attention (CaSA) module captures\ninformation across channels, significantly improving compression performance.\nAdditionally, we introduce a Mixed Local-Global Feed Forward Network (MLGFFN)\nwithin the Transformer block to enhance the extraction of diverse and rich\ninformation, which is crucial for effective compression. These innovations\ncollectively improve the transformation's ability to project data into a more\ndecorrelated latent space, thereby boosting overall compression efficiency.\nExperimental results demonstrate that our framework surpasses state-of-the-art\nLIC methods in rate-distortion performance.\n","authors":["Hamidreza Soltani","Erfan Ghasemi"],"pdf_url":"https://arxiv.org/pdf/2408.03842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03838v1","updated":"2024-08-07T15:24:25Z","published":"2024-08-07T15:24:25Z","title":"Using a Distance Sensor to Detect Deviations in a Planar Surface","summary":"  We investigate methods for determining if a planar surface contains geometric\ndeviations (e.g., protrusions, objects, divots, or cliffs) using only an\ninstantaneous measurement from a miniature optical time-of-flight sensor. The\nkey to our method is to utilize the entirety of information encoded in raw\ntime-of-flight data captured by off-the-shelf distance sensors. We provide an\nanalysis of the problem in which we identify the key ambiguity between geometry\nand surface photometrics. To overcome this challenging ambiguity, we fit a\nGaussian mixture model to a small dataset of planar surface measurements. This\nmodel implicitly captures the expected geometry and distribution of\nphotometrics of the planar surface and is used to identify measurements that\nare likely to contain deviations. We characterize our method on a variety of\nsurfaces and planar deviations across a range of scenarios. We find that our\nmethod utilizing raw time-of-flight data outperforms baselines which use only\nderived distance estimates. We build an example application in which our method\nenables mobile robot obstacle and cliff avoidance over a wide field-of-view.\n","authors":["Carter Sifferman","William Sun","Mohit Gupta","Michael Gleicher"],"pdf_url":"https://arxiv.org/pdf/2408.03838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03834v1","updated":"2024-08-07T15:17:51Z","published":"2024-08-07T15:17:51Z","title":"Target Prompting for Information Extraction with Vision Language Model","summary":"  The recent trend in the Large Vision and Language model has brought a new\nchange in how information extraction systems are built. VLMs have set a new\nbenchmark with their State-of-the-art techniques in understanding documents and\nbuilding question-answering systems across various industries. They are\nsignificantly better at generating text from document images and providing\naccurate answers to questions. However, there are still some challenges in\neffectively utilizing these models to build a precise conversational system.\nGeneral prompting techniques used with large language models are often not\nsuitable for these specially designed vision language models. The output\ngenerated by such generic input prompts is ordinary and may contain information\ngaps when compared with the actual content of the document. To obtain more\naccurate and specific answers, a well-targeted prompt is required by the vision\nlanguage model, along with the document image. In this paper, a technique is\ndiscussed called Target prompting, which focuses on explicitly targeting parts\nof document images and generating related answers from those specific regions\nonly. The paper also covers the evaluation of response for each prompting\ntechnique using different user queries and input prompts.\n","authors":["Dipankar Medhi"],"pdf_url":"https://arxiv.org/pdf/2408.03834v1.pdf","comment":"7 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.19588v2","updated":"2024-08-07T15:11:01Z","published":"2024-03-28T17:12:39Z","title":"DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs","summary":"  This paper revives Densely Connected Convolutional Networks (DenseNets) and\nreveals the underrated effectiveness over predominant ResNet-style\narchitectures. We believe DenseNets' potential was overlooked due to untouched\ntraining methods and traditional design elements not fully revealing their\ncapabilities. Our pilot study shows dense connections through concatenation are\nstrong, demonstrating that DenseNets can be revitalized to compete with modern\narchitectures. We methodically refine suboptimal components - architectural\nadjustments, block redesign, and improved training recipes towards widening\nDenseNets and boosting memory efficiency while keeping concatenation shortcuts.\nOur models, employing simple architectural elements, ultimately surpass Swin\nTransformer, ConvNeXt, and DeiT-III - key architectures in the residual\nlearning lineage. Furthermore, our models exhibit near state-of-the-art\nperformance on ImageNet-1K, competing with the very recent models and\ndownstream tasks, ADE20k semantic segmentation, and COCO object\ndetection/instance segmentation. Finally, we provide empirical analyses that\nuncover the merits of the concatenation over additive shortcuts, steering a\nrenewed preference towards DenseNet-style designs. Our code is available at\nhttps://github.com/naver-ai/rdnet.\n","authors":["Donghyun Kim","Byeongho Heo","Dongyoon Han"],"pdf_url":"https://arxiv.org/pdf/2403.19588v2.pdf","comment":"ECCV 2024. Code at https://github.com/naver-ai/rdnet"},{"id":"http://arxiv.org/abs/2401.00763v2","updated":"2024-08-07T15:10:15Z","published":"2024-01-01T14:06:55Z","title":"New Job, New Gender? Measuring the Social Bias in Image Generation\n  Models","summary":"  Image generation models can generate or edit images from a given text. Recent\nadvancements in image generation technology, exemplified by DALL-E and\nMidjourney, have been groundbreaking. These advanced models, despite their\nimpressive capabilities, are often trained on massive Internet datasets, making\nthem susceptible to generating content that perpetuates social stereotypes and\nbiases, which can lead to severe consequences. Prior research on assessing bias\nwithin image generation models suffers from several shortcomings, including\nlimited accuracy, reliance on extensive human labor, and lack of comprehensive\nanalysis. In this paper, we propose BiasPainter, a novel evaluation framework\nthat can accurately, automatically and comprehensively trigger social bias in\nimage generation models. BiasPainter uses a diverse range of seed images of\nindividuals and prompts the image generation models to edit these images using\ngender, race, and age-neutral queries. These queries span 62 professions, 39\nactivities, 57 types of objects, and 70 personality traits. The framework then\ncompares the edited images to the original seed images, focusing on the\nsignificant changes related to gender, race, and age. BiasPainter adopts a key\ninsight that these characteristics should not be modified when subjected to\nneutral prompts. Built upon this design, BiasPainter can trigger the social\nbias and evaluate the fairness of image generation models. We use BiasPainter\nto evaluate six widely-used image generation models, such as stable diffusion\nand Midjourney. Experimental results show that BiasPainter can successfully\ntrigger social bias in image generation models. According to our human\nevaluation, BiasPainter can achieve 90.8% accuracy on automatic bias detection,\nwhich is significantly higher than the results reported in previous work.\n","authors":["Wenxuan Wang","Haonan Bai","Jen-tse Huang","Yuxuan Wan","Youliang Yuan","Haoyi Qiu","Nanyun Peng","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2401.00763v2.pdf","comment":"ACM MM 2024 Oral"},{"id":"http://arxiv.org/abs/2408.03825v1","updated":"2024-08-07T15:01:08Z","published":"2024-08-07T15:01:08Z","title":"Towards Real-Time Gaussian Splatting: Accelerating 3DGS through\n  Photometric SLAM","summary":"  Initial applications of 3D Gaussian Splatting (3DGS) in Visual Simultaneous\nLocalization and Mapping (VSLAM) demonstrate the generation of high-quality\nvolumetric reconstructions from monocular video streams. However, despite these\npromising advancements, current 3DGS integrations have reduced tracking\nperformance and lower operating speeds compared to traditional VSLAM. To\naddress these issues, we propose integrating 3DGS with Direct Sparse Odometry,\na monocular photometric SLAM system. We have done preliminary experiments\nshowing that using Direct Sparse Odometry point cloud outputs, as opposed to\nstandard structure-from-motion methods, significantly shortens the training\ntime needed to achieve high-quality renders. Reducing 3DGS training time\nenables the development of 3DGS-integrated SLAM systems that operate in\nreal-time on mobile hardware. These promising initial findings suggest further\nexploration is warranted in combining traditional VSLAM systems with 3DGS.\n","authors":["Yan Song Hu","Dayou Mao","Yuhao Chen","John Zelek"],"pdf_url":"https://arxiv.org/pdf/2408.03825v1.pdf","comment":"This extended abstract has been submitted to be presented at an IEEE\n  conference. It will be made available online by IEEE but will not be\n  published in IEEE Xplore. Copyright may be transferred without notice, after\n  which this version may no longer be accessible"},{"id":"http://arxiv.org/abs/2408.03822v1","updated":"2024-08-07T14:56:34Z","published":"2024-08-07T14:56:34Z","title":"Compact 3D Gaussian Splatting for Static and Dynamic Radiance Fields","summary":"  3D Gaussian splatting (3DGS) has recently emerged as an alternative\nrepresentation that leverages a 3D Gaussian-based representation and introduces\nan approximated volumetric rendering, achieving very fast rendering speed and\npromising image quality. Furthermore, subsequent studies have successfully\nextended 3DGS to dynamic 3D scenes, demonstrating its wide range of\napplications. However, a significant drawback arises as 3DGS and its following\nmethods entail a substantial number of Gaussians to maintain the high fidelity\nof the rendered images, which requires a large amount of memory and storage. To\naddress this critical issue, we place a specific emphasis on two key\nobjectives: reducing the number of Gaussian points without sacrificing\nperformance and compressing the Gaussian attributes, such as view-dependent\ncolor and covariance. To this end, we propose a learnable mask strategy that\nsignificantly reduces the number of Gaussians while preserving high\nperformance. In addition, we propose a compact but effective representation of\nview-dependent color by employing a grid-based neural field rather than relying\non spherical harmonics. Finally, we learn codebooks to compactly represent the\ngeometric and temporal attributes by residual vector quantization. With model\ncompression techniques such as quantization and entropy coding, we consistently\nshow over 25x reduced storage and enhanced rendering speed compared to 3DGS for\nstatic scenes, while maintaining the quality of the scene representation. For\ndynamic scenes, our approach achieves more than 12x storage efficiency and\nretains a high-quality reconstruction compared to the existing state-of-the-art\nmethods. Our work provides a comprehensive framework for 3D scene\nrepresentation, achieving high performance, fast training, compactness, and\nreal-time rendering. Our project page is available at\nhttps://maincold2.github.io/c3dgs/.\n","authors":["Joo Chan Lee","Daniel Rho","Xiangyu Sun","Jong Hwan Ko","Eunbyung Park"],"pdf_url":"https://arxiv.org/pdf/2408.03822v1.pdf","comment":"Project page: https://maincold2.github.io/c3dgs/"},{"id":"http://arxiv.org/abs/2405.06342v2","updated":"2024-08-07T14:33:42Z","published":"2024-05-10T09:18:17Z","title":"Compression-Realized Deep Structural Network for Video Quality\n  Enhancement","summary":"  This paper focuses on the task of quality enhancement for compressed videos.\nAlthough deep network-based video restorers achieve impressive progress, most\nof the existing methods lack a structured design to optimally leverage the\npriors within compression codecs. Since the quality degradation of the video is\nprimarily induced by the compression algorithm, a new paradigm is urgently\nneeded for a more ``conscious'' process of quality enhancement. As a result, we\npropose the Compression-Realized Deep Structural Network (CRDS), introducing\nthree inductive biases aligned with the three primary processes in the classic\ncompression codec, merging the strengths of classical encoder architecture with\ndeep network capabilities. Inspired by the residual extraction and domain\ntransformation process in the codec, a pre-trained Latent Degradation Residual\nAuto-Encoder is proposed to transform video frames into a latent feature space,\nand the mutual neighborhood attention mechanism is integrated for precise\nmotion estimation and residual extraction. Furthermore, drawing inspiration\nfrom the quantization noise distribution of the codec, CRDS proposes a novel\nProgressive Denoising framework with intermediate supervision that decomposes\nthe quality enhancement into a series of simpler denoising sub-tasks.\nExperimental results on datasets like LDV 2.0 and MFQE 2.0 indicate our\napproach surpasses state-of-the-art models. Codes are available at\nhttps://github.com/shc15522/CRDS.\n","authors":["Hanchi Sun","Xiaohong Liu","Xinyang Jiang","Yifei Shen","Dongsheng Li","Xiongkuo Min","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2405.06342v2.pdf","comment":"Accepted by ACM MM'24"},{"id":"http://arxiv.org/abs/2407.18520v2","updated":"2024-08-07T14:33:14Z","published":"2024-07-26T05:29:24Z","title":"Text-Region Matching for Multi-Label Image Recognition with Missing\n  Labels","summary":"  Recently, large-scale visual language pre-trained (VLP) models have\ndemonstrated impressive performance across various downstream tasks. Motivated\nby these advancements, pioneering efforts have emerged in multi-label image\nrecognition with missing labels, leveraging VLP prompt-tuning technology.\nHowever, they usually cannot match text and vision features well, due to\ncomplicated semantics gaps and missing labels in a multi-label image. To tackle\nthis challenge, we propose \\textbf{T}ext-\\textbf{R}egion \\textbf{M}atching for\noptimizing \\textbf{M}ulti-\\textbf{L}abel prompt tuning, namely TRM-ML, a novel\nmethod for enhancing meaningful cross-modal matching. Compared to existing\nmethods, we advocate exploring the information of category-aware regions rather\nthan the entire image or pixels, which contributes to bridging the semantic gap\nbetween textual and visual representations in a one-to-one matching manner.\nConcurrently, we further introduce multimodal contrastive learning to narrow\nthe semantic gap between textual and visual modalities and establish\nintra-class and inter-class relationships. Additionally, to deal with missing\nlabels, we propose a multimodal category prototype that leverages intra- and\ninter-category semantic relationships to estimate unknown labels, facilitating\npseudo-label generation. Extensive experiments on the MS-COCO, PASCAL VOC,\nVisual Genome, NUS-WIDE, and CUB-200-211 benchmark datasets demonstrate that\nour proposed framework outperforms the state-of-the-art methods by a\nsignificant margin. Our code is available\nhere\\href{https://github.com/yu-gi-oh-leilei/TRM-ML}{\\raisebox{-1pt}{\\faGithub}}.\n","authors":["Leilei Ma","Hongxing Xie","Lei Wang","Yanping Fu","Dengdi Sun","Haifeng Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.18520v2.pdf","comment":"Accepted to ACM International Conference on Multimedia (ACM MM) 2024"},{"id":"http://arxiv.org/abs/2408.03790v1","updated":"2024-08-07T14:14:53Z","published":"2024-08-07T14:14:53Z","title":"Vision-Language Guidance for LiDAR-based Unsupervised 3D Object\n  Detection","summary":"  Accurate 3D object detection in LiDAR point clouds is crucial for autonomous\ndriving systems. To achieve state-of-the-art performance, the supervised\ntraining of detectors requires large amounts of human-annotated data, which is\nexpensive to obtain and restricted to predefined object categories. To mitigate\nmanual labeling efforts, recent unsupervised object detection approaches\ngenerate class-agnostic pseudo-labels for moving objects, subsequently serving\nas supervision signal to bootstrap a detector. Despite promising results, these\napproaches do not provide class labels or generalize well to static objects.\nFurthermore, they are mostly restricted to data containing multiple drives from\nthe same scene or images from a precisely calibrated and synchronized camera\nsetup. To overcome these limitations, we propose a vision-language-guided\nunsupervised 3D detection approach that operates exclusively on LiDAR point\nclouds. We transfer CLIP knowledge to classify point clusters of static and\nmoving objects, which we discover by exploiting the inherent spatio-temporal\ninformation of LiDAR point clouds for clustering, tracking, as well as box and\nlabel refinement. Our approach outperforms state-of-the-art unsupervised 3D\nobject detectors on the Waymo Open Dataset ($+23~\\text{AP}_{3D}$) and Argoverse\n2 ($+7.9~\\text{AP}_{3D}$) and provides class labels not solely based on object\nsize assumptions, marking a significant advancement in the field.\n","authors":["Christian Fruhwirth-Reisinger","Wei Lin","Dušan Malić","Horst Bischof","Horst Possegger"],"pdf_url":"https://arxiv.org/pdf/2408.03790v1.pdf","comment":"Accepted to BMVC 2024"},{"id":"http://arxiv.org/abs/2408.03789v1","updated":"2024-08-07T14:14:05Z","published":"2024-08-07T14:14:05Z","title":"Counterfactuals and Uncertainty-Based Explainable Paradigm for the\n  Automated Detection and Segmentation of Renal Cysts in Computed Tomography\n  Images: A Multi-Center Study","summary":"  Routine computed tomography (CT) scans often detect a wide range of renal\ncysts, some of which may be malignant. Early and precise localization of these\ncysts can significantly aid quantitative image analysis. Current segmentation\nmethods, however, do not offer sufficient interpretability at the feature and\npixel levels, emphasizing the necessity for an explainable framework that can\ndetect and rectify model inaccuracies. We developed an interpretable\nsegmentation framework and validated it on a multi-centric dataset. A\nVariational Autoencoder Generative Adversarial Network (VAE-GAN) was employed\nto learn the latent representation of 3D input patches and reconstruct input\nimages. Modifications in the latent representation using the gradient of the\nsegmentation model generated counterfactual explanations for varying dice\nsimilarity coefficients (DSC). Radiomics features extracted from these\ncounterfactual images, using a ground truth cyst mask, were analyzed to\ndetermine their correlation with segmentation performance. The DSCs for the\noriginal and VAE-GAN reconstructed images for counterfactual image generation\nshowed no significant differences. Counterfactual explanations highlighted how\nvariations in cyst image features influence segmentation outcomes and showed\nmodel discrepancies. Radiomics features correlating positively and negatively\nwith dice scores were identified. The uncertainty of the predicted segmentation\nmasks was estimated using posterior sampling of the weight space. The\ncombination of counterfactual explanations and uncertainty maps provided a\ndeeper understanding of the image features within the segmented renal cysts\nthat lead to high uncertainty. The proposed segmentation framework not only\nachieved high segmentation accuracy but also increased interpretability\nregarding how image features impact segmentation performance.\n","authors":["Zohaib Salahuddin","Abdalla Ibrahim","Sheng Kuang","Yousif Widaatalla","Razvan L. Miclea","Oliver Morin","Spencer Behr","Marnix P. M. Kop","Tom Marcelissen","Patricia Zondervan","Auke Jager","Philippe Lambin","Henry C Woodruff"],"pdf_url":"https://arxiv.org/pdf/2408.03789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.06646v2","updated":"2024-08-07T14:06:30Z","published":"2024-03-20T05:52:11Z","title":"Diffusion-based Human Motion Style Transfer with Semantic Guidance","summary":"  3D Human motion style transfer is a fundamental problem in computer graphic\nand animation processing. Existing AdaIN- based methods necessitate datasets\nwith balanced style distribution and content/style labels to train the\nclustered latent space. However, we may encounter a single unseen style example\nin practical scenarios, but not in sufficient quantity to constitute a style\ncluster for AdaIN-based methods. Therefore, in this paper, we propose a novel\ntwo-stage framework for few-shot style transfer learning based on the diffusion\nmodel. Specifically, in the first stage, we pre-train a diffusion-based\ntext-to-motion model as a generative prior so that it can cope with various\ncontent motion inputs. In the second stage, based on the single style example,\nwe fine-tune the pre-trained diffusion model in a few-shot manner to make it\ncapable of style transfer. The key idea is regarding the reverse process of\ndiffusion as a motion-style translation process since the motion styles can be\nviewed as special motion variations. During the fine-tuning for style transfer,\na simple yet effective semantic-guided style transfer loss coordinated with\nstyle example reconstruction loss is introduced to supervise the style transfer\nin CLIP semantic space. The qualitative and quantitative evaluations\ndemonstrate that our method can achieve state-of-the-art performance and has\npractical applications.\n","authors":["Lei Hu","Zihao Zhang","Yongjing Ye","Yiwen Xu","Shihong Xia"],"pdf_url":"https://arxiv.org/pdf/2405.06646v2.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2407.05771v2","updated":"2024-08-07T14:01:34Z","published":"2024-07-08T09:27:34Z","title":"Multi-times Monte Carlo Rendering for Inter-reflection Reconstruction","summary":"  Inverse rendering methods have achieved remarkable performance in\nreconstructing high-fidelity 3D objects with disentangled geometries,\nmaterials, and environmental light. However, they still face huge challenges in\nreflective surface reconstruction. Although recent methods model the light\ntrace to learn specularity, the ignorance of indirect illumination makes it\nhard to handle inter-reflections among multiple smooth objects. In this work,\nwe propose Ref-MC2 that introduces the multi-time Monte Carlo sampling which\ncomprehensively computes the environmental illumination and meanwhile considers\nthe reflective light from object surfaces. To address the computation challenge\nas the times of Monte Carlo sampling grow, we propose a specularity-adaptive\nsampling strategy, significantly reducing the computational complexity. Besides\nthe computational resource, higher geometry accuracy is also required because\ngeometric errors accumulate multiple times. Therefore, we further introduce a\nreflection-aware surface model to initialize the geometry and refine it during\ninverse rendering. We construct a challenging dataset containing scenes with\nmultiple objects and inter-reflections. Experiments show that our method\noutperforms other inverse rendering methods on various object groups. We also\nshow downstream applications, e.g., relighting and material editing, to\nillustrate the disentanglement ability of our method.\n","authors":["Tengjie Zhu","Zhuo Chen","Jingnan Gao","Yichao Yan","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2407.05771v2.pdf","comment":"10 pages,6 figures,NeurIPS 2024 Submitted"},{"id":"http://arxiv.org/abs/2407.15793v2","updated":"2024-08-07T13:59:46Z","published":"2024-07-22T16:51:28Z","title":"CLIP with Generative Latent Replay: a Strong Baseline for Incremental\n  Learning","summary":"  With the emergence of Transformers and Vision-Language Models (VLMs) such as\nCLIP, large pre-trained models have become a common strategy to enhance\nperformance in Continual Learning scenarios. This led to the development of\nnumerous prompting strategies to effectively fine-tune transformer-based models\nwithout succumbing to catastrophic forgetting. However, these methods struggle\nto specialize the model on domains significantly deviating from the\npre-training and preserving its zero-shot capabilities. In this work, we\npropose Continual Generative training for Incremental prompt-Learning, a novel\napproach to mitigate forgetting while adapting a VLM, which exploits generative\nreplay to align prompts to tasks. We also introduce a new metric to evaluate\nzero-shot capabilities within CL benchmarks. Through extensive experiments on\ndifferent domains, we demonstrate the effectiveness of our framework in\nadapting to new tasks while improving zero-shot capabilities. Further analysis\nreveals that our approach can bridge the gap with joint prompt tuning. The\ncodebase is available at https://github.com/aimagelab/mammoth.\n","authors":["Emanuele Frascaroli","Aniello Panariello","Pietro Buzzega","Lorenzo Bonicelli","Angelo Porrello","Simone Calderara"],"pdf_url":"https://arxiv.org/pdf/2407.15793v2.pdf","comment":"15 pages, 1 figure. Accepted at the The 35th British Machine Vision\n  Conference 2024 (BMVC 2024), Glasgow, UK"},{"id":"http://arxiv.org/abs/2408.03771v1","updated":"2024-08-07T13:47:32Z","published":"2024-08-07T13:47:32Z","title":"Methodological Explainability Evaluation of an Interpretable Deep\n  Learning Model for Post-Hepatectomy Liver Failure Prediction Incorporating\n  Counterfactual Explanations and Layerwise Relevance Propagation: A\n  Prospective In Silico Trial","summary":"  Artificial intelligence (AI)-based decision support systems have demonstrated\nvalue in predicting post-hepatectomy liver failure (PHLF) in hepatocellular\ncarcinoma (HCC). However, they often lack transparency, and the impact of model\nexplanations on clinicians' decisions has not been thoroughly evaluated.\nBuilding on prior research, we developed a variational autoencoder-multilayer\nperceptron (VAE-MLP) model for preoperative PHLF prediction. This model\nintegrated counterfactuals and layerwise relevance propagation (LRP) to provide\ninsights into its decision-making mechanism. Additionally, we proposed a\nmethodological framework for evaluating the explainability of AI systems. This\nframework includes qualitative and quantitative assessments of explanations\nagainst recognized biomarkers, usability evaluations, and an in silico clinical\ntrial. Our evaluations demonstrated that the model's explanation correlated\nwith established biomarkers and exhibited high usability at both the case and\nsystem levels. Furthermore, results from the three-track in silico clinical\ntrial showed that clinicians' prediction accuracy and confidence increased when\nAI explanations were provided.\n","authors":["Xian Zhong","Zohaib Salahuddin","Yi Chen","Henry C Woodruff","Haiyi Long","Jianyun Peng","Nuwan Udawatte","Roberto Casale","Ayoub Mokhtari","Xiaoer Zhang","Jiayao Huang","Qingyu Wu","Li Tan","Lili Chen","Dongming Li","Xiaoyan Xie","Manxia Lin","Philippe Lambin"],"pdf_url":"https://arxiv.org/pdf/2408.03771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10428v5","updated":"2024-08-07T13:44:06Z","published":"2023-03-18T14:46:44Z","title":"RCA: Region Conditioned Adaptation for Visual Abductive Reasoning","summary":"  Visual abductive reasoning aims to make likely explanations for visual\nobservations. We propose a simple yet effective Region Conditioned Adaptation,\na hybrid parameter-efficient fine-tuning method that equips the frozen CLIP\nwith the ability to infer explanations from local visual cues. We encode\n``local hints'' and ``global contexts'' into visual prompts of the CLIP model\nseparately at fine and coarse-grained levels. Adapters are used for fine-tuning\nCLIP models for downstream tasks and we design a new attention adapter, that\ndirectly steers the focus of the attention map with trainable query and key\nprojections of a frozen CLIP model. Finally, we train our new model with a\nmodified contrastive loss to regress the visual feature simultaneously toward\nfeatures of literal description and plausible explanations. The loss enables\nCLIP to maintain both perception and reasoning abilities. Experiments on the\nSherlock visual abductive reasoning benchmark show that the RCA significantly\noutstands previous SOTAs, ranking the \\nth{1} on the leaderboards (e.g., Human\nAcc: RCA 31.74 \\textit{vs} CPT-CLIP 29.58, higher =better). We also validate\nthe RCA is generalizable to local perception benchmarks like RefCOCO. We\nopen-source our project at\n\\textit{\\color{magenta}{\\url{https://github.com/LUNAProject22/RPA}}}.\n","authors":["Hao Zhang","Yeo Keat Ee","Basura Fernando"],"pdf_url":"https://arxiv.org/pdf/2303.10428v5.pdf","comment":"13 pages, 11 figures, ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2406.18944v3","updated":"2024-08-07T13:37:41Z","published":"2024-06-27T07:14:14Z","title":"Investigating and Defending Shortcut Learning in Personalized Diffusion\n  Models","summary":"  Personalized diffusion models have gained popularity for adapting pre-trained\ntext-to-image models to generate images of specific topics with minimal\ntraining data. However, these models are vulnerable to minor adversarial\nperturbations, leading to degraded performance on corrupted datasets. Such\nvulnerabilities are further exploited to craft protective perturbations on\nsensitive images like portraits that prevent unauthorized generation. In\nresponse, diffusion-based purification methods have been proposed to remove\nthese perturbations and retain generation performance. However, existing works\nturn to over-purifying the images, which causes information loss. In this\npaper, we take a closer look at the fine-tuning process of personalized\ndiffusion models through the lens of shortcut learning. And we propose a\nhypothesis explaining the manipulation mechanisms of existing perturbation\nmethods, demonstrating that perturbed images significantly deviate from their\noriginal prompts in the CLIP-based latent space. This misalignment during\nfine-tuning causes models to associate noisy patterns with identifiers,\nresulting in performance degradation. Based on these insights, we introduce a\nsystematic approach to maintain training performance through purification. Our\nmethod first purifies the images to realign them with their original semantic\nmeanings in latent space. Then, we introduce contrastive learning with negative\ntokens to decouple the learning of clean identities from noisy patterns, which\nshows a strong potential capacity against adaptive perturbation. Our study\nuncovers shortcut learning vulnerabilities in personalized diffusion models and\nprovides a firm evaluation framework for future protective perturbation\nresearch. Code is available at https://github.com/liuyixin-louis/DiffShortcut.\n","authors":["Yixin Liu","Ruoxi Chen","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2406.18944v3.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2408.03761v1","updated":"2024-08-07T13:30:58Z","published":"2024-08-07T13:30:58Z","title":"MMSummary: Multimodal Summary Generation for Fetal Ultrasound Video","summary":"  We present the first automated multimodal summary generation system,\nMMSummary, for medical imaging video, particularly with a focus on fetal\nultrasound analysis. Imitating the examination process performed by a human\nsonographer, MMSummary is designed as a three-stage pipeline, progressing from\nkeyframe detection to keyframe captioning and finally anatomy segmentation and\nmeasurement. In the keyframe detection stage, an innovative automated workflow\nis proposed to progressively select a concise set of keyframes, preserving\nsufficient video information without redundancy. Subsequently, we adapt a large\nlanguage model to generate meaningful captions for fetal ultrasound keyframes\nin the keyframe captioning stage. If a keyframe is captioned as fetal biometry,\nthe segmentation and measurement stage estimates biometric parameters by\nsegmenting the region of interest according to the textual prior. The MMSummary\nsystem provides comprehensive summaries for fetal ultrasound examinations and\nbased on reported experiments is estimated to reduce scanning time by\napproximately 31.5%, thereby suggesting the potential to enhance clinical\nworkflow efficiency.\n","authors":["Xiaoqing Guo","Qianhui Men","J. Alison Noble"],"pdf_url":"https://arxiv.org/pdf/2408.03761v1.pdf","comment":"MICCAI 2024"},{"id":"http://arxiv.org/abs/2408.01953v2","updated":"2024-08-07T13:24:38Z","published":"2024-08-04T07:59:17Z","title":"EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning","summary":"  Humans perceive and interact with the world with the awareness of\nequivariance, facilitating us in manipulating different objects in diverse\nposes. For robotic manipulation, such equivariance also exists in many\nscenarios. For example, no matter what the pose of a drawer is (translation,\nrotation and tilt), the manipulation strategy is consistent (grasp the handle\nand pull in a line). While traditional models usually do not have the awareness\nof equivariance for robotic manipulation, which might result in more data for\ntraining and poor performance in novel object poses, we propose our EqvAfford\nframework, with novel designs to guarantee the equivariance in point-level\naffordance learning for downstream robotic manipulation, with great performance\nand generalization ability on representative tasks on objects in diverse poses.\n","authors":["Yue Chen","Chenrui Tie","Ruihai Wu","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2408.01953v2.pdf","comment":"Accept to CVPRWorkshop on Equivariant Vision: From Theory to Practice\n  2024"},{"id":"http://arxiv.org/abs/2404.11317v2","updated":"2024-08-07T13:20:30Z","published":"2024-04-17T12:30:54Z","title":"Improving Composed Image Retrieval via Contrastive Learning with Scaling\n  Positives and Negatives","summary":"  The Composed Image Retrieval (CIR) task aims to retrieve target images using\na composed query consisting of a reference image and a modified text. Advanced\nmethods often utilize contrastive learning as the optimization objective, which\nbenefits from adequate positive and negative examples. However, the triplet for\nCIR incurs high manual annotation costs, resulting in limited positive\nexamples. Furthermore, existing methods commonly use in-batch negative\nsampling, which reduces the negative number available for the model. To address\nthe problem of lack of positives, we propose a data generation method by\nleveraging a multi-modal large language model to construct triplets for CIR. To\nintroduce more negatives during fine-tuning, we design a two-stage fine-tuning\nframework for CIR, whose second stage introduces plenty of static\nrepresentations of negatives to optimize the representation space rapidly. The\nabove two improvements can be effectively stacked and designed to be\nplug-and-play, easily applied to existing CIR models without changing their\noriginal architectures. Extensive experiments and ablation analysis demonstrate\nthat our method effectively scales positives and negatives and achieves\nstate-of-the-art results on both FashionIQ and CIRR datasets. In addition, our\nmethod also performs well in zero-shot composed image retrieval, providing a\nnew CIR solution for the low-resources scenario. Our code and data are released\nat https://github.com/BUAADreamer/SPN4CIR.\n","authors":["Zhangchi Feng","Richong Zhang","Zhijie Nie"],"pdf_url":"https://arxiv.org/pdf/2404.11317v2.pdf","comment":"Accepted to ACM MM 2024 Regular Papers"},{"id":"http://arxiv.org/abs/2408.03753v1","updated":"2024-08-07T13:06:29Z","published":"2024-08-07T13:06:29Z","title":"3iGS: Factorised Tensorial Illumination for 3D Gaussian Splatting","summary":"  The use of 3D Gaussians as representation of radiance fields has enabled high\nquality novel view synthesis at real-time rendering speed. However, the choice\nof optimising the outgoing radiance of each Gaussian independently as spherical\nharmonics results in unsatisfactory view dependent effects. In response to\nthese limitations, our work, Factorised Tensorial Illumination for 3D Gaussian\nSplatting, or 3iGS, improves upon 3D Gaussian Splatting (3DGS) rendering\nquality. Instead of optimising a single outgoing radiance parameter, 3iGS\nenhances 3DGS view-dependent effects by expressing the outgoing radiance as a\nfunction of a local illumination field and Bidirectional Reflectance\nDistribution Function (BRDF) features. We optimise a continuous incident\nillumination field through a Tensorial Factorisation representation, while\nseparately fine-tuning the BRDF features of each 3D Gaussian relative to this\nillumination field. Our methodology significantly enhances the rendering\nquality of specular view-dependent effects of 3DGS, while maintaining rapid\ntraining and rendering speeds.\n","authors":["Zhe Jun Tang","Tat-Jen Cham"],"pdf_url":"https://arxiv.org/pdf/2408.03753v1.pdf","comment":"The 18th European Conference on Computer Vision ECCV 2024"},{"id":"http://arxiv.org/abs/2408.03748v1","updated":"2024-08-07T13:01:10Z","published":"2024-08-07T13:01:10Z","title":"Data Generation Scheme for Thermal Modality with Edge-Guided Adversarial\n  Conditional Diffusion Model","summary":"  In challenging low light and adverse weather conditions,thermal vision\nalgorithms,especially object detection,have exhibited remarkable\npotential,contrasting with the frequent struggles encountered by visible vision\nalgorithms. Nevertheless,the efficacy of thermal vision algorithms driven by\ndeep learning models remains constrained by the paucity of available training\ndata samples. To this end,this paper introduces a novel approach termed the\nedge guided conditional diffusion model. This framework aims to produce\nmeticulously aligned pseudo thermal images at the pixel level,leveraging edge\ninformation extracted from visible images. By utilizing edges as contextual\ncues from the visible domain,the diffusion model achieves meticulous control\nover the delineation of objects within the generated images. To alleviate the\nimpacts of those visible-specific edge information that should not appear in\nthe thermal domain,a two-stage modality adversarial training strategy is\nproposed to filter them out from the generated images by differentiating the\nvisible and thermal modality. Extensive experiments on LLVIP demonstrate ECDM s\nsuperiority over existing state-of-the-art approaches in terms of image\ngeneration quality.\n","authors":["Guoqing Zhu","Honghu Pan","Qiang Wang","Chao Tian","Chao Yang","Zhenyu He"],"pdf_url":"https://arxiv.org/pdf/2408.03748v1.pdf","comment":"accepted by ACM MM 2024/ACM MM24"},{"id":"http://arxiv.org/abs/2305.18381v4","updated":"2024-08-07T12:59:31Z","published":"2023-05-28T06:53:41Z","title":"Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient\n  Dataset Distillation","summary":"  Data-efficient learning has garnered significant attention, especially given\nthe current trend of large multi-modal models. Recently, dataset distillation\nhas become an effective approach by synthesizing data samples that are\nessential for network training. However, it remains to be explored which\nsamples are essential for the dataset distillation process itself. In this\nwork, we study the data efficiency and selection for the dataset distillation\ntask. By re-formulating the dynamics of distillation, we provide insight into\nthe inherent redundancy in the real dataset, both theoretically and\nempirically. We propose to use the empirical loss value as a static data\npruning criterion. To further compensate for the variation of the data value in\ntraining, we find the most contributing samples based on their causal effects\non the distillation. The proposed selection strategy can efficiently exploit\nthe training dataset, outperform the previous SOTA distillation algorithms, and\nconsistently enhance the distillation algorithms, even on much larger-scale and\nmore heterogeneous datasets, e.g., full ImageNet-1K and Kinetics-400. We\nbelieve this paradigm will open up new avenues in the dynamics of distillation\nand pave the way for efficient dataset distillation. Our code is available on\nhttps://github.com/silicx/GoldFromOres-BiLP.\n","authors":["Yue Xu","Yong-Lu Li","Kaitong Cui","Ziyu Wang","Cewu Lu","Yu-Wing Tai","Chi-Keung Tang"],"pdf_url":"https://arxiv.org/pdf/2305.18381v4.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2408.03745v1","updated":"2024-08-07T12:58:39Z","published":"2024-08-07T12:58:39Z","title":"Intuitionistic Fuzzy Cognitive Maps for Interpretable Image\n  Classification","summary":"  The interpretability of machine learning models is critical, as users may be\nreluctant to rely on their inferences. Intuitionistic FCMs (iFCMs) have been\nproposed as an extension of FCMs offering a natural mechanism to assess the\nquality of their output through the estimation of hesitancy, a concept\nresembling to human hesitation in decision making. To address the challenge of\ninterpretable image classification, this paper introduces a novel framework,\nnamed Interpretable Intuitionistic FCM (I2FCM) which is domain-independent,\nsimple to implement, and can be applied on Convolutional Neural Network (CNN)\nmodels, rendering them interpretable. To the best of our knowledge this is the\nfirst time iFCMs are applied for image classification. Further novel\ncontributions include: a feature extraction process focusing on the most\ninformative image regions; a learning algorithm for data-driven determination\nof the intuitionistic fuzzy interconnections of the iFCM; an inherently\ninterpretable classification approach based on image contents. In the context\nof image classification, hesitancy is considered as a degree of inconfidence\nwith which an image is categorized to a class. The constructed iFCM model\ndistinguishes the most representative image semantics and analyses them\nutilizing cause-and-effect relations. The effectiveness of the introduced\nframework is evaluated on publicly available datasets, and the experimental\nresults confirm that it can provide enhanced classification performance, while\nproviding interpretable inferences.\n","authors":["Georgia Sovatzidi","Michael D. Vasilakakis","Dimitris K. Iakovidis"],"pdf_url":"https://arxiv.org/pdf/2408.03745v1.pdf","comment":"This work has been submitted for possible journal publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2408.03735v1","updated":"2024-08-07T12:42:09Z","published":"2024-08-07T12:42:09Z","title":"Advancing Multimodal Large Language Models with Quantization-Aware Scale\n  Learning for Efficient Adaptation","summary":"  This paper presents the first study to explore the potential of parameter\nquantization for multimodal large language models to alleviate the significant\nresource constraint encountered during vision-language instruction tuning. We\nintroduce a Quantization-aware Scale LeArning method based on multimodal\nWarmup, termed QSLAW. This method is grounded in two key innovations: (1) The\nlearning of group-wise scale factors for quantized LLM weights to mitigate the\nquantization error arising from activation outliers and achieve more effective\nvision-language instruction tuning; (2) The implementation of a multimodal\nwarmup that progressively integrates linguistic and multimodal training\nsamples, thereby preventing overfitting of the quantized model to multimodal\ndata while ensuring stable adaptation of multimodal large language models to\ndownstream vision-language tasks. Extensive experiments demonstrate that models\nquantized by QSLAW perform on par with, or even surpass, their full-precision\ncounterparts, while facilitating up to 1.4 times reduction in VL tuning time\nand GPU consumption. Our code is released at https://github.com/xjjxmu/QSLAW.\n","authors":["Jingjing Xie","Yuxin Zhang","Mingbao Lin","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2408.03735v1.pdf","comment":"Accepted by ACMMM2024"},{"id":"http://arxiv.org/abs/2408.03734v1","updated":"2024-08-07T12:42:06Z","published":"2024-08-07T12:42:06Z","title":"Soft-Hard Attention U-Net Model and Benchmark Dataset for Multiscale\n  Image Shadow Removal","summary":"  Effective shadow removal is pivotal in enhancing the visual quality of images\nin various applications, ranging from computer vision to digital photography.\nDuring the last decades physics and machine learning -based methodologies have\nbeen proposed; however, most of them have limited capacity in capturing complex\nshadow patterns due to restrictive model assumptions, neglecting the fact that\nshadows usually appear at different scales. Also, current datasets used for\nbenchmarking shadow removal are composed of a limited number of images with\nsimple scenes containing mainly uniform shadows cast by single objects, whereas\nonly a few of them include both manual shadow annotations and paired\nshadow-free images. Aiming to address all these limitations in the context of\nnatural scene imaging, including urban environments with complex scenes, the\ncontribution of this study is twofold: a) it proposes a novel deep learning\narchitecture, named Soft-Hard Attention U-net (SHAU), focusing on multiscale\nshadow removal; b) it provides a novel synthetic dataset, named Multiscale\nShadow Removal Dataset (MSRD), containing complex shadow patterns of multiple\nscales, aiming to serve as a privacy-preserving dataset for a more\ncomprehensive benchmarking of future shadow removal methodologies. Key\narchitectural components of SHAU are the soft and hard attention modules, which\nalong with multiscale feature extraction blocks enable effective shadow removal\nof different scales and intensities. The results demonstrate the effectiveness\nof SHAU over the relevant state-of-the-art shadow removal methods across\nvarious benchmark datasets, improving the Peak Signal-to-Noise Ratio and Root\nMean Square Error for the shadow area by 25.1% and 61.3%, respectively.\n","authors":["Eirini Cholopoulou","Dimitrios E. Diamantis","Dimitra-Christina C. Koutsiou","Dimitris K. Iakovidis"],"pdf_url":"https://arxiv.org/pdf/2408.03734v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2408.03717v1","updated":"2024-08-07T12:10:32Z","published":"2024-08-07T12:10:32Z","title":"Pick of the Bunch: Detecting Infrared Small Targets Beyond Hit-Miss\n  Trade-Offs via Selective Rank-Aware Attention","summary":"  Infrared small target detection faces the inherent challenge of precisely\nlocalizing dim targets amidst complex background clutter. Traditional\napproaches struggle to balance detection precision and false alarm rates. To\nbreak this dilemma, we propose SeRankDet, a deep network that achieves high\naccuracy beyond the conventional hit-miss trade-off, by following the ``Pick of\nthe Bunch'' principle. At its core lies our Selective Rank-Aware Attention\n(SeRank) module, employing a non-linear Top-K selection process that preserves\nthe most salient responses, preventing target signal dilution while maintaining\nconstant complexity. Furthermore, we replace the static concatenation typical\nin U-Net structures with our Large Selective Feature Fusion (LSFF) module, a\ndynamic fusion strategy that empowers SeRankDet with adaptive feature\nintegration, enhancing its ability to discriminate true targets from false\nalarms. The network's discernment is further refined by our Dilated Difference\nConvolution (DDC) module, which merges differential convolution aimed at\namplifying subtle target characteristics with dilated convolution to expand the\nreceptive field, thereby substantially improving target-background separation.\nDespite its lightweight architecture, the proposed SeRankDet sets new\nbenchmarks in state-of-the-art performance across multiple public datasets. The\ncode is available at https://github.com/GrokCV/SeRankDet.\n","authors":["Yimian Dai","Peiwen Pan","Yulei Qian","Yuxuan Li","Xiang Li","Jian Yang","Huan Wan"],"pdf_url":"https://arxiv.org/pdf/2408.03717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01669v2","updated":"2024-08-07T12:06:18Z","published":"2024-08-03T05:35:13Z","title":"SynopGround: A Large-Scale Dataset for Multi-Paragraph Video Grounding\n  from TV Dramas and Synopses","summary":"  Video grounding is a fundamental problem in multimodal content understanding,\naiming to localize specific natural language queries in an untrimmed video.\nHowever, current video grounding datasets merely focus on simple events and are\neither limited to shorter videos or brief sentences, which hinders the model\nfrom evolving toward stronger multimodal understanding capabilities. To address\nthese limitations, we present a large-scale video grounding dataset named\nSynopGround, in which more than 2800 hours of videos are sourced from popular\nTV dramas and are paired with accurately localized human-written synopses. Each\nparagraph in the synopsis serves as a language query and is manually annotated\nwith precise temporal boundaries in the long video. These paragraph queries are\ntightly correlated to each other and contain a wealth of abstract expressions\nsummarizing video storylines and specific descriptions portraying event\ndetails, which enables the model to learn multimodal perception on more\nintricate concepts over longer context dependencies. Based on the dataset, we\nfurther introduce a more complex setting of video grounding dubbed\nMulti-Paragraph Video Grounding (MPVG), which takes as input multiple\nparagraphs and a long video for grounding each paragraph query to its temporal\ninterval. In addition, we propose a novel Local-Global Multimodal Reasoner\n(LGMR) to explicitly model the local-global structures of long-term multimodal\ninputs for MPVG. Our method provides an effective baseline solution to the\nmulti-paragraph video grounding problem. Extensive experiments verify the\nproposed model's effectiveness as well as its superiority in long-term\nmulti-paragraph video grounding over prior state-of-the-arts. Dataset and code\nare publicly available. Project page: https://synopground.github.io/.\n","authors":["Chaolei Tan","Zihang Lin","Junfu Pu","Zhongang Qi","Wei-Yi Pei","Zhi Qu","Yexin Wang","Ying Shan","Wei-Shi Zheng","Jian-Fang Hu"],"pdf_url":"https://arxiv.org/pdf/2408.01669v2.pdf","comment":"Accepted to ACM MM 2024. Project page: https://synopground.github.io/"},{"id":"http://arxiv.org/abs/2408.01334v2","updated":"2024-08-07T12:01:58Z","published":"2024-08-02T15:32:42Z","title":"A Backbone for Long-Horizon Robot Task Understanding","summary":"  End-to-end robot learning, particularly for long-horizon tasks, often results\nin unpredictable outcomes and poor generalization. To address these challenges,\nwe propose a novel Therblig-based Backbone Framework (TBBF) to enhance robot\ntask understanding and transferability. This framework uses therbligs (basic\naction elements) as the backbone to decompose high-level robot tasks into\nelemental robot configurations, which are then integrated with current\nfoundation models to improve task understanding. The approach consists of two\nstages: offline training and online testing. During the offline training stage,\nwe developed the Meta-RGate SynerFusion (MGSF) network for accurate therblig\nsegmentation across various tasks. In the online testing stage, after a\none-shot demonstration of a new task is collected, our MGSF network extracts\nhigh-level knowledge, which is then encoded into the image using Action\nRegistration (ActionREG). Additionally, the Large Language Model\n(LLM)-Alignment Policy for Visual Correction (LAP-VC) is employed to ensure\nprecise action execution, facilitating trajectory transfer in novel robot\nscenarios. Experimental results validate these methods, achieving 94.37% recall\nin therblig segmentation and success rates of 94.4% and 80% in real-world\nonline robot testing for simple and complex scenarios, respectively.\nSupplementary material is available at:\nhttps://sites.google.com/view/therbligsbasedbackbone/home\n","authors":["Xiaoshuai Chen","Wei Chen","Dongmyoung Lee","Yukun Ge","Nicolas Rojas","Petar Kormushev"],"pdf_url":"https://arxiv.org/pdf/2408.01334v2.pdf","comment":"8 pages, 8 figures. This work is intended to be submitted to IEEE\n  Robotics and Automation Letters (RA-L) for possible publication"},{"id":"http://arxiv.org/abs/2305.12661v4","updated":"2024-08-07T11:37:02Z","published":"2023-05-22T03:04:22Z","title":"Semantic-guided modeling of spatial relation and object co-occurrence\n  for indoor scene recognition","summary":"  Exploring the semantic context in scene images is essential for indoor scene\nrecognition. However, due to the diverse intra-class spatial layouts and the\ncoexisting inter-class objects, modeling contextual relationships to adapt\nvarious image characteristics is a great challenge. Existing contextual\nmodeling methods for scene recognition exhibit two limitations: 1) They\ntypically model only one type of spatial relationship (order or metric) among\nobjects within scenes, with limited exploration of diverse spatial layouts. 2)\nThey often overlook the differences in coexisting objects across different\nscenes, suppressing scene recognition performance. To overcome these\nlimitations, we propose SpaCoNet, which simultaneously models Spatial relation\nand Co-occurrence of objects guided by semantic segmentation. Firstly, the\nSemantic Spatial Relation Module (SSRM) is constructed to model scene spatial\nfeatures. With the help of semantic segmentation, this module decouples spatial\ninformation from the scene image and thoroughly explores all spatial\nrelationships among objects in an end-to-end manner, thereby obtaining\nsemantic-based spatial features. Secondly, both spatial features from the SSRM\nand deep features from the Image Feature Extraction Module are allocated to\neach object, so as to distinguish the coexisting object across different\nscenes. Finally, utilizing the discriminative features above, we design a\nGlobal-Local Dependency Module to explore the long-range co-occurrence among\nobjects, and further generate a semantic-guided feature representation for\nindoor scene recognition. Experimental results on three widely used scene\ndatasets demonstrate the effectiveness and generality of the proposed method.\n","authors":["Chuanxin Song","Hanbo Wu","Xin Ma"],"pdf_url":"https://arxiv.org/pdf/2305.12661v4.pdf","comment":"Under second review at Expert Systems with Applications"},{"id":"http://arxiv.org/abs/2408.03703v1","updated":"2024-08-07T11:33:46Z","published":"2024-08-07T11:33:46Z","title":"CAS-ViT: Convolutional Additive Self-attention Vision Transformers for\n  Efficient Mobile Applications","summary":"  Vision Transformers (ViTs) mark a revolutionary advance in neural networks\nwith their token mixer's powerful global context capability. However, the\npairwise token affinity and complex matrix operations limit its deployment on\nresource-constrained scenarios and real-time applications, such as mobile\ndevices, although considerable efforts have been made in previous works. In\nthis paper, we introduce CAS-ViT: Convolutional Additive Self-attention Vision\nTransformers, to achieve a balance between efficiency and performance in mobile\napplications. Firstly, we argue that the capability of token mixers to obtain\nglobal contextual information hinges on multiple information interactions, such\nas spatial and channel domains. Subsequently, we construct a novel additive\nsimilarity function following this paradigm and present an efficient\nimplementation named Convolutional Additive Token Mixer (CATM). This\nsimplification leads to a significant reduction in computational overhead. We\nevaluate CAS-ViT across a variety of vision tasks, including image\nclassification, object detection, instance segmentation, and semantic\nsegmentation. Our experiments, conducted on GPUs, ONNX, and iPhones,\ndemonstrate that CAS-ViT achieves a competitive performance when compared to\nother state-of-the-art backbones, establishing it as a viable option for\nefficient mobile vision applications. Our code and model are available at:\n\\url{https://github.com/Tianfang-Zhang/CAS-ViT}\n","authors":["Tianfang Zhang","Lei Li","Yang Zhou","Wentao Liu","Chen Qian","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2408.03703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05278v2","updated":"2024-08-07T11:33:21Z","published":"2024-07-07T06:36:09Z","title":"HyperKAN: Kolmogorov-Arnold Networks make Hyperspectral Image\n  Classificators Smarter","summary":"  In traditional neural network architectures, a multilayer perceptron (MLP) is\ntypically employed as a classification block following the feature extraction\nstage. However, the Kolmogorov-Arnold Network (KAN) presents a promising\nalternative to MLP, offering the potential to enhance prediction accuracy. In\nthis paper, we propose the replacement of linear and convolutional layers of\ntraditional networks with KAN-based counterparts. These modifications allowed\nus to significantly increase the per-pixel classification accuracy for\nhyperspectral remote-sensing images. We modified seven different neural network\narchitectures for hyperspectral image classification and observed a substantial\nimprovement in the classification accuracy across all the networks. The\narchitectures considered in the paper include baseline MLP, state-of-the-art 1D\n(1DCNN) and 3D convolutional (two different 3DCNN, NM3DCNN), and transformer\n(SSFTT) architectures, as well as newly proposed M1DCNN. The greatest effect\nwas achieved for convolutional networks working exclusively on spectral data,\nand the best classification quality was achieved using a KAN-based transformer\narchitecture. All the experiments were conducted using seven openly available\nhyperspectral datasets. Our code is available at\nhttps://github.com/f-neumann77/HyperKAN.\n","authors":["Valeriy Lobanov","Nikita Firsov","Evgeny Myasnikov","Roman Khabibullin","Artem Nikonorov"],"pdf_url":"https://arxiv.org/pdf/2407.05278v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10271v2","updated":"2024-08-07T11:32:19Z","published":"2023-12-16T00:23:21Z","title":"Robustness of Deep Learning for Accelerated MRI: Benefits of Diverse\n  Training Data","summary":"  Deep learning based methods for image reconstruction are state-of-the-art for\na variety of imaging tasks. However, neural networks often perform worse if the\ntraining data differs significantly from the data they are applied to. For\nexample, a model trained for accelerated magnetic resonance imaging (MRI) on\none scanner performs worse on another scanner. In this work, we investigate the\nimpact of the training data on a model's performance and robustness for\naccelerated MRI. We find that models trained on the combination of various data\ndistributions, such as those obtained from different MRI scanners and\nanatomies, exhibit robustness equal or superior to models trained on the best\nsingle distribution for a specific target distribution. Thus training on such\ndiverse data tends to improve robustness. Furthermore, training on such a\ndiverse dataset does not compromise in-distribution performance, i.e., a model\ntrained on diverse data yields in-distribution performance at least as good as\nmodels trained on the more narrow individual distributions. Our results suggest\nthat training a model for imaging on a variety of distributions tends to yield\na more effective and robust model than maintaining separate models for\nindividual distributions.\n","authors":["Kang Lin","Reinhard Heckel"],"pdf_url":"https://arxiv.org/pdf/2312.10271v2.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2408.03695v1","updated":"2024-08-07T11:20:37Z","published":"2024-08-07T11:20:37Z","title":"Openstory++: A Large-scale Dataset and Benchmark for Instance-aware\n  Open-domain Visual Storytelling","summary":"  Recent image generation models excel at creating high-quality images from\nbrief captions. However, they fail to maintain consistency of multiple\ninstances across images when encountering lengthy contexts. This inconsistency\nis largely due to in existing training datasets the absence of granular\ninstance feature labeling in existing training datasets. To tackle these\nissues, we introduce Openstory++, a large-scale dataset combining additional\ninstance-level annotations with both images and text. Furthermore, we develop a\ntraining methodology that emphasizes entity-centric image-text generation,\nensuring that the models learn to effectively interweave visual and textual\ninformation. Specifically, Openstory++ streamlines the process of keyframe\nextraction from open-domain videos, employing vision-language models to\ngenerate captions that are then polished by a large language model for\nnarrative continuity. It surpasses previous datasets by offering a more\nexpansive open-domain resource, which incorporates automated captioning,\nhigh-resolution imagery tailored for instance count, and extensive frame\nsequences for temporal consistency. Additionally, we present Cohere-Bench, a\npioneering benchmark framework for evaluating the image generation tasks when\nlong multimodal context is provided, including the ability to keep the\nbackground, style, instances in the given context coherent. Compared to\nexisting benchmarks, our work fills critical gaps in multi-modal generation,\npropelling the development of models that can adeptly generate and interpret\ncomplex narratives in open-domain environments. Experiments conducted within\nCohere-Bench confirm the superiority of Openstory++ in nurturing high-quality\nvisual storytelling models, enhancing their ability to address open-domain\ngeneration tasks. More details can be found at https://openstorypp.github.io/\n","authors":["Zilyu Ye","Jinxiu Liu","Ruotian Peng","Jinjin Cao","Zhiyang Chen","Yiyang Zhang","Ziwei Xuan","Mingyuan Zhou","Xiaoqian Shen","Mohamed Elhoseiny","Qi Liu","Guo-Jun Qi"],"pdf_url":"https://arxiv.org/pdf/2408.03695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12670v3","updated":"2024-08-07T10:45:03Z","published":"2024-03-19T12:11:57Z","title":"Driving Animatronic Robot Facial Expression From Speech","summary":"  Animatronic robots hold the promise of enabling natural human-robot\ninteraction through lifelike facial expressions. However, generating realistic,\nspeech-synchronized robot expressions poses significant challenges due to the\ncomplexities of facial biomechanics and the need for responsive motion\nsynthesis. This paper introduces a novel, skinning-centric approach to drive\nanimatronic robot facial expressions from speech input. At its core, the\nproposed approach employs linear blend skinning (LBS) as a unifying\nrepresentation, guiding innovations in both embodiment design and motion\nsynthesis. LBS informs the actuation topology, facilitates human expression\nretargeting, and enables efficient speech-driven facial motion generation. This\napproach demonstrates the capability to produce highly realistic facial\nexpressions on an animatronic face in real-time at over 4000 fps on a single\nNvidia RTX 4090, significantly advancing robots' ability to replicate nuanced\nhuman expressions for natural interaction. To foster further research and\ndevelopment in this field, the code has been made publicly available at:\n\\url{https://github.com/library87/OpenRoboExp}.\n","authors":["Boren Li","Hang Li","Hangxin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.12670v3.pdf","comment":"8 pages, 6 figures, accepted to IROS 2024. For associated project\n  page, see https://library87.github.io/animatronic-face-iros24"},{"id":"http://arxiv.org/abs/2408.03677v1","updated":"2024-08-07T10:36:26Z","published":"2024-08-07T10:36:26Z","title":"L4DR: LiDAR-4DRadar Fusion for Weather-Robust 3D Object Detection","summary":"  LiDAR-based vision systems are integral for 3D object detection, which is\ncrucial for autonomous navigation. However, they suffer from performance\ndegradation in adverse weather conditions due to the quality deterioration of\nLiDAR point clouds. Fusing LiDAR with the weather-robust 4D radar sensor is\nexpected to solve this problem. However, the fusion of LiDAR and 4D radar is\nchallenging because they differ significantly in terms of data quality and the\ndegree of degradation in adverse weather. To address these issues, we introduce\nL4DR, a weather-robust 3D object detection method that effectively achieves\nLiDAR and 4D Radar fusion. Our L4DR includes Multi-Modal Encoding (MME) and\nForeground-Aware Denoising (FAD) technique to reconcile sensor gaps, which is\nthe first exploration of the complementarity of early fusion between LiDAR and\n4D radar. Additionally, we design an Inter-Modal and Intra-Modal ({IM}2 )\nparallel feature extraction backbone coupled with a Multi-Scale Gated Fusion\n(MSGF) module to counteract the varying degrees of sensor degradation under\nadverse weather conditions. Experimental evaluation on a VoD dataset with\nsimulated fog proves that L4DR is more adaptable to changing weather\nconditions. It delivers a significant performance increase under different fog\nlevels, improving the 3D mAP by up to 18.17% over the traditional LiDAR-only\napproach. Moreover, the results on the K-Radar dataset validate the consistent\nperformance improvement of L4DR in real-world adverse weather conditions.\n","authors":["Xun Huang","Ziyu Xu","Hai Wu","Jinlong Wang","Qiming Xia","Yan Xia","Jonathan Li","Kyle Gao","Chenglu Wen","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14068v2","updated":"2024-08-07T10:28:18Z","published":"2024-06-20T07:44:56Z","title":"Classifying Dry Eye Disease Patients from Healthy Controls Using Machine\n  Learning and Metabolomics Data","summary":"  Dry eye disease is a common disorder of the ocular surface, leading patients\nto seek eye care. Clinical signs and symptoms are currently used to diagnose\ndry eye disease. Metabolomics, a method for analyzing biological systems, has\nbeen found helpful in identifying distinct metabolites in patients and in\ndetecting metabolic profiles that may indicate dry eye disease at early stages.\nIn this study, we explored using machine learning and metabolomics information\nto identify which cataract patients suffered from dry eye disease. As there is\nno one-size-fits-all machine learning model for metabolomics data, choosing the\nmost suitable model can significantly affect the quality of predictions and\nsubsequent metabolomics analyses. To address this challenge, we conducted a\ncomparative analysis of nine machine learning models on three metabolomics data\nsets from cataract patients with and without dry eye disease. The models were\nevaluated and optimized using nested k-fold cross-validation. To assess the\nperformance of these models, we selected a set of suitable evaluation metrics\ntailored to the data set's challenges. The logistic regression model overall\nperformed the best, achieving the highest area under the curve score of 0.8378,\nbalanced accuracy of 0.735, Matthew's correlation coefficient of 0.5147, an\nF1-score of 0.8513, and a specificity of 0.5667. Additionally, following the\nlogistic regression, the XGBoost and Random Forest models also demonstrated\ngood performance.\n","authors":["Sajad Amouei Sheshkal","Morten Gundersen","Michael Alexander Riegler","Øygunn Aass Utheim","Kjell Gunnar Gundersen","Hugo Lewi Hammer"],"pdf_url":"https://arxiv.org/pdf/2406.14068v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01126v2","updated":"2024-08-07T10:25:08Z","published":"2024-08-02T09:07:31Z","title":"IG-SLAM: Instant Gaussian SLAM","summary":"  3D Gaussian Splatting has recently shown promising results as an alternative\nscene representation in SLAM systems to neural implicit representations.\nHowever, current methods either lack dense depth maps to supervise the mapping\nprocess or detailed training designs that consider the scale of the\nenvironment. To address these drawbacks, we present IG-SLAM, a dense RGB-only\nSLAM system that employs robust Dense-SLAM methods for tracking and combines\nthem with Gaussian Splatting. A 3D map of the environment is constructed using\naccurate pose and dense depth provided by tracking. Additionally, we utilize\ndepth uncertainty in map optimization to improve 3D reconstruction. Our decay\nstrategy in map optimization enhances convergence and allows the system to run\nat 10 fps in a single process. We demonstrate competitive performance with\nstate-of-the-art RGB-only SLAM systems while achieving faster operation speeds.\nWe present our experiments on the Replica, TUM-RGBD, ScanNet, and EuRoC\ndatasets. The system achieves photo-realistic 3D reconstruction in large-scale\nsequences, particularly in the EuRoC dataset.\n","authors":["F. Aykut Sarikamis","A. Aydin Alatan"],"pdf_url":"https://arxiv.org/pdf/2408.01126v2.pdf","comment":"8 pages, 3 page ref, 5 figures"},{"id":"http://arxiv.org/abs/2408.03663v1","updated":"2024-08-07T10:04:04Z","published":"2024-08-07T10:04:04Z","title":"Designing Extremely Memory-Efficient CNNs for On-device Vision Tasks","summary":"  In this paper, we introduce a memory-efficient CNN (convolutional neural\nnetwork), which enables resource-constrained low-end embedded and IoT devices\nto perform on-device vision tasks, such as image classification and object\ndetection, using extremely low memory, i.e., only 63 KB on ImageNet\nclassification. Based on the bottleneck block of MobileNet, we propose three\ndesign principles that significantly curtail the peak memory usage of a CNN so\nthat it can fit the limited KB memory of the low-end device. First, 'input\nsegmentation' divides an input image into a set of patches, including the\ncentral patch overlapped with the others, reducing the size (and memory\nrequirement) of a large input image. Second, 'patch tunneling' builds\nindependent tunnel-like paths consisting of multiple bottleneck blocks per\npatch, penetrating through the entire model from an input patch to the last\nlayer of the network, maintaining lightweight memory usage throughout the whole\nnetwork. Lastly, 'bottleneck reordering' rearranges the execution order of\nconvolution operations inside the bottleneck block such that the memory usage\nremains constant regardless of the size of the convolution output channels. The\nexperiment result shows that the proposed network classifies ImageNet with\nextremely low memory (i.e., 63 KB) while achieving competitive top-1 accuracy\n(i.e., 61.58\\%). To the best of our knowledge, the memory usage of the proposed\nnetwork is far smaller than state-of-the-art memory-efficient networks, i.e.,\nup to 89x and 3.1x smaller than MobileNet (i.e., 5.6 MB) and MCUNet (i.e., 196\nKB), respectively.\n","authors":["Jaewook Lee","Yoel Park","Seulki Lee"],"pdf_url":"https://arxiv.org/pdf/2408.03663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03657v1","updated":"2024-08-07T09:52:30Z","published":"2024-08-07T09:52:30Z","title":"PHOCUS: Physics-Based Deconvolution for Ultrasound Resolution\n  Enhancement","summary":"  Ultrasound is widely used in medical diagnostics allowing for accessible and\npowerful imaging but suffers from resolution limitations due to diffraction and\nthe finite aperture of the imaging system, which restricts diagnostic use. The\nimpulse function of an ultrasound imaging system is called the point spread\nfunction (PSF), which is convolved with the spatial distribution of reflectors\nin the image formation process. Recovering high-resolution reflector\ndistributions by removing image distortions induced by the convolution process\nimproves image clarity and detail. Conventionally, deconvolution techniques\nattempt to rectify the imaging system's dependent PSF, working directly on the\nradio-frequency (RF) data. However, RF data is often not readily accessible.\nTherefore, we introduce a physics-based deconvolution process using a modeled\nPSF, working directly on the more commonly available B-mode images. By\nleveraging Implicit Neural Representations (INRs), we learn a continuous\nmapping from spatial locations to their respective echogenicity values,\neffectively compensating for the discretized image space. Our contribution\nconsists of a novel methodology for retrieving a continuous echogenicity map\ndirectly from a B-mode image through a differentiable physics-based rendering\npipeline for ultrasound resolution enhancement. We qualitatively and\nquantitatively evaluate our approach on synthetic data, demonstrating\nimprovements over traditional methods in metrics such as PSNR and SSIM.\nFurthermore, we show qualitative enhancements on an ultrasound phantom and an\nin-vivo acquisition of a carotid artery.\n","authors":["Felix Duelmer","Walter Simson","Mohammad Farid Azampour","Magdalena Wysocki","Angelos Karlas","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2408.03657v1.pdf","comment":"Accepted at the Workshop of Advances in Simplifying Medical\n  Ultrasound at MICCAI 2024"},{"id":"http://arxiv.org/abs/2408.03654v1","updated":"2024-08-07T09:40:26Z","published":"2024-08-07T09:40:26Z","title":"Unsupervised Detection of Fetal Brain Anomalies using Denoising\n  Diffusion Models","summary":"  Congenital malformations of the brain are among the most common fetal\nabnormalities that impact fetal development. Previous anomaly detection methods\non ultrasound images are based on supervised learning, rely on manual\nannotations, and risk missing underrepresented categories. In this work, we\nframe fetal brain anomaly detection as an unsupervised task using diffusion\nmodels. To this end, we employ an inpainting-based Noise Agnostic Anomaly\nDetection approach that identifies the abnormality using\ndiffusion-reconstructed fetal brain images from multiple noise levels. Our\napproach only requires normal fetal brain ultrasound images for training,\naddressing the limited availability of abnormal data. Our experiments on a\nreal-world clinical dataset show the potential of using unsupervised methods\nfor fetal brain anomaly detection. Additionally, we comprehensively evaluate\nhow different noise types affect diffusion models in the fetal anomaly\ndetection domain.\n","authors":["Markus Ditlev Sjøgren Olsen","Jakob Ambsdorf","Manxi Lin","Caroline Taksøe-Vester","Morten Bo Søndergaard Svendsen","Anders Nymark Christensen","Mads Nielsen","Martin Grønnebæk Tolsgaard","Aasa Feragen","Paraskevas Pegios"],"pdf_url":"https://arxiv.org/pdf/2408.03654v1.pdf","comment":"Accepted at ASMUS@MICCAI 2024"},{"id":"http://arxiv.org/abs/2405.00354v2","updated":"2024-08-07T09:36:55Z","published":"2024-05-01T07:16:03Z","title":"CrossMatch: Enhance Semi-Supervised Medical Image Segmentation with\n  Perturbation Strategies and Knowledge Distillation","summary":"  Semi-supervised learning for medical image segmentation presents a unique\nchallenge of efficiently using limited labeled data while leveraging abundant\nunlabeled data. Despite advancements, existing methods often do not fully\nexploit the potential of the unlabeled data for enhancing model robustness and\naccuracy. In this paper, we introduce CrossMatch, a novel framework that\nintegrates knowledge distillation with dual perturbation strategies-image-level\nand feature-level-to improve the model's learning from both labeled and\nunlabeled data. CrossMatch employs multiple encoders and decoders to generate\ndiverse data streams, which undergo self-knowledge distillation to enhance\nconsistency and reliability of predictions across varied perturbations. Our\nmethod significantly surpasses other state-of-the-art techniques in standard\nbenchmarks by effectively minimizing the gap between training on labeled and\nunlabeled data and improving edge accuracy and generalization in medical image\nsegmentation. The efficacy of CrossMatch is demonstrated through extensive\nexperimental validations, showing remarkable performance improvements without\nincreasing computational costs. Code for this implementation is made available\nat https://github.com/AiEson/CrossMatch.git.\n","authors":["Bin Zhao","Chunshi Wang","Shuxue Ding"],"pdf_url":"https://arxiv.org/pdf/2405.00354v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15838v2","updated":"2024-08-07T09:34:25Z","published":"2024-07-22T17:55:22Z","title":"MMInstruct: A High-Quality Multi-Modal Instruction Tuning Dataset with\n  Extensive Diversity","summary":"  Despite the effectiveness of vision-language supervised fine-tuning in\nenhancing the performance of Vision Large Language Models (VLLMs). However,\nexisting visual instruction tuning datasets include the following limitations:\n(1) Instruction annotation quality: despite existing VLLMs exhibiting strong\nperformance, instructions generated by those advanced VLLMs may still suffer\nfrom inaccuracies, such as hallucinations. (2) Instructions and image\ndiversity: the limited range of instruction types and the lack of diversity in\nimage data may impact the model's ability to generate diversified and closer to\nreal-world scenarios outputs. To address these challenges, we construct a\nhigh-quality, diverse visual instruction tuning dataset MMInstruct, which\nconsists of 973K instructions from 24 domains. There are four instruction\ntypes: Judgement, Multiple-Choice, Long Visual Question Answering and Short\nVisual Question Answering. To construct MMInstruct, we propose an instruction\ngeneration data engine that leverages GPT-4V, GPT-3.5, and manual correction.\nOur instruction generation engine enables semi-automatic, low-cost, and\nmulti-domain instruction generation at 1/6 the cost of manual construction.\nThrough extensive experiment validation and ablation experiments, we\ndemonstrate that MMInstruct could significantly improve the performance of\nVLLMs, e.g., the model fine-tuning on MMInstruct achieves new state-of-the-art\nperformance on 10 out of 12 benchmarks. The code and data shall be available at\nhttps://github.com/yuecao0119/MMInstruct.\n","authors":["Yangzhou Liu","Yue Cao","Zhangwei Gao","Weiyun Wang","Zhe Chen","Wenhai Wang","Hao Tian","Lewei Lu","Xizhou Zhu","Tong Lu","Yu Qiao","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2407.15838v2.pdf","comment":"18 pages, 8 figures, technical report"},{"id":"http://arxiv.org/abs/2408.03651v1","updated":"2024-08-07T09:30:51Z","published":"2024-08-07T09:30:51Z","title":"SAM2-PATH: A better segment anything model for semantic segmentation in\n  digital pathology","summary":"  The semantic segmentation task in pathology plays an indispensable role in\nassisting physicians in determining the condition of tissue lesions. Foundation\nmodels, such as the SAM (Segment Anything Model) and SAM2, exhibit exceptional\nperformance in instance segmentation within everyday natural scenes. SAM-PATH\nhas also achieved impressive results in semantic segmentation within the field\nof pathology. However, in computational pathology, the models mentioned above\nstill have the following limitations. The pre-trained encoder models suffer\nfrom a scarcity of pathology image data; SAM and SAM2 are not suitable for\nsemantic segmentation. In this paper, we have designed a trainable\nKolmogorov-Arnold Networks(KAN) classification module within the SAM2 workflow,\nand we have introduced the largest pretrained vision encoder for histopathology\n(UNI) to date. Our proposed framework, SAM2-PATH, augments SAM2's capability to\nperform semantic segmentation in digital pathology autonomously, eliminating\nthe need for human provided input prompts. The experimental results demonstrate\nthat, after fine-tuning the KAN classification module and decoder, Our dataset\nhas achieved competitive results on publicly available pathology data. The code\nhas been open-sourced and can be found at the following address:\nhttps://github.com/simzhangbest/SAM2PATH.\n","authors":["Mingya Zhang","Liang Wang","Limei Gu","Zhao Li","Yaohui Wang","Tingshen Ling","Xianping Tao"],"pdf_url":"https://arxiv.org/pdf/2408.03651v1.pdf","comment":"6 pages , 3 figures"},{"id":"http://arxiv.org/abs/2408.02164v2","updated":"2024-08-07T09:23:36Z","published":"2024-08-04T23:21:46Z","title":"Rethinking Affect Analysis: A Protocol for Ensuring Fairness and\n  Consistency","summary":"  Evaluating affect analysis methods presents challenges due to inconsistencies\nin database partitioning and evaluation protocols, leading to unfair and biased\nresults. Previous studies claim continuous performance improvements, but our\nfindings challenge such assertions. Using these insights, we propose a unified\nprotocol for database partitioning that ensures fairness and comparability. We\nprovide detailed demographic annotations (in terms of race, gender and age),\nevaluation metrics, and a common framework for expression recognition, action\nunit detection and valence-arousal estimation. We also rerun the methods with\nthe new protocol and introduce a new leaderboards to encourage future research\nin affect recognition with a fairer comparison. Our annotations, code, and\npre-trained models are available on\n\\hyperlink{https://github.com/dkollias/Fair-Consistent-Affect-Analysis}{Github}.\n","authors":["Guanyu Hu","Dimitrios Kollias","Eleni Papadopoulou","Paraskevi Tzouveli","Jie Wei","Xinyu Yang"],"pdf_url":"https://arxiv.org/pdf/2408.02164v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2405.06841"},{"id":"http://arxiv.org/abs/2405.10885v2","updated":"2024-08-07T09:11:38Z","published":"2024-05-17T16:22:52Z","title":"FA-Depth: Toward Fast and Accurate Self-supervised Monocular Depth\n  Estimation","summary":"  Most existing methods often rely on complex models to predict scene depth\nwith high accuracy, resulting in slow inference that is not conducive to\ndeployment. To better balance precision and speed, we first designed SmallDepth\nbased on sparsity. Second, to enhance the feature representation ability of\nSmallDepth during training under the condition of equal complexity during\ninference, we propose an equivalent transformation module(ETM). Third, to\nimprove the ability of each layer in the case of a fixed SmallDepth to perceive\ndifferent context information and improve the robustness of SmallDepth to the\nleft-right direction and illumination changes, we propose pyramid loss. Fourth,\nto further improve the accuracy of SmallDepth, we utilized the proposed\nfunction approximation loss (APX) to transfer knowledge in the pretrained\nHQDecv2, obtained by optimizing the previous HQDec to address grid artifacts in\nsome regions, to SmallDepth. Extensive experiments demonstrate that each\nproposed component improves the precision of SmallDepth without changing the\ncomplexity of SmallDepth during inference, and the developed approach achieves\nstate-of-the-art results on KITTI at an inference speed of more than 500 frames\nper second and with approximately 2 M parameters. The code and models will be\npublicly available at https://github.com/fwucas/FA-Depth.\n","authors":["Fei Wang","Jun Cheng"],"pdf_url":"https://arxiv.org/pdf/2405.10885v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14973v2","updated":"2024-08-07T08:53:25Z","published":"2024-03-22T06:04:11Z","title":"Pose-Aware Self-Supervised Learning with Viewpoint Trajectory\n  Regularization","summary":"  Learning visual features from unlabeled images has proven successful for\nsemantic categorization, often by mapping different $views$ of the same object\nto the same feature to achieve recognition invariance. However, visual\nrecognition involves not only identifying $what$ an object is but also\nunderstanding $how$ it is presented. For example, seeing a car from the side\nversus head-on is crucial for deciding whether to stay put or jump out of the\nway. While unsupervised feature learning for downstream viewpoint reasoning is\nimportant, it remains under-explored, partly due to the lack of a standardized\nevaluation method and benchmarks.\n  We introduce a new dataset of adjacent image triplets obtained from a\nviewpoint trajectory, without any semantic or pose labels. We benchmark both\nsemantic classification and pose estimation accuracies on the same visual\nfeature. Additionally, we propose a viewpoint trajectory regularization loss\nfor learning features from unlabeled image triplets. Our experiments\ndemonstrate that this approach helps develop a visual representation that\nencodes object identity and organizes objects by their poses, retaining\nsemantic classification accuracy while achieving emergent global pose awareness\nand better generalization to novel objects. Our dataset and code are available\nat http://pwang.pw/trajSSL/.\n","authors":["Jiayun Wang","Yubei Chen","Stella X. Yu"],"pdf_url":"https://arxiv.org/pdf/2403.14973v2.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2408.03637v1","updated":"2024-08-07T08:52:21Z","published":"2024-08-07T08:52:21Z","title":"TALE: Training-free Cross-domain Image Composition via Adaptive Latent\n  Manipulation and Energy-guided Optimization","summary":"  We present TALE, a novel training-free framework harnessing the generative\ncapabilities of text-to-image diffusion models to address the cross-domain\nimage composition task that focuses on flawlessly incorporating user-specified\nobjects into a designated visual contexts regardless of domain disparity.\nPrevious methods often involve either training auxiliary networks or finetuning\ndiffusion models on customized datasets, which are expensive and may undermine\nthe robust textual and visual priors of pre-trained diffusion models. Some\nrecent works attempt to break the barrier by proposing training-free\nworkarounds that rely on manipulating attention maps to tame the denoising\nprocess implicitly. However, composing via attention maps does not necessarily\nyield desired compositional outcomes. These approaches could only retain some\nsemantic information and usually fall short in preserving identity\ncharacteristics of input objects or exhibit limited background-object style\nadaptation in generated images. In contrast, TALE is a novel method that\noperates directly on latent space to provide explicit and effective guidance\nfor the composition process to resolve these problems. Specifically, we equip\nTALE with two mechanisms dubbed Adaptive Latent Manipulation and Energy-guided\nLatent Optimization. The former formulates noisy latents conducive to\ninitiating and steering the composition process by directly leveraging\nbackground and foreground latents at corresponding timesteps, and the latter\nexploits designated energy functions to further optimize intermediate latents\nconforming to specific conditions that complement the former to generate\ndesired final results. Our experiments demonstrate that TALE surpasses prior\nbaselines and attains state-of-the-art performance in image-guided composition\nacross various photorealistic and artistic domains.\n","authors":["Kien T. Pham","Jingye Chen","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2408.03637v1.pdf","comment":"The 32nd ACM Multimedia Conference (MM '24)"},{"id":"http://arxiv.org/abs/2408.03632v1","updated":"2024-08-07T08:43:58Z","published":"2024-08-07T08:43:58Z","title":"Concept Conductor: Orchestrating Multiple Personalized Concepts in\n  Text-to-Image Synthesis","summary":"  The customization of text-to-image models has seen significant advancements,\nyet generating multiple personalized concepts remains a challenging task.\nCurrent methods struggle with attribute leakage and layout confusion when\nhandling multiple concepts, leading to reduced concept fidelity and semantic\nconsistency. In this work, we introduce a novel training-free framework,\nConcept Conductor, designed to ensure visual fidelity and correct layout in\nmulti-concept customization. Concept Conductor isolates the sampling processes\nof multiple custom models to prevent attribute leakage between different\nconcepts and corrects erroneous layouts through self-attention-based spatial\nguidance. Additionally, we present a concept injection technique that employs\nshape-aware masks to specify the generation area for each concept. This\ntechnique injects the structure and appearance of personalized concepts through\nfeature fusion in the attention layers, ensuring harmony in the final image.\nExtensive qualitative and quantitative experiments demonstrate that Concept\nConductor can consistently generate composite images with accurate layouts\nwhile preserving the visual details of each concept. Compared to existing\nbaselines, Concept Conductor shows significant performance improvements. Our\nmethod supports the combination of any number of concepts and maintains high\nfidelity even when dealing with visually similar concepts. The code and models\nare available at https://github.com/Nihukat/Concept-Conductor.\n","authors":["Zebin Yao","Fangxiang Feng","Ruifan Li","Xiaojie Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03632v1.pdf","comment":"Github Page: https://github.com/Nihukat/Concept-Conductor"},{"id":"http://arxiv.org/abs/2408.03627v1","updated":"2024-08-07T08:39:33Z","published":"2024-08-07T08:39:33Z","title":"Weakly Contrastive Learning via Batch Instance Discrimination and\n  Feature Clustering for Small Sample SAR ATR","summary":"  In recent years, impressive performance of deep learning technology has been\nrecognized in Synthetic Aperture Radar (SAR) Automatic Target Recognition\n(ATR). Since a large amount of annotated data is required in this technique, it\nposes a trenchant challenge to the issue of obtaining a high recognition rate\nthrough less labeled data. To overcome this problem, inspired by the\ncontrastive learning, we proposed a novel framework named Batch Instance\nDiscrimination and Feature Clustering (BIDFC). In this framework, different\nfrom that of the objective of general contrastive learning methods, embedding\ndistance between samples should be moderate because of the high similarity\nbetween samples in the SAR images. Consequently, our flexible framework is\nequipped with adjustable distance between embedding, which we term as weakly\ncontrastive learning. Technically, instance labels are assigned to the\nunlabeled data in per batch and random augmentation and training are performed\nfew times on these augmented data. Meanwhile, a novel Dynamic-Weighted Variance\nloss (DWV loss) function is also posed to cluster the embedding of enhanced\nversions for each sample. Experimental results on the moving and stationary\ntarget acquisition and recognition (MSTAR) database indicate a 91.25%\nclassification accuracy of our method fine-tuned on only 3.13% training data.\nEven though a linear evaluation is performed on the same training data, the\naccuracy can still reach 90.13%. We also verified the effectiveness of BIDFC in\nOpenSarShip database, indicating that our method can be generalized to other\ndatasets. Our code is avaliable at:\nhttps://github.com/Wenlve-Zhou/BIDFC-master.\n","authors":["Yikui Zhai","Wenlve Zhou","Bing Sun","Jingwen Li","Qirui Ke","Zilu Ying","Junying Gan","Chaoyun Mai","Ruggero Donida Labati","Vincenzo Piuri","Fabio Scotti"],"pdf_url":"https://arxiv.org/pdf/2408.03627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03624v1","updated":"2024-08-07T08:34:48Z","published":"2024-08-07T08:34:48Z","title":"AgentsCoMerge: Large Language Model Empowered Collaborative Decision\n  Making for Ramp Merging","summary":"  Ramp merging is one of the bottlenecks in traffic systems, which commonly\ncause traffic congestion, accidents, and severe carbon emissions. In order to\naddress this essential issue and enhance the safety and efficiency of connected\nand autonomous vehicles (CAVs) at multi-lane merging zones, we propose a novel\ncollaborative decision-making framework, named AgentsCoMerge, to leverage large\nlanguage models (LLMs). Specifically, we first design a scene observation and\nunderstanding module to allow an agent to capture the traffic environment. Then\nwe propose a hierarchical planning module to enable the agent to make decisions\nand plan trajectories based on the observation and the agent's own state. In\naddition, in order to facilitate collaboration among multiple agents, we\nintroduce a communication module to enable the surrounding agents to exchange\nnecessary information and coordinate their actions. Finally, we develop a\nreinforcement reflection guided training paradigm to further enhance the\ndecision-making capability of the framework. Extensive experiments are\nconducted to evaluate the performance of our proposed method, demonstrating its\nsuperior efficiency and effectiveness for multi-agent collaborative\ndecision-making under various ramp merging scenarios.\n","authors":["Senkang Hu","Zhengru Fang","Zihan Fang","Yiqin Deng","Xianhao Chen","Yuguang Fang","Sam Kwong"],"pdf_url":"https://arxiv.org/pdf/2408.03624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.09914v4","updated":"2024-08-07T08:20:43Z","published":"2023-04-19T18:32:49Z","title":"The Face of Populism: Examining Differences in Facial Emotional\n  Expressions of Political Leaders Using Machine Learning","summary":"  Populist rhetoric employed on online media is characterized as deeply\nimpassioned and often imbued with strong emotions. The aim of this paper is to\nempirically investigate the differences in affective nonverbal communication of\npolitical leaders. We use a deep-learning approach to process a sample of 220\nYouTube videos of political leaders from 15 different countries, analyze their\nfacial expressions of emotion and then examine differences in average emotion\nscores representing the relative presence of 6 emotional states (anger,\ndisgust, fear, happiness, sadness, and surprise) and a neutral expression for\neach frame of the YouTube video. Based on a sample of manually coded images, we\nfind that this deep-learning approach has 53-60\\% agreement with human labels.\nWe observe statistically significant differences in the average score of\nnegative emotions between groups of leaders with varying degrees of populist\nrhetoric.\n","authors":["Sara Major","Aleksandar Tomašević"],"pdf_url":"https://arxiv.org/pdf/2304.09914v4.pdf","comment":"Version 4.0: Annotation study added, supplementary information\n  extended"},{"id":"http://arxiv.org/abs/2408.03616v1","updated":"2024-08-07T08:17:34Z","published":"2024-08-07T08:17:34Z","title":"Distillation Learning Guided by Image Reconstruction for One-Shot\n  Medical Image Segmentation","summary":"  Traditional one-shot medical image segmentation (MIS) methods use\nregistration networks to propagate labels from a reference atlas or rely on\ncomprehensive sampling strategies to generate synthetic labeled data for\ntraining. However, these methods often struggle with registration errors and\nlow-quality synthetic images, leading to poor performance and generalization.\nTo overcome this, we introduce a novel one-shot MIS framework based on\nknowledge distillation, which allows the network to directly 'see' real images\nthrough a distillation process guided by image reconstruction. It focuses on\nanatomical structures in a single labeled image and a few unlabeled ones. A\nregistration-based data augmentation network creates realistic, labeled\nsamples, while a feature distillation module helps the student network learn\nsegmentation from these samples, guided by the teacher network. During\ninference, the streamlined student network accurately segments new images.\nEvaluations on three public datasets (OASIS for T1 brain MRI, BCV for abdomen\nCT, and VerSe for vertebrae CT) show superior segmentation performance and\ngeneralization across different medical image datasets and modalities compared\nto leading methods. Our code is available at\nhttps://github.com/NoviceFodder/OS-MedSeg.\n","authors":["Feng Zhou","Yanjie Zhou","Longjie Wang","Yun Peng","David E. Carlson","Liyun Tu"],"pdf_url":"https://arxiv.org/pdf/2408.03616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03612v1","updated":"2024-08-07T08:08:08Z","published":"2024-08-07T08:08:08Z","title":"JARViS: Detecting Actions in Video Using Unified Actor-Scene Context\n  Relation Modeling","summary":"  Video action detection (VAD) is a formidable vision task that involves the\nlocalization and classification of actions within the spatial and temporal\ndimensions of a video clip. Among the myriad VAD architectures, two-stage VAD\nmethods utilize a pre-trained person detector to extract the region of interest\nfeatures, subsequently employing these features for action detection. However,\nthe performance of two-stage VAD methods has been limited as they depend solely\non localized actor features to infer action semantics. In this study, we\npropose a new two-stage VAD framework called Joint Actor-scene context Relation\nmodeling based on Visual Semantics (JARViS), which effectively consolidates\ncross-modal action semantics distributed globally across spatial and temporal\ndimensions using Transformer attention. JARViS employs a person detector to\nproduce densely sampled actor features from a keyframe. Concurrently, it uses a\nvideo backbone to create spatio-temporal scene features from a video clip.\nFinally, the fine-grained interactions between actors and scenes are modeled\nthrough a Unified Action-Scene Context Transformer to directly output the final\nset of actions in parallel. Our experimental results demonstrate that JARViS\noutperforms existing methods by significant margins and achieves\nstate-of-the-art performance on three popular VAD datasets, including AVA,\nUCF101-24, and JHMDB51-21.\n","authors":["Seok Hwan Lee","Taein Son","Soo Won Seo","Jisong Kim","Jun Won Choi"],"pdf_url":"https://arxiv.org/pdf/2408.03612v1.pdf","comment":"31 pages, 10 figures"},{"id":"http://arxiv.org/abs/2408.03608v1","updated":"2024-08-07T07:54:19Z","published":"2024-08-07T07:54:19Z","title":"InPer: Whole-Process Domain Generalization via Causal Intervention and\n  Perturbation","summary":"  Despite the considerable advancements achieved by deep neural networks, their\nperformance tends to degenerate when the test environment diverges from the\ntraining ones. Domain generalization (DG) solves this issue by learning\nrepresentations independent of domain-related information, thus facilitating\nextrapolation to unseen environments. Existing approaches typically focus on\nformulating tailored training objectives to extract shared features from the\nsource data. However, the disjointed training and testing procedures may\ncompromise robustness, particularly in the face of unforeseen variations during\ndeployment. In this paper, we propose a novel and holistic framework based on\ncausality, named InPer, designed to enhance model generalization by\nincorporating causal intervention during training and causal perturbation\nduring testing. Specifically, during the training phase, we employ\nentropy-based causal intervention (EnIn) to refine the selection of causal\nvariables. To identify samples with anti-interference causal variables from the\ntarget domain, we propose a novel metric, homeostatic score, through causal\nperturbation (HoPer) to construct a prototype classifier in test time.\nExperimental results across multiple cross-domain tasks confirm the efficacy of\nInPer.\n","authors":["Luyao Tang","Yuxuan Yuan","Chaoqi Chen","Xinghao Ding","Yue Huang"],"pdf_url":"https://arxiv.org/pdf/2408.03608v1.pdf","comment":"Accepted by BMVC2024"},{"id":"http://arxiv.org/abs/2408.03598v1","updated":"2024-08-07T07:35:17Z","published":"2024-08-07T07:35:17Z","title":"PRISM: PRogressive dependency maxImization for Scale-invariant image\n  Matching","summary":"  Image matching aims at identifying corresponding points between a pair of\nimages. Currently, detector-free methods have shown impressive performance in\nchallenging scenarios, thanks to their capability of generating dense matches\nand global receptive field. However, performing feature interaction and\nproposing matches across the entire image is unnecessary, because not all image\nregions contribute to the matching process. Interacting and matching in\nunmatchable areas can introduce errors, reducing matching accuracy and\nefficiency. Meanwhile, the scale discrepancy issue still troubles existing\nmethods. To address above issues, we propose PRogressive dependency\nmaxImization for Scale-invariant image Matching (PRISM), which jointly prunes\nirrelevant patch features and tackles the scale discrepancy. To do this, we\nfirstly present a Multi-scale Pruning Module (MPM) to adaptively prune\nirrelevant features by maximizing the dependency between the two feature sets.\nMoreover, we design the Scale-Aware Dynamic Pruning Attention (SADPA) to\naggregate information from different scales via a hierarchical design. Our\nmethod's superior matching performance and generalization capability are\nconfirmed by leading accuracy across various evaluation benchmarks and\ndownstream tasks. The code is publicly available at\nhttps://github.com/Master-cai/PRISM.\n","authors":["Xudong Cai","Yongcai Wang","Lun Luo","Minhang Wang","Deying Li","Jintao Xu","Weihao Gu","Rui Ai"],"pdf_url":"https://arxiv.org/pdf/2408.03598v1.pdf","comment":"15 pages, 8 figures, ACM MM 2024. Supplementary materials are\n  included"},{"id":"http://arxiv.org/abs/2405.19722v2","updated":"2024-08-07T07:28:02Z","published":"2024-05-30T06:07:57Z","title":"QClusformer: A Quantum Transformer-based Framework for Unsupervised\n  Visual Clustering","summary":"  Unsupervised vision clustering, a cornerstone in computer vision, has been\nstudied for decades, yielding significant outcomes across numerous vision\ntasks. However, these algorithms involve substantial computational demands when\nconfronted with vast amounts of unlabeled data. Conversely, quantum computing\nholds promise in expediting unsupervised algorithms when handling large-scale\ndatabases. In this study, we introduce QClusformer, a pioneering\nTransformer-based framework leveraging quantum machines to tackle unsupervised\nvision clustering challenges. Specifically, we design the Transformer\narchitecture, including the self-attention module and transformer blocks, from\na quantum perspective to enable execution on quantum hardware. In addition, we\npresent QClusformer, a variant based on the Transformer architecture, tailored\nfor unsupervised vision clustering tasks. By integrating these elements into an\nend-to-end framework, QClusformer consistently outperforms previous methods\nrunning on classical computers. Empirical evaluations across diverse\nbenchmarks, including MS-Celeb-1M and DeepFashion, underscore the superior\nperformance of QClusformer compared to state-of-the-art methods.\n","authors":["Xuan-Bac Nguyen","Hoang-Quan Nguyen","Samuel Yen-Chi Chen","Samee U. Khan","Hugh Churchill","Khoa Luu"],"pdf_url":"https://arxiv.org/pdf/2405.19722v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03596v1","updated":"2024-08-07T07:24:15Z","published":"2024-08-07T07:24:15Z","title":"Hierarchical Quantum Control Gates for Functional MRI Understanding","summary":"  Quantum computing has emerged as a powerful tool for solving complex problems\nintractable for classical computers, particularly in popular fields such as\ncryptography, optimization, and neurocomputing. In this paper, we present a new\nquantum-based approach named the Hierarchical Quantum Control Gates (HQCG)\nmethod for efficient understanding of Functional Magnetic Resonance Imaging\n(fMRI) data. This approach includes two novel modules: the Local Quantum\nControl Gate (LQCG) and the Global Quantum Control Gate (GQCG), which are\ndesigned to extract local and global features of fMRI signals, respectively.\nOur method operates end-to-end on a quantum machine, leveraging quantum\nmechanics to learn patterns within extremely high-dimensional fMRI signals,\nsuch as 30,000 samples which is a challenge for classical computers. Empirical\nresults demonstrate that our approach significantly outperforms classical\nmethods. Additionally, we found that the proposed quantum model is more stable\nand less prone to overfitting than the classical methods.\n","authors":["Xuan-Bac Nguyen","Hoang-Quan Nguyen","Hugh Churchill","Samee U. Khan","Khoa Luu"],"pdf_url":"https://arxiv.org/pdf/2408.03596v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03592v1","updated":"2024-08-07T07:12:52Z","published":"2024-08-07T07:12:52Z","title":"HistoSPACE: Histology-Inspired Spatial Transcriptome Prediction And\n  Characterization Engine","summary":"  Spatial transcriptomics (ST) enables the visualization of gene expression\nwithin the context of tissue morphology. This emerging discipline has the\npotential to serve as a foundation for developing tools to design precision\nmedicines. However, due to the higher costs and expertise required for such\nexperiments, its translation into a regular clinical practice might be\nchallenging. Despite the implementation of modern deep learning to enhance\ninformation obtained from histological images using AI, efforts have been\nconstrained by limitations in the diversity of information. In this paper, we\ndeveloped a model, HistoSPACE that explore the diversity of histological images\navailable with ST data to extract molecular insights from tissue image. Our\nproposed study built an image encoder derived from universal image autoencoder.\nThis image encoder was connected to convolution blocks to built the final\nmodel. It was further fine tuned with the help of ST-Data. This model is\nnotably lightweight in compared to traditional histological models. Our\ndeveloped model demonstrates significant efficiency compared to contemporary\nalgorithms, revealing a correlation of 0.56 in leave-one-out cross-validation.\nFinally, its robustness was validated through an independent dataset, showing a\nwell matched preditction with predefined disease pathology.\n","authors":["Shivam Kumar","Samrat Chatterjee"],"pdf_url":"https://arxiv.org/pdf/2408.03592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03591v1","updated":"2024-08-07T07:09:14Z","published":"2024-08-07T07:09:14Z","title":"Focal Depth Estimation: A Calibration-Free, Subject- and Daytime\n  Invariant Approach","summary":"  In an era where personalized technology is increasingly intertwined with\ndaily life, traditional eye-tracking systems and autofocal glasses face a\nsignificant challenge: the need for frequent, user-specific calibration, which\nimpedes their practicality. This study introduces a groundbreaking\ncalibration-free method for estimating focal depth, leveraging machine learning\ntechniques to analyze eye movement features within short sequences. Our\napproach, distinguished by its innovative use of LSTM networks and\ndomain-specific feature engineering, achieves a mean absolute error (MAE) of\nless than 10 cm, setting a new focal depth estimation accuracy standard. This\nadvancement promises to enhance the usability of autofocal glasses and pave the\nway for their seamless integration into extended reality environments, marking\na significant leap forward in personalized visual technology.\n","authors":["Benedikt W. Hosp","Björn Severitt","Rajat Agarwala","Evgenia Rusak","Yannick Sauer","Siegfried Wahl"],"pdf_url":"https://arxiv.org/pdf/2408.03591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02922v2","updated":"2024-08-07T06:44:24Z","published":"2024-08-06T03:15:18Z","title":"Pose Magic: Efficient and Temporally Consistent Human Pose Estimation\n  with a Hybrid Mamba-GCN Network","summary":"  Current state-of-the-art (SOTA) methods in 3D Human Pose Estimation (HPE) are\nprimarily based on Transformers. However, existing Transformer-based 3D HPE\nbackbones often encounter a trade-off between accuracy and computational\nefficiency. To resolve the above dilemma, in this work, we leverage recent\nadvances in state space models and utilize Mamba for high-quality and efficient\nlong-range modeling. Nonetheless, Mamba still faces challenges in precisely\nexploiting local dependencies between joints. To address these issues, we\npropose a new attention-free hybrid spatiotemporal architecture named Hybrid\nMamba-GCN (Pose Magic). This architecture introduces local enhancement with GCN\nby capturing relationships between neighboring joints, thus producing new\nrepresentations to complement Mamba's outputs. By adaptively fusing\nrepresentations from Mamba and GCN, Pose Magic demonstrates superior capability\nin learning the underlying 3D structure. To meet the requirements of real-time\ninference, we also provide a fully causal version. Extensive experiments show\nthat Pose Magic achieves new SOTA results ($\\downarrow 0.9 mm$) while saving\n$74.1\\%$ FLOPs. In addition, Pose Magic exhibits optimal motion consistency and\nthe ability to generalize to unseen sequence lengths.\n","authors":["Xinyi Zhang","Qiqi Bao","Qinpeng Cui","Wenming Yang","Qingmin Liao"],"pdf_url":"https://arxiv.org/pdf/2408.02922v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11936v2","updated":"2024-08-07T06:43:51Z","published":"2024-07-16T17:26:50Z","title":"Thermal Imaging and Radar for Remote Sleep Monitoring of Breathing and\n  Apnea","summary":"  Polysomnography (PSG), the current gold standard method for monitoring and\ndetecting sleep disorders, is cumbersome and costly. At-home testing solutions,\nknown as home sleep apnea testing (HSAT), exist. However, they are\ncontact-based, a feature which limits the ability of some patient populations\nto tolerate testing and discourages widespread deployment. Previous work on\nnon-contact sleep monitoring for sleep apnea detection either estimates\nrespiratory effort using radar or nasal airflow using a thermal camera, but has\nnot compared the two or used them together. We conducted a study on 10\nparticipants, ages 34 - 78, with suspected sleep disorders using a hardware\nsetup with a synchronized radar and thermal camera. We show the first\ncomparison of radar and thermal imaging for sleep monitoring, and find that our\nthermal imaging method outperforms radar significantly. Our thermal imaging\nmethod detects apneas with an accuracy of 0.99, a precision of 0.68, a recall\nof 0.74, an F1 score of 0.71, and an intra-class correlation of 0.70; our radar\nmethod detects apneas with an accuracy of 0.83, a precision of 0.13, a recall\nof 0.86, an F1 score of 0.22, and an intra-class correlation of 0.13. We also\npresent a novel proposal for classifying obstructive and central sleep apnea by\nleveraging a multimodal setup. This method could be used accurately detect and\nclassify apneas during sleep with non-contact sensors, thereby improving\ndiagnostic capacities in patient populations unable to tolerate current\ntechnology.\n","authors":["Kai Del Regno","Alexander Vilesov","Adnan Armouti","Anirudh Bindiganavale Harish","Selim Emir Can","Ashley Kita","Achuta Kadambi"],"pdf_url":"https://arxiv.org/pdf/2407.11936v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03574v1","updated":"2024-08-07T06:26:04Z","published":"2024-08-07T06:26:04Z","title":"Teach CLIP to Develop a Number Sense for Ordinal Regression","summary":"  Ordinal regression is a fundamental problem within the field of computer\nvision, with customised well-trained models on specific tasks. While\npre-trained vision-language models (VLMs) have exhibited impressive performance\non various vision tasks, their potential for ordinal regression has received\nless exploration. In this study, we first investigate CLIP's potential for\nordinal regression, from which we expect the model could generalise to\ndifferent ordinal regression tasks and scenarios. Unfortunately, vanilla CLIP\nfails on this task, since current VLMs have a well-documented limitation of\nencapsulating compositional concepts such as number sense. We propose a simple\nyet effective method called NumCLIP to improve the quantitative understanding\nof VLMs. We disassemble the exact image to number-specific text matching\nproblem into coarse classification and fine prediction stages. We discretize\nand phrase each numerical bin with common language concept to better leverage\nthe available pre-trained alignment in CLIP. To consider the inherent\ncontinuous property of ordinal regression, we propose a novel fine-grained\ncross-modal ranking-based regularisation loss specifically designed to keep\nboth semantic and ordinal alignment in CLIP's feature space. Experimental\nresults on three general ordinal regression tasks demonstrate the effectiveness\nof NumCLIP, with 10% and 3.83% accuracy improvement on historical image dating\nand image aesthetics assessment task, respectively. Code is publicly available\nat https://github.com/xmed-lab/NumCLIP.\n","authors":["Yao Du","Qiang Zhai","Weihang Dai","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2408.03574v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2408.03568v1","updated":"2024-08-07T06:11:25Z","published":"2024-08-07T06:11:25Z","title":"A comparative study of generative adversarial networks for image\n  recognition algorithms based on deep learning and traditional methods","summary":"  In this paper, an image recognition algorithm based on the combination of\ndeep learning and generative adversarial network (GAN) is studied, and compared\nwith traditional image recognition methods. The purpose of this study is to\nevaluate the advantages and application prospects of deep learning technology,\nespecially GAN, in the field of image recognition. Firstly, this paper reviews\nthe basic principles and techniques of traditional image recognition methods,\nincluding the classical algorithms based on feature extraction such as SIFT,\nHOG and their combination with support vector machine (SVM), random forest, and\nother classifiers. Then, the working principle, network structure, and unique\nadvantages of GAN in image generation and recognition are introduced. In order\nto verify the effectiveness of GAN in image recognition, a series of\nexperiments are designed and carried out using multiple public image data sets\nfor training and testing. The experimental results show that compared with\ntraditional methods, GAN has excellent performance in processing complex\nimages, recognition accuracy, and anti-noise ability. Specifically, Gans are\nbetter able to capture high-dimensional features and details of images,\nsignificantly improving recognition performance. In addition, Gans shows unique\nadvantages in dealing with image noise, partial missing information, and\ngenerating high-quality images.\n","authors":["Yihao Zhong","Yijing Wei","Yingbin Liang","Xiqing Liu","Rongwei Ji","Yiru Cang"],"pdf_url":"https://arxiv.org/pdf/2408.03568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03567v1","updated":"2024-08-07T06:10:45Z","published":"2024-08-07T06:10:45Z","title":"Unlocking Exocentric Video-Language Data for Egocentric Video\n  Representation Learning","summary":"  We present EMBED (Egocentric Models Built with Exocentric Data), a method\ndesigned to transform exocentric video-language data for egocentric video\nrepresentation learning. Large-scale exocentric data covers diverse activities\nwith significant potential for egocentric learning, but inherent disparities\nbetween egocentric and exocentric data pose challenges in utilizing one view\nfor the other seamlessly. Egocentric videos predominantly feature close-up\nhand-object interactions, whereas exocentric videos offer a broader perspective\non human activities. Additionally, narratives in egocentric datasets are\ntypically more action-centric and closely linked with the visual content, in\ncontrast to the narrative styles found in exocentric datasets. To address these\nchallenges, we employ a data transformation framework to adapt exocentric data\nfor egocentric training, focusing on identifying specific video clips that\nemphasize hand-object interactions and transforming narration styles to align\nwith egocentric perspectives. By applying both vision and language style\ntransfer, our framework creates a new egocentric dataset derived from\nexocentric video-language data. Through extensive evaluations, we demonstrate\nthe effectiveness of EMBED, achieving state-of-the-art results across various\negocentric downstream tasks, including an absolute improvement of 4.7% on the\nEpic-Kitchens-100 multi-instance retrieval and 6.2% on the EGTEA classification\nbenchmarks in zero-shot settings. Furthermore, EMBED enables egocentric\nvideo-language models to perform competitively in exocentric tasks. Finally, we\nshowcase EMBED's application across various exocentric datasets, exhibiting\nstrong generalization capabilities when applied to different exocentric\ndatasets.\n","authors":["Zi-Yi Dou","Xitong Yang","Tushar Nagarajan","Huiyu Wang","Jing Huang","Nanyun Peng","Kris Kitani","Fu-Jen Chu"],"pdf_url":"https://arxiv.org/pdf/2408.03567v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11755v3","updated":"2024-08-07T06:05:42Z","published":"2024-03-18T13:03:24Z","title":"Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs","summary":"  Prompt ensembling of Large Language Model (LLM) generated category-specific\nprompts has emerged as an effective method to enhance zero-shot recognition\nability of Vision-Language Models (VLMs). To obtain these category-specific\nprompts, the present methods rely on hand-crafting the prompts to the LLMs for\ngenerating VLM prompts for the downstream tasks. However, this requires\nmanually composing these task-specific prompts and still, they might not cover\nthe diverse set of visual concepts and task-specific styles associated with the\ncategories of interest. To effectively take humans out of the loop and\ncompletely automate the prompt generation process for zero-shot recognition, we\npropose Meta-Prompting for Visual Recognition (MPVR). Taking as input only\nminimal information about the target task, in the form of its short natural\nlanguage description, and a list of associated class labels, MPVR automatically\nproduces a diverse set of category-specific prompts resulting in a strong\nzero-shot classifier. MPVR generalizes effectively across various popular\nzero-shot image recognition benchmarks belonging to widely different domains\nwhen tested with multiple LLMs and VLMs. For example, MPVR obtains a zero-shot\nrecognition improvement over CLIP by up to 19.8% and 18.2% (5.0% and 4.5% on\naverage over 20 datasets) leveraging GPT and Mixtral LLMs, respectively\n","authors":["M. Jehanzeb Mirza","Leonid Karlinsky","Wei Lin","Sivan Doveh","Jakub Micorek","Mateusz Kozinski","Hilde Kuehne","Horst Possegger"],"pdf_url":"https://arxiv.org/pdf/2403.11755v3.pdf","comment":"ECCV Camera Ready. Code & Data:\n  https://jmiemirza.github.io/Meta-Prompting/"},{"id":"http://arxiv.org/abs/2408.02085v3","updated":"2024-08-07T06:04:31Z","published":"2024-08-04T16:50:07Z","title":"Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data\n  Assessment and Selection for Instruction Tuning of Language Models","summary":"  Instruction tuning plays a critical role in aligning large language models\n(LLMs) with human preference. Despite the vast amount of open instruction\ndatasets, naively training a LLM on all existing instructions may not be\noptimal and practical. To pinpoint the most beneficial datapoints, data\nassessment and selection methods have been proposed in the fields of natural\nlanguage processing (NLP) and deep learning. However, under the context of\ninstruction tuning, there still exists a gap in knowledge on what kind of data\nevaluation metrics can be employed and how they can be integrated into the\nselection mechanism. To bridge this gap, we present a comprehensive review on\nexisting literature of data assessment and selection especially for instruction\ntuning of LLMs. We systematically categorize all applicable methods into\nquality-based, diversity-based, and importance-based ones where a unified,\nfine-grained taxonomy is structured. For each category, representative methods\nare elaborated to describe the landscape of relevant research. In addition,\ncomparison between latest methods is conducted on their officially reported\nresults to provide in-depth discussions on their limitations. Finally, we\nsummarize the open challenges and propose the promosing avenues for future\nstudies. All related contents are available at\nhttps://github.com/yuleiqin/fantastic-data-engineering.\n","authors":["Yulei Qin","Yuncheng Yang","Pengcheng Guo","Gang Li","Hang Shao","Yuchen Shi","Zihan Xu","Yun Gu","Ke Li","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2408.02085v3.pdf","comment":"review, survey, 28 pages, 2 figures, 4 tables"},{"id":"http://arxiv.org/abs/2402.14461v2","updated":"2024-08-07T06:01:18Z","published":"2024-02-22T11:40:49Z","title":"S^2Former-OR: Single-Stage Bi-Modal Transformer for Scene Graph\n  Generation in OR","summary":"  Scene graph generation (SGG) of surgical procedures is crucial in enhancing\nholistically cognitive intelligence in the operating room (OR). However,\nprevious works have primarily relied on multi-stage learning, where the\ngenerated semantic scene graphs depend on intermediate processes with pose\nestimation and object detection. This pipeline may potentially compromise the\nflexibility of learning multimodal representations, consequently constraining\nthe overall effectiveness. In this study, we introduce a novel single-stage\nbi-modal transformer framework for SGG in the OR, termed S^2Former-OR, aimed to\ncomplementally leverage multi-view 2D scenes and 3D point clouds for SGG in an\nend-to-end manner. Concretely, our model embraces a View-Sync Transfusion\nscheme to encourage multi-view visual information interaction. Concurrently, a\nGeometry-Visual Cohesion operation is designed to integrate the synergic 2D\nsemantic features into 3D point cloud features. Moreover, based on the\naugmented feature, we propose a novel relation-sensitive transformer decoder\nthat embeds dynamic entity-pair queries and relational trait priors, which\nenables the direct prediction of entity-pair relations for graph generation\nwithout intermediate steps. Extensive experiments have validated the superior\nSGG performance and lower computational cost of S^2Former-OR on 4D-OR\nbenchmark, compared with current OR-SGG methods, e.g., 3 percentage points\nPrecision increase and 24.2M reduction in model parameters. We further compared\nour method with generic single-stage SGG methods with broader metrics for a\ncomprehensive evaluation, with consistently better performance achieved.\n","authors":["Jialun Pei","Diandian Guo","Jingyang Zhang","Manxi Lin","Yueming Jin","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2402.14461v2.pdf","comment":"This work has been accepted by TMI2024"},{"id":"http://arxiv.org/abs/2408.03564v1","updated":"2024-08-07T05:56:05Z","published":"2024-08-07T05:56:05Z","title":"Underwater litter monitoring using consumer-grade aerial-aquatic speedy\n  scanner (AASS) and deep learning based super-resolution reconstruction and\n  detection network","summary":"  Underwater litter is widely spread across aquatic environments such as lakes,\nrivers, and oceans, significantly impacting natural ecosystems. Current\nmonitoring technologies for detecting underwater litter face limitations in\nsurvey efficiency, cost, and environmental conditions, highlighting the need\nfor efficient, consumer-grade technologies for automatic detection. This\nresearch introduces the Aerial-Aquatic Speedy Scanner (AASS) combined with\nSuper-Resolution Reconstruction (SRR) and an improved YOLOv8 detection network.\nAASS enhances data acquisition efficiency over traditional methods, capturing\nhigh-quality images that accurately identify underwater waste. SRR improves\nimage-resolution by mitigating motion blur and insufficient resolution, thereby\nenhancing detection tasks. Specifically, the RCAN model achieved the highest\nmean average precision (mAP) of 78.6% for detection accuracy on reconstructed\nimages among the tested SRR models. With a magnification factor of 4, the SRR\ntest set shows an improved mAP compared to the conventional bicubic set. These\nresults demonstrate the effectiveness of the proposed method in detecting\nunderwater litter.\n","authors":["Fan Zhao","Yongying Liu","Jiaqi Wang","Yijia Chen","Dianhan Xi","Xinlei Shao","Shigeru Tabeta","Katsunori Mizuno"],"pdf_url":"https://arxiv.org/pdf/2408.03564v1.pdf","comment":"The earlier version of this conference paper was accepted at OCEANS\n  2024-Halifax, Canada and was selected for inclusion in the Student Poster\n  Competition (SPC) Program"},{"id":"http://arxiv.org/abs/2403.19026v3","updated":"2024-08-07T05:54:04Z","published":"2024-03-27T21:43:12Z","title":"EgoNav: Egocentric Scene-aware Human Trajectory Prediction","summary":"  Wearable collaborative robots stand to assist human wearers who need fall\nprevention assistance or wear exoskeletons. Such a robot needs to be able to\nconstantly adapt to the surrounding scene based on egocentric vision, and\npredict the ego motion of the wearer. In this work, we leveraged body-mounted\ncameras and sensors to anticipate the trajectory of human wearers through\ncomplex surroundings. To facilitate research in ego-motion prediction, we have\ncollected a comprehensive walking scene navigation dataset centered on the\nuser's perspective. We then present a method to predict human motion\nconditioning on the surrounding static scene. Our method leverages a diffusion\nmodel to produce a distribution of potential future trajectories, taking into\naccount the user's observation of the environment. To that end, we introduce a\ncompact representation to encode the user's visual memory of the surroundings,\nas well as an efficient sample-generating technique to speed up real-time\ninference of a diffusion model. We ablate our model and compare it to\nbaselines, and results show that our model outperforms existing methods on key\nmetrics of collision avoidance and trajectory mode coverage.\n","authors":["Weizhuo Wang","C. Karen Liu","Monroe Kennedy III"],"pdf_url":"https://arxiv.org/pdf/2403.19026v3.pdf","comment":"13 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.03559v1","updated":"2024-08-07T05:47:15Z","published":"2024-08-07T05:47:15Z","title":"Monitoring of Hermit Crabs Using drone-captured imagery and Deep\n  Learning based Super-Resolution Reconstruction and Improved YOLOv8","summary":"  Hermit crabs play a crucial role in coastal ecosystems by dispersing seeds,\ncleaning up debris, and disturbing soil. They serve as vital indicators of\nmarine environmental health, responding to climate change and pollution.\nTraditional survey methods, like quadrat sampling, are labor-intensive,\ntime-consuming, and environmentally dependent. This study presents an\ninnovative approach combining UAV-based remote sensing with Super-Resolution\nReconstruction (SRR) and the CRAB-YOLO detection network, a modification of\nYOLOv8s, to monitor hermit crabs. SRR enhances image quality by addressing\nissues such as motion blur and insufficient resolution, significantly improving\ndetection accuracy over conventional low-resolution fuzzy images. The CRAB-YOLO\nnetwork integrates three improvements for detection accuracy, hermit crab\ncharacteristics, and computational efficiency, achieving state-of-the-art\n(SOTA) performance compared to other mainstream detection models. The RDN\nnetworks demonstrated the best image reconstruction performance, and CRAB-YOLO\nachieved a mean average precision (mAP) of 69.5% on the SRR test set, a 40%\nimprovement over the conventional Bicubic method with a magnification factor of\n4. These results indicate that the proposed method is effective in detecting\nhermit crabs, offering a cost-effective and automated solution for extensive\nhermit crab monitoring, thereby aiding coastal benthos conservation.\n","authors":["Fan Zhao","Yijia Chen","Dianhan Xi","Yongying Liu","Jiaqi Wang","Shigeru Tabeta","Katsunori Mizuno"],"pdf_url":"https://arxiv.org/pdf/2408.03559v1.pdf","comment":"The earlier version of this conference paper was presented at OCEANS\n  2024-Singapore and was selected for inclusion in the Student Poster\n  Competition (SPC) Program"},{"id":"http://arxiv.org/abs/2408.03558v1","updated":"2024-08-07T05:47:06Z","published":"2024-08-07T05:47:06Z","title":"D2Styler: Advancing Arbitrary Style Transfer with Discrete Diffusion\n  Methods","summary":"  In image processing, one of the most challenging tasks is to render an\nimage's semantic meaning using a variety of artistic approaches. Existing\ntechniques for arbitrary style transfer (AST) frequently experience\nmode-collapse, over-stylization, or under-stylization due to a disparity\nbetween the style and content images. We propose a novel framework called\nD$^2$Styler (Discrete Diffusion Styler) that leverages the discrete\nrepresentational capability of VQ-GANs and the advantages of discrete\ndiffusion, including stable training and avoidance of mode collapse. Our method\nuses Adaptive Instance Normalization (AdaIN) features as a context guide for\nthe reverse diffusion process. This makes it easy to move features from the\nstyle image to the content image without bias. The proposed method\nsubstantially enhances the visual quality of style-transferred images, allowing\nthe combination of content and style in a visually appealing manner. We take\nstyle images from the WikiArt dataset and content images from the COCO dataset.\nExperimental results demonstrate that D$^2$Styler produces high-quality\nstyle-transferred images and outperforms twelve existing methods on nearly all\nthe metrics. The qualitative results and ablation studies provide further\ninsights into the efficacy of our technique. The code is available at\nhttps://github.com/Onkarsus13/D2Styler.\n","authors":["Onkar Susladkar","Gayatri Deshmukh","Sparsh Mittal","Parth Shastri"],"pdf_url":"https://arxiv.org/pdf/2408.03558v1.pdf","comment":"Paper accepted at 27th International Conference on Pattern\n  Recognition (ICPR), 2024"},{"id":"http://arxiv.org/abs/2106.11760v5","updated":"2024-08-07T05:28:30Z","published":"2021-06-19T06:25:10Z","title":"Fingerprinting Image-to-Image Generative Adversarial Networks","summary":"  Generative Adversarial Networks (GANs) have been widely used in various\napplication scenarios. Since the production of a commercial GAN requires\nsubstantial computational and human resources, the copyright protection of GANs\nis urgently needed. This paper presents a novel fingerprinting scheme for the\nIntellectual Property (IP) protection of image-to-image GANs based on a trusted\nthird party. We break through the stealthiness and robustness bottlenecks\nsuffered by previous fingerprinting methods for classification models being\nnaively transferred to GANs. Specifically, we innovatively construct a\ncomposite deep learning model from the target GAN and a classifier. Then we\ngenerate fingerprint samples from this composite model, and embed them in the\nclassifier for effective ownership verification. This scheme inspires some\nconcrete methodologies to practically protect the modern image-to-image\ntranslation GANs. Theoretical analysis proves that these methods can satisfy\ndifferent security requirements necessary for IP protection. We also conduct\nextensive experiments to show that our solutions outperform existing\nstrategies.\n","authors":["Guanlin Li","Guowen Xu","Han Qiu","Shangwei Guo","Run Wang","Jiwei Li","Tianwei Zhang","Rongxing Lu"],"pdf_url":"https://arxiv.org/pdf/2106.11760v5.pdf","comment":"Accepted by EuroS&P 2024"},{"id":"http://arxiv.org/abs/2408.03551v1","updated":"2024-08-07T05:23:52Z","published":"2024-08-07T05:23:52Z","title":"VPOcc: Exploiting Vanishing Point for Monocular 3D Semantic Occupancy\n  Prediction","summary":"  Monocular 3D semantic occupancy prediction is becoming important in robot\nvision due to the compactness of using a single RGB camera. However, existing\nmethods often do not adequately account for camera perspective geometry,\nresulting in information imbalance along the depth range of the image. To\naddress this issue, we propose a vanishing point (VP) guided monocular 3D\nsemantic occupancy prediction framework named VPOcc. Our framework consists of\nthree novel modules utilizing VP. First, in the VPZoomer module, we initially\nutilize VP in feature extraction to achieve information balanced feature\nextraction across the scene by generating a zoom-in image based on VP. Second,\nwe perform perspective geometry-aware feature aggregation by sampling points\ntowards VP using a VP-guided cross-attention (VPCA) module. Finally, we create\nan information-balanced feature volume by effectively fusing original and\nzoom-in voxel feature volumes with a balanced feature volume fusion (BVFV)\nmodule. Experiments demonstrate that our method achieves state-of-the-art\nperformance for both IoU and mIoU on SemanticKITTI and SSCBench-KITTI360. These\nresults are obtained by effectively addressing the information imbalance in\nimages through the utilization of VP. Our code will be available at\nwww.github.com/anonymous.\n","authors":["Junsu Kim","Junhee Lee","Ukcheol Shin","Jean Oh","Kyungdon Joo"],"pdf_url":"https://arxiv.org/pdf/2408.03551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.01089v4","updated":"2024-08-07T05:22:57Z","published":"2023-02-02T13:22:18Z","title":"Curriculum Learning for ab initio Deep Learned Refractive Optics","summary":"  Deep optical optimization has recently emerged as a new paradigm for\ndesigning computational imaging systems using only the output image as the\nobjective. However, it has been limited to either simple optical systems\nconsisting of a single element such as a diffractive optical element (DOE) or\nmetalens, or the fine-tuning of compound lenses from good initial designs. Here\nwe present a DeepLens design method based on curriculum learning, which is able\nto learn optical designs of compound lenses ab initio from randomly initialized\nsurfaces without human intervention, therefore overcoming the need for a good\ninitial design. We demonstrate the effectiveness of our approach by fully\nautomatically designing both classical imaging lenses and a large field-of-view\nextended depth-of-field computational lens in a cellphone-style form factor,\nwith highly aspheric surfaces and a short back focal length.\n","authors":["Xinge Yang","Qiang Fu","Wolfgang Heidrich"],"pdf_url":"https://arxiv.org/pdf/2302.01089v4.pdf","comment":"Automatically design computational lenses from scratch with\n  differentiable ray tracing"},{"id":"http://arxiv.org/abs/2407.11652v5","updated":"2024-08-07T05:14:24Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v5.pdf","comment":"I found critical errors in the manuscript affecting its validity. I\n  need to correct these before resubmitting. Major changes to methodology and\n  results are underway, significantly altering the content. I will resubmit the\n  revised version"},{"id":"http://arxiv.org/abs/2404.06605v3","updated":"2024-08-07T05:10:43Z","published":"2024-04-09T20:24:29Z","title":"RoadBEV: Road Surface Reconstruction in Bird's Eye View","summary":"  Road surface conditions, especially geometry profiles, enormously affect\ndriving performance of autonomous vehicles. Vision-based online road\nreconstruction promisingly captures road information in advance. Existing\nsolutions like monocular depth estimation and stereo matching suffer from\nmodest performance. The recent technique of Bird's-Eye-View (BEV) perception\nprovides immense potential to more reliable and accurate reconstruction. This\npaper uniformly proposes two simple yet effective models for road elevation\nreconstruction in BEV named RoadBEV-mono and RoadBEV-stereo, which estimate\nroad elevation with monocular and stereo images, respectively. The former\ndirectly fits elevation values based on voxel features queried from image view,\nwhile the latter efficiently recognizes road elevation patterns based on BEV\nvolume representing correlation between left and right voxel features.\nInsightful analyses reveal their consistence and difference with the\nperspective view. Experiments on real-world dataset verify the models'\neffectiveness and superiority. Elevation errors of RoadBEV-mono and\nRoadBEV-stereo achieve 1.83 cm and 0.50 cm, respectively. Our models are\npromising for practical road preview, providing essential information for\npromoting safety and comfort of autonomous vehicles. The code is released at\nhttps://github.com/ztsrxh/RoadBEV\n","authors":["Tong Zhao","Lei Yang","Yichen Xie","Mingyu Ding","Masayoshi Tomizuka","Yintao Wei"],"pdf_url":"https://arxiv.org/pdf/2404.06605v3.pdf","comment":"Accepted by IEEE TITS https://ieeexplore.ieee.org/document/10618926"},{"id":"http://arxiv.org/abs/2305.10856v3","updated":"2024-08-07T04:59:01Z","published":"2023-05-18T10:18:59Z","title":"Spatial-Frequency Discriminability for Revealing Adversarial\n  Perturbations","summary":"  The vulnerability of deep neural networks to adversarial perturbations has\nbeen widely perceived in the computer vision community. From a security\nperspective, it poses a critical risk for modern vision systems, e.g., the\npopular Deep Learning as a Service (DLaaS) frameworks. For protecting deep\nmodels while not modifying them, current algorithms typically detect\nadversarial patterns through discriminative decomposition for natural and\nadversarial data. However, these decompositions are either biased towards\nfrequency resolution or spatial resolution, thus failing to capture adversarial\npatterns comprehensively. Also, when the detector relies on few fixed features,\nit is practical for an adversary to fool the model while evading the detector\n(i.e., defense-aware attack). Motivated by such facts, we propose a\ndiscriminative detector relying on a spatial-frequency Krawtchouk\ndecomposition. It expands the above works from two aspects: 1) the introduced\nKrawtchouk basis provides better spatial-frequency discriminability, capturing\nthe differences between natural and adversarial data comprehensively in both\nspatial and frequency distributions, w.r.t. the common trigonometric or wavelet\nbasis; 2) the extensive features formed by the Krawtchouk decomposition allows\nfor adaptive feature selection and secrecy mechanism, significantly increasing\nthe difficulty of the defense-aware attack, w.r.t. the detector with few fixed\nfeatures. Theoretical and numerical analyses demonstrate the uniqueness and\nusefulness of our detector, exhibiting competitive scores on several deep\nmodels and image sets against a variety of adversarial attacks.\n","authors":["Chao Wang","Shuren Qi","Zhiqiu Huang","Yushu Zhang","Rushi Lan","Xiaochun Cao","Feng-Lei Fan"],"pdf_url":"https://arxiv.org/pdf/2305.10856v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03545v1","updated":"2024-08-07T04:50:05Z","published":"2024-08-07T04:50:05Z","title":"CLIP-based Point Cloud Classification via Point Cloud to Image\n  Translation","summary":"  Point cloud understanding is an inherently challenging problem because of the\nsparse and unordered structure of the point cloud in the 3D space. Recently,\nContrastive Vision-Language Pre-training (CLIP) based point cloud\nclassification model i.e. PointCLIP has added a new direction in the point\ncloud classification research domain. In this method, at first multi-view depth\nmaps are extracted from the point cloud and passed through the CLIP visual\nencoder. To transfer the 3D knowledge to the network, a small network called an\nadapter is fine-tuned on top of the CLIP visual encoder. PointCLIP has two\nlimitations. Firstly, the point cloud depth maps lack image information which\nis essential for tasks like classification and recognition. Secondly, the\nadapter only relies on the global representation of the multi-view features.\nMotivated by this observation, we propose a Pretrained Point Cloud to Image\nTranslation Network (PPCITNet) that produces generalized colored images along\nwith additional salient visual cues to the point cloud depth maps so that it\ncan achieve promising performance on point cloud classification and\nunderstanding. In addition, we propose a novel viewpoint adapter that combines\nthe view feature processed by each viewpoint as well as the global intertwined\nknowledge that exists across the multi-view features. The experimental results\ndemonstrate the superior performance of the proposed model over existing\nstate-of-the-art CLIP-based models on ModelNet10, ModelNet40, and ScanobjectNN\ndatasets.\n","authors":["Shuvozit Ghose","Manyi Li","Yiming Qian","Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03545v1.pdf","comment":"Accepted by ICPR2024"},{"id":"http://arxiv.org/abs/2408.03542v1","updated":"2024-08-07T04:42:10Z","published":"2024-08-07T04:42:10Z","title":"Automatic identification of the area covered by acorn trees in the\n  dehesa (pastureland) Extremadura of Spain","summary":"  The acorn is the fruit of the oak and is an important crop in the Spanish\ndehesa extreme\\~na, especially for the value it provides in the Iberian pig\nfood to obtain the \"acorn\" certification. For this reason, we want to maximise\nthe production of Iberian pigs with the appropriate weight. Hence the need to\nknow the area covered by the crowns of the acorn trees, to determine the\ncovered wooded area (CWA, from the Spanish Superficie Arbolada Cubierta SAC)\nand thereby estimate the number of Iberian pigs that can be released per\nhectare, as indicated by the royal decree 4/2014. In this work, we propose the\nautomatic estimation of the CWA, through aerial digital images (orthophotos) of\nthe pastureland of Extremadura, and with this, to offer the possibility of\ndetermining the number of Iberian pigs to be released in a specific plot of\nland. Among the main issues for automatic detection are, first, the correct\nidentification of acorn trees, secondly, correctly discriminating the shades of\nthe acorn trees and, finally, detect the arbuscles (young acorn trees not yet\nproductive, or shrubs that are not oaks). These difficulties represent a real\nchallenge, both for the automatic segmentation process and for manual\nsegmentation. In this work, the proposed method for automatic segmentation is\nbased on the clustering algorithm proposed by Gustafson-Kessel (GK) but the\nmodified version of Babuska (GK-B) and on the use of real orthophotos. The\nobtained results are promising both in their comparison with the real images\nand when compared with the images segmented by hand. The whole set of\northophotos used in this work correspond to an approximate area of 142\nhectares, and the results are of great interest to producers of certified\n\"acorn\" pork.\n","authors":["Ojeda-Magaña Benjamin","Ruelas Ruben","Quintanilla-Dominguez Joel","Gomez-Barba Leopoldo","Lopez de Herrera Juan","Robledo-Hernandez Jose","Tarquis Ana"],"pdf_url":"https://arxiv.org/pdf/2408.03542v1.pdf","comment":"22 pages, 15 Figures, 2 Tables"},{"id":"http://arxiv.org/abs/2408.03540v1","updated":"2024-08-07T04:38:03Z","published":"2024-08-07T04:38:03Z","title":"PoseMamba: Monocular 3D Human Pose Estimation with Bidirectional\n  Global-Local Spatio-Temporal State Space Model","summary":"  Transformers have significantly advanced the field of 3D human pose\nestimation (HPE). However, existing transformer-based methods primarily use\nself-attention mechanisms for spatio-temporal modeling, leading to a quadratic\ncomplexity, unidirectional modeling of spatio-temporal relationships, and\ninsufficient learning of spatial-temporal correlations. Recently, the Mamba\narchitecture, utilizing the state space model (SSM), has exhibited superior\nlong-range modeling capabilities in a variety of vision tasks with linear\ncomplexity. In this paper, we propose PoseMamba, a novel purely SSM-based\napproach with linear complexity for 3D human pose estimation in monocular\nvideo. Specifically, we propose a bidirectional global-local spatio-temporal\nSSM block that comprehensively models human joint relations within individual\nframes as well as temporal correlations across frames. Within this\nbidirectional global-local spatio-temporal SSM block, we introduce a reordering\nstrategy to enhance the local modeling capability of the SSM. This strategy\nprovides a more logical geometric scanning order and integrates it with the\nglobal SSM, resulting in a combined global-local spatial scan. We have\nquantitatively and qualitatively evaluated our approach using two benchmark\ndatasets: Human3.6M and MPI-INF-3DHP. Extensive experiments demonstrate that\nPoseMamba achieves state-of-the-art performance on both datasets while\nmaintaining a smaller model size and reducing computational costs. The code and\nmodels will be released.\n","authors":["Yunlong Huang","Junshuo Liu","Ke Xian","Robert Caiming Qiu"],"pdf_url":"https://arxiv.org/pdf/2408.03540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03538v1","updated":"2024-08-07T04:35:06Z","published":"2024-08-07T04:35:06Z","title":"PRTGS: Precomputed Radiance Transfer of Gaussian Splats for Real-Time\n  High-Quality Relighting","summary":"  We proposed Precomputed RadianceTransfer of GaussianSplats (PRTGS), a\nreal-time high-quality relighting method for Gaussian splats in low-frequency\nlighting environments that captures soft shadows and interreflections by\nprecomputing 3D Gaussian splats' radiance transfer. Existing studies have\ndemonstrated that 3D Gaussian splatting (3DGS) outperforms neural fields'\nefficiency for dynamic lighting scenarios. However, the current relighting\nmethod based on 3DGS still struggles to compute high-quality shadow and\nindirect illumination in real time for dynamic light, leading to unrealistic\nrendering results. We solve this problem by precomputing the expensive\ntransport simulations required for complex transfer functions like shadowing,\nthe resulting transfer functions are represented as dense sets of vectors or\nmatrices for every Gaussian splat. We introduce distinct precomputing methods\ntailored for training and rendering stages, along with unique ray tracing and\nindirect lighting precomputation techniques for 3D Gaussian splats to\naccelerate training speed and compute accurate indirect lighting related to\nenvironment light. Experimental analyses demonstrate that our approach achieves\nstate-of-the-art visual quality while maintaining competitive training times\nand allows high-quality real-time (30+ fps) relighting for dynamic light and\nrelatively complex scenes at 1080p resolution.\n","authors":["Yijia Guo","Yuanxi Bai","Liwen Hu","Ziyi Guo","Mianzhi Liu","Yu Cai","Tiejun Huang","Lei Ma"],"pdf_url":"https://arxiv.org/pdf/2408.03538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04346v2","updated":"2024-08-07T04:00:57Z","published":"2024-07-05T08:37:10Z","title":"MobileFlow: A Multimodal LLM For Mobile GUI Agent","summary":"  Currently, the integration of mobile Graphical User Interfaces (GUIs) is\nubiquitous in most people's daily lives. And the ongoing evolution of\nmultimodal large-scale models, such as GPT-4v, Qwen-VL-Max, has significantly\nbolstered the capabilities of GUI comprehension and user action analysis,\nshowcasing the potentiality of intelligent GUI assistants. However, current GUI\nAgents often need to access page layout information through calling system\nAPIs, which may pose privacy risks. Fixing GUI (such as mobile interfaces) to a\ncertain low resolution might result in the loss of fine-grained image details.\nAt the same time, the multimodal large models built for GUI Agents currently\nhave poor understanding and decision-making abilities for Chinese GUI\ninterfaces, making them difficult to apply to a large number of Chinese apps.\nThis paper introduces MobileFlow, a multimodal large language model\nmeticulously crafted for mobile GUI agents. Transforming from the open-source\nmodel Qwen-VL-Chat into GUI domain, MobileFlow contains approximately 21\nbillion parameters and is equipped with novel hybrid visual encoders, making it\npossible for variable resolutions of image inputs and good support for\nmultilingual GUI. By incorporating Mixture of Experts (MoE) expansions and\npioneering alignment training strategies, MobileFlow has the capacity to fully\ninterpret image data and comprehend user instructions for GUI interaction\ntasks. Finally, MobileFlow outperforms Qwen-VL-Max and GPT-4v in terms of task\nexecution by GUI agents on both public and our proposed evaluation metrics, and\nhas been successfully deployed in real-world business contexts, proving its\neffectiveness for practical applications.\n","authors":["Songqin Nong","Jiali Zhu","Rui Wu","Jiongchao Jin","Shuo Shan","Xiutian Huang","Wenhao Xu"],"pdf_url":"https://arxiv.org/pdf/2407.04346v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18403v5","updated":"2024-08-07T03:30:30Z","published":"2023-05-28T15:15:48Z","title":"LoRAPrune: Structured Pruning Meets Low-Rank Parameter-Efficient\n  Fine-Tuning","summary":"  Large Language Models (LLMs), such as LLaMA and T5, have shown exceptional\nperformance across various tasks through fine-tuning. Although low-rank\nadaption (LoRA) has emerged to cheaply fine-tune these LLMs on downstream\ntasks, their deployment is still hindered by the vast model scale and\ncomputational costs. Post-training model pruning offers a way to compress LLMs.\nHowever, the current pruning methods designed for LLMs are not compatible with\nLoRA. This is due to their utilization of unstructured pruning on LLMs,\nimpeding the merging of LoRA weights, or their dependence on the gradients of\npre-trained weights to guide pruning, which can impose significant memory\noverhead. To this end, we propose LoRAPrune, a new framework that delivers an\naccurate structured pruned model in a highly memory-efficient manner.\nSpecifically, we first design a LoRA-guided pruning criterion, which uses the\nweights and gradients of LoRA, rather than the gradients of pre-trained weights\nfor importance estimation. We subsequently integrate this criterion into an\niterative pruning process, effectively removing redundant channels and heads.\nExtensive experimental results demonstrate the superior performance of our\nLoRAPrune over existing approaches on the LLaMA series models. At a 50\\%\ncompression rate, LoRAPrune demonstrates superior performance over LLM-Pruner,\nachieving a reduction in perplexity by 4.81 on WikiText2 and 3.46 on PTB, while\nalso decreasing memory usage by 52.6%. Besides, LoRAPrune also matches\nsemi-structural pruning across multiple LLMs, proving its wide applicability.\nThe code is available at https://github.com/aim-uofa/LoRAPrune.\n","authors":["Mingyang Zhang","Hao Chen","Chunhua Shen","Zhen Yang","Linlin Ou","Xinyi Yu","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2305.18403v5.pdf","comment":"accepted by acl 2024 findings"},{"id":"http://arxiv.org/abs/2408.03521v1","updated":"2024-08-07T03:16:33Z","published":"2024-08-07T03:16:33Z","title":"SwinShadow: Shifted Window for Ambiguous Adjacent Shadow Detection","summary":"  Shadow detection is a fundamental and challenging task in many computer\nvision applications. Intuitively, most shadows come from the occlusion of light\nby the object itself, resulting in the object and its shadow being contiguous\n(referred to as the adjacent shadow in this paper). In this case, when the\ncolor of the object is similar to that of the shadow, existing methods struggle\nto achieve accurate detection. To address this problem, we present SwinShadow,\na transformer-based architecture that fully utilizes the powerful shifted\nwindow mechanism for detecting adjacent shadows. The mechanism operates in two\nsteps. Initially, it applies local self-attention within a single window,\nenabling the network to focus on local details. Subsequently, it shifts the\nattention windows to facilitate inter-window attention, enabling the capture of\na broader range of adjacent information. These combined steps significantly\nimprove the network's capacity to distinguish shadows from nearby objects. And\nthe whole process can be divided into three parts: encoder, decoder, and\nfeature integration. During encoding, we adopt Swin Transformer to acquire\nhierarchical features. Then during decoding, for shallow layers, we propose a\ndeep supervision (DS) module to suppress the false positives and boost the\nrepresentation capability of shadow features for subsequent processing, while\nfor deep layers, we leverage a double attention (DA) module to integrate local\nand shifted window in one stage to achieve a larger receptive field and enhance\nthe continuity of information. Ultimately, a new multi-level aggregation (MLA)\nmechanism is applied to fuse the decoded features for mask prediction.\nExtensive experiments on three shadow detection benchmark datasets, SBU, UCF,\nand ISTD, demonstrate that our network achieves good performance in terms of\nbalance error rate (BER).\n","authors":["Yonghui Wang","Shaokai Liu","Li Li","Wengang Zhou","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2408.03521v1.pdf","comment":null}]},"2024-08-08T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2407.19674v4","updated":"2024-08-08T02:39:15Z","published":"2024-07-29T03:30:09Z","title":"Advancing Prompt Learning through an External Layer","summary":"  Prompt learning represents a promising method for adapting pre-trained\nvision-language models (VLMs) to various downstream tasks by learning a set of\ntext embeddings. One challenge inherent to these methods is the poor\ngeneralization performance due to the invalidity of the learned text embeddings\nfor unseen tasks. A straightforward approach to bridge this gap is to freeze\nthe text embeddings in prompts, which results in a lack of capacity to adapt\nVLMs for downstream tasks. To address this dilemma, we propose a paradigm\ncalled EnPrompt with a novel External Layer (EnLa). Specifically, we propose a\ntextual external layer and learnable visual embeddings for adapting VLMs to\ndownstream tasks. The learnable external layer is built upon valid embeddings\nof pre-trained CLIP. This design considers the balance of learning capabilities\nbetween the two branches. To align the textual and visual features, we propose\na novel two-pronged approach: i) we introduce the optimal transport as the\ndiscrepancy metric to align the vision and text modalities, and ii) we\nintroduce a novel strengthening feature to enhance the interaction between\nthese two modalities. Four representative experiments (i.e., base-to-novel\ngeneralization, few-shot learning, cross-dataset generalization, domain shifts\ngeneralization) across 15 datasets demonstrate that our method outperforms the\nexisting prompt learning method.\n","authors":["Fangming Cui","Xun Yang","Chao Wu","Liang Xiao","Xinmei Tian"],"pdf_url":"https://arxiv.org/pdf/2407.19674v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04633v1","updated":"2024-08-08T17:59:58Z","published":"2024-08-08T17:59:58Z","title":"LiDAR-Event Stereo Fusion with Hallucinations","summary":"  Event stereo matching is an emerging technique to estimate depth from\nneuromorphic cameras; however, events are unlikely to trigger in the absence of\nmotion or the presence of large, untextured regions, making the correspondence\nproblem extremely challenging. Purposely, we propose integrating a stereo event\ncamera with a fixed-frequency active sensor -- e.g., a LiDAR -- collecting\nsparse depth measurements, overcoming the aforementioned limitations. Such\ndepth hints are used by hallucinating -- i.e., inserting fictitious events --\nthe stacks or raw input streams, compensating for the lack of information in\nthe absence of brightness changes. Our techniques are general, can be adapted\nto any structured representation to stack events and outperform\nstate-of-the-art fusion methods applied to event-based stereo.\n","authors":["Luca Bartolomei","Matteo Poggi","Andrea Conti","Stefano Mattoccia"],"pdf_url":"https://arxiv.org/pdf/2408.04633v1.pdf","comment":"ECCV 2024. Code: https://github.com/bartn8/eventvppstereo/ - Project\n  Page: https://eventvppstereo.github.io/"},{"id":"http://arxiv.org/abs/2408.04632v1","updated":"2024-08-08T17:59:46Z","published":"2024-08-08T17:59:46Z","title":"Arctic-TILT. Business Document Understanding at Sub-Billion Scale","summary":"  The vast portion of workloads employing LLMs involves answering questions\ngrounded on PDF or scan content. We introduce the Arctic-TILT achieving\naccuracy on par with models 1000$\\times$ its size on these use cases. It can be\nfine-tuned and deployed on a single 24GB GPU, lowering operational costs while\nprocessing Visually Rich Documents with up to 400k tokens. The model\nestablishes state-of-the-art results on seven diverse Document Understanding\nbenchmarks, as well as provides reliable confidence scores and quick inference,\nwhich are essential for processing files in large-scale or time-sensitive\nenterprise environments.\n","authors":["Łukasz Borchmann","Michał Pietruszka","Wojciech Jaśkowski","Dawid Jurkiewicz","Piotr Halama","Paweł Józiak","Łukasz Garncarek","Paweł Liskowski","Karolina Szyndler","Andrzej Gretkowski","Julita Ołtusek","Gabriela Nowakowska","Artur Zawłocki","Łukasz Duhr","Paweł Dyda","Michał Turski"],"pdf_url":"https://arxiv.org/pdf/2408.04632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04631v1","updated":"2024-08-08T17:59:38Z","published":"2024-08-08T17:59:38Z","title":"Puppet-Master: Scaling Interactive Video Generation as a Motion Prior\n  for Part-Level Dynamics","summary":"  We present Puppet-Master, an interactive video generative model that can\nserve as a motion prior for part-level dynamics. At test time, given a single\nimage and a sparse set of motion trajectories (i.e., drags), Puppet-Master can\nsynthesize a video depicting realistic part-level motion faithful to the given\ndrag interactions. This is achieved by fine-tuning a large-scale pre-trained\nvideo diffusion model, for which we propose a new conditioning architecture to\ninject the dragging control effectively. More importantly, we introduce the\nall-to-first attention mechanism, a drop-in replacement for the widely adopted\nspatial attention modules, which significantly improves generation quality by\naddressing the appearance and background issues in existing models. Unlike\nother motion-conditioned video generators that are trained on in-the-wild\nvideos and mostly move an entire object, Puppet-Master is learned from\nObjaverse-Animation-HQ, a new dataset of curated part-level motion clips. We\npropose a strategy to automatically filter out sub-optimal animations and\naugment the synthetic renderings with meaningful motion trajectories.\nPuppet-Master generalizes well to real images across various categories and\noutperforms existing methods in a zero-shot manner on a real-world benchmark.\nSee our project page for more results: vgg-puppetmaster.github.io.\n","authors":["Ruining Li","Chuanxia Zheng","Christian Rupprecht","Andrea Vedaldi"],"pdf_url":"https://arxiv.org/pdf/2408.04631v1.pdf","comment":"Project page: https://vgg-puppetmaster.github.io/"},{"id":"http://arxiv.org/abs/2408.04628v1","updated":"2024-08-08T17:58:06Z","published":"2024-08-08T17:58:06Z","title":"LogogramNLP: Comparing Visual and Textual Representations of Ancient\n  Logographic Writing Systems for NLP","summary":"  Standard natural language processing (NLP) pipelines operate on symbolic\nrepresentations of language, which typically consist of sequences of discrete\ntokens. However, creating an analogous representation for ancient logographic\nwriting systems is an extremely labor intensive process that requires expert\nknowledge. At present, a large portion of logographic data persists in a purely\nvisual form due to the absence of transcription -- this issue poses a\nbottleneck for researchers seeking to apply NLP toolkits to study ancient\nlogographic languages: most of the relevant data are images of writing.\n  This paper investigates whether direct processing of visual representations\nof language offers a potential solution. We introduce LogogramNLP, the first\nbenchmark enabling NLP analysis of ancient logographic languages, featuring\nboth transcribed and visual datasets for four writing systems along with\nannotations for tasks like classification, translation, and parsing. Our\nexperiments compare systems that employ recent visual and text encoding\nstrategies as backbones. The results demonstrate that visual representations\noutperform textual representations for some investigated tasks, suggesting that\nvisual processing pipelines may unlock a large amount of cultural heritage data\nof logographic languages for NLP-based analyses.\n","authors":["Danlu Chen","Freda Shi","Aditi Agarwal","Jacobo Myerston","Taylor Berg-Kirkpatrick"],"pdf_url":"https://arxiv.org/pdf/2408.04628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07061v2","updated":"2024-08-08T17:52:16Z","published":"2024-01-13T12:32:29Z","title":"Dual-View Data Hallucination with Semantic Relation Guidance for\n  Few-Shot Image Recognition","summary":"  Learning to recognize novel concepts from just a few image samples is very\nchallenging as the learned model is easily overfitted on the few data and\nresults in poor generalizability. One promising but underexplored solution is\nto compensate the novel classes by generating plausible samples. However, most\nexisting works of this line exploit visual information only, rendering the\ngenerated data easy to be distracted by some challenging factors contained in\nthe few available samples. Being aware of the semantic information in the\ntextual modality that reflects human concepts, this work proposes a novel\nframework that exploits semantic relations to guide dual-view data\nhallucination for few-shot image recognition. The proposed framework enables\ngenerating more diverse and reasonable data samples for novel classes through\neffective information transfer from base classes. Specifically, an\ninstance-view data hallucination module hallucinates each sample of a novel\nclass to generate new data by employing local semantic correlated attention and\nglobal semantic feature fusion derived from base classes. Meanwhile, a\nprototype-view data hallucination module exploits semantic-aware measure to\nestimate the prototype of a novel class and the associated distribution from\nthe few samples, which thereby harvests the prototype as a more stable sample\nand enables resampling a large number of samples. We conduct extensive\nexperiments and comparisons with state-of-the-art methods on several popular\nfew-shot benchmarks to verify the effectiveness of the proposed framework.\n","authors":["Hefeng Wu","Guangzhi Ye","Ziyang Zhou","Ling Tian","Qing Wang","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2401.07061v2.pdf","comment":"Accepted by IEEE Transactions on Multimedia"},{"id":"http://arxiv.org/abs/2408.04610v1","updated":"2024-08-08T17:28:32Z","published":"2024-08-08T17:28:32Z","title":"Quantifying the Impact of Population Shift Across Age and Sex for\n  Abdominal Organ Segmentation","summary":"  Deep learning-based medical image segmentation has seen tremendous progress\nover the last decade, but there is still relatively little transfer into\nclinical practice. One of the main barriers is the challenge of domain\ngeneralisation, which requires segmentation models to maintain high performance\nacross a wide distribution of image data. This challenge is amplified by the\nmany factors that contribute to the diverse appearance of medical images, such\nas acquisition conditions and patient characteristics. The impact of shifting\npatient characteristics such as age and sex on segmentation performance remains\nrelatively under-studied, especially for abdominal organs, despite that this is\ncrucial for ensuring the fairness of the segmentation model. We perform the\nfirst study to determine the impact of population shift with respect to age and\nsex on abdominal CT image segmentation, by leveraging two large public\ndatasets, and introduce a novel metric to quantify the impact. We find that\npopulation shift is a challenge similar in magnitude to cross-dataset shift for\nabdominal organ segmentation, and that the effect is asymmetric and\ndataset-dependent. We conclude that dataset diversity in terms of known patient\ncharacteristics is not necessarily equivalent to dataset diversity in terms of\nimage features. This implies that simple population matching to ensure good\ngeneralisation and fairness may be insufficient, and we recommend that fairness\nresearch should be directed towards better understanding and quantifying\nmedical image dataset diversity in terms of performance-relevant\ncharacteristics such as organ morphology.\n","authors":["Kate Čevora","Ben Glocker","Wenjia Bai"],"pdf_url":"https://arxiv.org/pdf/2408.04610v1.pdf","comment":"This paper has been accepted for publication by the MICCAI 2024\n  Fairness of AI in Medical Imaging (FAIMI) Workshop"},{"id":"http://arxiv.org/abs/2408.04606v1","updated":"2024-08-08T17:26:56Z","published":"2024-08-08T17:26:56Z","title":"Enhanced Prototypical Part Network (EPPNet) For Explainable Image\n  Classification Via Prototypes","summary":"  Explainable Artificial Intelligence (xAI) has the potential to enhance the\ntransparency and trust of AI-based systems. Although accurate predictions can\nbe made using Deep Neural Networks (DNNs), the process used to arrive at such\npredictions is usually hard to explain. In terms of perceptibly human-friendly\nrepresentations, such as word phrases in text or super-pixels in images,\nprototype-based explanations can justify a model's decision. In this work, we\nintroduce a DNN architecture for image classification, the Enhanced\nPrototypical Part Network (EPPNet), which achieves strong performance while\ndiscovering relevant prototypes that can be used to explain the classification\nresults. This is achieved by introducing a novel cluster loss that helps to\ndiscover more relevant human-understandable prototypes. We also introduce a\nfaithfulness score to evaluate the explainability of the results based on the\ndiscovered prototypes. Our score not only accounts for the relevance of the\nlearned prototypes but also the performance of a model. Our evaluations on the\nCUB-200-2011 dataset show that the EPPNet outperforms state-of-the-art\nxAI-based methods, in terms of both classification accuracy and explainability\n","authors":["Bhushan Atote","Victor Sanchez"],"pdf_url":"https://arxiv.org/pdf/2408.04606v1.pdf","comment":"Accepted at the International Conference on Image Processing (ICIP),\n  IEEE (2024), we will update the new version after published through IEEE"},{"id":"http://arxiv.org/abs/2408.04605v1","updated":"2024-08-08T17:24:54Z","published":"2024-08-08T17:24:54Z","title":"Fall Detection for Industrial Setups Using YOLOv8 Variants","summary":"  This paper presents the development of an industrial fall detection system\nutilizing YOLOv8 variants, enhanced by our proposed augmentation pipeline to\nincrease dataset variance and improve detection accuracy. Among the models\nevaluated, the YOLOv8m model, consisting of 25.9 million parameters and 79.1\nGFLOPs, demonstrated a respectable balance between computational efficiency and\ndetection performance, achieving a mean Average Precision (mAP) of 0.971 at 50%\nIntersection over Union (IoU) across both \"Fall Detected\" and \"Human in Motion\"\ncategories. Although the YOLOv8l and YOLOv8x models presented higher precision\nand recall, particularly in fall detection, their higher computational demands\nand model size make them less suitable for resource-constrained environments.\n","authors":["Gracile Astlin Pereira"],"pdf_url":"https://arxiv.org/pdf/2408.04605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04604v1","updated":"2024-08-08T17:24:03Z","published":"2024-08-08T17:24:03Z","title":"Towards High-resolution 3D Anomaly Detection via Group-Level Feature\n  Contrastive Learning","summary":"  High-resolution point clouds~(HRPCD) anomaly detection~(AD) plays a critical\nrole in precision machining and high-end equipment manufacturing. Despite\nconsiderable 3D-AD methods that have been proposed recently, they still cannot\nmeet the requirements of the HRPCD-AD task. There are several challenges: i) It\nis difficult to directly capture HRPCD information due to large amounts of\npoints at the sample level; ii) The advanced transformer-based methods usually\nobtain anisotropic features, leading to degradation of the representation; iii)\nThe proportion of abnormal areas is very small, which makes it difficult to\ncharacterize. To address these challenges, we propose a novel group-level\nfeature-based network, called Group3AD, which has a significantly efficient\nrepresentation ability. First, we design an Intercluster Uniformity\nNetwork~(IUN) to present the mapping of different groups in the feature space\nas several clusters, and obtain a more uniform distribution between clusters\nrepresenting different parts of the point clouds in the feature space. Then, an\nIntracluster Alignment Network~(IAN) is designed to encourage groups within the\ncluster to be distributed tightly in the feature space. In addition, we propose\nan Adaptive Group-Center Selection~(AGCS) based on geometric information to\nimprove the pixel density of potential anomalous regions during inference. The\nexperimental results verify the effectiveness of our proposed Group3AD, which\nsurpasses Reg3D-AD by the margin of 5\\% in terms of object-level AUROC on\nReal3D-AD. We provide the code and supplementary information on our website:\nhttps://github.com/M-3LAB/Group3AD.\n","authors":["Hongze Zhu","Guoyang Xie","Chengbin Hou","Tao Dai","Can Gao","Jinbao Wang","Linlin Shen"],"pdf_url":"https://arxiv.org/pdf/2408.04604v1.pdf","comment":"ACMMM24, 12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.04600v1","updated":"2024-08-08T17:20:08Z","published":"2024-08-08T17:20:08Z","title":"Improving Network Interpretability via Explanation Consistency\n  Evaluation","summary":"  While deep neural networks have achieved remarkable performance, they tend to\nlack transparency in prediction. The pursuit of greater interpretability in\nneural networks often results in a degradation of their original performance.\nSome works strive to improve both interpretability and performance, but they\nprimarily depend on meticulously imposed conditions. In this paper, we propose\na simple yet effective framework that acquires more explainable activation\nheatmaps and simultaneously increase the model performance, without the need\nfor any extra supervision. Specifically, our concise framework introduces a new\nmetric, i.e., explanation consistency, to reweight the training samples\nadaptively in model learning. The explanation consistency metric is utilized to\nmeasure the similarity between the model's visual explanations of the original\nsamples and those of semantic-preserved adversarial samples, whose background\nregions are perturbed by using image adversarial attack techniques. Our\nframework then promotes the model learning by paying closer attention to those\ntraining samples with a high difference in explanations (i.e., low explanation\nconsistency), for which the current model cannot provide robust\ninterpretations. Comprehensive experimental results on various benchmarks\ndemonstrate the superiority of our framework in multiple aspects, including\nhigher recognition accuracy, greater data debiasing capability, stronger\nnetwork robustness, and more precise localization ability on both regular\nnetworks and interpretable networks. We also provide extensive ablation studies\nand qualitative analyses to unveil the detailed contribution of each component.\n","authors":["Hefeng Wu","Hao Jiang","Keze Wang","Ziyi Tang","Xianghuan He","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2408.04600v1.pdf","comment":"To appear in IEEE Transactions on Multimedia"},{"id":"http://arxiv.org/abs/2408.04594v1","updated":"2024-08-08T17:10:16Z","published":"2024-08-08T17:10:16Z","title":"Img-Diff: Contrastive Data Synthesis for Multimodal Large Language\n  Models","summary":"  High-performance Multimodal Large Language Models (MLLMs) rely heavily on\ndata quality. This study introduces a novel dataset named Img-Diff, designed to\nenhance fine-grained image recognition in MLLMs by leveraging insights from\ncontrastive learning and image difference captioning. By analyzing object\ndifferences between similar images, we challenge models to identify both\nmatching and distinct components. We utilize the Stable-Diffusion-XL model and\nadvanced image editing techniques to create pairs of similar images that\nhighlight object replacements. Our methodology includes a Difference Area\nGenerator for object differences identifying, followed by a Difference Captions\nGenerator for detailed difference descriptions. The result is a relatively\nsmall but high-quality dataset of \"object replacement\" samples. We use the the\nproposed dataset to fine-tune state-of-the-art (SOTA) MLLMs such as MGM-7B,\nyielding comprehensive improvements of performance scores over SOTA models that\ntrained with larger-scale datasets, in numerous image difference and Visual\nQuestion Answering tasks. For instance, our trained models notably surpass the\nSOTA models GPT-4V and Gemini on the MMVP benchmark. Besides, we investigate\nalternative methods for generating image difference data through \"object\nremoval\" and conduct thorough evaluation to confirm the dataset's diversity,\nquality, and robustness, presenting several insights on synthesis of such\ncontrastive dataset. To encourage further research and advance the field of\nmultimodal data synthesis and enhancement of MLLMs' fundamental capabilities\nfor image understanding, we release our codes and dataset at\nhttps://github.com/modelscope/data-juicer/tree/ImgDiff.\n","authors":["Qirui Jiao","Daoyuan Chen","Yilun Huang","Yaliang Li","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2408.04594v1.pdf","comment":"14 pages, 9 figures, 7 tables"},{"id":"http://arxiv.org/abs/2408.04593v1","updated":"2024-08-08T17:08:57Z","published":"2024-08-08T17:08:57Z","title":"SAM 2 in Robotic Surgery: An Empirical Evaluation for Robustness and\n  Generalization in Surgical Video Segmentation","summary":"  The recent Segment Anything Model (SAM) 2 has demonstrated remarkable\nfoundational competence in semantic segmentation, with its memory mechanism and\nmask decoder further addressing challenges in video tracking and object\nocclusion, thereby achieving superior results in interactive segmentation for\nboth images and videos. Building upon our previous empirical studies, we\nfurther explore the zero-shot segmentation performance of SAM 2 in\nrobot-assisted surgery based on prompts, alongside its robustness against\nreal-world corruption. For static images, we employ two forms of prompts:\n1-point and bounding box, while for video sequences, the 1-point prompt is\napplied to the initial frame. Through extensive experimentation on the MICCAI\nEndoVis 2017 and EndoVis 2018 benchmarks, SAM 2, when utilizing bounding box\nprompts, outperforms state-of-the-art (SOTA) methods in comparative\nevaluations. The results with point prompts also exhibit a substantial\nenhancement over SAM's capabilities, nearing or even surpassing existing\nunprompted SOTA methodologies. Besides, SAM 2 demonstrates improved inference\nspeed and less performance degradation against various image corruption.\nAlthough slightly unsatisfactory results remain in specific edges or regions,\nSAM 2's robust adaptability to 1-point prompts underscores its potential for\ndownstream surgical tasks with limited prompt requirements.\n","authors":["Jieming Yu","An Wang","Wenzhen Dong","Mengya Xu","Mobarakol Islam","Jie Wang","Long Bai","Hongliang Ren"],"pdf_url":"https://arxiv.org/pdf/2408.04593v1.pdf","comment":"Empirical study. Previous work \"SAM Meets Robotic Surgery\" is\n  accessible at: arXiv:2308.07156"},{"id":"http://arxiv.org/abs/2408.04591v1","updated":"2024-08-08T17:04:06Z","published":"2024-08-08T17:04:06Z","title":"HiLo: A Learning Framework for Generalized Category Discovery Robust to\n  Domain Shifts","summary":"  Generalized Category Discovery (GCD) is a challenging task in which, given a\npartially labelled dataset, models must categorize all unlabelled instances,\nregardless of whether they come from labelled categories or from new ones. In\nthis paper, we challenge a remaining assumption in this task: that all images\nshare the same domain. Specifically, we introduce a new task and method to\nhandle GCD when the unlabelled data also contains images from different domains\nto the labelled set. Our proposed `HiLo' networks extract High-level semantic\nand Low-level domain features, before minimizing the mutual information between\nthe representations. Our intuition is that the clusterings based on domain\ninformation and semantic information should be independent. We further extend\nour method with a specialized domain augmentation tailored for the GCD task, as\nwell as a curriculum learning approach. Finally, we construct a benchmark from\ncorrupted fine-grained datasets as well as a large-scale evaluation on\nDomainNet with real-world domain shifts, reimplementing a number of GCD\nbaselines in this setting. We demonstrate that HiLo outperforms SoTA category\ndiscovery models by a large margin on all evaluations.\n","authors":["Hongjun Wang","Sagar Vaze","Kai Han"],"pdf_url":"https://arxiv.org/pdf/2408.04591v1.pdf","comment":"39 pages, 9 figures, 26 tables"},{"id":"http://arxiv.org/abs/2408.04586v1","updated":"2024-08-08T16:56:03Z","published":"2024-08-08T16:56:03Z","title":"Sampling for View Synthesis: From Local Light Field Fusion to Neural\n  Radiance Fields and Beyond","summary":"  Capturing and rendering novel views of complex real-world scenes is a\nlong-standing problem in computer graphics and vision, with applications in\naugmented and virtual reality, immersive experiences and 3D photography. The\nadvent of deep learning has enabled revolutionary advances in this area,\nclassically known as image-based rendering. However, previous approaches\nrequire intractably dense view sampling or provide little or no guidance for\nhow users should sample views of a scene to reliably render high-quality novel\nviews. Local light field fusion proposes an algorithm for practical view\nsynthesis from an irregular grid of sampled views that first expands each\nsampled view into a local light field via a multiplane image scene\nrepresentation, then renders novel views by blending adjacent local light\nfields. Crucially, we extend traditional plenoptic sampling theory to derive a\nbound that specifies precisely how densely users should sample views of a given\nscene when using our algorithm. We achieve the perceptual quality of Nyquist\nrate view sampling while using up to 4000x fewer views. Subsequent developments\nhave led to new scene representations for deep learning with view synthesis,\nnotably neural radiance fields, but the problem of sparse view synthesis from a\nsmall number of images has only grown in importance. We reprise some of the\nrecent results on sparse and even single image view synthesis, while posing the\nquestion of whether prescriptive sampling guidelines are feasible for the new\ngeneration of image-based rendering algorithms.\n","authors":["Ravi Ramamoorthi"],"pdf_url":"https://arxiv.org/pdf/2408.04586v1.pdf","comment":"Article written for Frontiers of Science Award, International\n  Congress on Basic Science, 2024"},{"id":"http://arxiv.org/abs/2312.02647v2","updated":"2024-08-08T16:47:26Z","published":"2023-12-05T10:39:37Z","title":"TPA3D: Triplane Attention for Fast Text-to-3D Generation","summary":"  Due to the lack of large-scale text-3D correspondence data, recent text-to-3D\ngeneration works mainly rely on utilizing 2D diffusion models for synthesizing\n3D data. Since diffusion-based methods typically require significant\noptimization time for both training and inference, the use of GAN-based models\nwould still be desirable for fast 3D generation. In this work, we propose\nTriplane Attention for text-guided 3D generation (TPA3D), an end-to-end\ntrainable GAN-based deep learning model for fast text-to-3D generation. With\nonly 3D shape data and their rendered 2D images observed during training, our\nTPA3D is designed to retrieve detailed visual descriptions for synthesizing the\ncorresponding 3D mesh data. This is achieved by the proposed attention\nmechanisms on the extracted sentence and word-level text features. In our\nexperiments, we show that TPA3D generates high-quality 3D textured shapes\naligned with fine-grained descriptions, while impressive computation efficiency\ncan be observed.\n","authors":["Bin-Shih Wu","Hong-En Chen","Sheng-Yu Huang","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2312.02647v2.pdf","comment":"ECCV2024"},{"id":"http://arxiv.org/abs/2408.04579v1","updated":"2024-08-08T16:40:15Z","published":"2024-08-08T16:40:15Z","title":"SAM2-Adapter: Evaluating & Adapting Segment Anything 2 in Downstream\n  Tasks: Camouflage, Shadow, Medical Image Segmentation, and More","summary":"  The advent of large models, also known as foundation models, has\nsignificantly transformed the AI research landscape, with models like Segment\nAnything (SAM) achieving notable success in diverse image segmentation\nscenarios. Despite its advancements, SAM encountered limitations in handling\nsome complex low-level segmentation tasks like camouflaged object and medical\nimaging. In response, in 2023, we introduced SAM-Adapter, which demonstrated\nimproved performance on these challenging tasks. Now, with the release of\nSegment Anything 2 (SAM2), a successor with enhanced architecture and a larger\ntraining corpus, we reassess these challenges. This paper introduces\nSAM2-Adapter, the first adapter designed to overcome the persistent limitations\nobserved in SAM2 and achieve new state-of-the-art (SOTA) results in specific\ndownstream tasks including medical image segmentation, camouflaged (concealed)\nobject detection, and shadow detection. SAM2-Adapter builds on the\nSAM-Adapter's strengths, offering enhanced generalizability and composability\nfor diverse applications. We present extensive experimental results\ndemonstrating SAM2-Adapter's effectiveness. We show the potential and encourage\nthe research community to leverage the SAM2 model with our SAM2-Adapter for\nachieving superior segmentation outcomes. Code, pre-trained models, and data\nprocessing protocols are available at\nhttp://tianrun-chen.github.io/SAM-Adaptor/\n","authors":["Tianrun Chen","Ankang Lu","Lanyun Zhu","Chaotao Ding","Chunan Yu","Deyi Ji","Zejian Li","Lingyun Sun","Papa Mao","Ying Zang"],"pdf_url":"https://arxiv.org/pdf/2408.04579v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2304.09148"},{"id":"http://arxiv.org/abs/2408.04567v1","updated":"2024-08-08T16:27:37Z","published":"2024-08-08T16:27:37Z","title":"Sketch2Scene: Automatic Generation of Interactive 3D Game Scenes from\n  User's Casual Sketches","summary":"  3D Content Generation is at the heart of many computer graphics applications,\nincluding video gaming, film-making, virtual and augmented reality, etc. This\npaper proposes a novel deep-learning based approach for automatically\ngenerating interactive and playable 3D game scenes, all from the user's casual\nprompts such as a hand-drawn sketch. Sketch-based input offers a natural, and\nconvenient way to convey the user's design intention in the content creation\nprocess. To circumvent the data-deficient challenge in learning (i.e. the lack\nof large training data of 3D scenes), our method leverages a pre-trained 2D\ndenoising diffusion model to generate a 2D image of the scene as the conceptual\nguidance. In this process, we adopt the isometric projection mode to factor out\nunknown camera poses while obtaining the scene layout. From the generated\nisometric image, we use a pre-trained image understanding method to segment the\nimage into meaningful parts, such as off-ground objects, trees, and buildings,\nand extract the 2D scene layout. These segments and layouts are subsequently\nfed into a procedural content generation (PCG) engine, such as a 3D video game\nengine like Unity or Unreal, to create the 3D scene. The resulting 3D scene can\nbe seamlessly integrated into a game development environment and is readily\nplayable. Extensive tests demonstrate that our method can efficiently generate\nhigh-quality and interactive 3D game scenes with layouts that closely follow\nthe user's intention.\n","authors":["Yongzhi Xu","Yonhon Ng","Yifu Wang","Inkyu Sa","Yunfei Duan","Yang Li","Pan Ji","Hongdong Li"],"pdf_url":"https://arxiv.org/pdf/2408.04567v1.pdf","comment":"Project Page: https://xrvisionlabs.github.io/Sketch2Scene/"},{"id":"http://arxiv.org/abs/2307.02694v3","updated":"2024-08-08T16:24:52Z","published":"2023-07-05T23:53:55Z","title":"Loss Functions and Metrics in Deep Learning","summary":"  When training or evaluating deep learning models, two essential parts are\npicking the proper loss function and deciding on performance metrics. In this\npaper, we provide a comprehensive overview of the most common loss functions\nand metrics used across many different types of deep learning tasks, from\ngeneral tasks such as regression and classification to more specific tasks in\nComputer Vision and Natural Language Processing. We introduce the formula for\neach loss and metric, discuss their strengths and limitations, and describe how\nthese methods can be applied to various problems within deep learning. We hope\nthis work serves as a reference for researchers and practitioners in the field,\nhelping them make informed decisions when selecting the most appropriate loss\nfunction and performance metrics for their deep learning projects.\n","authors":["Juan Terven","Diana M. Cordova-Esparza","Alfonso Ramirez-Pedraza","Edgar A. Chavez-Urbiola","Julio A. Romero-Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2307.02694v3.pdf","comment":"76 pages, 4 figures, 13 tables, 127 equations"},{"id":"http://arxiv.org/abs/2407.14153v3","updated":"2024-08-08T16:20:02Z","published":"2024-07-19T09:32:30Z","title":"ESP-MedSAM: Efficient Self-Prompting SAM for Universal\n  Domain-Generalized Image Segmentation","summary":"  The universality of deep neural networks across different modalities and\ntheir generalization capabilities to unseen domains play an essential role in\nmedical image segmentation. The recent Segment Anything Model (SAM) has\ndemonstrated its potential in both settings. However, the huge computational\ncosts, demand for manual annotations as prompts and conflict-prone decoding\nprocess of SAM degrade its generalizability and applicability in clinical\nscenarios. To address these issues, we propose an efficient self-prompting SAM\nfor universal domain-generalized medical image segmentation, named ESP-MedSAM.\nSpecifically, we first devise the Multi-Modal Decoupled Knowledge Distillation\n(MMDKD) strategy to construct a lightweight semi-parameter sharing image\nencoder that produces discriminative visual features for diverse modalities.\nFurther, we introduce the Self-Patch Prompt Generator (SPPG) to automatically\ngenerate high-quality dense prompt embeddings for guiding segmentation\ndecoding. Finally, we design the Query-Decoupled Modality Decoder (QDMD) that\nleverages a one-to-one strategy to provide an independent decoding channel for\nevery modality. Extensive experiments indicate that ESP-MedSAM outperforms\nstate-of-the-arts in diverse medical imaging segmentation tasks, displaying\nsuperior modality universality and generalization capabilities. Especially,\nESP-MedSAM uses only 4.5\\% parameters compared to SAM-H. The source code is\navailable at https://github.com/xq141839/ESP-MedSAM.\n","authors":["Qing Xu","Jiaxuan Li","Xiangjian He","Ziyu Liu","Zhen Chen","Wenting Duan","Chenxin Li","Maggie M. He","Fiseha B. Tesema","Wooi P. Cheah","Yi Wang","Rong Qu","Jonathan M. Garibaldi"],"pdf_url":"https://arxiv.org/pdf/2407.14153v3.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2208.03561v2","updated":"2024-08-08T16:12:51Z","published":"2022-08-06T18:30:53Z","title":"Study of detecting behavioral signatures within DeepFake videos","summary":"  There is strong interest in the generation of synthetic video imagery of\npeople talking for various purposes, including entertainment, communication,\ntraining, and advertisement. With the development of deep fake generation\nmodels, synthetic video imagery will soon be visually indistinguishable to the\nnaked eye from a naturally capture video. In addition, many methods are\ncontinuing to improve to avoid more careful, forensic visual analysis. Some\ndeep fake videos are produced through the use of facial puppetry, which\ndirectly controls the head and face of the synthetic image through the\nmovements of the actor, allow the actor to 'puppet' the image of another. In\nthis paper, we address the question of whether one person's movements can be\ndistinguished from the original speaker by controlling the visual appearance of\nthe speaker but transferring the behavior signals from another source. We\nconduct a study by comparing synthetic imagery that: 1) originates from a\ndifferent person speaking a different utterance, 2) originates from the same\nperson speaking a different utterance, and 3) originates from a different\nperson speaking the same utterance. Our study shows that synthetic videos in\nall three cases are seen as less real and less engaging than the original\nsource video. Our results indicate that there could be a behavioral signature\nthat is detectable from a person's movements that is separate from their visual\nappearance, and that this behavioral signature could be used to distinguish a\ndeep fake from a properly captured video.\n","authors":["Qiaomu Miao","Sinhwa Kang","Stacy Marsella","Steve DiPaola","Chao Wang","Ari Shapiro"],"pdf_url":"https://arxiv.org/pdf/2208.03561v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2406.01561v3","updated":"2024-08-08T16:00:01Z","published":"2024-06-03T17:44:11Z","title":"Long and Short Guidance in Score identity Distillation for One-Step\n  Text-to-Image Generation","summary":"  Diffusion-based text-to-image generation models trained on extensive\ntext-image pairs have shown the capacity to generate photorealistic images\nconsistent with textual descriptions. However, a significant limitation of\nthese models is their slow sample generation, which requires iterative\nrefinement through the same network. In this paper, we enhance Score identity\nDistillation (SiD) by developing long and short classifier-free guidance (LSG)\nto efficiently distill pretrained Stable Diffusion models without using real\ntraining data. SiD aims to optimize a model-based explicit score matching loss,\nutilizing a score-identity-based approximation alongside the proposed LSG for\npractical computation. By training exclusively with fake images synthesized\nwith its one-step generator, SiD equipped with LSG rapidly improves FID and\nCLIP scores, achieving state-of-the-art FID performance while maintaining a\ncompetitive CLIP score. Specifically, its data-free distillation of Stable\nDiffusion 1.5 achieves a record low FID of 8.15 on the COCO-2014 validation\nset, with a CLIP score of 0.304 at an LSG scale of 1.5, and an FID of 9.56 with\na CLIP score of 0.313 at an LSG scale of 2. Our code and distilled one-step\ntext-to-image generators are available at\nhttps://github.com/mingyuanzhou/SiD-LSG.\n","authors":["Mingyuan Zhou","Zhendong Wang","Huangjie Zheng","Hai Huang"],"pdf_url":"https://arxiv.org/pdf/2406.01561v3.pdf","comment":"Code and model checkpoints available at\n  https://github.com/mingyuanzhou/SiD-LSG"},{"id":"http://arxiv.org/abs/2404.02282v3","updated":"2024-08-08T15:25:44Z","published":"2024-04-02T20:15:43Z","title":"Smooth Deep Saliency","summary":"  In this work, we investigate methods to reduce the noise in deep saliency\nmaps coming from convolutional downsampling. Those methods make the\ninvestigated models more interpretable for gradient-based saliency maps,\ncomputed in hidden layers. We evaluate the faithfulness of those methods using\ninsertion and deletion metrics, finding that saliency maps computed in hidden\nlayers perform better compared to both the input layer and GradCAM. We test our\napproach on different models trained for image classification on ImageNet1K,\nand models trained for tumor detection on Camelyon16 and in-house real-world\ndigital pathology scans of stained tissue samples. Our results show that the\ncheckerboard noise in the gradient gets reduced, resulting in smoother and\ntherefore easier to interpret saliency maps.\n","authors":["Rudolf Herdt","Maximilian Schmidt","Daniel Otero Baguer","Peter Maaß"],"pdf_url":"https://arxiv.org/pdf/2404.02282v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04523v1","updated":"2024-08-08T15:24:07Z","published":"2024-08-08T15:24:07Z","title":"Depth Any Canopy: Leveraging Depth Foundation Models for Canopy Height\n  Estimation","summary":"  Estimating global tree canopy height is crucial for forest conservation and\nclimate change applications. However, capturing high-resolution ground truth\ncanopy height using LiDAR is expensive and not available globally. An efficient\nalternative is to train a canopy height estimator to operate on single-view\nremotely sensed imagery. The primary obstacle to this approach is that these\nmethods require significant training data to generalize well globally and\nacross uncommon edge cases. Recent monocular depth estimation foundation models\nhave show strong zero-shot performance even for complex scenes. In this paper\nwe leverage the representations learned by these models to transfer to the\nremote sensing domain for measuring canopy height. Our findings suggest that\nour proposed Depth Any Canopy, the result of fine-tuning the Depth Anything v2\nmodel for canopy height estimation, provides a performant and efficient\nsolution, surpassing the current state-of-the-art with superior or comparable\nperformance using only a fraction of the computational resources and\nparameters. Furthermore, our approach requires less than \\$1.30 in compute and\nresults in an estimated carbon footprint of 0.14 kgCO2. Code, experimental\nresults, and model checkpoints are openly available at\nhttps://github.com/DarthReca/depth-any-canopy.\n","authors":["Daniele Rege Cambrin","Isaac Corley","Paolo Garza"],"pdf_url":"https://arxiv.org/pdf/2408.04523v1.pdf","comment":"Accepted at ECCV 2024 CV4E Workshop"},{"id":"http://arxiv.org/abs/2408.04515v1","updated":"2024-08-08T15:15:48Z","published":"2024-08-08T15:15:48Z","title":"Saliency Detection in Educational Videos: Analyzing the Performance of\n  Current Models, Identifying Limitations and Advancement Directions","summary":"  Identifying the regions of a learning resource that a learner pays attention\nto is crucial for assessing the material's impact and improving its design and\nrelated support systems. Saliency detection in videos addresses the automatic\nrecognition of attention-drawing regions in single frames. In educational\nsettings, the recognition of pertinent regions in a video's visual stream can\nenhance content accessibility and information retrieval tasks such as video\nsegmentation, navigation, and summarization. Such advancements can pave the way\nfor the development of advanced AI-assisted technologies that support learning\nwith greater efficacy. However, this task becomes particularly challenging for\neducational videos due to the combination of unique characteristics such as\ntext, voice, illustrations, animations, and more. To the best of our knowledge,\nthere is currently no study that evaluates saliency detection approaches in\neducational videos. In this paper, we address this gap by evaluating four\nstate-of-the-art saliency detection approaches for educational videos. We\nreproduce the original studies and explore the replication capabilities for\ngeneral-purpose (non-educational) datasets. Then, we investigate the\ngeneralization capabilities of the models and evaluate their performance on\neducational videos. We conduct a comprehensive analysis to identify common\nfailure scenarios and possible areas of improvement. Our experimental results\nshow that educational videos remain a challenging context for generic video\nsaliency detection models.\n","authors":["Evelyn Navarrete","Ralph Ewerth","Anett Hoppe"],"pdf_url":"https://arxiv.org/pdf/2408.04515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16677v2","updated":"2024-08-08T15:02:13Z","published":"2024-03-25T12:14:48Z","title":"FOOL: Addressing the Downlink Bottleneck in Satellite Computing with\n  Neural Feature Compression","summary":"  Nanosatellite constellations equipped with sensors capturing large geographic\nregions provide unprecedented opportunities for Earth observation. As\nconstellation sizes increase, network contention poses a downlink bottleneck.\nOrbital Edge Computing (OEC) leverages limited onboard compute resources to\nreduce transfer costs by processing the raw captures at the source. However,\ncurrent solutions have limited practicability due to reliance on crude\nfiltering methods or over-prioritizing particular downstream tasks.\n  This work presents FOOL, an OEC-native and task-agnostic feature compression\nmethod that preserves prediction performance. FOOL partitions high-resolution\nsatellite imagery to maximize throughput. Further, it embeds context and\nleverages inter-tile dependencies to lower transfer costs with negligible\noverhead. While FOOL is a feature compressor, it can recover images with\ncompetitive scores on quality measures at lower bitrates. We extensively\nevaluate transfer cost reduction by including the peculiarity of intermittently\navailable network connections in low earth orbit. Lastly, we test the\nfeasibility of our system for standardized nanosatellite form factors. We\ndemonstrate that FOOL permits downlinking over 100x the data volume without\nrelying on prior information on the downstream tasks.\n","authors":["Alireza Furutanpey","Qiyang Zhang","Philipp Raith","Tobias Pfandzelter","Shangguang Wang","Schahram Dustdar"],"pdf_url":"https://arxiv.org/pdf/2403.16677v2.pdf","comment":"18 pages, double column, 19 figures, 7 tables, Revision 1"},{"id":"http://arxiv.org/abs/2408.04491v1","updated":"2024-08-08T14:41:32Z","published":"2024-08-08T14:41:32Z","title":"Towards Synergistic Deep Learning Models for Volumetric Cirrhotic Liver\n  Segmentation in MRIs","summary":"  Liver cirrhosis, a leading cause of global mortality, requires precise\nsegmentation of ROIs for effective disease monitoring and treatment planning.\nExisting segmentation models often fail to capture complex feature interactions\nand generalize across diverse datasets. To address these limitations, we\npropose a novel synergistic theory that leverages complementary latent spaces\nfor enhanced feature interaction modeling. Our proposed architecture,\nnnSynergyNet3D integrates continuous and discrete latent spaces for 3D volumes\nand features auto-configured training. This approach captures both fine-grained\nand coarse features, enabling effective modeling of intricate feature\ninteractions. We empirically validated nnSynergyNet3D on a private dataset of\n628 high-resolution T1 abdominal MRI scans from 339 patients. Our model\noutperformed the baseline nnUNet3D by approximately 2%. Additionally, zero-shot\ntesting on healthy liver CT scans from the public LiTS dataset demonstrated\nsuperior cross-modal generalization capabilities. These results highlight the\npotential of synergistic latent space models to improve segmentation accuracy\nand robustness, thereby enhancing clinical workflows by ensuring consistency\nacross CT and MRI modalities.\n","authors":["Vandan Gorade","Onkar Susladkar","Gorkem Durak","Elif Keles","Ertugrul Aktas","Timurhan Cebeci","Alpay Medetalibeyoglu","Daniela Ladner","Debesh Jha","Ulas Bagci"],"pdf_url":"https://arxiv.org/pdf/2408.04491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06657v3","updated":"2024-08-08T14:34:52Z","published":"2023-03-12T13:13:05Z","title":"Color Mismatches in Stereoscopic Video: Real-World Dataset and Deep\n  Correction Method","summary":"  Stereoscopic videos can contain color mismatches between the left and right\nviews due to minor variations in camera settings, lenses, and even object\nreflections captured from different positions. The presence of color mismatches\ncan lead to viewer discomfort and headaches. This problem can be solved by\ntransferring color between stereoscopic views, but traditional methods often\nlack quality, while neural-network-based methods can easily overfit on\nartificial data. The scarcity of stereoscopic videos with real-world color\nmismatches hinders the evaluation of different methods' performance. Therefore,\nwe filmed a video dataset, which includes both distorted frames with color\nmismatches and ground-truth data, using a beam-splitter. Our second\ncontribution is a deep multiscale neural network that solves the\ncolor-mismatch-correction task by leveraging stereo correspondences. The\nexperimental results demonstrate the effectiveness of the proposed method on a\nconventional dataset, but there remains room for improvement on challenging\nreal-world data.\n","authors":["Egor Chistov","Nikita Alutis","Dmitriy Vatolin"],"pdf_url":"https://arxiv.org/pdf/2303.06657v3.pdf","comment":"The code and datasets are at\n  https://github.com/egorchistov/color-transfer/"},{"id":"http://arxiv.org/abs/2407.15787v2","updated":"2024-08-08T14:33:12Z","published":"2024-07-22T16:47:29Z","title":"Unsupervised Mastoidectomy for Cochlear CT Mesh Reconstruction Using\n  Highly Noisy Data","summary":"  Cochlear Implant (CI) procedures involve inserting an array of electrodes\ninto the cochlea located inside the inner ear. Mastoidectomy is a surgical\nprocedure that uses a high-speed drill to remove part of the mastoid region of\nthe temporal bone, providing safe access to the cochlea through the middle and\ninner ear. We aim to develop an intraoperative navigation system that registers\nplans created using 3D preoperative Computerized Tomography (CT) volumes with\nthe 2D surgical microscope view. Herein, we propose a method to synthesize the\nmastoidectomy volume using only the preoperative CT scan, where the mastoid is\nintact. We introduce an unsupervised learning framework designed to synthesize\nmastoidectomy. For model training purposes, this method uses postoperative CT\nscans to avoid manual data cleaning or labeling, even when the region removed\nduring mastoidectomy is visible but affected by metal artifacts, low\nsignal-to-noise ratio, or electrode wiring. Our approach estimates\nmastoidectomy regions with a mean dice score of 70.0%. This approach represents\na major step forward for CI intraoperative navigation by predicting realistic\nmastoidectomy-removed regions in preoperative planning that can be used to\nregister the pre-surgery plan to intraoperative microscopy.\n","authors":["Yike Zhang","Dingjie Su","Eduardo Davalos","Jack H. Noble"],"pdf_url":"https://arxiv.org/pdf/2407.15787v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12904v3","updated":"2024-08-08T14:21:35Z","published":"2023-10-19T16:57:49Z","title":"Unsupervised Object Localization in the Era of Self-Supervised ViTs: A\n  Survey","summary":"  The recent enthusiasm for open-world vision systems show the high interest of\nthe community to perform perception tasks outside of the closed-vocabulary\nbenchmark setups which have been so popular until now. Being able to discover\nobjects in images/videos without knowing in advance what objects populate the\ndataset is an exciting prospect. But how to find objects without knowing\nanything about them? Recent works show that it is possible to perform\nclass-agnostic unsupervised object localization by exploiting self-supervised\npre-trained features. We propose here a survey of unsupervised object\nlocalization methods that discover objects in images without requiring any\nmanual annotation in the era of self-supervised ViTs. We gather links of\ndiscussed methods in the repository\nhttps://github.com/valeoai/Awesome-Unsupervised-Object-Localization.\n","authors":["Oriane Siméoni","Éloi Zablocki","Spyros Gidaris","Gilles Puy","Patrick Pérez"],"pdf_url":"https://arxiv.org/pdf/2310.12904v3.pdf","comment":"IJCV 2024"},{"id":"http://arxiv.org/abs/2408.04482v1","updated":"2024-08-08T14:19:11Z","published":"2024-08-08T14:19:11Z","title":"SegXAL: Explainable Active Learning for Semantic Segmentation in Driving\n  Scene Scenarios","summary":"  Most of the sophisticated AI models utilize huge amounts of annotated data\nand heavy training to achieve high-end performance. However, there are certain\nchallenges that hinder the deployment of AI models \"in-the-wild\" scenarios,\ni.e., inefficient use of unlabeled data, lack of incorporation of human\nexpertise, and lack of interpretation of the results. To mitigate these\nchallenges, we propose a novel Explainable Active Learning (XAL) model,\nXAL-based semantic segmentation model \"SegXAL\", that can (i) effectively\nutilize the unlabeled data, (ii) facilitate the \"Human-in-the-loop\" paradigm,\nand (iii) augment the model decisions in an interpretable way. In particular,\nwe investigate the application of the SegXAL model for semantic segmentation in\ndriving scene scenarios. The SegXAL model proposes the image regions that\nrequire labeling assistance from Oracle by dint of explainable AI (XAI) and\nuncertainty measures in a weakly-supervised manner. Specifically, we propose a\nnovel Proximity-aware Explainable-AI (PAE) module and Entropy-based Uncertainty\n(EBU) module to get an Explainable Error Mask, which enables the machine\nteachers/human experts to provide intuitive reasoning behind the results and to\nsolicit feedback to the AI system via an active learning strategy. Such a\nmechanism bridges the semantic gap between man and machine through\ncollaborative intelligence, where humans and AI actively enhance each other's\ncomplementary strengths. A novel high-confidence sample selection technique\nbased on the DICE similarity coefficient is also presented within the SegXAL\nframework. Extensive quantitative and qualitative analyses are carried out in\nthe benchmarking Cityscape dataset. Results show the outperformance of our\nproposed SegXAL against other state-of-the-art models.\n","authors":["Sriram Mandalika","Athira Nambiar"],"pdf_url":"https://arxiv.org/pdf/2408.04482v1.pdf","comment":"17 pages, 7 figures. To appear in the proceedings of the 27th\n  International Conference on Pattern Recognition (ICPR), 01-05 December, 2024,\n  Kolkata, India"},{"id":"http://arxiv.org/abs/2408.04471v1","updated":"2024-08-08T14:01:12Z","published":"2024-08-08T14:01:12Z","title":"What could go wrong? Discovering and describing failure modes in\n  computer vision","summary":"  Deep learning models are effective, yet brittle. Even carefully trained,\ntheir behavior tends to be hard to predict when confronted with\nout-of-distribution samples. In this work, our goal is to propose a simple yet\neffective solution to predict and describe via natural language potential\nfailure modes of computer vision models. Given a pretrained model and a set of\nsamples, our aim is to find sentences that accurately describe the visual\nconditions in which the model underperforms. In order to study this important\ntopic and foster future research on it, we formalize the problem of\nLanguage-Based Error Explainability (LBEE) and propose a set of metrics to\nevaluate and compare different methods for this task. We propose solutions that\noperate in a joint vision-and-language embedding space, and can characterize\nthrough language descriptions model failures caused, e.g., by objects unseen\nduring training or adverse visual conditions. We experiment with different\ntasks, such as classification under the presence of dataset bias and semantic\nsegmentation in unseen environments, and show that the proposed methodology\nisolates nontrivial sentences associated with specific error causes. We hope\nour work will help practitioners better understand the behavior of models,\nincreasing their overall safety and interpretability.\n","authors":["Gabriela Csurka","Tyler L. Hayes","Diane Larlus","Riccardo Volpi"],"pdf_url":"https://arxiv.org/pdf/2408.04471v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07128v2","updated":"2024-08-08T13:58:51Z","published":"2023-12-12T10:04:11Z","title":"MS-Twins: Multi-Scale Deep Self-Attention Networks for Medical Image\n  Segmentation","summary":"  Chest X-ray is one of the most common radiological examination types for the\ndiagnosis of chest diseases. Nowadays, the automatic classification technology\nof radiological images has been widely used in clinical diagnosis and treatment\nplans. However, each disease has its own different response characteristic\nreceptive field region, which is the main challenge for chest disease\nclassification tasks. Besides, the imbalance of sample data categories further\nincreases the difficulty of tasks. To solve these problems, we propose a new\nmulti-label chest disease image classification scheme based on a multi-scale\nattention network. In this scheme, multi-scale information is iteratively fused\nto focus on regions with a high probability of disease, to effectively mine\nmore meaningful information from data, and the classification performance can\nbe improved only by image level annotation. We also designed a new loss\nfunction to improve the rationality of visual perception and the performance of\nmulti-label image classification by forcing the consistency of attention\nregions before and after image transformation. A comprehensive experiment was\ncarried out on the public Chest X-Ray14 and CheXpert datasets to achieve state\nof the art results, which verified the effectiveness of this method in chest\nX-ray image classification.\n","authors":["Jing Xu"],"pdf_url":"https://arxiv.org/pdf/2312.07128v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12539v3","updated":"2024-08-08T13:57:35Z","published":"2023-11-21T11:33:15Z","title":"GMISeg: General Medical Image Segmentation without Re-Training","summary":"  The online shopping behavior has the characteristics of rich granularity\ndimension and data sparsity and previous researches on user behavior prediction\ndid not seriously discuss feature selection and ensemble design. In this paper,\nwe proposed a SE-Stacking model based on information fusion and ensemble\nlearning for user purchase behavior prediction. After successfully utilizing\nthe ensemble feature selection method to screen purchase-related factors, we\nused the Stacking algorithm for user purchase behavior prediction. In our\nefforts to avoid the deviation of prediction results, we optimized the model by\nselecting ten different kinds of models as base learners and modifying relevant\nparameters specifically for them. The experiments conducted on a\npublicly-available dataset shows that the SE-Stacking model can achieve a\n98.40% F1-score, about 0.09% higher than the optimal base models. The\nSE-Stacking model not only has a good application in the prediction of user\npurchase behavior but also has practical value combining with the actual\ne-commerce scene. At the same time, it has important significance for academic\nresearch and the development of this field.\n","authors":["Jing Xu"],"pdf_url":"https://arxiv.org/pdf/2311.12539v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12487v2","updated":"2024-08-08T13:40:06Z","published":"2024-05-21T04:10:26Z","title":"3DSS-Mamba: 3D-Spectral-Spatial Mamba for Hyperspectral Image\n  Classification","summary":"  Hyperspectral image (HSI) classification constitutes the fundamental research\nin remote sensing fields. Convolutional Neural Networks (CNNs) and Transformers\nhave demonstrated impressive capability in capturing spectral-spatial\ncontextual dependencies. However, these architectures suffer from limited\nreceptive fields and quadratic computational complexity, respectively.\nFortunately, recent Mamba architectures built upon the State Space Model\nintegrate the advantages of long-range sequence modeling and linear\ncomputational efficiency, exhibiting substantial potential in low-dimensional\nscenarios. Motivated by this, we propose a novel 3D-Spectral-Spatial Mamba\n(3DSS-Mamba) framework for HSI classification, allowing for global\nspectral-spatial relationship modeling with greater computational efficiency.\nTechnically, a spectral-spatial token generation (SSTG) module is designed to\nconvert the HSI cube into a set of 3D spectral-spatial tokens. To overcome the\nlimitations of traditional Mamba, which is confined to modeling causal\nsequences and inadaptable to high-dimensional scenarios, a 3D-Spectral-Spatial\nSelective Scanning (3DSS) mechanism is introduced, which performs pixel-wise\nselective scanning on 3D hyperspectral tokens along the spectral and spatial\ndimensions. Five scanning routes are constructed to investigate the impact of\ndimension prioritization. The 3DSS scanning mechanism combined with\nconventional mapping operations forms the 3D-spectral-spatial mamba block\n(3DMB), enabling the extraction of global spectral-spatial semantic\nrepresentations. Experimental results and analysis demonstrate that the\nproposed method outperforms the state-of-the-art methods on HSI classification\nbenchmarks.\n","authors":["Yan He","Bing Tu","Bo Liu","Jun Li","Antonio Plaza"],"pdf_url":"https://arxiv.org/pdf/2405.12487v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03166v4","updated":"2024-08-08T13:32:21Z","published":"2024-02-05T16:35:29Z","title":"RRWNet: Recursive Refinement Network for Effective Retinal Artery/Vein\n  Segmentation and Classification","summary":"  The caliber and configuration of retinal blood vessels serve as important\nbiomarkers for various diseases and medical conditions. A thorough analysis of\nthe retinal vasculature requires the segmentation of the blood vessels and\ntheir classification into arteries and veins, typically performed on color\nfundus images obtained by retinography. However, manually performing these\ntasks is labor-intensive and prone to human error. While several automated\nmethods have been proposed to address this task, the current state of art faces\nchallenges due to manifest classification errors affecting the topological\nconsistency of segmentation maps. In this work, we introduce RRWNet, a novel\nend-to-end deep learning framework that addresses this limitation. The\nframework consists of a fully convolutional neural network that recursively\nrefines semantic segmentation maps, correcting manifest classification errors\nand thus improving topological consistency. In particular, RRWNet is composed\nof two specialized subnetworks: a Base subnetwork that generates base\nsegmentation maps from the input images, and a Recursive Refinement subnetwork\nthat iteratively and recursively improves these maps. Evaluation on three\ndifferent public datasets demonstrates the state-of-the-art performance of the\nproposed method, yielding more topologically consistent segmentation maps with\nfewer manifest classification errors than existing approaches. In addition, the\nRecursive Refinement module within RRWNet proves effective in post-processing\nsegmentation maps from other methods, further demonstrating its potential. The\nmodel code, weights, and predictions will be publicly available at\nhttps://github.com/j-morano/rrwnet.\n","authors":["José Morano","Guilherme Aresta","Hrvoje Bogunović"],"pdf_url":"https://arxiv.org/pdf/2402.03166v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04439v1","updated":"2024-08-08T13:10:03Z","published":"2024-08-08T13:10:03Z","title":"Deep Learning for identifying systolic complexes in SCG traces: a\n  cross-dataset analysis","summary":"  The seismocardiographic signal is a promising alternative to the traditional\nECG in the analysis of the cardiac activity. In particular, the systolic\ncomplex is known to be the most informative part of the seismocardiogram, thus\nrequiring further analysis. State-of-art solutions to detect the systolic\ncomplex are based on Deep Learning models, which have been proven effective in\npioneering studies. However, these solutions have only been tested in a\ncontrolled scenario considering only clean signals acquired from users\nmaintained still in supine position. On top of that, all these studies consider\ndata coming from a single dataset, ignoring the benefits and challenges related\nto a cross-dataset scenario. In this work, a cross-dataset experimental\nanalysis was performed considering also data from a real-world scenario. Our\nfindings prove the effectiveness of a deep learning solution, while showing the\nimportance of a personalization step to contrast the domain shift, namely a\nchange in data distribution between training and testing data. Finally, we\ndemonstrate the benefits of a multi-channels approach, leveraging the\ninformation extracted from both accelerometers and gyroscopes data.\n","authors":["Michele Craighero","Sarah Solbiati","Federica Mozzini","Enrico Caiani","Giacomo Boracchi"],"pdf_url":"https://arxiv.org/pdf/2408.04439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04426v1","updated":"2024-08-08T12:51:23Z","published":"2024-08-08T12:51:23Z","title":"A Review of 3D Reconstruction Techniques for Deformable Tissues in\n  Robotic Surgery","summary":"  As a crucial and intricate task in robotic minimally invasive surgery,\nreconstructing surgical scenes using stereo or monocular endoscopic video holds\nimmense potential for clinical applications. NeRF-based techniques have\nrecently garnered attention for the ability to reconstruct scenes implicitly.\nOn the other hand, Gaussian splatting-based 3D-GS represents scenes explicitly\nusing 3D Gaussians and projects them onto a 2D plane as a replacement for the\ncomplex volume rendering in NeRF. However, these methods face challenges\nregarding surgical scene reconstruction, such as slow inference, dynamic\nscenes, and surgical tool occlusion. This work explores and reviews\nstate-of-the-art (SOTA) approaches, discussing their innovations and\nimplementation principles. Furthermore, we replicate the models and conduct\ntesting and evaluation on two datasets. The test results demonstrate that with\nadvancements in these techniques, achieving real-time, high-quality\nreconstructions becomes feasible.\n","authors":["Mengya Xu","Ziqi Guo","An Wang","Long Bai","Hongliang Ren"],"pdf_url":"https://arxiv.org/pdf/2408.04426v1.pdf","comment":"To appear in MICCAI 2024 EARTH Workshop. Code availability:\n  https://github.com/Epsilon404/surgicalnerf"},{"id":"http://arxiv.org/abs/2406.18284v2","updated":"2024-08-08T12:18:30Z","published":"2024-06-26T12:09:59Z","title":"RealTalk: Real-time and Realistic Audio-driven Face Generation with 3D\n  Facial Prior-guided Identity Alignment Network","summary":"  Person-generic audio-driven face generation is a challenging task in computer\nvision. Previous methods have achieved remarkable progress in audio-visual\nsynchronization, but there is still a significant gap between current results\nand practical applications. The challenges are two-fold: 1) Preserving unique\nindividual traits for achieving high-precision lip synchronization. 2)\nGenerating high-quality facial renderings in real-time performance. In this\npaper, we propose a novel generalized audio-driven framework RealTalk, which\nconsists of an audio-to-expression transformer and a high-fidelity\nexpression-to-face renderer. In the first component, we consider both identity\nand intra-personal variation features related to speaking lip movements. By\nincorporating cross-modal attention on the enriched facial priors, we can\neffectively align lip movements with audio, thus attaining greater precision in\nexpression prediction. In the second component, we design a lightweight facial\nidentity alignment (FIA) module which includes a lip-shape control structure\nand a face texture reference structure. This novel design allows us to generate\nfine details in real-time, without depending on sophisticated and inefficient\nfeature alignment modules. Our experimental results, both quantitative and\nqualitative, on public datasets demonstrate the clear advantages of our method\nin terms of lip-speech synchronization and generation quality. Furthermore, our\nmethod is efficient and requires fewer computational resources, making it\nwell-suited to meet the needs of practical applications.\n","authors":["Xiaozhong Ji","Chuming Lin","Zhonggan Ding","Ying Tai","Junwei Zhu","Xiaobin Hu","Donghao Luo","Yanhao Ge","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2406.18284v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04407v1","updated":"2024-08-08T12:16:14Z","published":"2024-08-08T12:16:14Z","title":"Clutter Classification Using Deep Learning in Multiple Stages","summary":"  Path loss prediction for wireless communications is highly dependent on the\nlocal environment. Propagation models including clutter information have been\nshown to significantly increase model accuracy. This paper explores the\napplication of deep learning to satellite imagery to identify environmental\nclutter types automatically. Recognizing these clutter types has numerous uses,\nbut our main application is to use clutter information to enhance propagation\nprediction models. Knowing the type of obstruction (tree, building, and further\nclassifications) can improve the prediction accuracy of key propagation metrics\nsuch as path loss.\n","authors":["Ryan Dempsey","Jonathan Ethier"],"pdf_url":"https://arxiv.org/pdf/2408.04407v1.pdf","comment":"SoutheastCon 2024"},{"id":"http://arxiv.org/abs/2403.08214v2","updated":"2024-08-08T11:51:15Z","published":"2024-03-13T03:23:50Z","title":"P2LHAP:Wearable sensor-based human activity recognition, segmentation\n  and forecast through Patch-to-Label Seq2Seq Transformer","summary":"  Traditional deep learning methods struggle to simultaneously segment,\nrecognize, and forecast human activities from sensor data. This limits their\nusefulness in many fields such as healthcare and assisted living, where\nreal-time understanding of ongoing and upcoming activities is crucial. This\npaper introduces P2LHAP, a novel Patch-to-Label Seq2Seq framework that tackles\nall three tasks in a efficient single-task model. P2LHAP divides sensor data\nstreams into a sequence of \"patches\", served as input tokens, and outputs a\nsequence of patch-level activity labels including the predicted future\nactivities. A unique smoothing technique based on surrounding patch labels, is\nproposed to identify activity boundaries accurately. Additionally, P2LHAP\nlearns patch-level representation by sensor signal channel-independent\nTransformer encoders and decoders. All channels share embedding and Transformer\nweights across all sequences. Evaluated on three public datasets, P2LHAP\nsignificantly outperforms the state-of-the-art in all three tasks,\ndemonstrating its effectiveness and potential for real-world applications.\n","authors":["Shuangjian Li","Tao Zhu","Mingxing Nie","Huansheng Ning","Zhenyu Liu","Liming Chen"],"pdf_url":"https://arxiv.org/pdf/2403.08214v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09630v2","updated":"2024-08-08T11:38:21Z","published":"2024-03-14T17:58:33Z","title":"GenAD: Generalized Predictive Model for Autonomous Driving","summary":"  In this paper, we introduce the first large-scale video prediction model in\nthe autonomous driving discipline. To eliminate the restriction of high-cost\ndata collection and empower the generalization ability of our model, we acquire\nmassive data from the web and pair it with diverse and high-quality text\ndescriptions. The resultant dataset accumulates over 2000 hours of driving\nvideos, spanning areas all over the world with diverse weather conditions and\ntraffic scenarios. Inheriting the merits from recent latent diffusion models,\nour model, dubbed GenAD, handles the challenging dynamics in driving scenes\nwith novel temporal reasoning blocks. We showcase that it can generalize to\nvarious unseen driving datasets in a zero-shot manner, surpassing general or\ndriving-specific video prediction counterparts. Furthermore, GenAD can be\nadapted into an action-conditioned prediction model or a motion planner,\nholding great potential for real-world driving applications.\n","authors":["Jiazhi Yang","Shenyuan Gao","Yihang Qiu","Li Chen","Tianyu Li","Bo Dai","Kashyap Chitta","Penghao Wu","Jia Zeng","Ping Luo","Jun Zhang","Andreas Geiger","Yu Qiao","Hongyang Li"],"pdf_url":"https://arxiv.org/pdf/2403.09630v2.pdf","comment":"CVPR 2024 Highlight Paper. Dataset:\n  https://github.com/OpenDriveLab/DriveAGI"},{"id":"http://arxiv.org/abs/2408.01669v3","updated":"2024-08-08T11:19:37Z","published":"2024-08-03T05:35:13Z","title":"SynopGround: A Large-Scale Dataset for Multi-Paragraph Video Grounding\n  from TV Dramas and Synopses","summary":"  Video grounding is a fundamental problem in multimodal content understanding,\naiming to localize specific natural language queries in an untrimmed video.\nHowever, current video grounding datasets merely focus on simple events and are\neither limited to shorter videos or brief sentences, which hinders the model\nfrom evolving toward stronger multimodal understanding capabilities. To address\nthese limitations, we present a large-scale video grounding dataset named\nSynopGround, in which more than 2800 hours of videos are sourced from popular\nTV dramas and are paired with accurately localized human-written synopses. Each\nparagraph in the synopsis serves as a language query and is manually annotated\nwith precise temporal boundaries in the long video. These paragraph queries are\ntightly correlated to each other and contain a wealth of abstract expressions\nsummarizing video storylines and specific descriptions portraying event\ndetails, which enables the model to learn multimodal perception on more\nintricate concepts over longer context dependencies. Based on the dataset, we\nfurther introduce a more complex setting of video grounding dubbed\nMulti-Paragraph Video Grounding (MPVG), which takes as input multiple\nparagraphs and a long video for grounding each paragraph query to its temporal\ninterval. In addition, we propose a novel Local-Global Multimodal Reasoner\n(LGMR) to explicitly model the local-global structures of long-term multimodal\ninputs for MPVG. Our method provides an effective baseline solution to the\nmulti-paragraph video grounding problem. Extensive experiments verify the\nproposed model's effectiveness as well as its superiority in long-term\nmulti-paragraph video grounding over prior state-of-the-arts. Dataset and code\nare publicly available. Project page: https://synopground.github.io/.\n","authors":["Chaolei Tan","Zihang Lin","Junfu Pu","Zhongang Qi","Wei-Yi Pei","Zhi Qu","Yexin Wang","Ying Shan","Wei-Shi Zheng","Jian-Fang Hu"],"pdf_url":"https://arxiv.org/pdf/2408.01669v3.pdf","comment":"Accepted to ACM MM 2024. Project page: https://synopground.github.io/"},{"id":"http://arxiv.org/abs/2408.04367v1","updated":"2024-08-08T10:55:55Z","published":"2024-08-08T10:55:55Z","title":"MultiViPerFrOG: A Globally Optimized Multi-Viewpoint Perception\n  Framework for Camera Motion and Tissue Deformation","summary":"  Reconstructing the 3D shape of a deformable environment from the information\ncaptured by a moving depth camera is highly relevant to surgery. The underlying\nchallenge is the fact that simultaneously estimating camera motion and tissue\ndeformation in a fully deformable scene is an ill-posed problem, especially\nfrom a single arbitrarily moving viewpoint. Current solutions are often\norgan-specific and lack the robustness required to handle large deformations.\nHere we propose a multi-viewpoint global optimization framework that can\nflexibly integrate the output of low-level perception modules (data\nassociation, depth, and relative scene flow) with kinematic and scene-modeling\npriors to jointly estimate multiple camera motions and absolute scene flow. We\nuse simulated noisy data to show three practical examples that successfully\nconstrain the convergence to a unique solution. Overall, our method shows\nrobustness to combined noisy input measures and can process hundreds of points\nin a few milliseconds. MultiViPerFrOG builds a generalized learning-free\nscaffolding for spatio-temporal encoding that can unlock advanced surgical\nscene representations and will facilitate the development of the\ncomputer-assisted-surgery technologies of the future.\n","authors":["Guido Caccianiga","Julian Nubert","Cesar Cadena","Marco Hutter","Katherine J. Kuchenbecker"],"pdf_url":"https://arxiv.org/pdf/2408.04367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04360v1","updated":"2024-08-08T10:47:02Z","published":"2024-08-08T10:47:02Z","title":"Detecting Car Speed using Object Detection and Depth Estimation: A Deep\n  Learning Framework","summary":"  Road accidents are quite common in almost every part of the world, and, in\nmajority, fatal accidents are attributed to over speeding of vehicles. The\ntendency to over speeding is usually tried to be controlled using check points\nat various parts of the road but not all traffic police have the device to\ncheck speed with existing speed estimating devices such as LIDAR based, or\nRadar based guns. The current project tries to address the issue of vehicle\nspeed estimation with handheld devices such as mobile phones or wearable\ncameras with network connection to estimate the speed using deep learning\nframeworks.\n","authors":["Subhasis Dasgupta","Arshi Naaz","Jayeeta Choudhury","Nancy Lahiri"],"pdf_url":"https://arxiv.org/pdf/2408.04360v1.pdf","comment":"This is the pre-print of the paper which was accepted for oral\n  presentation and publication in the proceedings of IEEE CONIT 2024, organized\n  at Pune from June 21 to 23, 2024. The paper is 6 pages long and it contains\n  11 figures and 1 table. This is not the final version of the paper"},{"id":"http://arxiv.org/abs/2408.04347v1","updated":"2024-08-08T10:16:02Z","published":"2024-08-08T10:16:02Z","title":"AggSS: An Aggregated Self-Supervised Approach for Class-Incremental\n  Learning","summary":"  This paper investigates the impact of self-supervised learning, specifically\nimage rotations, on various class-incremental learning paradigms. Here, each\nimage with a predefined rotation is considered as a new class for training. At\ninference, all image rotation predictions are aggregated for the final\nprediction, a strategy we term Aggregated Self-Supervision (AggSS). We observe\na shift in the deep neural network's attention towards intrinsic object\nfeatures as it learns through AggSS strategy. This learning approach\nsignificantly enhances class-incremental learning by promoting robust feature\nlearning. AggSS serves as a plug-and-play module that can be seamlessly\nincorporated into any class-incremental learning framework, leveraging its\npowerful feature learning capabilities to enhance performance across various\nclass-incremental learning approaches. Extensive experiments conducted on\nstandard incremental learning datasets CIFAR-100 and ImageNet-Subset\ndemonstrate the significant role of AggSS in improving performance within these\nparadigms.\n","authors":["Jayateja Kalla","Soma Biswas"],"pdf_url":"https://arxiv.org/pdf/2408.04347v1.pdf","comment":"Accepted in BMVC 2024"},{"id":"http://arxiv.org/abs/2407.06704v2","updated":"2024-08-08T09:41:40Z","published":"2024-07-09T09:31:15Z","title":"Self-supervised visual learning from interactions with objects","summary":"  Self-supervised learning (SSL) has revolutionized visual representation\nlearning, but has not achieved the robustness of human vision. A reason for\nthis could be that SSL does not leverage all the data available to humans\nduring learning. When learning about an object, humans often purposefully turn\nor move around objects and research suggests that these interactions can\nsubstantially enhance their learning. Here we explore whether such\nobject-related actions can boost SSL. For this, we extract the actions\nperformed to change from one ego-centric view of an object to another in four\nvideo datasets. We then introduce a new loss function to learn visual and\naction embeddings by aligning the performed action with the representations of\ntwo images extracted from the same clip. This permits the performed actions to\nstructure the latent visual representation. Our experiments show that our\nmethod consistently outperforms previous methods on downstream category\nrecognition. In our analysis, we find that the observed improvement is\nassociated with a better viewpoint-wise alignment of different objects from the\nsame category. Overall, our work demonstrates that embodied interactions with\nobjects can improve SSL of object categories.\n","authors":["Arthur Aubret","Céline Teulière","Jochen Triesch"],"pdf_url":"https://arxiv.org/pdf/2407.06704v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08995v2","updated":"2024-08-08T09:40:29Z","published":"2023-03-15T23:59:18Z","title":"Fast and Accurate Object Detection on Asymmetrical Receptive Field","summary":"  Object detection has been used in a wide range of industries. For example, in\nautonomous driving, the task of object detection is to accurately and\nefficiently identify and locate a large number of predefined classes of object\ninstances (vehicles, pedestrians, traffic signs, etc.) from videos of roads. In\nrobotics, the industry robot needs to recognize specific machine elements. In\nthe security field, the camera should accurately recognize each face of people.\nWith the wide application of deep learning, the accuracy and efficiency of\nobject detection have been greatly improved, but object detection based on deep\nlearning still faces challenges. Different applications of object detection\nhave different requirements, including highly accurate detection,\nmulti-category object detection, real-time detection, robustness to occlusions,\netc. To address the above challenges, based on extensive literature research,\nthis paper analyzes methods for improving and optimizing mainstream object\ndetection algorithms from the perspective of evolution of one-stage and\ntwo-stage object detection algorithms. Furthermore, this article proposes\nmethods for improving object detection accuracy from the perspective of\nchanging receptive fields. The new model is based on the original YOLOv5 (You\nLook Only Once) with some modifications. The structure of the head part of\nYOLOv5 is modified by adding asymmetrical pooling layers. As a result, the\naccuracy of the algorithm is improved while ensuring the speed. The\nperformances of the new model in this article are compared with original YOLOv5\nmodel and analyzed from several parameters. And the evaluation of the new model\nis presented in four situations. Moreover, the summary and outlooks are made on\nthe problems to be solved and the research directions in the future.\n","authors":["Tianhao Lin"],"pdf_url":"https://arxiv.org/pdf/2303.08995v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20183v3","updated":"2024-08-08T09:40:10Z","published":"2024-03-29T13:57:46Z","title":"HARMamba: Efficient and Lightweight Wearable Sensor Human Activity\n  Recognition Based on Bidirectional Mamba","summary":"  Wearable sensor-based human activity recognition (HAR) is a critical research\ndomain in activity perception. However, achieving high efficiency and long\nsequence recognition remains a challenge. Despite the extensive investigation\nof temporal deep learning models, such as CNNs, RNNs, and transformers, their\nextensive parameters often pose significant computational and memory\nconstraints, rendering them less suitable for resource-constrained mobile\nhealth applications. This study introduces HARMamba, an innovative light-weight\nand versatile HAR architecture that combines selective bidirectional State\nSpaces Model and hardware-aware design. To optimize real-time resource\nconsumption in practical scenarios, HARMamba employs linear recursive\nmechanisms and parameter discretization, allowing it to selectively focus on\nrelevant input sequences while efficiently fusing scan and recompute\noperations. The model employs independent channels to process sensor data\nstreams, dividing each channel into patches and appending classification tokens\nto the end of the sequence. It utilizes position embedding to represent the\nsequence order. The patch sequence is subsequently processed by HARMamba Block,\nand the classification head finally outputs the activity category. The HARMamba\nBlock serves as the fundamental component of the HARMamba architecture,\nenabling the effective capture of more discriminative activity sequence\nfeatures. HARMamba outperforms contemporary state-of-the-art frameworks,\ndelivering comparable or better accuracy with significantly reducing\ncomputational and memory demands. It's effectiveness has been extensively\nvalidated on 4 publically available datasets namely PAMAP2, WISDM, UNIMIB SHAR\nand UCI. The F1 scores of HARMamba on the four datasets are 99.74%, 99.20%,\n88.23% and 97.01%, respectively.\n","authors":["Shuangjian Li","Tao Zhu","Furong Duan","Liming Chen","Huansheng Ning","Christopher Nugent","Yaping Wan"],"pdf_url":"https://arxiv.org/pdf/2403.20183v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04331v1","updated":"2024-08-08T09:31:24Z","published":"2024-08-08T09:31:24Z","title":"Enhancing Journalism with AI: A Study of Contextualized Image Captioning\n  for News Articles using LLMs and LMMs","summary":"  Large language models (LLMs) and large multimodal models (LMMs) have\nsignificantly impacted the AI community, industry, and various economic\nsectors. In journalism, integrating AI poses unique challenges and\nopportunities, particularly in enhancing the quality and efficiency of news\nreporting. This study explores how LLMs and LMMs can assist journalistic\npractice by generating contextualised captions for images accompanying news\narticles. We conducted experiments using the GoodNews dataset to evaluate the\nability of LMMs (BLIP-2, GPT-4v, or LLaVA) to incorporate one of two types of\ncontext: entire news articles, or extracted named entities. In addition, we\ncompared their performance to a two-stage pipeline composed of a captioning\nmodel (BLIP-2, OFA, or ViT-GPT2) with post-hoc contextualisation with LLMs\n(GPT-4 or LLaMA). We assess a diversity of models, and we find that while the\nchoice of contextualisation model is a significant factor for the two-stage\npipelines, this is not the case in the LMMs, where smaller, open-source models\nperform well compared to proprietary, GPT-powered ones. Additionally, we found\nthat controlling the amount of provided context enhances performance. These\nresults highlight the limitations of a fully automated approach and underscore\nthe necessity for an interactive, human-in-the-loop strategy.\n","authors":["Aliki Anagnostopoulou","Thiago Gouvea","Daniel Sonntag"],"pdf_url":"https://arxiv.org/pdf/2408.04331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.08389v3","updated":"2024-08-08T09:28:22Z","published":"2023-05-15T07:12:19Z","title":"Edit As You Wish: Video Caption Editing with Multi-grained User Control","summary":"  Automatically narrating videos in natural language complying with user\nrequests, i.e. Controllable Video Captioning task, can help people manage\nmassive videos with desired intentions. However, existing works suffer from two\nshortcomings: 1) the control signal is single-grained which can not satisfy\ndiverse user intentions; 2) the video description is generated in a single\nround which can not be further edited to meet dynamic needs. In this paper, we\npropose a novel \\textbf{V}ideo \\textbf{C}aption \\textbf{E}diting \\textbf{(VCE)}\ntask to automatically revise an existing video description guided by\nmulti-grained user requests. Inspired by human writing-revision habits, we\ndesign the user command as a pivotal triplet \\{\\textit{operation, position,\nattribute}\\} to cover diverse user needs from coarse-grained to fine-grained.\nTo facilitate the VCE task, we \\textit{automatically} construct an open-domain\nbenchmark dataset named VATEX-EDIT and \\textit{manually} collect an e-commerce\ndataset called EMMAD-EDIT. We further propose a specialized small-scale model\n(i.e., OPA) compared with two generalist Large Multi-modal Models to perform an\nexhaustive analysis of the novel task. For evaluation, we adopt comprehensive\nmetrics considering caption fluency, command-caption consistency, and\nvideo-caption alignment. Experiments reveal the task challenges of fine-grained\nmulti-modal semantics understanding and processing. Our datasets, codes, and\nevaluation tools are available at https://github.com/yaolinli/VCE.\n","authors":["Linli Yao","Yuanmeng Zhang","Ziheng Wang","Xinglin Hou","Tiezheng Ge","Yuning Jiang","Xu Sun","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2305.08389v3.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2408.04326v1","updated":"2024-08-08T09:09:37Z","published":"2024-08-08T09:09:37Z","title":"Multi-Scale and Detail-Enhanced Segment Anything Model for Salient\n  Object Detection","summary":"  Salient Object Detection (SOD) aims to identify and segment the most\nprominent objects in images. Advanced SOD methods often utilize various\nConvolutional Neural Networks (CNN) or Transformers for deep feature\nextraction. However, these methods still deliver low performance and poor\ngeneralization in complex cases. Recently, Segment Anything Model (SAM) has\nbeen proposed as a visual fundamental model, which gives strong segmentation\nand generalization capabilities. Nonetheless, SAM requires accurate prompts of\ntarget objects, which are unavailable in SOD. Additionally, SAM lacks the\nutilization of multi-scale and multi-level information, as well as the\nincorporation of fine-grained details. To address these shortcomings, we\npropose a Multi-scale and Detail-enhanced SAM (MDSAM) for SOD. Specifically, we\nfirst introduce a Lightweight Multi-Scale Adapter (LMSA), which allows SAM to\nlearn multi-scale information with very few trainable parameters. Then, we\npropose a Multi-Level Fusion Module (MLFM) to comprehensively utilize the\nmulti-level information from the SAM's encoder. Finally, we propose a Detail\nEnhancement Module (DEM) to incorporate SAM with fine-grained details.\nExperimental results demonstrate the superior performance of our model on\nmultiple SOD datasets and its strong generalization on other segmentation\ntasks. The source code is released at https://github.com/BellyBeauty/MDSAM.\n","authors":["Shixuan Gao","Pingping Zhang","Tianyu Yan","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2408.04326v1.pdf","comment":"This work is accepted by ACM MM2024"},{"id":"http://arxiv.org/abs/2408.04318v1","updated":"2024-08-08T08:52:29Z","published":"2024-08-08T08:52:29Z","title":"Deep Transfer Learning for Kidney Cancer Diagnosis","summary":"  Many incurable diseases prevalent across global societies stem from various\ninfluences, including lifestyle choices, economic conditions, social factors,\nand genetics. Research predominantly focuses on these diseases due to their\nwidespread nature, aiming to decrease mortality, enhance treatment options, and\nimprove healthcare standards. Among these, kidney disease stands out as a\nparticularly severe condition affecting men and women worldwide. Nonetheless,\nthere is a pressing need for continued research into innovative, early\ndiagnostic methods to develop more effective treatments for such diseases.\nRecently, automatic diagnosis of Kidney Cancer has become an important\nchallenge especially when using deep learning (DL) due to the importance of\ntraining medical datasets, which in most cases are difficult and expensive to\nobtain. Furthermore, in most cases, algorithms require data from the same\ndomain and a powerful computer with efficient storage capacity. To overcome\nthis issue, a new type of learning known as transfer learning (TL) has been\nproposed that can produce impressive results based on other different\npre-trained data. This paper presents, to the best of the authors' knowledge,\nthe first comprehensive survey of DL-based TL frameworks for kidney cancer\ndiagnosis. This is a strong contribution to help researchers understand the\ncurrent challenges and perspectives of this topic. Hence, the main limitations\nand advantages of each framework are identified and detailed critical analyses\nare provided. Looking ahead, the article identifies promising directions for\nfuture research. Moving on, the discussion is concluded by reflecting on the\npivotal role of TL in the development of precision medicine and its effects on\nclinical practice and research in oncology.\n","authors":["Yassine Habchi","Hamza Kheddar","Yassine Himeur","Abdelkrim Boukabou","Shadi Atalla","Wathiq Mansoor","Hussain Al-Ahmad"],"pdf_url":"https://arxiv.org/pdf/2408.04318v1.pdf","comment":"32 pages, 8 figures and 8 tables"},{"id":"http://arxiv.org/abs/2403.11868v7","updated":"2024-08-08T08:45:08Z","published":"2024-03-18T15:22:09Z","title":"View-Consistent 3D Editing with Gaussian Splatting","summary":"  The advent of 3D Gaussian Splatting (3DGS) has revolutionized 3D editing,\noffering efficient, high-fidelity rendering and enabling precise local\nmanipulations. Currently, diffusion-based 2D editing models are harnessed to\nmodify multi-view rendered images, which then guide the editing of 3DGS models.\nHowever, this approach faces a critical issue of multi-view inconsistency,\nwhere the guidance images exhibit significant discrepancies across views,\nleading to mode collapse and visual artifacts of 3DGS. To this end, we\nintroduce View-consistent Editing (VcEdit), a novel framework that seamlessly\nincorporates 3DGS into image editing processes, ensuring multi-view consistency\nin edited guidance images and effectively mitigating mode collapse issues.\nVcEdit employs two innovative consistency modules: the Cross-attention\nConsistency Module and the Editing Consistency Module, both designed to reduce\ninconsistencies in edited images. By incorporating these consistency modules\ninto an iterative pattern, VcEdit proficiently resolves the issue of multi-view\ninconsistency, facilitating high-quality 3DGS editing across a diverse range of\nscenes. Further video results are shown in http://vcedit.github.io.\n","authors":["Yuxuan Wang","Xuanyu Yi","Zike Wu","Na Zhao","Long Chen","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11868v7.pdf","comment":"accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2407.11652v6","updated":"2024-08-08T08:44:29Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v6.pdf","comment":"I found critical errors in the manuscript affecting its validity. I\n  need to correct these before resubmitting. Major changes to methodology and\n  results are underway, significantly altering the content. I will resubmit the\n  revised version"},{"id":"http://arxiv.org/abs/2401.10110v4","updated":"2024-08-08T08:42:31Z","published":"2024-01-18T16:27:09Z","title":"SVIPTR: Fast and Efficient Scene Text Recognition with Vision Permutable\n  Extractor","summary":"  Scene Text Recognition (STR) is an important and challenging upstream task\nfor building structured information databases, that involves recognizing text\nwithin images of natural scenes. Although current state-of-the-art (SOTA)\nmodels for STR exhibit high performance, they typically suffer from low\ninference efficiency due to their reliance on hybrid architectures comprised of\nvisual encoders and sequence decoders. In this work, we propose a VIsion\nPermutable extractor for fast and efficient Scene Text Recognition (SVIPTR),\nwhich achieves an impressive balance between high performance and rapid\ninference speeds in the domain of STR. Specifically, SVIPTR leverages a\nvisual-semantic extractor with a pyramid structure, characterized by the\nPermutation and combination of local and global self-attention layers. This\ndesign results in a lightweight and efficient model and its inference is\ninsensitive to input length. Extensive experimental results on various standard\ndatasets for both Chinese and English scene text recognition validate the\nsuperiority of SVIPTR. Notably, the SVIPTR-T (Tiny) variant delivers highly\ncompetitive accuracy on par with other lightweight models and achieves SOTA\ninference speeds. Meanwhile, the SVIPTR-L (Large) attains SOTA accuracy in\nsingle-encoder-type models, while maintaining a low parameter count and\nfavorable inference speed. Our proposed method provides a compelling solution\nfor the STR challenge, which greatly benefits real-world applications requiring\nfast and efficient STR. The code is publicly available at\nhttps://github.com/cxfyxl/VIPTR.\n","authors":["Xianfu Cheng","Weixiao Zhou","Xiang Li","Jian Yang","Hang Zhang","Tao Sun","Wei Zhang","Yuying Mai","Tongliang Li","Xiaoming Chen","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2401.10110v4.pdf","comment":"10 pages, 4 figures, 6 tables"},{"id":"http://arxiv.org/abs/2408.04300v1","updated":"2024-08-08T08:35:21Z","published":"2024-08-08T08:35:21Z","title":"An Explainable Non-local Network for COVID-19 Diagnosis","summary":"  The CNN has achieved excellent results in the automatic classification of\nmedical images. In this study, we propose a novel deep residual 3D attention\nnon-local network (NL-RAN) to classify CT images included COVID-19, common\npneumonia, and normal to perform rapid and explainable COVID-19 diagnosis. We\nbuilt a deep residual 3D attention non-local network that could achieve\nend-to-end training. The network is embedded with a nonlocal module to capture\nglobal information, while a 3D attention module is embedded to focus on the\ndetails of the lesion so that it can directly analyze the 3D lung CT and output\nthe classification results. The output of the attention module can be used as a\nheat map to increase the interpretability of the model. 4079 3D CT scans were\nincluded in this study. Each scan had a unique label (novel coronavirus\npneumonia, common pneumonia, and normal). The CT scans cohort was randomly\nsplit into a training set of 3263 scans, a validation set of 408 scans, and a\ntesting set of 408 scans. And compare with existing mainstream classification\nmethods, such as CovNet, CBAM, ResNet, etc. Simultaneously compare the\nvisualization results with visualization methods such as CAM. Model performance\nwas evaluated using the Area Under the ROC Curve(AUC), precision, and F1-score.\nThe NL-RAN achieved the AUC of 0.9903, the precision of 0.9473, and the\nF1-score of 0.9462, surpass all the classification methods compared. The heat\nmap output by the attention module is also clearer than the heat map output by\nCAM. Our experimental results indicate that our proposed method performs\nsignificantly better than existing methods. In addition, the first attention\nmodule outputs a heat map containing detailed outline information to increase\nthe interpretability of the model. Our experiments indicate that the inference\nof our model is fast. It can provide real-time assistance with diagnosis.\n","authors":["Jingfu Yang","Peng Huang","Jing Hu","Shu Hu","Siwei Lyu","Xin Wang","Jun Guo","Xi Wu"],"pdf_url":"https://arxiv.org/pdf/2408.04300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04299v1","updated":"2024-08-08T08:25:38Z","published":"2024-08-08T08:25:38Z","title":"Respiratory Subtraction for Pulmonary Microwave Ablation Evaluation","summary":"  Currently, lung cancer is a leading cause of global cancer mortality, often\nnecessitating minimally invasive interventions. Microwave ablation (MWA) is\nextensively utilized for both primary and secondary lung tumors. Although\nnumerous clinical guidelines and standards for MWA have been established, the\nclinical evaluation of ablation surgery remains challenging and requires\nlong-term patient follow-up for confirmation. In this paper, we propose a\nmethod termed respiratory subtraction to evaluate lung tumor ablation therapy\nperformance based on pre- and post-operative image guidance. Initially,\npreoperative images undergo coarse rigid registration to their corresponding\npostoperative positions, followed by further non-rigid registration.\nSubsequently, subtraction images are generated by subtracting the registered\npreoperative images from the postoperative ones. Furthermore, to enhance the\nclinical assessment of MWA treatment performance, we devise a quantitative\nanalysis metric to evaluate ablation efficacy by comparing differences between\ntumor areas and treatment areas. To the best of our knowledge, this is the\npioneering work in the field to facilitate the assessment of MWA surgery\nperformance on pulmonary tumors. Extensive experiments involving 35 clinical\ncases further validate the efficacy of the respiratory subtraction method. The\nexperimental results confirm the effectiveness of the respiratory subtraction\nmethod and the proposed quantitative evaluation metric in assessing lung tumor\ntreatment.\n","authors":["Wan Li","Xinyun Zhong","Wei Li","Song Zhang","Moheng Rong","Yan Xi","Peng Yuan","Zechen Wang","Xiaolei Jiang","Rongxi Yi","Hui Tang","Yang Chen","Chaohui Tong","Zhan Wu","Feng Wang"],"pdf_url":"https://arxiv.org/pdf/2408.04299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04294v1","updated":"2024-08-08T08:17:50Z","published":"2024-08-08T08:17:50Z","title":"Dual-branch PolSAR Image Classification Based on GraphMAE and Local\n  Feature Extraction","summary":"  The annotation of polarimetric synthetic aperture radar (PolSAR) images is a\nlabor-intensive and time-consuming process. Therefore, classifying PolSAR\nimages with limited labels is a challenging task in remote sensing domain. In\nrecent years, self-supervised learning approaches have proven effective in\nPolSAR image classification with sparse labels. However, we observe a lack of\nresearch on generative selfsupervised learning in the studied task. Motivated\nby this, we propose a dual-branch classification model based on generative\nself-supervised learning in this paper. The first branch is a\nsuperpixel-branch, which learns superpixel-level polarimetric representations\nusing a generative self-supervised graph masked autoencoder. To acquire finer\nclassification results, a convolutional neural networks-based pixel-branch is\nfurther incorporated to learn pixel-level features. Classification with fused\ndual-branch features is finally performed to obtain the predictions.\nExperimental results on the benchmark Flevoland dataset demonstrate that our\napproach yields promising classification results.\n","authors":["Yuchen Wang","Ziyi Guo","Haixia Bi","Danfeng Hong","Chen Xu"],"pdf_url":"https://arxiv.org/pdf/2408.04294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04290v1","updated":"2024-08-08T08:06:42Z","published":"2024-08-08T08:06:42Z","title":"Efficient and Accurate Pneumonia Detection Using a Novel Multi-Scale\n  Transformer Approach","summary":"  Pneumonia, a severe respiratory disease, poses significant diagnostic\nchallenges, especially in underdeveloped regions. Traditional diagnostic\nmethods, such as chest X-rays, suffer from variability in interpretation among\nradiologists, necessitating reliable automated tools. In this study, we propose\na novel approach combining deep learning and transformer-based attention\nmechanisms to enhance pneumonia detection from chest X-rays. Our method begins\nwith lung segmentation using a TransUNet model that integrates our specialized\ntransformer module, which has fewer parameters compared to common transformers\nwhile maintaining performance. This model is trained on the \"Chest Xray Masks\nand Labels\" dataset and then applied to the Kermany and Cohen datasets to\nisolate lung regions, enhancing subsequent classification tasks. For\nclassification, we employ pre-trained ResNet models (ResNet-50 and ResNet-101)\nto extract multi-scale feature maps, processed through our modified transformer\nmodule. By employing our specialized transformer, we attain superior results\nwith significantly fewer parameters compared to common transformer models. Our\napproach achieves high accuracy rates of 92.79% on the Kermany dataset and\n95.11% on the Cohen dataset, ensuring robust and efficient performance suitable\nfor resource-constrained environments.\n\"https://github.com/amirrezafateh/Multi-Scale-Transformer-Pneumonia\"\n","authors":["Alireza Saber","Pouria Parhami","Alimihammad Siahkarzadeh","Amirreza Fateh"],"pdf_url":"https://arxiv.org/pdf/2408.04290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04158v2","updated":"2024-08-08T07:51:10Z","published":"2024-06-06T15:18:59Z","title":"Sparse Multi-baseline SAR Cross-modal 3D Reconstruction of Vehicle\n  Targets","summary":"  Multi-baseline SAR 3D imaging faces significant challenges due to data\nsparsity. In recent years, deep learning techniques have achieved notable\nsuccess in enhancing the quality of sparse SAR 3D imaging. However, previous\nwork typically rely on full-aperture high-resolution radar images to supervise\nthe training of deep neural networks (DNNs), utilizing only single-modal\ninformation from radar data. Consequently, imaging performance is limited, and\nacquiring full-aperture data for multi-baseline SAR is costly and sometimes\nimpractical in real-world applications. In this paper, we propose a Cross-Modal\nReconstruction Network (CMR-Net), which integrates differentiable render and\ncross-modal supervision with optical images to reconstruct highly sparse\nmulti-baseline SAR 3D images of vehicle targets into visually structured and\nhigh-resolution images. We meticulously designed the network architecture and\ntraining strategies to enhance network generalization capability. Remarkably,\nCMR-Net, trained solely on simulated data, demonstrates high-resolution\nreconstruction capabilities on both publicly available simulation datasets and\nreal measured datasets, outperforming traditional sparse reconstruction\nalgorithms based on compressed sensing and other learning-based methods.\nAdditionally, using optical images as supervision provides a cost-effective way\nto build training datasets, reducing the difficulty of method dissemination.\nOur work showcases the broad prospects of deep learning in multi-baseline SAR\n3D imaging and offers a novel path for researching radar imaging based on\ncross-modal learning theory.\n","authors":["Da Li","Guoqiang Zhao","Houjun Sun","Jiacheng Bao"],"pdf_url":"https://arxiv.org/pdf/2406.04158v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09767v2","updated":"2024-08-08T07:44:14Z","published":"2023-12-15T13:15:42Z","title":"DreamTalk: When Emotional Talking Head Generation Meets Diffusion\n  Probabilistic Models","summary":"  Emotional talking head generation has attracted growing attention. Previous\nmethods, which are mainly GAN-based, still struggle to consistently produce\nsatisfactory results across diverse emotions and cannot conveniently specify\npersonalized emotions. In this work, we leverage powerful diffusion models to\naddress the issue and propose DreamTalk, a framework that employs meticulous\ndesign to unlock the potential of diffusion models in generating emotional\ntalking heads. Specifically, DreamTalk consists of three crucial components: a\ndenoising network, a style-aware lip expert, and a style predictor. The\ndiffusion-based denoising network can consistently synthesize high-quality\naudio-driven face motions across diverse emotions. To enhance lip-motion\naccuracy and emotional fullness, we introduce a style-aware lip expert that can\nguide lip-sync while preserving emotion intensity. To more conveniently specify\npersonalized emotions, a diffusion-based style predictor is utilized to predict\nthe personalized emotion directly from the audio, eliminating the need for\nextra emotion reference. By this means, DreamTalk can consistently generate\nvivid talking faces across diverse emotions and conveniently specify\npersonalized emotions. Extensive experiments validate DreamTalk's effectiveness\nand superiority. The code is available at\nhttps://github.com/ali-vilab/dreamtalk.\n","authors":["Yifeng Ma","Shiwei Zhang","Jiayu Wang","Xiang Wang","Yingya Zhang","Zhidong Deng"],"pdf_url":"https://arxiv.org/pdf/2312.09767v2.pdf","comment":"Project Page: https://dreamtalk-project.github.io"},{"id":"http://arxiv.org/abs/2408.04273v1","updated":"2024-08-08T07:14:57Z","published":"2024-08-08T07:14:57Z","title":"SG-JND: Semantic-Guided Just Noticeable Distortion Predictor For Image\n  Compression","summary":"  Just noticeable distortion (JND), representing the threshold of distortion in\nan image that is minimally perceptible to the human visual system (HVS), is\ncrucial for image compression algorithms to achieve a trade-off between\ntransmission bit rate and image quality. However, traditional JND prediction\nmethods only rely on pixel-level or sub-band level features, lacking the\nability to capture the impact of image content on JND. To bridge this gap, we\npropose a Semantic-Guided JND (SG-JND) network to leverage semantic information\nfor JND prediction. In particular, SG-JND consists of three essential modules:\nthe image preprocessing module extracts semantic-level patches from images, the\nfeature extraction module extracts multi-layer features by utilizing the\ncross-scale attention layers, and the JND prediction module regresses the\nextracted features into the final JND value. Experimental results show that\nSG-JND achieves the state-of-the-art performance on two publicly available JND\ndatasets, which demonstrates the effectiveness of SG-JND and highlight the\nsignificance of incorporating semantic information in JND assessment.\n","authors":["Linhan Cao","Wei Sun","Xiongkuo Min","Jun Jia","Zicheng Zhang","Zijian Chen","Yucheng Zhu","Lizhou Liu","Qiubo Chen","Jing Chen","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2408.04273v1.pdf","comment":"Accepted by ICIP 2024"},{"id":"http://arxiv.org/abs/2408.04268v1","updated":"2024-08-08T07:11:57Z","published":"2024-08-08T07:11:57Z","title":"Evaluating Modern Approaches in 3D Scene Reconstruction: NeRF vs\n  Gaussian-Based Methods","summary":"  Exploring the capabilities of Neural Radiance Fields (NeRF) and\nGaussian-based methods in the context of 3D scene reconstruction, this study\ncontrasts these modern approaches with traditional Simultaneous Localization\nand Mapping (SLAM) systems. Utilizing datasets such as Replica and ScanNet, we\nassess performance based on tracking accuracy, mapping fidelity, and view\nsynthesis. Findings reveal that NeRF excels in view synthesis, offering unique\ncapabilities in generating new perspectives from existing data, albeit at\nslower processing speeds. Conversely, Gaussian-based methods provide rapid\nprocessing and significant expressiveness but lack comprehensive scene\ncompletion. Enhanced by global optimization and loop closure techniques, newer\nmethods like NICE-SLAM and SplaTAM not only surpass older frameworks such as\nORB-SLAM2 in terms of robustness but also demonstrate superior performance in\ndynamic and complex environments. This comparative analysis bridges theoretical\nresearch with practical implications, shedding light on future developments in\nrobust 3D scene reconstruction across various real-world applications.\n","authors":["Yiming Zhou","Zixuan Zeng","Andi Chen","Xiaofan Zhou","Haowei Ni","Shiyao Zhang","Panfeng Li","Liangxi Liu","Mengyao Zheng","Xupeng Chen"],"pdf_url":"https://arxiv.org/pdf/2408.04268v1.pdf","comment":"Accepted by 2024 6th International Conference on Data-driven\n  Optimization of Complex Systems"},{"id":"http://arxiv.org/abs/2401.10373v2","updated":"2024-08-08T07:06:40Z","published":"2024-01-18T20:43:43Z","title":"Harmonized Spatial and Spectral Learning for Robust and Generalized\n  Medical Image Segmentation","summary":"  Deep learning has demonstrated remarkable achievements in medical image\nsegmentation. However, prevailing deep learning models struggle with poor\ngeneralization due to (i) intra-class variations, where the same class appears\ndifferently in different samples, and (ii) inter-class independence, resulting\nin difficulties capturing intricate relationships between distinct objects,\nleading to higher false negative cases. This paper presents a novel approach\nthat synergies spatial and spectral representations to enhance\ndomain-generalized medical image segmentation. We introduce the innovative\nSpectral Correlation Coefficient objective to improve the model's capacity to\ncapture middle-order features and contextual long-range dependencies. This\nobjective complements traditional spatial objectives by incorporating valuable\nspectral information. Extensive experiments reveal that optimizing this\nobjective with existing architectures like UNet and TransUNet significantly\nenhances generalization, interpretability, and noise robustness, producing more\nconfident predictions. For instance, in cardiac segmentation, we observe a 0.81\npp and 1.63 pp (pp = percentage point) improvement in DSC over UNet and\nTransUNet, respectively. Our interpretability study demonstrates that, in most\ntasks, objectives optimized with UNet outperform even TransUNet by introducing\nglobal contextual information alongside local details. These findings\nunderscore the versatility and effectiveness of our proposed method across\ndiverse imaging modalities and medical domains.\n","authors":["Vandan Gorade","Sparsh Mittal","Debesh Jha","Rekha Singhal","Ulas Bagci"],"pdf_url":"https://arxiv.org/pdf/2401.10373v2.pdf","comment":"Early Accepted at ICPR-2024 for Oral Presentation"},{"id":"http://arxiv.org/abs/2408.04262v1","updated":"2024-08-08T06:59:32Z","published":"2024-08-08T06:59:32Z","title":"CoBooM: Codebook Guided Bootstrapping for Medical Image Representation\n  Learning","summary":"  Self-supervised learning (SSL) has emerged as a promising paradigm for\nmedical image analysis by harnessing unannotated data. Despite their potential,\nthe existing SSL approaches overlook the high anatomical similarity inherent in\nmedical images. This makes it challenging for SSL methods to capture diverse\nsemantic content in medical images consistently. This work introduces a novel\nand generalized solution that implicitly exploits anatomical similarities by\nintegrating codebooks in SSL. The codebook serves as a concise and informative\ndictionary of visual patterns, which not only aids in capturing nuanced\nanatomical details but also facilitates the creation of robust and generalized\nfeature representations. In this context, we propose CoBooM, a novel framework\nfor self-supervised medical image learning by integrating continuous and\ndiscrete representations. The continuous component ensures the preservation of\nfine-grained details, while the discrete aspect facilitates coarse-grained\nfeature extraction through the structured embedding space. To understand the\neffectiveness of CoBooM, we conduct a comprehensive evaluation of various\nmedical datasets encompassing chest X-rays and fundus images. The experimental\nresults reveal a significant performance gain in classification and\nsegmentation tasks.\n","authors":["Azad Singh","Deepak Mishra"],"pdf_url":"https://arxiv.org/pdf/2408.04262v1.pdf","comment":"Accepted in MICCAI 2024"},{"id":"http://arxiv.org/abs/2408.04261v1","updated":"2024-08-08T06:58:48Z","published":"2024-08-08T06:58:48Z","title":"Unveiling Hidden Visual Information: A Reconstruction Attack Against\n  Adversarial Visual Information Hiding","summary":"  This paper investigates the security vulnerabilities of\nadversarial-example-based image encryption by executing data reconstruction\n(DR) attacks on encrypted images. A representative image encryption method is\nthe adversarial visual information hiding (AVIH), which uses type-I adversarial\nexample training to protect gallery datasets used in image recognition tasks.\nIn the AVIH method, the type-I adversarial example approach creates images that\nappear completely different but are still recognized by machines as the\noriginal ones. Additionally, the AVIH method can restore encrypted images to\ntheir original forms using a predefined private key generative model. For the\nbest security, assigning a unique key to each image is recommended; however,\nstorage limitations may necessitate some images sharing the same key model.\nThis raises a crucial security question for AVIH: How many images can safely\nshare the same key model without being compromised by a DR attack? To address\nthis question, we introduce a dual-strategy DR attack against the AVIH\nencryption method by incorporating (1) generative-adversarial loss and (2)\naugmented identity loss, which prevent DR from overfitting -- an issue akin to\nthat in machine learning. Our numerical results validate this approach through\nimage recognition and re-identification benchmarks, demonstrating that our\nstrategy can significantly enhance the quality of reconstructed images, thereby\nrequiring fewer key-sharing encrypted images. Our source code to reproduce our\nresults will be available soon.\n","authors":["Jonggyu Jang","Hyeonsu Lyu","Seongjin Hwang","Hyun Jong Yang"],"pdf_url":"https://arxiv.org/pdf/2408.04261v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2408.04258v1","updated":"2024-08-08T06:56:33Z","published":"2024-08-08T06:56:33Z","title":"UHNet: An Ultra-Lightweight and High-Speed Edge Detection Network","summary":"  Edge detection is crucial in medical image processing, enabling precise\nextraction of structural information to support lesion identification and image\nanalysis. Traditional edge detection models typically rely on complex\nConvolutional Neural Networks and Vision Transformer architectures. Due to\ntheir numerous parameters and high computational demands, these models are\nlimited in their application on resource-constrained devices. This paper\npresents an ultra-lightweight edge detection model (UHNet), characterized by\nits minimal parameter count, rapid computation speed, negligible of\npre-training costs, and commendable performance. UHNet boasts impressive\nperformance metrics with 42.3k parameters, 166 FPS, and 0.79G FLOPs. By\nemploying an innovative feature extraction module and optimized residual\nconnection method, UHNet significantly reduces model complexity and\ncomputational requirements. Additionally, a lightweight feature fusion strategy\nis explored, enhancing detection accuracy. Experimental results on the BSDS500,\nNYUD, and BIPED datasets validate that UHNet achieves remarkable edge detection\nperformance while maintaining high efficiency. This work not only provides new\ninsights into the design of lightweight edge detection models but also\ndemonstrates the potential and application prospects of the UHNet model in\nengineering applications such as medical image processing. The codes are\navailable at https://github.com/stoneLi20cv/UHNet\n","authors":["Fuzhang Li","Chuan Lin"],"pdf_url":"https://arxiv.org/pdf/2408.04258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03030v2","updated":"2024-08-08T06:32:30Z","published":"2024-08-06T08:24:47Z","title":"Nighttime Pedestrian Detection Based on Fore-Background Contrast\n  Learning","summary":"  The significance of background information is frequently overlooked in\ncontemporary research concerning channel attention mechanisms. This study\naddresses the issue of suboptimal single-spectral nighttime pedestrian\ndetection performance under low-light conditions by incorporating background\ninformation into the channel attention mechanism. Despite numerous studies\nfocusing on the development of efficient channel attention mechanisms, the\nrelevance of background information has been largely disregarded. By adopting a\ncontrast learning approach, we reexamine channel attention with regard to\npedestrian objects and background information for nighttime pedestrian\ndetection, resulting in the proposed Fore-Background Contrast Attention (FBCA).\nFBCA possesses two primary attributes: (1) channel descriptors form remote\ndependencies with global spatial feature information; (2) the integration of\nbackground information enhances the distinction between channels concentrating\non low-light pedestrian features and those focusing on background information.\nConsequently, the acquired channel descriptors exhibit a higher semantic level\nand spatial accuracy. Experimental outcomes demonstrate that FBCA significantly\noutperforms existing methods in single-spectral nighttime pedestrian detection,\nachieving state-of-the-art results on the NightOwls and TJU-DHD-pedestrian\ndatasets. Furthermore, this methodology also yields performance improvements\nfor the multispectral LLVIP dataset. These findings indicate that integrating\nbackground information into the channel attention mechanism effectively\nmitigates detector performance degradation caused by illumination factors in\nnighttime scenarios.\n","authors":["He Yao","Yongjun Zhang","Huachun Jian","Li Zhang","Ruzhong Cheng"],"pdf_url":"https://arxiv.org/pdf/2408.03030v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04249v1","updated":"2024-08-08T06:29:32Z","published":"2024-08-08T06:29:32Z","title":"InstantStyleGaussian: Efficient Art Style Transfer with 3D Gaussian\n  Splatting","summary":"  We present InstantStyleGaussian, an innovative 3D style transfer method based\non the 3D Gaussian Splatting (3DGS) scene representation. By inputting a target\nstyle image, it quickly generates new 3D GS scenes. Our approach operates on\npre-reconstructed GS scenes, combining diffusion models with an improved\niterative dataset update strategy. It utilizes diffusion models to generate\ntarget style images, adds these new images to the training dataset, and uses\nthis dataset to iteratively update and optimize the GS scenes. Extensive\nexperimental results demonstrate that our method ensures high-quality stylized\nscenes while offering significant advantages in style transfer speed and\nconsistency.\n","authors":["Xin-Yi Yu","Jun-Xin Yu","Li-Bo Zhou","Yan Wei","Lin-Lin Ou"],"pdf_url":"https://arxiv.org/pdf/2408.04249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04243v1","updated":"2024-08-08T06:16:00Z","published":"2024-08-08T06:16:00Z","title":"MU-MAE: Multimodal Masked Autoencoders-Based One-Shot Learning","summary":"  With the exponential growth of multimedia data, leveraging multimodal sensors\npresents a promising approach for improving accuracy in human activity\nrecognition. Nevertheless, accurately identifying these activities using both\nvideo data and wearable sensor data presents challenges due to the\nlabor-intensive data annotation, and reliance on external pretrained models or\nadditional data. To address these challenges, we introduce Multimodal Masked\nAutoencoders-Based One-Shot Learning (Mu-MAE). Mu-MAE integrates a multimodal\nmasked autoencoder with a synchronized masking strategy tailored for wearable\nsensors. This masking strategy compels the networks to capture more meaningful\nspatiotemporal features, which enables effective self-supervised pretraining\nwithout the need for external data. Furthermore, Mu-MAE leverages the\nrepresentation extracted from multimodal masked autoencoders as prior\ninformation input to a cross-attention multimodal fusion layer. This fusion\nlayer emphasizes spatiotemporal features requiring attention across different\nmodalities while highlighting differences from other classes, aiding in the\nclassification of various classes in metric-based one-shot learning.\nComprehensive evaluations on MMAct one-shot classification show that Mu-MAE\noutperforms all the evaluated approaches, achieving up to an 80.17% accuracy\nfor five-way one-shot multimodal classification, without the use of additional\ndata.\n","authors":["Rex Liu","Xin Liu"],"pdf_url":"https://arxiv.org/pdf/2408.04243v1.pdf","comment":"IEEE MIPR 2024"},{"id":"http://arxiv.org/abs/2408.04235v1","updated":"2024-08-08T05:41:09Z","published":"2024-08-08T05:41:09Z","title":"LLDif: Diffusion Models for Low-light Emotion Recognition","summary":"  This paper introduces LLDif, a novel diffusion-based facial expression\nrecognition (FER) framework tailored for extremely low-light (LL) environments.\nImages captured under such conditions often suffer from low brightness and\nsignificantly reduced contrast, presenting challenges to conventional methods.\nThese challenges include poor image quality that can significantly reduce the\naccuracy of emotion recognition. LLDif addresses these issues with a novel\ntwo-stage training process that combines a Label-aware CLIP (LA-CLIP), an\nembedding prior network (PNET), and a transformer-based network adept at\nhandling the noise of low-light images. The first stage involves LA-CLIP\ngenerating a joint embedding prior distribution (EPD) to guide the LLformer in\nlabel recovery. In the second stage, the diffusion model (DM) refines the EPD\ninference, ultilising the compactness of EPD for precise predictions.\nExperimental evaluations on various LL-FER datasets have shown that LLDif\nachieves competitive performance, underscoring its potential to enhance FER\napplications in challenging lighting conditions.\n","authors":["Zhifeng Wang","Kaihao Zhang","Ramesh Sankaranarayana"],"pdf_url":"https://arxiv.org/pdf/2408.04235v1.pdf","comment":"Accepted by ICPR2024"},{"id":"http://arxiv.org/abs/2408.04227v1","updated":"2024-08-08T05:30:59Z","published":"2024-08-08T05:30:59Z","title":"Physical prior guided cooperative learning framework for joint\n  turbulence degradation estimation and infrared video restoration","summary":"  Infrared imaging and turbulence strength measurements are in widespread\ndemand in many fields. This paper introduces a Physical Prior Guided\nCooperative Learning (P2GCL) framework to jointly enhance atmospheric\nturbulence strength estimation and infrared image restoration. P2GCL involves a\ncyclic collaboration between two models, i.e., a TMNet measures turbulence\nstrength and outputs the refractive index structure constant (Cn2) as a\nphysical prior, a TRNet conducts infrared image sequence restoration based on\nCn2 and feeds the restored images back to the TMNet to boost the measurement\naccuracy. A novel Cn2-guided frequency loss function and a physical constraint\nloss are introduced to align the training process with physical theories.\nExperiments demonstrate P2GCL achieves the best performance for both turbulence\nstrength estimation (improving Cn2 MAE by 0.0156, enhancing R2 by 0.1065) and\nimage restoration (enhancing PSNR by 0.2775 dB), validating the significant\nimpact of physical prior guided cooperative learning.\n","authors":["Ziran Zhang","Yuhang Tang","Zhigang Wang","Yueting Chen","Bin Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.04227v1.pdf","comment":"21"},{"id":"http://arxiv.org/abs/2408.04224v1","updated":"2024-08-08T05:17:27Z","published":"2024-08-08T05:17:27Z","title":"Cross-View Meets Diffusion: Aerial Image Synthesis with Geometry and\n  Text Guidance","summary":"  Aerial imagery analysis is critical for many research fields. However,\nobtaining frequent high-quality aerial images is not always accessible due to\nits high effort and cost requirements. One solution is to use the\nGround-to-Aerial (G2A) technique to synthesize aerial images from easily\ncollectible ground images. However, G2A is rarely studied, because of its\nchallenges, including but not limited to, the drastic view changes, occlusion,\nand range of visibility. In this paper, we present a novel Geometric Preserving\nGround-to-Aerial (G2A) image synthesis (GPG2A) model that can generate\nrealistic aerial images from ground images. GPG2A consists of two stages. The\nfirst stage predicts the Bird's Eye View (BEV) segmentation (referred to as the\nBEV layout map) from the ground image. The second stage synthesizes the aerial\nimage from the predicted BEV layout map and text descriptions of the ground\nimage. To train our model, we present a new multi-modal cross-view dataset,\nnamely VIGORv2 which is built upon VIGOR with newly collected aerial images,\nmaps, and text descriptions. Our extensive experiments illustrate that GPG2A\nsynthesizes better geometry-preserved aerial images than existing models. We\nalso present two applications, data augmentation for cross-view\ngeo-localization and sketch-based region search, to further verify the\neffectiveness of our GPG2A. The code and data will be publicly available.\n","authors":["Ahmad Arrabi","Xiaohan Zhang","Waqas Sultan","Chen Chen","Safwan Wshah"],"pdf_url":"https://arxiv.org/pdf/2408.04224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15143v2","updated":"2024-08-08T05:15:07Z","published":"2024-07-21T12:32:00Z","title":"Rethinking Feature Backbone Fine-tuning for Remote Sensing Object\n  Detection","summary":"  Recently, numerous methods have achieved impressive performance in remote\nsensing object detection, relying on convolution or transformer architectures.\nSuch detectors typically have a feature backbone to extract useful features\nfrom raw input images. For the remote sensing domain, a common practice among\ncurrent detectors is to initialize the backbone with pre-training on ImageNet\nconsisting of natural scenes. Fine-tuning the backbone is then typically\nrequired to generate features suitable for remote-sensing images. However, this\ncould hinder the extraction of basic visual features in long-term training,\nthus restricting performance improvement. To mitigate this issue, we propose a\nnovel method named DBF (Dynamic Backbone Freezing) for feature backbone\nfine-tuning on remote sensing object detection. Our method aims to handle the\ndilemma of whether the backbone should extract low-level generic features or\npossess specific knowledge of the remote sensing domain, by introducing a\nmodule called 'Freezing Scheduler' to dynamically manage the update of backbone\nfeatures during training. Extensive experiments on DOTA and DIOR-R show that\nour approach enables more accurate model learning while substantially reducing\ncomputational costs. Our method can be seamlessly adopted without additional\neffort due to its straightforward design.\n","authors":["Yechan Kim","JongHyun Park","SooYeon Kim","Moongu Jeon"],"pdf_url":"https://arxiv.org/pdf/2407.15143v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2408.04223v1","updated":"2024-08-08T05:14:07Z","published":"2024-08-08T05:14:07Z","title":"VideoQA in the Era of LLMs: An Empirical Study","summary":"  Video Large Language Models (Video-LLMs) are flourishing and has advanced\nmany video-language tasks. As a golden testbed, Video Question Answering\n(VideoQA) plays pivotal role in Video-LLM developing. This work conducts a\ntimely and comprehensive study of Video-LLMs' behavior in VideoQA, aiming to\nelucidate their success and failure modes, and provide insights towards more\nhuman-like video understanding and question answering. Our analyses demonstrate\nthat Video-LLMs excel in VideoQA; they can correlate contextual cues and\ngenerate plausible responses to questions about varied video contents. However,\nmodels falter in handling video temporality, both in reasoning about temporal\ncontent ordering and grounding QA-relevant temporal moments. Moreover, the\nmodels behave unintuitively - they are unresponsive to adversarial video\nperturbations while being sensitive to simple variations of candidate answers\nand questions. Also, they do not necessarily generalize better. The findings\ndemonstrate Video-LLMs' QA capability in standard condition yet highlight their\nsevere deficiency in robustness and interpretability, suggesting the urgent\nneed on rationales in Video-LLM developing.\n","authors":["Junbin Xiao","Nanxin Huang","Hangyu Qin","Dongyang Li","Yicong Li","Fengbin Zhu","Zhulin Tao","Jianxing Yu","Liang Lin","Tat-Seng Chua","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2408.04223v1.pdf","comment":"Preprint. Under Review"},{"id":"http://arxiv.org/abs/2408.04221v1","updated":"2024-08-08T05:09:02Z","published":"2024-08-08T05:09:02Z","title":"Connective Viewpoints of Signal-to-Noise Diffusion Models","summary":"  Diffusion models (DM) have become fundamental components of generative\nmodels, excelling across various domains such as image creation, audio\ngeneration, and complex data interpolation. Signal-to-Noise diffusion models\nconstitute a diverse family covering most state-of-the-art diffusion models.\nWhile there have been several attempts to study Signal-to-Noise (S2N) diffusion\nmodels from various perspectives, there remains a need for a comprehensive\nstudy connecting different viewpoints and exploring new perspectives. In this\nstudy, we offer a comprehensive perspective on noise schedulers, examining\ntheir role through the lens of the signal-to-noise ratio (SNR) and its\nconnections to information theory. Building upon this framework, we have\ndeveloped a generalized backward equation to enhance the performance of the\ninference process.\n","authors":["Khanh Doan","Long Tung Vuong","Tuan Nguyen","Anh Tuan Bui","Quyen Tran","Thanh-Toan Do","Dinh Phung","Trung Le"],"pdf_url":"https://arxiv.org/pdf/2408.04221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02369v2","updated":"2024-08-08T04:54:47Z","published":"2024-08-05T10:38:50Z","title":"The NPU-ASLP System Description for Visual Speech Recognition in CNVSRC\n  2024","summary":"  This paper delineates the visual speech recognition (VSR) system introduced\nby the NPU-ASLP (Team 237) in the second Chinese Continuous Visual Speech\nRecognition Challenge (CNVSRC 2024), engaging in all four tracks, including the\nfixed and open tracks of Single-Speaker VSR Task and Multi-Speaker VSR Task. In\nterms of data processing, we leverage the lip motion extractor from the\nbaseline1 to produce multiscale video data. Besides, various augmentation\ntechniques are applied during training, encompassing speed perturbation, random\nrotation, horizontal flipping, and color transformation. The VSR model adopts\nan end-to-end architecture with joint CTC/attention loss, introducing Enhanced\nResNet3D visual frontend, E-Branchformer encoder, and Bi-directional\nTransformer decoder. Our approach yields a 30.47% CER for the Single-Speaker\nTask and 34.30% CER for the Multi-Speaker Task, securing second place in the\nopen track of the Single-Speaker Task and first place in the other three\ntracks.\n","authors":["He Wang","Lei Xie"],"pdf_url":"https://arxiv.org/pdf/2408.02369v2.pdf","comment":"2 pages, 2 figures, CNVSRC 2024 System Report"},{"id":"http://arxiv.org/abs/2312.02934v4","updated":"2024-08-08T04:42:52Z","published":"2023-12-05T18:05:14Z","title":"WoVoGen: World Volume-aware Diffusion for Controllable Multi-camera\n  Driving Scene Generation","summary":"  Generating multi-camera street-view videos is critical for augmenting\nautonomous driving datasets, addressing the urgent demand for extensive and\nvaried data. Due to the limitations in diversity and challenges in handling\nlighting conditions, traditional rendering-based methods are increasingly being\nsupplanted by diffusion-based methods. However, a significant challenge in\ndiffusion-based methods is ensuring that the generated sensor data preserve\nboth intra-world consistency and inter-sensor coherence. To address these\nchallenges, we combine an additional explicit world volume and propose the\nWorld Volume-aware Multi-camera Driving Scene Generator (WoVoGen). This system\nis specifically designed to leverage 4D world volume as a foundational element\nfor video generation. Our model operates in two distinct phases: (i)\nenvisioning the future 4D temporal world volume based on vehicle control\nsequences, and (ii) generating multi-camera videos, informed by this envisioned\n4D temporal world volume and sensor interconnectivity. The incorporation of the\n4D world volume empowers WoVoGen not only to generate high-quality street-view\nvideos in response to vehicle control inputs but also to facilitate scene\nediting tasks.\n","authors":["Jiachen Lu","Ze Huang","Zeyu Yang","Jiahui Zhang","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.02934v4.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2408.04212v1","updated":"2024-08-08T04:34:29Z","published":"2024-08-08T04:34:29Z","title":"Is SAM 2 Better than SAM in Medical Image Segmentation?","summary":"  Segment Anything Model (SAM) demonstrated impressive performance in zero-shot\npromptable segmentation on natural images. The recently released Segment\nAnything Model 2 (SAM 2) model claims to have better performance than SAM on\nimages while extending the model's capabilities to video segmentation. It is\nimportant to evaluate the recent model's ability in medical image segmentation\nin a zero-shot promptable manner. In this work, we performed extensive studies\nwith multiple datasets from different imaging modalities to compare the\nperformance between SAM and SAM 2. We used two point prompt strategies: (i)\nsingle positive prompt near the centroid of the target structure and (ii)\nadditional positive prompts placed randomly within the target structure. The\nevaluation included 21 unique organ-modality combinations including abdominal\nstructures, cardiac structures, and fetal head images acquired from publicly\navailable MRI, CT, and Ultrasound datasets. The preliminary results, based on\n2D images, indicate that while SAM 2 may perform slightly better in a few\ncases, but it does not in general surpass SAM for medical image segmentation.\nEspecially when the contrast is lower like in CT, Ultrasound images, SAM 2\nperforms poorly than SAM. For MRI images, SAM 2 performs at par or better than\nSAM. Similar to SAM, SAM 2 also suffers from over-segmentation issue especially\nwhen the boundaries of the to-be-segmented organ is fuzzy in nature.\n","authors":["Sourya Sengupta","Satrajit Chakrabarty","Ravi Soni"],"pdf_url":"https://arxiv.org/pdf/2408.04212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.11528v3","updated":"2024-08-08T04:18:34Z","published":"2021-06-22T03:44:03Z","title":"Recent Deep Semi-supervised Learning Approaches and Related Works","summary":"  This work proposes an overview of the recent semi-supervised learning\napproaches and related works. Despite the remarkable success of neural networks\nin various applications, there exist a few formidable constraints, including\nthe need for a large amount of labeled data. Therefore, semi-supervised\nlearning, which is a learning scheme in which scarce labels and a larger amount\nof unlabeled data are utilized to train models (e.g., deep neural networks), is\ngetting more important. Based on the key assumptions of semi-supervised\nlearning, which are the manifold assumption, cluster assumption, and continuity\nassumption, the work reviews the recent semi-supervised learning approaches. In\nparticular, the methods in regard to using deep neural networks in a\nsemi-supervised learning setting are primarily discussed. In addition, the\nexisting works are first classified based on the underlying idea and explained,\nthen the holistic approaches that unify the aforementioned ideas are detailed.\n","authors":["Gyeongho Kim"],"pdf_url":"https://arxiv.org/pdf/2106.11528v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11106v2","updated":"2024-08-08T04:03:27Z","published":"2023-10-17T09:44:30Z","title":"3D Structure-guided Network for Tooth Alignment in 2D Photograph","summary":"  Orthodontics focuses on rectifying misaligned teeth (i.e., malocclusions),\naffecting both masticatory function and aesthetics. However, orthodontic\ntreatment often involves complex, lengthy procedures. As such, generating a 2D\nphotograph depicting aligned teeth prior to orthodontic treatment is crucial\nfor effective dentist-patient communication and, more importantly, for\nencouraging patients to accept orthodontic intervention. In this paper, we\npropose a 3D structure-guided tooth alignment network that takes 2D photographs\nas input (e.g., photos captured by smartphones) and aligns the teeth within the\n2D image space to generate an orthodontic comparison photograph featuring\naesthetically pleasing, aligned teeth. Notably, while the process operates\nwithin a 2D image space, our method employs 3D intra-oral scanning models\ncollected in clinics to learn about orthodontic treatment, i.e., projecting the\npre- and post-orthodontic 3D tooth structures onto 2D tooth contours, followed\nby a diffusion model to learn the mapping relationship. Ultimately, the aligned\ntooth contours are leveraged to guide the generation of a 2D photograph with\naesthetically pleasing, aligned teeth and realistic textures. We evaluate our\nnetwork on various facial photographs, demonstrating its exceptional\nperformance and strong applicability within the orthodontic industry.\n","authors":["Yulong Dou","Lanzhuju Mei","Dinggang Shen","Zhiming Cui"],"pdf_url":"https://arxiv.org/pdf/2310.11106v2.pdf","comment":"Accepted by The 34th British Machine Vision Conference (BMVC 2023)\n  Our BMVC webpage is https://proceedings.bmvc2023.org/322/"},{"id":"http://arxiv.org/abs/2407.19451v3","updated":"2024-08-08T04:01:03Z","published":"2024-07-28T10:05:11Z","title":"Perm: A Parametric Representation for Multi-Style 3D Hair Modeling","summary":"  We present Perm, a learned parametric model of human 3D hair designed to\nfacilitate various hair-related applications. Unlike previous work that jointly\nmodels the global hair shape and local strand details, we propose to\ndisentangle them using a PCA-based strand representation in the frequency\ndomain, thereby allowing more precise editing and output control. Specifically,\nwe leverage our strand representation to fit and decompose hair geometry\ntextures into low- to high-frequency hair structures. These decomposed textures\nare later parameterized with different generative models, emulating common\nstages in the hair modeling process. We conduct extensive experiments to\nvalidate the architecture design of \\textsc{Perm}, and finally deploy the\ntrained model as a generic prior to solve task-agnostic problems, further\nshowcasing its flexibility and superiority in tasks such as 3D hair\nparameterization, hairstyle interpolation, single-view hair reconstruction, and\nhair-conditioned image generation. Our code, data, and supplemental can be\nfound at our project page: https://cs.yale.edu/homes/che/projects/perm/\n","authors":["Chengan He","Xin Sun","Zhixin Shu","Fujun Luan","Sören Pirk","Jorge Alejandro Amador Herrera","Dominik L. Michels","Tuanfeng Y. Wang","Meng Zhang","Holly Rushmeier","Yi Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.19451v3.pdf","comment":"Project page: https://cs.yale.edu/homes/che/projects/perm/"},{"id":"http://arxiv.org/abs/2402.17485v3","updated":"2024-08-08T03:48:38Z","published":"2024-02-27T13:10:11Z","title":"EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with\n  Audio2Video Diffusion Model under Weak Conditions","summary":"  In this work, we tackle the challenge of enhancing the realism and\nexpressiveness in talking head video generation by focusing on the dynamic and\nnuanced relationship between audio cues and facial movements. We identify the\nlimitations of traditional techniques that often fail to capture the full\nspectrum of human expressions and the uniqueness of individual facial styles.\nTo address these issues, we propose EMO, a novel framework that utilizes a\ndirect audio-to-video synthesis approach, bypassing the need for intermediate\n3D models or facial landmarks. Our method ensures seamless frame transitions\nand consistent identity preservation throughout the video, resulting in highly\nexpressive and lifelike animations. Experimental results demonsrate that EMO is\nable to produce not only convincing speaking videos but also singing videos in\nvarious styles, significantly outperforming existing state-of-the-art\nmethodologies in terms of expressiveness and realism.\n","authors":["Linrui Tian","Qi Wang","Bang Zhang","Liefeng Bo"],"pdf_url":"https://arxiv.org/pdf/2402.17485v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.06342v3","updated":"2024-08-08T03:25:02Z","published":"2024-05-10T09:18:17Z","title":"Compression-Realized Deep Structural Network for Video Quality\n  Enhancement","summary":"  This paper focuses on the task of quality enhancement for compressed videos.\nAlthough deep network-based video restorers achieve impressive progress, most\nof the existing methods lack a structured design to optimally leverage the\npriors within compression codecs. Since the quality degradation of the video is\nprimarily induced by the compression algorithm, a new paradigm is urgently\nneeded for a more ``conscious'' process of quality enhancement. As a result, we\npropose the Compression-Realized Deep Structural Network (CRDS), introducing\nthree inductive biases aligned with the three primary processes in the classic\ncompression codec, merging the strengths of classical encoder architecture with\ndeep network capabilities. Inspired by the residual extraction and domain\ntransformation process in the codec, a pre-trained Latent Degradation Residual\nAuto-Encoder is proposed to transform video frames into a latent feature space,\nand the mutual neighborhood attention mechanism is integrated for precise\nmotion estimation and residual extraction. Furthermore, drawing inspiration\nfrom the quantization noise distribution of the codec, CRDS proposes a novel\nProgressive Denoising framework with intermediate supervision that decomposes\nthe quality enhancement into a series of simpler denoising sub-tasks.\nExperimental results on datasets like LDV 2.0 and MFQE 2.0 indicate our\napproach surpasses state-of-the-art models.\n","authors":["Hanchi Sun","Xiaohong Liu","Xinyang Jiang","Yifei Shen","Dongsheng Li","Xiongkuo Min","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2405.06342v3.pdf","comment":"Accepted by ACM MM'24"},{"id":"http://arxiv.org/abs/2301.01156v3","updated":"2024-08-08T03:22:43Z","published":"2023-01-03T15:33:48Z","title":"Reference Twice: A Simple and Unified Baseline for Few-Shot Instance\n  Segmentation","summary":"  Few-Shot Instance Segmentation (FSIS) requires detecting and segmenting novel\nclasses with limited support examples. Existing methods based on Region\nProposal Networks (RPNs) face two issues: 1) Overfitting suppresses novel class\nobjects; 2) Dual-branch models require complex spatial correlation strategies\nto prevent spatial information loss when generating class prototypes. We\nintroduce a unified framework, Reference Twice (RefT), to exploit the\nrelationship between support and query features for FSIS and related tasks. Our\nthree main contributions are: 1) A novel transformer-based baseline that avoids\noverfitting, offering a new direction for FSIS; 2) Demonstrating that support\nobject queries encode key factors after base training, allowing query features\nto be enhanced twice at both feature and query levels using simple\ncross-attention, thus avoiding complex spatial correlation interaction; 3)\nIntroducing a class-enhanced base knowledge distillation loss to address the\nissue of DETR-like models struggling with incremental settings due to the input\nprojection layer, enabling easy extension to incremental FSIS. Extensive\nexperimental evaluations on the COCO dataset under three FSIS settings\ndemonstrate that our method performs favorably against existing approaches\nacross different shots, \\eg, $+8.2/+9.4$ performance gain over state-of-the-art\nmethods with 10/30-shots. Source code and models will be available at\nhttps://github.com/hanyue1648/RefT.\n","authors":["Yue Han","Jiangning Zhang","Yabiao Wang","Chengjie Wang","Yong Liu","Lu Qi","Xiangtai Li","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2301.01156v3.pdf","comment":"Accepted by T-PAMI"},{"id":"http://arxiv.org/abs/2406.18037v2","updated":"2024-08-08T03:16:23Z","published":"2024-06-26T03:10:57Z","title":"Towards Synchronous Memorizability and Generalizability with\n  Site-Modulated Diffusion Replay for Cross-Site Continual Segmentation","summary":"  The ability to learn sequentially from different data sites is crucial for a\ndeep network in solving practical medical image diagnosis problems due to\nprivacy restrictions and storage limitations. However, adapting on incoming\nsite leads to catastrophic forgetting on past sites and decreases\ngeneralizablity on unseen sites. Existing Continual Learning (CL) and Domain\nGeneralization (DG) methods have been proposed to solve these two challenges\nrespectively, but none of them can address both simultaneously. Recognizing\nthis limitation, this paper proposes a novel training paradigm, learning\ntowards Synchronous Memorizability and Generalizability (SMG-Learning). To\nachieve this, we create the orientational gradient alignment to ensure\nmemorizability on previous sites, and arbitrary gradient alignment to enhance\ngeneralizability on unseen sites. This approach is named as Parallel Gradient\nAlignment (PGA). Furthermore, we approximate the PGA as dual meta-objectives\nusing the first-order Taylor expansion to reduce computational cost of aligning\ngradients. Considering that performing gradient alignments, especially for\nprevious sites, is not feasible due to the privacy constraints, we design a\nSite-Modulated Diffusion (SMD) model to generate images with site-specific\nlearnable prompts, replaying images have similar data distributions as previous\nsites. We evaluate our method on two medical image segmentation tasks, where\ndata from different sites arrive sequentially. Experimental results show that\nour method efficiently enhances both memorizability and generalizablity better\nthan other state-of-the-art methods, delivering satisfactory performance across\nall sites. Our code will be available at:\nhttps://github.com/dyxu-cuhkcse/SMG-Learning.\n","authors":["Dunyuan Xu","Xi Wang","Jingyang Zhang","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2406.18037v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2408.04187v1","updated":"2024-08-08T03:11:12Z","published":"2024-08-08T03:11:12Z","title":"Medical Graph RAG: Towards Safe Medical Large Language Model via Graph\n  Retrieval-Augmented Generation","summary":"  We introduce a novel graph-based Retrieval-Augmented Generation (RAG)\nframework specifically designed for the medical domain, called\n\\textbf{MedGraphRAG}, aimed at enhancing Large Language Model (LLM)\ncapabilities and generating evidence-based results, thereby improving safety\nand reliability when handling private medical data. Our comprehensive pipeline\nbegins with a hybrid static-semantic approach to document chunking,\nsignificantly improving context capture over traditional methods. Extracted\nentities are used to create a three-tier hierarchical graph structure, linking\nentities to foundational medical knowledge sourced from medical papers and\ndictionaries. These entities are then interconnected to form meta-graphs, which\nare merged based on semantic similarities to develop a comprehensive global\ngraph. This structure supports precise information retrieval and response\ngeneration. The retrieval process employs a U-retrieve method to balance global\nawareness and indexing efficiency of the LLM. Our approach is validated through\na comprehensive ablation study comparing various methods for document chunking,\ngraph construction, and information retrieval. The results not only demonstrate\nthat our hierarchical graph construction method consistently outperforms\nstate-of-the-art models on multiple medical Q\\&A benchmarks, but also confirms\nthat the responses generated include source documentation, significantly\nenhancing the reliability of medical LLMs in practical applications. Code will\nbe at: https://github.com/MedicineToken/Medical-Graph-RAG/tree/main\n","authors":["Junde Wu","Jiayuan Zhu","Yunli Qi"],"pdf_url":"https://arxiv.org/pdf/2408.04187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.11703v3","updated":"2024-08-08T03:09:21Z","published":"2022-02-23T18:58:56Z","title":"Paying U-Attention to Textures: Multi-Stage Hourglass Vision Transformer\n  for Universal Texture Synthesis","summary":"  We present a novel U-Attention vision Transformer for universal texture\nsynthesis. We exploit the natural long-range dependencies enabled by the\nattention mechanism to allow our approach to synthesize diverse textures while\npreserving their structures in a single inference. We propose a hierarchical\nhourglass backbone that attends to the global structure and performs patch\nmapping at varying scales in a coarse-to-fine-to-coarse stream. Completed by\nskip connection and convolution designs that propagate and fuse information at\ndifferent scales, our hierarchical U-Attention architecture unifies attention\nto features from macro structures to micro details, and progressively refines\nsynthesis results at successive stages. Our method achieves stronger 2$\\times$\nsynthesis than previous work on both stochastic and structured textures while\ngeneralizing to unseen textures without fine-tuning. Ablation studies\ndemonstrate the effectiveness of each component of our architecture.\n","authors":["Shouchang Guo","Valentin Deschaintre","Douglas Noll","Arthur Roullier"],"pdf_url":"https://arxiv.org/pdf/2202.11703v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04424v2","updated":"2024-08-08T03:01:31Z","published":"2023-12-07T16:49:09Z","title":"Cascade-Zero123: One Image to Highly Consistent 3D with Self-Prompted\n  Nearby Views","summary":"  Synthesizing multi-view 3D from one single image is a significant but\nchallenging task. Zero-1-to-3 methods have achieved great success by lifting a\n2D latent diffusion model to the 3D scope. The target view image is generated\nwith a single-view source image and the camera pose as condition information.\nHowever, due to the high sparsity of the single input image, Zero-1-to-3 tends\nto produce geometry and appearance inconsistency across views, especially for\ncomplex objects. To tackle this issue, we propose to supply more condition\ninformation for the generation model but in a self-prompt way. A cascade\nframework is constructed with two Zero-1-to-3 models, named Cascade-Zero123,\nwhich progressively extract 3D information from the source image. Specifically,\nseveral nearby views are first generated by the first model and then fed into\nthe second-stage model along with the source image as generation conditions.\nWith amplified self-prompted condition images, our Cascade-Zero123 generates\nmore consistent novel-view images than Zero-1-to-3. Experiment results\ndemonstrate remarkable promotion, especially for various complex and\nchallenging scenes, involving insects, humans, transparent objects, and stacked\nmultiple objects etc. More demos and code are available at\nhttps://cascadezero123.github.io.\n","authors":["Yabo Chen","Jiemin Fang","Yuyang Huang","Taoran Yi","Xiaopeng Zhang","Lingxi Xie","Xinggang Wang","Wenrui Dai","Hongkai Xiong","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2312.04424v2.pdf","comment":"ECCV 2024. Project page: https://cascadezero123.github.io/"},{"id":"http://arxiv.org/abs/2408.03361v2","updated":"2024-08-08T02:43:06Z","published":"2024-08-06T17:59:21Z","title":"GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards\n  General Medical AI","summary":"  Large Vision-Language Models (LVLMs) are capable of handling diverse data\ntypes such as imaging, text, and physiological signals, and can be applied in\nvarious fields. In the medical field, LVLMs have a high potential to offer\nsubstantial assistance for diagnosis and treatment. Before that, it is crucial\nto develop benchmarks to evaluate LVLMs' effectiveness in various medical\napplications. Current benchmarks are often built upon specific academic\nliterature, mainly focusing on a single domain, and lacking varying perceptual\ngranularities. Thus, they face specific challenges, including limited clinical\nrelevance, incomplete evaluations, and insufficient guidance for interactive\nLVLMs. To address these limitations, we developed the GMAI-MMBench, the most\ncomprehensive general medical AI benchmark with well-categorized data structure\nand multi-perceptual granularity to date. It is constructed from 285 datasets\nacross 39 medical image modalities, 18 clinical-related tasks, 18 departments,\nand 4 perceptual granularities in a Visual Question Answering (VQA) format.\nAdditionally, we implemented a lexical tree structure that allows users to\ncustomize evaluation tasks, accommodating various assessment needs and\nsubstantially supporting medical AI research and applications. We evaluated 50\nLVLMs, and the results show that even the advanced GPT-4o only achieves an\naccuracy of 52%, indicating significant room for improvement. Moreover, we\nidentified five key insufficiencies in current cutting-edge LVLMs that need to\nbe addressed to advance the development of better medical applications. We\nbelieve that GMAI-MMBench will stimulate the community to build the next\ngeneration of LVLMs toward GMAI.\n  Project Page: https://uni-medical.github.io/GMAI-MMBench.github.io/\n","authors":["Pengcheng Chen","Jin Ye","Guoan Wang","Yanjun Li","Zhongying Deng","Wei Li","Tianbin Li","Haodong Duan","Ziyan Huang","Yanzhou Su","Benyou Wang","Shaoting Zhang","Bin Fu","Jianfei Cai","Bohan Zhuang","Eric J Seibel","Junjun He","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2408.03361v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04175v1","updated":"2024-08-08T02:38:19Z","published":"2024-08-08T02:38:19Z","title":"pyBregMan: A Python library for Bregman Manifolds","summary":"  A Bregman manifold is a synonym for a dually flat space in information\ngeometry which admits as a canonical divergence a Bregman divergence. Bregman\nmanifolds are induced by smooth strictly convex functions like the cumulant or\npartition functions of regular exponential families, the negative entropy of\nmixture families, or the characteristic functions of regular cones just to list\na few such convex Bregman generators. We describe the design of pyBregMan, a\nlibrary which implements generic operations on Bregman manifolds and\ninstantiate several common Bregman manifolds used in information sciences. At\nthe core of the library is the notion of Legendre-Fenchel duality inducing a\ncanonical pair of dual potential functions and dual Bregman divergences. The\nlibrary also implements the Fisher-Rao manifolds of categorical/multinomial\ndistributions and multivariate normal distributions. To demonstrate the use of\nthe pyBregMan kernel manipulating those Bregman and Fisher-Rao manifolds, the\nlibrary also provides several core algorithms for various applications in\nstatistics, machine learning, information fusion, and so on.\n","authors":["Frank Nielsen","Alexander Soen"],"pdf_url":"https://arxiv.org/pdf/2408.04175v1.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2406.07822v2","updated":"2024-08-08T02:36:04Z","published":"2024-06-12T02:43:19Z","title":"Tell Me What's Next: Textual Foresight for Generic UI Representations","summary":"  Mobile app user interfaces (UIs) are rich with action, text, structure, and\nimage content that can be utilized to learn generic UI representations for\ntasks like automating user commands, summarizing content, and evaluating the\naccessibility of user interfaces. Prior work has learned strong visual\nrepresentations with local or global captioning losses, but fails to retain\nboth granularities. To combat this, we propose Textual Foresight, a novel\npretraining objective for learning UI screen representations. Textual Foresight\ngenerates global text descriptions of future UI states given a current UI and\nlocal action taken. Our approach requires joint reasoning over elements and\nentire screens, resulting in improved UI features: on generation tasks, UI\nagents trained with Textual Foresight outperform state-of-the-art by 2% with\n28x fewer images. We train with our newly constructed mobile app dataset,\nOpenApp, which results in the first public dataset for app UI representation\nlearning. OpenApp enables new baselines, and we find Textual Foresight improves\naverage task performance over them by 5.7% while having access to 2x less data.\n","authors":["Andrea Burns","Kate Saenko","Bryan A. Plummer"],"pdf_url":"https://arxiv.org/pdf/2406.07822v2.pdf","comment":"Accepted to ACL 2024 Findings. Data and code to be released at\n  https://github.com/aburns4/textualforesight"},{"id":"http://arxiv.org/abs/2408.04172v1","updated":"2024-08-08T02:34:41Z","published":"2024-08-08T02:34:41Z","title":"MultiColor: Image Colorization by Learning from Multiple Color Spaces","summary":"  Deep networks have shown impressive performance in the image restoration\ntasks, such as image colorization. However, we find that previous approaches\nrely on the digital representation from single color model with a specific\nmapping function, a.k.a., color space, during the colorization pipeline. In\nthis paper, we first investigate the modeling of different color spaces, and\nfind each of them exhibiting distinctive characteristics with unique\ndistribution of colors. The complementarity among multiple color spaces leads\nto benefits for the image colorization task.\n  We present MultiColor, a new learning-based approach to automatically\ncolorize grayscale images that combines clues from multiple color spaces.\nSpecifically, we employ a set of dedicated colorization modules for individual\ncolor space. Within each module, a transformer decoder is first employed to\nrefine color query embeddings and then a color mapper produces color channel\nprediction using the embeddings and semantic features. With these predicted\ncolor channels representing various color spaces, a complementary network is\ndesigned to exploit the complementarity and generate pleasing and reasonable\ncolorized images. We conduct extensive experiments on real-world datasets, and\nthe results demonstrate superior performance over the state-of-the-arts.\n","authors":["Xiangcheng Du","Zhao Zhou","Yanlong Wang","Zhuoyao Wang","Yingbin Zheng","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2408.04172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04171v1","updated":"2024-08-08T02:32:48Z","published":"2024-08-08T02:32:48Z","title":"Rotation center identification based on geometric relationships for\n  rotary motion deblurring","summary":"  Non-blind rotary motion deblurring (RMD) aims to recover the latent clear\nimage from a rotary motion blurred (RMB) image. The rotation center is a\ncrucial input parameter in non-blind RMD methods. Existing methods directly\nestimate the rotation center from the RMB image. However they always suffer\nsignificant errors, and the performance of RMD is limited. For the assembled\nimaging systems, the position of the rotation center remains fixed. Leveraging\nthis prior knowledge, we propose a geometric-based method for rotation center\nidentification and analyze its error range. Furthermore, we construct a RMB\nimaging system. The experiment demonstrates that our method achieves less than\n1-pixel error along a single axis (x-axis or y-axis). We utilize the\nconstructed imaging system to capture real RMB images, and experimental results\nshow that our method can help existing RMD approaches yield better RMD images.\n","authors":["Jinhui Qin","Yong Ma","Jun Huang","Fan Fan","You Du"],"pdf_url":"https://arxiv.org/pdf/2408.04171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04170v1","updated":"2024-08-08T02:31:04Z","published":"2024-08-08T02:31:04Z","title":"M2EF-NNs: Multimodal Multi-instance Evidence Fusion Neural Networks for\n  Cancer Survival Prediction","summary":"  Accurate cancer survival prediction is crucial for assisting clinical doctors\nin formulating treatment plans. Multimodal data, including histopathological\nimages and genomic data, offer complementary and comprehensive information that\ncan greatly enhance the accuracy of this task. However, the current methods,\ndespite yielding promising results, suffer from two notable limitations: they\ndo not effectively utilize global context and disregard modal uncertainty. In\nthis study, we put forward a neural network model called M2EF-NNs, which\nleverages multimodal and multi-instance evidence fusion techniques for accurate\ncancer survival prediction. Specifically, to capture global information in the\nimages, we use a pre-trained Vision Transformer (ViT) model to obtain patch\nfeature embeddings of histopathological images. Then, we introduce a multimodal\nattention module that uses genomic embeddings as queries and learns the\nco-attention mapping between genomic and histopathological images to achieve an\nearly interaction fusion of multimodal information and better capture their\ncorrelations. Subsequently, we are the first to apply the Dempster-Shafer\nevidence theory (DST) to cancer survival prediction. We parameterize the\ndistribution of class probabilities using the processed multimodal features and\nintroduce subjective logic to estimate the uncertainty associated with\ndifferent modalities. By combining with the Dempster-Shafer theory, we can\ndynamically adjust the weights of class probabilities after multimodal fusion\nto achieve trusted survival prediction. Finally, Experimental validation on the\nTCGA datasets confirms the significant improvements achieved by our proposed\nmethod in cancer survival prediction and enhances the reliability of the model.\n","authors":["Hui Luo","Jiashuang Huang","Hengrong Ju","Tianyi Zhou","Weiping Ding"],"pdf_url":"https://arxiv.org/pdf/2408.04170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08759v2","updated":"2024-08-08T02:28:17Z","published":"2024-06-13T02:41:11Z","title":"GaussianForest: Hierarchical-Hybrid 3D Gaussian Splatting for Compressed\n  Scene Modeling","summary":"  The field of novel-view synthesis has recently witnessed the emergence of 3D\nGaussian Splatting, which represents scenes in a point-based manner and renders\nthrough rasterization. This methodology, in contrast to Radiance Fields that\nrely on ray tracing, demonstrates superior rendering quality and speed.\nHowever, the explicit and unstructured nature of 3D Gaussians poses a\nsignificant storage challenge, impeding its broader application. To address\nthis challenge, we introduce the Gaussian-Forest modeling framework, which\nhierarchically represents a scene as a forest of hybrid 3D Gaussians. Each\nhybrid Gaussian retains its unique explicit attributes while sharing implicit\nones with its sibling Gaussians, thus optimizing parameterization with\nsignificantly fewer variables. Moreover, adaptive growth and pruning strategies\nare designed, ensuring detailed representation in complex regions and a notable\nreduction in the number of required Gaussians. Extensive experiments\ndemonstrate that Gaussian-Forest not only maintains comparable speed and\nquality but also achieves a compression rate surpassing 10 times, marking a\nsignificant advancement in efficient scene modeling. Codes will be available at\nhttps://github.com/Xian-Bei/GaussianForest.\n","authors":["Fengyi Zhang","Yadan Luo","Tianjun Zhang","Lin Zhang","Zi Huang"],"pdf_url":"https://arxiv.org/pdf/2406.08759v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01976v2","updated":"2024-08-08T02:15:41Z","published":"2024-08-04T09:44:47Z","title":"Single-Point Supervised High-Resolution Dynamic Network for Infrared\n  Small Target Detection","summary":"  Infrared small target detection (IRSTD) tasks are extremely challenging for\ntwo main reasons: 1) it is difficult to obtain accurate labelling information\nthat is critical to existing methods, and 2) infrared (IR) small target\ninformation is easily lost in deep networks. To address these issues, we\npropose a single-point supervised high-resolution dynamic network (SSHD-Net).\nIn contrast to existing methods, we achieve state-of-the-art (SOTA) detection\nperformance using only single-point supervision. Specifically, we first design\na high-resolution cross-feature extraction module (HCEM), that achieves\nbi-directional feature interaction through stepped feature cascade channels\n(SFCC). It balances network depth and feature resolution to maintain deep IR\nsmall-target information. Secondly, the effective integration of global and\nlocal features is achieved through the dynamic coordinate fusion module (DCFM),\nwhich enhances the anti-interference ability in complex backgrounds. In\naddition, we introduce the high-resolution multilevel residual module (HMRM) to\nenhance the semantic information extraction capability. Finally, we design the\nadaptive target localization detection head (ATLDH) to improve detection\naccuracy. Experiments on the publicly available datasets NUDT-SIRST and\nIRSTD-1k demonstrate the effectiveness of our method. Compared to other SOTA\nmethods, our method can achieve better detection performance with only a single\npoint of supervision.\n","authors":["Jing Wu","Rixiang Ni","Feng Huang","Zhaobing Qiu","Liqiong Chen","Changhai Luo","Yunxiang Li","Youli Li"],"pdf_url":"https://arxiv.org/pdf/2408.01976v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04158v1","updated":"2024-08-08T02:03:10Z","published":"2024-08-08T02:03:10Z","title":"Efficient Single Image Super-Resolution with Entropy Attention and\n  Receptive Field Augmentation","summary":"  Transformer-based deep models for single image super-resolution (SISR) have\ngreatly improved the performance of lightweight SISR tasks in recent years.\nHowever, they often suffer from heavy computational burden and slow inference\ndue to the complex calculation of multi-head self-attention (MSA), seriously\nhindering their practical application and deployment. In this work, we present\nan efficient SR model to mitigate the dilemma between model efficiency and SR\nperformance, which is dubbed Entropy Attention and Receptive Field Augmentation\nnetwork (EARFA), and composed of a novel entropy attention (EA) and a shifting\nlarge kernel attention (SLKA). From the perspective of information theory, EA\nincreases the entropy of intermediate features conditioned on a Gaussian\ndistribution, providing more informative input for subsequent reasoning. On the\nother hand, SLKA extends the receptive field of SR models with the assistance\nof channel shifting, which also favors to boost the diversity of hierarchical\nfeatures. Since the implementation of EA and SLKA does not involve complex\ncomputations (such as extensive matrix multiplications), the proposed method\ncan achieve faster nonlinear inference than Transformer-based SR models while\nmaintaining better SR performance. Extensive experiments show that the proposed\nmodel can significantly reduce the delay of model inference while achieving the\nSR performance comparable with other advanced models.\n","authors":["Xiaole Zhao","Linze Li","Chengxing Xie","Xiaoming Zhang","Ting Jiang","Wenjie Lin","Shuaicheng Liu","Tianrui Li"],"pdf_url":"https://arxiv.org/pdf/2408.04158v1.pdf","comment":"Accepted to ACM MM 2024"},{"id":"http://arxiv.org/abs/2406.01916v2","updated":"2024-08-08T01:50:52Z","published":"2024-06-04T02:57:09Z","title":"FastLGS: Speeding up Language Embedded Gaussians with Feature Grid\n  Mapping","summary":"  The semantically interactive radiance field has always been an appealing task\nfor its potential to facilitate user-friendly and automated real-world 3D scene\nunderstanding applications. However, it is a challenging task to achieve high\nquality, efficiency and zero-shot ability at the same time with semantics in\nradiance fields. In this work, we present FastLGS, an approach that supports\nreal-time open-vocabulary query within 3D Gaussian Splatting (3DGS) under high\nresolution. We propose the semantic feature grid to save multi-view CLIP\nfeatures which are extracted based on Segment Anything Model (SAM) masks, and\nmap the grids to low dimensional features for semantic field training through\n3DGS. Once trained, we can restore pixel-aligned CLIP embeddings through\nfeature grids from rendered features for open-vocabulary queries. Comparisons\nwith other state-of-the-art methods prove that FastLGS can achieve the first\nplace performance concerning both speed and accuracy, where FastLGS is 98x\nfaster than LERF and 4x faster than LangSplat. Meanwhile, experiments show that\nFastLGS is adaptive and compatible with many downstream tasks, such as 3D\nsegmentation and 3D object inpainting, which can be easily applied to other 3D\nmanipulation systems.\n","authors":["Yuzhou Ji","He Zhu","Junshu Tang","Wuyi Liu","Zhizhong Zhang","Yuan Xie","Xin Tan"],"pdf_url":"https://arxiv.org/pdf/2406.01916v2.pdf","comment":null}],"Robotics":[{"id":"http://arxiv.org/abs/2408.04593v1","updated":"2024-08-08T17:08:57Z","published":"2024-08-08T17:08:57Z","title":"SAM 2 in Robotic Surgery: An Empirical Evaluation for Robustness and\n  Generalization in Surgical Video Segmentation","summary":"  The recent Segment Anything Model (SAM) 2 has demonstrated remarkable\nfoundational competence in semantic segmentation, with its memory mechanism and\nmask decoder further addressing challenges in video tracking and object\nocclusion, thereby achieving superior results in interactive segmentation for\nboth images and videos. Building upon our previous empirical studies, we\nfurther explore the zero-shot segmentation performance of SAM 2 in\nrobot-assisted surgery based on prompts, alongside its robustness against\nreal-world corruption. For static images, we employ two forms of prompts:\n1-point and bounding box, while for video sequences, the 1-point prompt is\napplied to the initial frame. Through extensive experimentation on the MICCAI\nEndoVis 2017 and EndoVis 2018 benchmarks, SAM 2, when utilizing bounding box\nprompts, outperforms state-of-the-art (SOTA) methods in comparative\nevaluations. The results with point prompts also exhibit a substantial\nenhancement over SAM's capabilities, nearing or even surpassing existing\nunprompted SOTA methodologies. Besides, SAM 2 demonstrates improved inference\nspeed and less performance degradation against various image corruption.\nAlthough slightly unsatisfactory results remain in specific edges or regions,\nSAM 2's robust adaptability to 1-point prompts underscores its potential for\ndownstream surgical tasks with limited prompt requirements.\n","authors":["Jieming Yu","An Wang","Wenzhen Dong","Mengya Xu","Mobarakol Islam","Jie Wang","Long Bai","Hongliang Ren"],"pdf_url":"https://arxiv.org/pdf/2408.04593v1.pdf","comment":"Empirical study. Previous work \"SAM Meets Robotic Surgery\" is\n  accessible at: arXiv:2308.07156"},{"id":"http://arxiv.org/abs/2408.04587v1","updated":"2024-08-08T16:56:07Z","published":"2024-08-08T16:56:07Z","title":"FORGE: Force-Guided Exploration for Robust Contact-Rich Manipulation\n  under Uncertainty","summary":"  We present FORGE, a method that enables sim-to-real transfer of contact-rich\nmanipulation policies in the presence of significant pose uncertainty. FORGE\ncombines a force threshold mechanism with a dynamics randomization scheme\nduring policy learning in simulation, to enable the robust transfer of the\nlearned policies to the real robot. At deployment, FORGE policies, conditioned\non a maximum allowable force, adaptively perform contact-rich tasks while\nrespecting the specified force threshold, regardless of the controller gains.\nAdditionally, FORGE autonomously predicts a termination action once the task\nhas succeeded. We demonstrate that FORGE can be used to learn a variety of\nrobust contact-rich policies, enabling multi-stage assembly of a planetary gear\nsystem, which requires success across three assembly tasks: nut-threading,\ninsertion, and gear meshing. Project website can be accessed at\nhttps://noseworm.github.io/forge/.\n","authors":["Michael Noseworthy","Bingjie Tang","Bowen Wen","Ankur Handa","Nicholas Roy","Dieter Fox","Fabio Ramos","Yashraj Narang","Iretiayo Akinola"],"pdf_url":"https://arxiv.org/pdf/2408.04587v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11984v3","updated":"2024-08-08T15:46:41Z","published":"2023-09-21T11:41:22Z","title":"State Representations as Incentives for Reinforcement Learning Agents: A\n  Sim2Real Analysis on Robotic Grasping","summary":"  Choosing an appropriate representation of the environment for the underlying\ndecision-making process of the reinforcement learning agent is not always\nstraightforward. The state representation should be inclusive enough to allow\nthe agent to informatively decide on its actions and disentangled enough to\nsimplify policy training and the corresponding sim2real transfer. Given this\noutlook, this work examines the effect of various representations in\nincentivizing the agent to solve a specific robotic task: antipodal and planar\nobject grasping. A continuum of state representations is defined, starting from\nhand-crafted numerical states to encoded image-based representations, with\ndecreasing levels of induced task-specific knowledge. The effects of each\nrepresentation on the ability of the agent to solve the task in simulation and\nthe transferability of the learned policy to the real robot are examined and\ncompared against a model-based approach with complete system knowledge. The\nresults show that reinforcement learning agents using numerical states can\nperform on par with non-learning baselines. Furthermore, we find that agents\nusing image-based representations from pre-trained environment embedding\nvectors perform better than end-to-end trained agents, and hypothesize that\nseparation of representation learning from reinforcement learning can benefit\nsim2real transfer. Finally, we conclude that incentivizing the state\nrepresentation with task-specific knowledge facilitates faster convergence for\nagent training and increases success rates in sim2real robot control.\n","authors":["Panagiotis Petropoulakis","Ludwig Gräf","Mohammadhossein Malmir","Josip Josifovski","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2309.11984v3.pdf","comment":"Accepted to IEEE International Conference on Systems, Man, and\n  Cybernetics (SMC) 2024"},{"id":"http://arxiv.org/abs/2309.10092v4","updated":"2024-08-08T14:56:23Z","published":"2023-09-18T19:05:25Z","title":"Conformal Temporal Logic Planning using Large Language Models","summary":"  This paper addresses planning problems for mobile robots. We consider\nmissions that require accomplishing multiple high-level sub-tasks, expressed in\nnatural language (NL), in a temporal and logical order. To formally define the\nmission, we treat these sub-tasks as atomic predicates in a Linear Temporal\nLogic (LTL) formula. We refer to this task specification framework as LTL-NL.\nOur goal is to design plans, defined as sequences of robot actions,\naccomplishing LTL-NL tasks. This action planning problem cannot be solved\ndirectly by existing LTL planners because of the NL nature of atomic\npredicates. To address it, we propose HERACLEs, a hierarchical neuro-symbolic\nplanner that relies on a novel integration of (i) existing symbolic planners\ngenerating high-level task plans determining the order at which the NL\nsub-tasks should be accomplished; (ii) pre-trained Large Language Models (LLMs)\nto design sequences of robot actions based on these task plans; and (iii)\nconformal prediction acting as a formal interface between (i) and (ii) and\nmanaging uncertainties due to LLM imperfections. We show, both theoretically\nand empirically, that HERACLEs can achieve user-defined mission success rates.\nFinally, we provide comparative experiments demonstrating that HERACLEs\noutperforms LLM-based planners that require the mission to be defined solely\nusing NL. Additionally, we present examples demonstrating that our approach\nenhances user-friendliness compared to conventional symbolic approaches.\n","authors":["Jun Wang","Jiaming Tong","Kaiyuan Tan","Yevgeniy Vorobeychik","Yiannis Kantaros"],"pdf_url":"https://arxiv.org/pdf/2309.10092v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04485v1","updated":"2024-08-08T14:23:23Z","published":"2024-08-08T14:23:23Z","title":"A Learning-Based Model Predictive Contouring Control for Vehicle Evasive\n  Manoeuvres","summary":"  This paper presents a novel Learning-based Model Predictive Contouring\nControl (L-MPCC) algorithm for evasive manoeuvres at the limit of handling. The\nalgorithm uses the Student-t Process (STP) to minimise model mismatches and\nuncertainties online. The proposed STP captures the mismatches between the\nprediction model and the measured lateral tyre forces and yaw rate. The\nmismatches correspond to the posterior means provided to the prediction model\nto improve its accuracy. Simultaneously, the posterior covariances are\npropagated to the vehicle lateral velocity and yaw rate along the prediction\nhorizon. The STP posterior covariance directly depends on the variance of\nobserved data, so its variance is more significant when the online measurements\ndiffer from the recorded ones in the training set and smaller in the opposite\ncase. Thus, these covariances can be utilised in the L-MPCC's cost function to\nminimise the vehicle state uncertainties. In a high-fidelity simulation\nenvironment, we demonstrate that the proposed L-MPCC can successfully avoid\nobstacles, keeping the vehicle stable while driving a double lane change\nmanoeuvre at a higher velocity than an MPCC without STP. Furthermore, the\nproposed controller yields a significantly lower peak sideslip angle, improving\nthe vehicle's manoeuvrability compared to an L-MPCC with a Gaussian Process.\n","authors":["Alberto Bertipaglia","Mohsen Alirezaei","Riender Happee","Barys Shyrokau"],"pdf_url":"https://arxiv.org/pdf/2408.04485v1.pdf","comment":"The work will be presented at AVEC'24 in Milan"},{"id":"http://arxiv.org/abs/2408.04482v1","updated":"2024-08-08T14:19:11Z","published":"2024-08-08T14:19:11Z","title":"SegXAL: Explainable Active Learning for Semantic Segmentation in Driving\n  Scene Scenarios","summary":"  Most of the sophisticated AI models utilize huge amounts of annotated data\nand heavy training to achieve high-end performance. However, there are certain\nchallenges that hinder the deployment of AI models \"in-the-wild\" scenarios,\ni.e., inefficient use of unlabeled data, lack of incorporation of human\nexpertise, and lack of interpretation of the results. To mitigate these\nchallenges, we propose a novel Explainable Active Learning (XAL) model,\nXAL-based semantic segmentation model \"SegXAL\", that can (i) effectively\nutilize the unlabeled data, (ii) facilitate the \"Human-in-the-loop\" paradigm,\nand (iii) augment the model decisions in an interpretable way. In particular,\nwe investigate the application of the SegXAL model for semantic segmentation in\ndriving scene scenarios. The SegXAL model proposes the image regions that\nrequire labeling assistance from Oracle by dint of explainable AI (XAI) and\nuncertainty measures in a weakly-supervised manner. Specifically, we propose a\nnovel Proximity-aware Explainable-AI (PAE) module and Entropy-based Uncertainty\n(EBU) module to get an Explainable Error Mask, which enables the machine\nteachers/human experts to provide intuitive reasoning behind the results and to\nsolicit feedback to the AI system via an active learning strategy. Such a\nmechanism bridges the semantic gap between man and machine through\ncollaborative intelligence, where humans and AI actively enhance each other's\ncomplementary strengths. A novel high-confidence sample selection technique\nbased on the DICE similarity coefficient is also presented within the SegXAL\nframework. Extensive quantitative and qualitative analyses are carried out in\nthe benchmarking Cityscape dataset. Results show the outperformance of our\nproposed SegXAL against other state-of-the-art models.\n","authors":["Sriram Mandalika","Athira Nambiar"],"pdf_url":"https://arxiv.org/pdf/2408.04482v1.pdf","comment":"17 pages, 7 figures. To appear in the proceedings of the 27th\n  International Conference on Pattern Recognition (ICPR), 01-05 December, 2024,\n  Kolkata, India"},{"id":"http://arxiv.org/abs/2408.04426v1","updated":"2024-08-08T12:51:23Z","published":"2024-08-08T12:51:23Z","title":"A Review of 3D Reconstruction Techniques for Deformable Tissues in\n  Robotic Surgery","summary":"  As a crucial and intricate task in robotic minimally invasive surgery,\nreconstructing surgical scenes using stereo or monocular endoscopic video holds\nimmense potential for clinical applications. NeRF-based techniques have\nrecently garnered attention for the ability to reconstruct scenes implicitly.\nOn the other hand, Gaussian splatting-based 3D-GS represents scenes explicitly\nusing 3D Gaussians and projects them onto a 2D plane as a replacement for the\ncomplex volume rendering in NeRF. However, these methods face challenges\nregarding surgical scene reconstruction, such as slow inference, dynamic\nscenes, and surgical tool occlusion. This work explores and reviews\nstate-of-the-art (SOTA) approaches, discussing their innovations and\nimplementation principles. Furthermore, we replicate the models and conduct\ntesting and evaluation on two datasets. The test results demonstrate that with\nadvancements in these techniques, achieving real-time, high-quality\nreconstructions becomes feasible.\n","authors":["Mengya Xu","Ziqi Guo","An Wang","Long Bai","Hongliang Ren"],"pdf_url":"https://arxiv.org/pdf/2408.04426v1.pdf","comment":"To appear in MICCAI 2024 EARTH Workshop. Code availability:\n  https://github.com/Epsilon404/surgicalnerf"},{"id":"http://arxiv.org/abs/2408.04423v1","updated":"2024-08-08T12:47:52Z","published":"2024-08-08T12:47:52Z","title":"UNMuTe: Unifying Navigation and Multimodal Dialogue-like Text Generation","summary":"  Smart autonomous agents are becoming increasingly important in various\nreal-life applications, including robotics and autonomous vehicles. One crucial\nskill that these agents must possess is the ability to interact with their\nsurrounding entities, such as other agents or humans. In this work, we aim at\nbuilding an intelligent agent that can efficiently navigate in an environment\nwhile being able to interact with an oracle (or human) in natural language and\nask for directions when it is unsure about its navigation performance. The\ninteraction is started by the agent that produces a question, which is then\nanswered by the oracle on the basis of the shortest trajectory to the goal. The\nprocess can be performed multiple times during navigation, thus enabling the\nagent to hold a dialogue with the oracle. To this end, we propose a novel\ncomputational model, named UNMuTe, that consists of two main components: a\ndialogue model and a navigator. Specifically, the dialogue model is based on a\nGPT-2 decoder that handles multimodal data consisting of both text and images.\nFirst, the dialogue model is trained to generate question-answer pairs: the\nquestion is generated using the current image, while the answer is produced\nleveraging future images on the path toward the goal. Subsequently, a VLN model\nis trained to follow the dialogue predicting navigation actions or triggering\nthe dialogue model if it needs help. In our experimental analysis, we show that\nUNMuTe achieves state-of-the-art performance on the main navigation tasks\nimplying dialogue, i.e. Cooperative Vision and Dialogue Navigation (CVDN) and\nNavigation from Dialogue History (NDH), proving that our approach is effective\nin generating useful questions and answers to guide navigation.\n","authors":["Niyati Rawal","Roberto Bigazzi","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2408.04423v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18247v3","updated":"2024-08-08T12:15:18Z","published":"2023-10-27T16:34:00Z","title":"Guided Data Augmentation for Offline Reinforcement Learning and\n  Imitation Learning","summary":"  In offline reinforcement learning (RL), an RL agent learns to solve a task\nusing only a fixed dataset of previously collected data. While offline RL has\nbeen successful in learning real-world robot control policies, it typically\nrequires large amounts of expert-quality data to learn effective policies that\ngeneralize to out-of-distribution states. Unfortunately, such data is often\ndifficult and expensive to acquire in real-world tasks. Several recent works\nhave leveraged data augmentation (DA) to inexpensively generate additional\ndata, but most DA works apply augmentations in a random fashion and ultimately\nproduce highly suboptimal augmented experience. In this work, we propose Guided\nData Augmentation (GuDA), a human-guided DA framework that generates\nexpert-quality augmented data. The key insight behind GuDA is that while it may\nbe difficult to demonstrate the sequence of actions required to produce expert\ndata, a user can often easily characterize when an augmented trajectory segment\nrepresents progress toward task completion. Thus, a user can restrict the space\nof possible augmentations to automatically reject suboptimal augmented data. To\nextract a policy from GuDA, we use off-the-shelf offline reinforcement learning\nand behavior cloning algorithms. We evaluate GuDA on a physical robot soccer\ntask as well as simulated D4RL navigation tasks, a simulated autonomous driving\ntask, and a simulated soccer task. Empirically, GuDA enables learning given a\nsmall initial dataset of potentially suboptimal experience and outperforms a\nrandom DA strategy as well as a model-based DA strategy.\n","authors":["Nicholas E. Corrado","Yuxiao Qu","John U. Balis","Adam Labiosa","Josiah P. Hanna"],"pdf_url":"https://arxiv.org/pdf/2310.18247v3.pdf","comment":"RLC 2024"},{"id":"http://arxiv.org/abs/2408.04380v1","updated":"2024-08-08T11:34:31Z","published":"2024-08-08T11:34:31Z","title":"Deep Generative Models in Robotics: A Survey on Learning from Multimodal\n  Demonstrations","summary":"  Learning from Demonstrations, the field that proposes to learn robot behavior\nmodels from data, is gaining popularity with the emergence of deep generative\nmodels. Although the problem has been studied for years under names such as\nImitation Learning, Behavioral Cloning, or Inverse Reinforcement Learning,\nclassical methods have relied on models that don't capture complex data\ndistributions well or don't scale well to large numbers of demonstrations. In\nrecent years, the robot learning community has shown increasing interest in\nusing deep generative models to capture the complexity of large datasets. In\nthis survey, we aim to provide a unified and comprehensive review of the last\nyear's progress in the use of deep generative models in robotics. We present\nthe different types of models that the community has explored, such as\nenergy-based models, diffusion models, action value maps, or generative\nadversarial networks. We also present the different types of applications in\nwhich deep generative models have been used, from grasp generation to\ntrajectory generation or cost learning. One of the most important elements of\ngenerative models is the generalization out of distributions. In our survey, we\nreview the different decisions the community has made to improve the\ngeneralization of the learned models. Finally, we highlight the research\nchallenges and propose a number of future directions for learning deep\ngenerative models in robotics.\n","authors":["Julen Urain","Ajay Mandlekar","Yilun Du","Mahi Shafiullah","Danfei Xu","Katerina Fragkiadaki","Georgia Chalvatzaki","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2408.04380v1.pdf","comment":"20 pages, 11 figures, submitted to TRO"},{"id":"http://arxiv.org/abs/2405.07556v2","updated":"2024-08-08T10:06:47Z","published":"2024-05-13T08:36:06Z","title":"Safety-Aware Human-Lead Vehicle Platooning by Proactively Reacting to\n  Uncertain Human Behaving","summary":"  Human-Lead Cooperative Adaptive Cruise Control (HL-CACC) is regarded as a\npromising vehicle platooning technology in real-world implementation. By\nutilizing a Human-driven Vehicle (HV) as the platoon leader, HL-CACC reduces\nthe cost and enhances the reliability of perception and decision-making.\nHowever, state-of-the-art HL-CACC technology still has a great limitation on\ndriving safety for the lack of considering the leading human driver's uncertain\nbehaving. In this study, a HL-CACC controller is designed based on Stochastic\nModel Predictive Control (SMPC). It is enabled to predict the driving intention\nof the leading Connected Human-Driven Vehicle (CHV). The proposed controller\nhas the following features: i) enhanced perceived safety in oscillating\ntraffic; ii) guaranteed safety against hard brakes; iii) computational\nefficient for real-time implementation. The proposed controller is evaluated on\na PreScan&Simulink simulation platform. Real vehicle trajectory data is\ncollected for the calibration of simulation. Results reveal that the proposed\ncontroller: i) improves perceived safety by 19.17% in oscillating traffic; ii)\nenhances actual safety by 7.76% against hard brake; iii) is confirmed with\nstring stability. The computation time is approximately 3 milliseconds when\nrunning on a laptop equipped with an Intel i5-13500H CPU. This indicates the\nproposed controller is ready for real-time implementation.\n","authors":["Jia Hu","Shuhan Wang","Yiming Zhang","Haoran Wang"],"pdf_url":"https://arxiv.org/pdf/2405.07556v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17298v2","updated":"2024-08-08T10:04:19Z","published":"2024-04-26T10:06:58Z","title":"Automatic Target-Less Camera-LiDAR Calibration From Motion and Deep\n  Point Correspondences","summary":"  Sensor setups of robotic platforms commonly include both camera and LiDAR as\nthey provide complementary information. However, fusing these two modalities\ntypically requires a highly accurate calibration between them. In this paper,\nwe propose MDPCalib which is a novel method for camera-LiDAR calibration that\nrequires neither human supervision nor any specific target objects. Instead, we\nutilize sensor motion estimates from visual and LiDAR odometry as well as deep\nlearning-based 2D-pixel-to-3D-point correspondences that are obtained without\nin-domain retraining. We represent camera-LiDAR calibration as an optimization\nproblem and minimize the costs induced by constraints from sensor motion and\npoint correspondences. In extensive experiments, we demonstrate that our\napproach yields highly accurate extrinsic calibration parameters and is robust\nto random initialization. Additionally, our approach generalizes to a wide\nrange of sensor setups, which we demonstrate by employing it on various robotic\nplatforms including a self-driving perception car, a quadruped robot, and a\nUAV. To make our calibration method publicly accessible, we release the code on\nour project website at http://calibration.cs.uni-freiburg.de.\n","authors":["Kürsat Petek","Niclas Vödisch","Johannes Meyer","Daniele Cattaneo","Abhinav Valada","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2404.17298v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04295v1","updated":"2024-08-08T08:18:05Z","published":"2024-08-08T08:18:05Z","title":"Assigning Credit with Partial Reward Decoupling in Multi-Agent Proximal\n  Policy Optimization","summary":"  Multi-agent proximal policy optimization (MAPPO) has recently demonstrated\nstate-of-the-art performance on challenging multi-agent reinforcement learning\ntasks. However, MAPPO still struggles with the credit assignment problem,\nwherein the sheer difficulty in ascribing credit to individual agents' actions\nscales poorly with team size. In this paper, we propose a multi-agent\nreinforcement learning algorithm that adapts recent developments in credit\nassignment to improve upon MAPPO. Our approach leverages partial reward\ndecoupling (PRD), which uses a learned attention mechanism to estimate which of\na particular agent's teammates are relevant to its learning updates. We use\nthis estimate to dynamically decompose large groups of agents into smaller,\nmore manageable subgroups. We empirically demonstrate that our approach,\nPRD-MAPPO, decouples agents from teammates that do not influence their expected\nfuture reward, thereby streamlining credit assignment. We additionally show\nthat PRD-MAPPO yields significantly higher data efficiency and asymptotic\nperformance compared to both MAPPO and other state-of-the-art methods across\nseveral multi-agent tasks, including StarCraft II. Finally, we propose a\nversion of PRD-MAPPO that is applicable to \\textit{shared} reward settings,\nwhere PRD was previously not applicable, and empirically show that this also\nleads to performance improvements over MAPPO.\n","authors":["Aditya Kapoor","Benjamin Freed","Howie Choset","Jeff Schneider"],"pdf_url":"https://arxiv.org/pdf/2408.04295v1.pdf","comment":"20 pages, 5 figures, 12 tables, Reinforcement Learning Journal and\n  Reinforcement Learning Conference 2024"},{"id":"http://arxiv.org/abs/2408.04266v1","updated":"2024-08-08T07:10:16Z","published":"2024-08-08T07:10:16Z","title":"BPMP-Tracker: A Versatile Aerial Target Tracker Using Bernstein\n  Polynomial Motion Primitives","summary":"  This letter presents a versatile trajectory planning pipeline for aerial\ntracking. The proposed tracker is capable of handling various chasing settings\nsuch as complex unstructured environments, crowded dynamic obstacles and\nmultiple-target following. Among the entire pipeline, we focus on developing a\npredictor for future target motion and a chasing trajectory planner. For rapid\ncomputation, we employ the sample-check-select strategy: modules sample a set\nof candidate movements, check multiple constraints, and then select the best\ntrajectory. Also, we leverage the properties of Bernstein polynomials for quick\ncalculations. The prediction module predicts the trajectories of the targets,\nwhich do not overlap with static and dynamic obstacles. Then the trajectory\nplanner outputs a trajectory, ensuring various conditions such as occlusion and\ncollision avoidance, the visibility of all targets within a camera image and\ndynamical limits. We fully test the proposed tracker in simulations and\nhardware experiments under challenging scenarios, including dual-target\nfollowing, environments with dozens of dynamic obstacles and complex indoor and\noutdoor spaces.\n","authors":["Yunwoo Lee","Jungwon Park","Boseong Jeon","Seungwoo Jung","H. Jin Kim"],"pdf_url":"https://arxiv.org/pdf/2408.04266v1.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2112.09988v7","updated":"2024-08-08T07:04:58Z","published":"2021-12-18T19:54:35Z","title":"Smooth Model Predictive Path Integral Control without Smoothing","summary":"  We present a sampling-based control approach that can generate smooth actions\nfor general nonlinear systems without external smoothing algorithms. Model\nPredictive Path Integral (MPPI) control has been utilized in numerous robotic\napplications due to its appealing characteristics to solve non-convex\noptimization problems. However, the stochastic nature of sampling-based methods\ncan cause significant chattering in the resulting commands. Chattering becomes\nmore prominent in cases where the environment changes rapidly, possibly even\ncausing the MPPI to diverge. To address this issue, we propose a method that\nseamlessly combines MPPI with an input-lifting strategy. In addition, we\nintroduce a new action cost to smooth control sequence during trajectory\nrollouts while preserving the information theoretic interpretation of MPPI,\nwhich was derived from non-affine dynamics. We validate our method in two\nnonlinear control tasks with neural network dynamics: a pendulum swing-up task\nand a challenging autonomous driving task. The experimental results demonstrate\nthat our method outperforms the MPPI baselines with additionally applied\nsmoothing algorithms.\n","authors":["Taekyung Kim","Gyuhyun Park","Kiho Kwak","Jihwan Bae","Wonsuk Lee"],"pdf_url":"https://arxiv.org/pdf/2112.09988v7.pdf","comment":"Accepted to IEEE Robotics and Automation Letters (and IROS 2022). Our\n  video can be found at https://youtu.be/fyngK8PCoyM"},{"id":"http://arxiv.org/abs/2408.02912v2","updated":"2024-08-08T07:02:20Z","published":"2024-08-06T02:53:55Z","title":"KOI: Accelerating Online Imitation Learning via Hybrid Key-state\n  Guidance","summary":"  Online Imitation Learning methods struggle with the gap between extensive\nonline exploration space and limited expert trajectories, which hinder\nefficient exploration due to inaccurate task-aware reward estimation. Inspired\nby the findings from cognitive neuroscience that task decomposition could\nfacilitate cognitive processing for efficient learning, we hypothesize that an\nagent could estimate precise task-aware imitation rewards for efficient online\nexploration by decomposing the target task into the objectives of \"what to do\"\nand the mechanisms of \"how to do\". In this work, we introduce the hybrid\nKey-state guided Online Imitation (KOI) learning approach, which leverages the\nintegration of semantic and motion key states as guidance for task-aware reward\nestimation. Initially, we utilize the visual-language models to segment the\nexpert trajectory into semantic key states, indicating the objectives of \"what\nto do\". Within the intervals between semantic key states, optical flow is\nemployed to capture motion key states to understand the process of \"how to do\".\nBy integrating a thorough grasp of both semantic and motion key states, we\nrefine the trajectory-matching reward computation, encouraging task-aware\nexploration for efficient online imitation learning. Our experiment results\nprove that our method is more sample efficient in the Meta-World and LIBERO\nenvironments. We also conduct real-world robotic manipulation experiments to\nvalidate the efficacy of our method, demonstrating the practical applicability\nof our KOI method.\n","authors":["Jingxian Lu","Wenke Xia","Dong Wang","Zhigang Wang","Bin Zhao","Di Hu","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.02912v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19913v2","updated":"2024-08-08T06:38:31Z","published":"2024-03-29T01:53:24Z","title":"MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of\n  Large Language Models","summary":"  Large language models such as ChatGPT and GPT-4 have recently achieved\nastonishing performance on a variety of natural language processing tasks. In\nthis paper, we propose MANGO, a benchmark to evaluate their capabilities to\nperform text-based mapping and navigation. Our benchmark includes 53 mazes\ntaken from a suite of textgames: each maze is paired with a walkthrough that\nvisits every location but does not cover all possible paths. The task is\nquestion-answering: for each maze, a large language model reads the walkthrough\nand answers hundreds of mapping and navigation questions such as \"How should\nyou go to Attic from West of House?\" and \"Where are we if we go north and east\nfrom Cellar?\". Although these questions are easy to humans, it turns out that\neven GPT-4, the best-to-date language model, performs poorly at answering them.\nFurther, our experiments suggest that a strong mapping and navigation ability\nwould benefit large language models in performing relevant downstream tasks,\nsuch as playing textgames. Our MANGO benchmark will facilitate future research\non methods that improve the mapping and navigation capabilities of language\nmodels. We host our leaderboard, data, code, and evaluation program at\nhttps://mango.ttic.edu and https://github.com/oaklight/mango/.\n","authors":["Peng Ding","Jiading Fang","Peng Li","Kangrui Wang","Xiaochen Zhou","Mo Yu","Jing Li","Matthew R. Walter","Hongyuan Mei"],"pdf_url":"https://arxiv.org/pdf/2403.19913v2.pdf","comment":"COLM 2024 camera-ready"},{"id":"http://arxiv.org/abs/2311.03148v3","updated":"2024-08-08T06:14:39Z","published":"2023-11-06T14:44:24Z","title":"Collision Avoidance using Iterative Dynamic and Nonlinear Programming\n  with Adaptive Grid Refinements","summary":"  Nonlinear optimal control problems for trajectory planning with obstacle\navoidance present several challenges. While general-purpose optimizers and\ndynamic programming methods struggle when adopted separately, their combination\nenabled by a penalty approach is capable of handling highly nonlinear systems\nwhile overcoming the curse of dimensionality. Nevertheless, using dynamic\nprogramming with a fixed state space discretization limits the set of reachable\nsolutions, hindering convergence or requiring enormous memory resources for\nuniformly spaced grids. In this work we solve this issue by incorporating an\nadaptive refinement of the state space grid, splitting cells where needed to\nbetter capture the problem structure while requiring less discretization points\noverall. Numerical results on a space manipulator demonstrate the improved\nrobustness and efficiency of the combined method with respect to the single\ncomponents.\n","authors":["Rebecca Richter","Alberto De Marchi","Matthias Gerdts"],"pdf_url":"https://arxiv.org/pdf/2311.03148v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07717v3","updated":"2024-08-08T05:11:20Z","published":"2024-04-11T13:09:37Z","title":"Reflectance Estimation for Proximity Sensing by Vision-Language Models:\n  Utilizing Distributional Semantics for Low-Level Cognition in Robotics","summary":"  Large language models (LLMs) and vision-language models (VLMs) have been\nincreasingly used in robotics for high-level cognition, but their use for\nlow-level cognition, such as interpreting sensor information, remains\nunderexplored. In robotic grasping, estimating the reflectance of objects is\ncrucial for successful grasping, as it significantly impacts the distance\nmeasured by proximity sensors. We investigate whether LLMs can estimate\nreflectance from object names alone, leveraging the embedded human knowledge in\ndistributional semantics, and if the latent structure of language in VLMs\npositively affects image-based reflectance estimation. In this paper, we verify\nthat 1) LLMs such as GPT-3.5 and GPT-4 can estimate an object's reflectance\nusing only text as input; and 2) VLMs such as CLIP can increase their\ngeneralization capabilities in reflectance estimation from images. Our\nexperiments show that GPT-4 can estimate an object's reflectance using only\ntext input with a mean error of 14.7%, lower than the image-only ResNet.\nMoreover, CLIP achieved the lowest mean error of 11.8%, while GPT-3.5 obtained\na competitive 19.9% compared to ResNet's 17.8%. These results suggest that the\ndistributional semantics in LLMs and VLMs increases their generalization\ncapabilities, and the knowledge acquired by VLMs benefits from the latent\nstructure of language.\n","authors":["Masashi Osada","Gustavo A. Garcia Ricardez","Yosuke Suzuki","Tadahiro Taniguchi"],"pdf_url":"https://arxiv.org/pdf/2404.07717v3.pdf","comment":"24 pages, 13 figures, submitted to Advanced Robotics Special Issue on\n  Real-World Robot Applications of the Foundation Models"},{"id":"http://arxiv.org/abs/2408.04215v1","updated":"2024-08-08T04:49:24Z","published":"2024-08-08T04:49:24Z","title":"Temporal Logic Planning via Zero-Shot Policy Composition","summary":"  This work develops a zero-shot mechanism for an agent to satisfy a Linear\nTemporal Logic (LTL) specification given existing task primitives. Oftentimes,\nautonomous robots need to satisfy spatial and temporal goals that are unknown\nuntil run time. Prior research addresses the problem by learning policies that\nare capable of executing a high-level task specified using LTL, but they\nincorporate the specification into the learning process; therefore, any change\nto the specification requires retraining the policy. Other related research\naddresses the problem by creating skill-machines which, given a specification\nchange, do not require full policy retraining but require fine-tuning on the\nskill-machine to guarantee satisfaction. We present a more a flexible approach\n-- to learn a set of minimum-violation (MV) task primitive policies that can be\nused to satisfy arbitrary LTL specifications without retraining or fine-tuning.\nTask primitives can be learned offline using reinforcement learning (RL)\nmethods and combined using Boolean composition at deployment. This work focuses\non creating and pruning a transition system (TS) representation of the\nenvironment in order to solve for deterministic, non-ambiguous, and feasible\nsolutions to LTL specifications given an environment and a set of MV task\nprimitive policies. We show that our pruned TS is deterministic, contains no\nunrealizable transitions, and is sound. Through simulation, we show that our\napproach is executable and we verify our MV policies produce the expected\nsymbols.\n","authors":["Taylor Bergeron","Zachary Serlin","Kevin Leahy"],"pdf_url":"https://arxiv.org/pdf/2408.04215v1.pdf","comment":"16 pages, 11 figures"},{"id":"http://arxiv.org/abs/2408.04200v1","updated":"2024-08-08T03:54:48Z","published":"2024-08-08T03:54:48Z","title":"Koopman Operators in Robot Learning","summary":"  Koopman operator theory offers a rigorous treatment of dynamics and has been\nemerging as a powerful modeling and learning-based control method enabling\nsignificant advancements across various domains of robotics. Due to its ability\nto represent nonlinear dynamics as a linear operator, Koopman theory offers a\nfresh lens through which to understand and tackle the modeling and control of\ncomplex robotic systems. Moreover, it enables incremental updates and is\ncomputationally inexpensive making it particularly appealing for real-time\napplications and online active learning. This review comprehensively presents\nrecent research results on advancing Koopman operator theory across diverse\ndomains of robotics, encompassing aerial, legged, wheeled, underwater, soft,\nand manipulator robotics. Furthermore, it offers practical tutorials to help\nnew users get started as well as a treatise of more advanced topics leading to\nan outlook on future directions and open research questions. Taken together,\nthese provide insights into the potential evolution of Koopman theory as\napplied to the field of robotics.\n","authors":["Lu Shi","Masih Haseli","Giorgos Mamakoukas","Daniel Bruder","Ian Abraham","Todd Murphey","Jorge Cortes","Konstantinos Karydis"],"pdf_url":"https://arxiv.org/pdf/2408.04200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04198v1","updated":"2024-08-08T03:49:22Z","published":"2024-08-08T03:49:22Z","title":"F1tenth Autonomous Racing With Offline Reinforcement Learning Methods","summary":"  Autonomous racing serves as a critical platform for evaluating automated\ndriving systems and enhancing vehicle mobility intelligence. This work\ninvestigates offline reinforcement learning methods to train agents within the\ndynamic F1tenth racing environment. The study begins by exploring the\nchallenges of online training in the Austria race track environment, where\nagents consistently fail to complete the laps. Consequently, this research\npivots towards an offline strategy, leveraging `expert' demonstration dataset\nto facilitate agent training. A waypoint-based suboptimal controller is\ndeveloped to gather data with successful lap episodes. This data is then\nemployed to train offline learning-based algorithms, with a subsequent analysis\nof the agents' cross-track performance, evaluating their zero-shot\ntransferability from seen to unseen scenarios and their capacity to adapt to\nchanges in environment dynamics. Beyond mere algorithm benchmarking in\nautonomous racing scenarios, this study also introduces and describes the\nmachinery of our return-conditioned decision tree-based policy, comparing its\nperformance with methods that employ fully connected neural networks,\nTransformers, and Diffusion Policies and highlighting some insights into method\nselection for training autonomous agents in driving interactions.\n","authors":["Prajwal Koirala","Cody Fleming"],"pdf_url":"https://arxiv.org/pdf/2408.04198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04195v1","updated":"2024-08-08T03:31:04Z","published":"2024-08-08T03:31:04Z","title":"Design and Implementation of Smart Infrastructures and Connected\n  Vehicles in A Mini-city Platform","summary":"  This paper presents a 1/10th scale mini-city platform used as a testing bed\nfor evaluating autonomous and connected vehicles. Using the mini-city platform,\nwe can evaluate different driving scenarios including human-driven and\nautonomous driving. We provide a unique, visual feature-rich environment for\nevaluating computer vision methods. The conducted experiments utilize onboard\nsensors mounted on a robotic platform we built, allowing them to navigate in a\ncontrolled real-world urban environment. The designed city is occupied by cars,\nstop signs, a variety of residential and business buildings, and complex\nintersections mimicking an urban area. Furthermore, We have designed an\nintelligent infrastructure at one of the intersections in the city which helps\nsafer and more efficient navigation in the presence of multiple cars and\npedestrians. We have used the mini-city platform for the analysis of three\ndifferent applications: city mapping, depth estimation in challenging occluded\nenvironments, and smart infrastructure for connected vehicles. Our smart\ninfrastructure is among the first to develop and evaluate\nVehicle-to-Infrastructure (V2I) communication at intersections. The\nintersection-related result shows how inaccuracy in perception, including\nmapping and localization, can affect safety. The proposed mini-city platform\ncan be considered as a baseline environment for developing research and\neducation in intelligent transportation systems.\n","authors":["Daniel Vargas","Ethan Haque","Matthew Carroll","Daniel Perez","Tyler Roman","Phong Nguyen","Golnaz Habibi"],"pdf_url":"https://arxiv.org/pdf/2408.04195v1.pdf","comment":"8 pages, 9 figures, Presented at 2024 IEEE ITSC Conference, 23\n  Citations"},{"id":"http://arxiv.org/abs/2408.04142v1","updated":"2024-08-08T01:00:45Z","published":"2024-08-08T01:00:45Z","title":"Everyday Finger: A Robotic Finger that Meets the Needs of Everyday\n  Interactive Manipulation","summary":"  We provide the mechanical and dynamical requirements for a robotic finger\ncapable of performing thirty diverse everyday tasks. To match these\nrequirements, we present a finger design based on series-elastic actuation that\nwe call the everyday finger. Our focus is to make the fingers as compact as\npossible while achieving the desired performance. We evaluated everyday fingers\nby constructing a two-finger robotic hand that was tested on various\nperformance parameters and tasks like picking and placing dishes in a rack,\npicking thin and flat objects like paper and delicate objects such as\nstrawberries. Videos are available at the project website:\nhttps://sites.google.com/view/everydayfinger.\n","authors":["Rubén Castro Ornelas","Tomás Cantú","Isabel Sperandio","Alexander H. Slocum","Pulkit Agrawal"],"pdf_url":"https://arxiv.org/pdf/2408.04142v1.pdf","comment":"9.5 pages + references, 14 figures, extended/updated version of\n  article to appear in IEEE ICRA 2024 proceedings"}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2408.04631v1","updated":"2024-08-08T17:59:38Z","published":"2024-08-08T17:59:38Z","title":"Puppet-Master: Scaling Interactive Video Generation as a Motion Prior\n  for Part-Level Dynamics","summary":"  We present Puppet-Master, an interactive video generative model that can\nserve as a motion prior for part-level dynamics. At test time, given a single\nimage and a sparse set of motion trajectories (i.e., drags), Puppet-Master can\nsynthesize a video depicting realistic part-level motion faithful to the given\ndrag interactions. This is achieved by fine-tuning a large-scale pre-trained\nvideo diffusion model, for which we propose a new conditioning architecture to\ninject the dragging control effectively. More importantly, we introduce the\nall-to-first attention mechanism, a drop-in replacement for the widely adopted\nspatial attention modules, which significantly improves generation quality by\naddressing the appearance and background issues in existing models. Unlike\nother motion-conditioned video generators that are trained on in-the-wild\nvideos and mostly move an entire object, Puppet-Master is learned from\nObjaverse-Animation-HQ, a new dataset of curated part-level motion clips. We\npropose a strategy to automatically filter out sub-optimal animations and\naugment the synthetic renderings with meaningful motion trajectories.\nPuppet-Master generalizes well to real images across various categories and\noutperforms existing methods in a zero-shot manner on a real-world benchmark.\nSee our project page for more results: vgg-puppetmaster.github.io.\n","authors":["Ruining Li","Chuanxia Zheng","Christian Rupprecht","Andrea Vedaldi"],"pdf_url":"https://arxiv.org/pdf/2408.04631v1.pdf","comment":"Project page: https://vgg-puppetmaster.github.io/"},{"id":"http://arxiv.org/abs/2408.04628v1","updated":"2024-08-08T17:58:06Z","published":"2024-08-08T17:58:06Z","title":"LogogramNLP: Comparing Visual and Textual Representations of Ancient\n  Logographic Writing Systems for NLP","summary":"  Standard natural language processing (NLP) pipelines operate on symbolic\nrepresentations of language, which typically consist of sequences of discrete\ntokens. However, creating an analogous representation for ancient logographic\nwriting systems is an extremely labor intensive process that requires expert\nknowledge. At present, a large portion of logographic data persists in a purely\nvisual form due to the absence of transcription -- this issue poses a\nbottleneck for researchers seeking to apply NLP toolkits to study ancient\nlogographic languages: most of the relevant data are images of writing.\n  This paper investigates whether direct processing of visual representations\nof language offers a potential solution. We introduce LogogramNLP, the first\nbenchmark enabling NLP analysis of ancient logographic languages, featuring\nboth transcribed and visual datasets for four writing systems along with\nannotations for tasks like classification, translation, and parsing. Our\nexperiments compare systems that employ recent visual and text encoding\nstrategies as backbones. The results demonstrate that visual representations\noutperform textual representations for some investigated tasks, suggesting that\nvisual processing pipelines may unlock a large amount of cultural heritage data\nof logographic languages for NLP-based analyses.\n","authors":["Danlu Chen","Freda Shi","Aditi Agarwal","Jacobo Myerston","Taylor Berg-Kirkpatrick"],"pdf_url":"https://arxiv.org/pdf/2408.04628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04619v1","updated":"2024-08-08T17:49:07Z","published":"2024-08-08T17:49:07Z","title":"Transformer Explainer: Interactive Learning of Text-Generative Models","summary":"  Transformers have revolutionized machine learning, yet their inner workings\nremain opaque to many. We present Transformer Explainer, an interactive\nvisualization tool designed for non-experts to learn about Transformers through\nthe GPT-2 model. Our tool helps users understand complex Transformer concepts\nby integrating a model overview and enabling smooth transitions across\nabstraction levels of mathematical operations and model structures. It runs a\nlive GPT-2 instance locally in the user's browser, empowering users to\nexperiment with their own input and observe in real-time how the internal\ncomponents and parameters of the Transformer work together to predict the next\ntokens. Our tool requires no installation or special hardware, broadening the\npublic's education access to modern generative AI techniques. Our open-sourced\ntool is available at https://poloclub.github.io/transformer-explainer/. A video\ndemo is available at https://youtu.be/ECR4oAwocjs.\n","authors":["Aeree Cho","Grace C. Kim","Alexander Karpekov","Alec Helbling","Zijie J. Wang","Seongmin Lee","Benjamin Hoover","Duen Horng Chau"],"pdf_url":"https://arxiv.org/pdf/2408.04619v1.pdf","comment":"To be presented at IEEE VIS 2024"},{"id":"http://arxiv.org/abs/2408.04614v1","updated":"2024-08-08T17:42:32Z","published":"2024-08-08T17:42:32Z","title":"Better Alignment with Instruction Back-and-Forth Translation","summary":"  We propose a new method, instruction back-and-forth translation, to construct\nhigh-quality synthetic data grounded in world knowledge for aligning large\nlanguage models (LLMs). Given documents from a web corpus, we generate and\ncurate synthetic instructions using the backtranslation approach proposed by Li\net al.(2023a), and rewrite the responses to improve their quality further based\non the initial documents. Fine-tuning with the resulting (backtranslated\ninstruction, rewritten response) pairs yields higher win rates on AlpacaEval\nthan using other common instruction datasets such as Humpback, ShareGPT, Open\nOrca, Alpaca-GPT4 and Self-instruct. We also demonstrate that rewriting the\nresponses with an LLM outperforms direct distillation, and the two generated\ntext distributions exhibit significant distinction in embedding space. Further\nanalysis shows that our backtranslated instructions are of higher quality than\nother sources of synthetic instructions, while our responses are more diverse\nand complex than those obtained from distillation. Overall we find that\ninstruction back-and-forth translation combines the best of both worlds --\nmaking use of the information diversity and quantity found on the web, while\nensuring the quality of the responses which is necessary for effective\nalignment.\n","authors":["Thao Nguyen","Jeffrey Li","Sewoong Oh","Ludwig Schmidt","Jason Weston","Luke Zettlemoyer","Xian Li"],"pdf_url":"https://arxiv.org/pdf/2408.04614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04595v1","updated":"2024-08-08T17:11:36Z","published":"2024-08-08T17:11:36Z","title":"Inference with the Upper Confidence Bound Algorithm","summary":"  In this paper, we discuss the asymptotic behavior of the Upper Confidence\nBound (UCB) algorithm in the context of multiarmed bandit problems and discuss\nits implication in downstream inferential tasks. While inferential tasks become\nchallenging when data is collected in a sequential manner, we argue that this\nproblem can be alleviated when the sequential algorithm at hand satisfies\ncertain stability property. This notion of stability is motivated from the\nseminal work of Lai and Wei (1982). Our first main result shows that such a\nstability property is always satisfied for the UCB algorithm, and as a result\nthe sample means for each arm are asymptotically normal. Next, we examine the\nstability properties of the UCB algorithm when the number of arms $K$ is\nallowed to grow with the number of arm pulls $T$. We show that in such a case\nthe arms are stable when $\\frac{\\log K}{\\log T} \\rightarrow 0$, and the number\nof near-optimal arms are large.\n","authors":["Koulik Khamaru","Cun-Hui Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.04595v1.pdf","comment":"17 pages, 1 figure"},{"id":"http://arxiv.org/abs/2408.04594v1","updated":"2024-08-08T17:10:16Z","published":"2024-08-08T17:10:16Z","title":"Img-Diff: Contrastive Data Synthesis for Multimodal Large Language\n  Models","summary":"  High-performance Multimodal Large Language Models (MLLMs) rely heavily on\ndata quality. This study introduces a novel dataset named Img-Diff, designed to\nenhance fine-grained image recognition in MLLMs by leveraging insights from\ncontrastive learning and image difference captioning. By analyzing object\ndifferences between similar images, we challenge models to identify both\nmatching and distinct components. We utilize the Stable-Diffusion-XL model and\nadvanced image editing techniques to create pairs of similar images that\nhighlight object replacements. Our methodology includes a Difference Area\nGenerator for object differences identifying, followed by a Difference Captions\nGenerator for detailed difference descriptions. The result is a relatively\nsmall but high-quality dataset of \"object replacement\" samples. We use the the\nproposed dataset to fine-tune state-of-the-art (SOTA) MLLMs such as MGM-7B,\nyielding comprehensive improvements of performance scores over SOTA models that\ntrained with larger-scale datasets, in numerous image difference and Visual\nQuestion Answering tasks. For instance, our trained models notably surpass the\nSOTA models GPT-4V and Gemini on the MMVP benchmark. Besides, we investigate\nalternative methods for generating image difference data through \"object\nremoval\" and conduct thorough evaluation to confirm the dataset's diversity,\nquality, and robustness, presenting several insights on synthesis of such\ncontrastive dataset. To encourage further research and advance the field of\nmultimodal data synthesis and enhancement of MLLMs' fundamental capabilities\nfor image understanding, we release our codes and dataset at\nhttps://github.com/modelscope/data-juicer/tree/ImgDiff.\n","authors":["Qirui Jiao","Daoyuan Chen","Yilun Huang","Yaliang Li","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2408.04594v1.pdf","comment":"14 pages, 9 figures, 7 tables"},{"id":"http://arxiv.org/abs/2408.02666v2","updated":"2024-08-08T17:09:58Z","published":"2024-08-05T17:57:02Z","title":"Self-Taught Evaluators","summary":"  Model-based evaluation is at the heart of successful model development -- as\na reward model for training, and as a replacement for human evaluation. To\ntrain such evaluators, the standard approach is to collect a large amount of\nhuman preference judgments over model responses, which is costly and the data\nbecomes stale as models improve. In this work, we present an approach that aims\nto im-prove evaluators without human annotations, using synthetic training data\nonly. Starting from unlabeled instructions, our iterative self-improvement\nscheme generates contrasting model outputs and trains an LLM-as-a-Judge to\nproduce reasoning traces and final judgments, repeating this training at each\nnew iteration using the improved predictions. Without any labeled preference\ndata, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct)\nfrom 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms\ncommonly used LLM judges such as GPT-4 and matches the performance of the\ntop-performing reward models trained with labeled examples.\n","authors":["Tianlu Wang","Ilia Kulikov","Olga Golovneva","Ping Yu","Weizhe Yuan","Jane Dwivedi-Yu","Richard Yuanzhe Pang","Maryam Fazel-Zarandi","Jason Weston","Xian Li"],"pdf_url":"https://arxiv.org/pdf/2408.02666v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04591v1","updated":"2024-08-08T17:04:06Z","published":"2024-08-08T17:04:06Z","title":"HiLo: A Learning Framework for Generalized Category Discovery Robust to\n  Domain Shifts","summary":"  Generalized Category Discovery (GCD) is a challenging task in which, given a\npartially labelled dataset, models must categorize all unlabelled instances,\nregardless of whether they come from labelled categories or from new ones. In\nthis paper, we challenge a remaining assumption in this task: that all images\nshare the same domain. Specifically, we introduce a new task and method to\nhandle GCD when the unlabelled data also contains images from different domains\nto the labelled set. Our proposed `HiLo' networks extract High-level semantic\nand Low-level domain features, before minimizing the mutual information between\nthe representations. Our intuition is that the clusterings based on domain\ninformation and semantic information should be independent. We further extend\nour method with a specialized domain augmentation tailored for the GCD task, as\nwell as a curriculum learning approach. Finally, we construct a benchmark from\ncorrupted fine-grained datasets as well as a large-scale evaluation on\nDomainNet with real-world domain shifts, reimplementing a number of GCD\nbaselines in this setting. We demonstrate that HiLo outperforms SoTA category\ndiscovery models by a large margin on all evaluations.\n","authors":["Hongjun Wang","Sagar Vaze","Kai Han"],"pdf_url":"https://arxiv.org/pdf/2408.04591v1.pdf","comment":"39 pages, 9 figures, 26 tables"},{"id":"http://arxiv.org/abs/2401.12731v2","updated":"2024-08-08T16:56:42Z","published":"2024-01-23T13:04:02Z","title":"The Distributional Uncertainty of the SHAP score in Explainable Machine\n  Learning","summary":"  Attribution scores reflect how important the feature values in an input\nentity are for the output of a machine learning model. One of the most popular\nattribution scores is the SHAP score, which is an instantiation of the general\nShapley value used in coalition game theory. The definition of this score\nrelies on a probability distribution on the entity population. Since the exact\ndistribution is generally unknown, it needs to be assigned subjectively or be\nestimated from data, which may lead to misleading feature scores. In this\npaper, we propose a principled framework for reasoning on SHAP scores under\nunknown entity population distributions. In our framework, we consider an\nuncertainty region that contains the potential distributions, and the SHAP\nscore of a feature becomes a function defined over this region. We study the\nbasic problems of finding maxima and minima of this function, which allows us\nto determine tight ranges for the SHAP scores of all features. In particular,\nwe pinpoint the complexity of these problems, and other related ones, showing\nthem to be NP-complete. Finally, we present experiments on a real-world\ndataset, showing that our framework may contribute to a more robust feature\nscoring.\n","authors":["Santiago Cifuentes","Leopoldo Bertossi","Nina Pardal","Sergio Abriola","Maria Vanina Martinez","Miguel Romero"],"pdf_url":"https://arxiv.org/pdf/2401.12731v2.pdf","comment":"In ECAI 2024 proceedings"},{"id":"http://arxiv.org/abs/2408.04586v1","updated":"2024-08-08T16:56:03Z","published":"2024-08-08T16:56:03Z","title":"Sampling for View Synthesis: From Local Light Field Fusion to Neural\n  Radiance Fields and Beyond","summary":"  Capturing and rendering novel views of complex real-world scenes is a\nlong-standing problem in computer graphics and vision, with applications in\naugmented and virtual reality, immersive experiences and 3D photography. The\nadvent of deep learning has enabled revolutionary advances in this area,\nclassically known as image-based rendering. However, previous approaches\nrequire intractably dense view sampling or provide little or no guidance for\nhow users should sample views of a scene to reliably render high-quality novel\nviews. Local light field fusion proposes an algorithm for practical view\nsynthesis from an irregular grid of sampled views that first expands each\nsampled view into a local light field via a multiplane image scene\nrepresentation, then renders novel views by blending adjacent local light\nfields. Crucially, we extend traditional plenoptic sampling theory to derive a\nbound that specifies precisely how densely users should sample views of a given\nscene when using our algorithm. We achieve the perceptual quality of Nyquist\nrate view sampling while using up to 4000x fewer views. Subsequent developments\nhave led to new scene representations for deep learning with view synthesis,\nnotably neural radiance fields, but the problem of sparse view synthesis from a\nsmall number of images has only grown in importance. We reprise some of the\nrecent results on sparse and even single image view synthesis, while posing the\nquestion of whether prescriptive sampling guidelines are feasible for the new\ngeneration of image-based rendering algorithms.\n","authors":["Ravi Ramamoorthi"],"pdf_url":"https://arxiv.org/pdf/2408.04586v1.pdf","comment":"Article written for Frontiers of Science Award, International\n  Congress on Basic Science, 2024"},{"id":"http://arxiv.org/abs/2408.04583v1","updated":"2024-08-08T16:48:33Z","published":"2024-08-08T16:48:33Z","title":"Unveiling the Power of Sparse Neural Networks for Feature Selection","summary":"  Sparse Neural Networks (SNNs) have emerged as powerful tools for efficient\nfeature selection. Leveraging the dynamic sparse training (DST) algorithms\nwithin SNNs has demonstrated promising feature selection capabilities while\ndrastically reducing computational overheads. Despite these advancements,\nseveral critical aspects remain insufficiently explored for feature selection.\nQuestions persist regarding the choice of the DST algorithm for network\ntraining, the choice of metric for ranking features/neurons, and the\ncomparative performance of these methods across diverse datasets when compared\nto dense networks. This paper addresses these gaps by presenting a\ncomprehensive systematic analysis of feature selection with sparse neural\nnetworks. Moreover, we introduce a novel metric considering sparse neural\nnetwork characteristics, which is designed to quantify feature importance\nwithin the context of SNNs. Our findings show that feature selection with SNNs\ntrained with DST algorithms can achieve, on average, more than $50\\%$ memory\nand $55\\%$ FLOPs reduction compared to the dense networks, while outperforming\nthem in terms of the quality of the selected features. Our code and the\nsupplementary material are available on GitHub\n(\\url{https://github.com/zahraatashgahi/Neuron-Attribution}).\n","authors":["Zahra Atashgahi","Tennison Liu","Mykola Pechenizkiy","Raymond Veldhuis","Decebal Constantin Mocanu","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2408.04583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04575v1","updated":"2024-08-08T16:36:24Z","published":"2024-08-08T16:36:24Z","title":"SCENE: Evaluating Explainable AI Techniques Using Soft Counterfactuals","summary":"  Explainable Artificial Intelligence (XAI) is essential for enhancing the\ntransparency and accountability of AI models, especially in natural language\nprocessing (NLP) tasks. This paper introduces SCENE (Soft Counterfactual\nEvaluation for Natural language Explainability), a novel evaluation method that\nleverages large language models (LLMs) to generate Soft Counterfactual\nexplanations in a zero-shot manner. By focusing on token-based substitutions,\nSCENE creates contextually appropriate and seman-tically meaningful Soft\nCounterfactuals without extensive fine-tuning. SCENE adopts Validitysoft and\nCsoft metrics to evaluate the effectiveness of model-agnostic XAI methods in\ntext classification tasks. Applied to CNN, RNN, and BERT architectures, SCENE\nprovides valuable insights into the strengths and limitations of various XAI\ntechniques.\n","authors":["Haoran Zheng","Utku Pamuksuz"],"pdf_url":"https://arxiv.org/pdf/2408.04575v1.pdf","comment":"10 pages, 5 tables"},{"id":"http://arxiv.org/abs/2408.00161v2","updated":"2024-08-08T16:31:05Z","published":"2024-07-31T21:12:21Z","title":"Automatic Generation of Behavioral Test Cases For Natural Language\n  Processing Using Clustering and Prompting","summary":"  Recent work in behavioral testing for natural language processing (NLP)\nmodels, such as Checklist, is inspired by related paradigms in software\nengineering testing. They allow evaluation of general linguistic capabilities\nand domain understanding, hence can help evaluate conceptual soundness and\nidentify model weaknesses. However, a major challenge is the creation of test\ncases. The current packages rely on semi-automated approach using manual\ndevelopment which requires domain expertise and can be time consuming. This\npaper introduces an automated approach to develop test cases by exploiting the\npower of large language models and statistical techniques. It clusters the text\nrepresentations to carefully construct meaningful groups and then apply\nprompting techniques to automatically generate Minimal Functionality Tests\n(MFT). The well-known Amazon Reviews corpus is used to demonstrate our\napproach. We analyze the behavioral test profiles across four different\nclassification algorithms and discuss the limitations and strengths of those\nmodels.\n","authors":["Ying Li","Rahul Singh","Tarun Joshi","Agus Sudjianto"],"pdf_url":"https://arxiv.org/pdf/2408.00161v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04568v1","updated":"2024-08-08T16:28:22Z","published":"2024-08-08T16:28:22Z","title":"Learning Fine-Grained Grounded Citations for Attributed Large Language\n  Models","summary":"  Despite the impressive performance on information-seeking tasks, large\nlanguage models (LLMs) still struggle with hallucinations. Attributed LLMs,\nwhich augment generated text with in-line citations, have shown potential in\nmitigating hallucinations and improving verifiability. However, current\napproaches suffer from suboptimal citation quality due to their reliance on\nin-context learning. Furthermore, the practice of citing only coarse document\nidentifiers makes it challenging for users to perform fine-grained\nverification. In this work, we introduce FRONT, a training framework designed\nto teach LLMs to generate Fine-Grained Grounded Citations. By grounding model\noutputs in fine-grained supporting quotes, these quotes guide the generation of\ngrounded and consistent responses, not only improving citation quality but also\nfacilitating fine-grained verification. Experiments on the ALCE benchmark\ndemonstrate the efficacy of FRONT in generating superior grounded responses and\nhighly supportive citations. With LLaMA-2-7B, the framework significantly\noutperforms all the baselines, achieving an average of 14.21% improvement in\ncitation quality across all datasets, even surpassing ChatGPT.\n","authors":["Lei Huang","Xiaocheng Feng","Weitao Ma","Yuxuan Gu","Weihong Zhong","Xiachong Feng","Weijiang Yu","Weihua Peng","Duyu Tang","Dandan Tu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2408.04568v1.pdf","comment":"Accepted by ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2307.02694v3","updated":"2024-08-08T16:24:52Z","published":"2023-07-05T23:53:55Z","title":"Loss Functions and Metrics in Deep Learning","summary":"  When training or evaluating deep learning models, two essential parts are\npicking the proper loss function and deciding on performance metrics. In this\npaper, we provide a comprehensive overview of the most common loss functions\nand metrics used across many different types of deep learning tasks, from\ngeneral tasks such as regression and classification to more specific tasks in\nComputer Vision and Natural Language Processing. We introduce the formula for\neach loss and metric, discuss their strengths and limitations, and describe how\nthese methods can be applied to various problems within deep learning. We hope\nthis work serves as a reference for researchers and practitioners in the field,\nhelping them make informed decisions when selecting the most appropriate loss\nfunction and performance metrics for their deep learning projects.\n","authors":["Juan Terven","Diana M. Cordova-Esparza","Alfonso Ramirez-Pedraza","Edgar A. Chavez-Urbiola","Julio A. Romero-Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2307.02694v3.pdf","comment":"76 pages, 4 figures, 13 tables, 127 equations"},{"id":"http://arxiv.org/abs/2306.14694v3","updated":"2024-08-08T16:22:29Z","published":"2023-06-26T13:39:36Z","title":"Dialectical Reconciliation via Structured Argumentative Dialogues","summary":"  We present a novel framework designed to extend model reconciliation\napproaches, commonly used in human-aware planning, for enhanced human-AI\ninteraction. By adopting a structured argumentation-based dialogue paradigm,\nour framework enables dialectical reconciliation to address knowledge\ndiscrepancies between an explainer (AI agent) and an explainee (human user),\nwhere the goal is for the explainee to understand the explainer's decision. We\nformally describe the operational semantics of our proposed framework,\nproviding theoretical guarantees. We then evaluate the framework's efficacy\n``in the wild'' via computational and human-subject experiments. Our findings\nsuggest that our framework offers a promising direction for fostering effective\nhuman-AI interactions in domains where explainability is important.\n","authors":["Stylianos Loukas Vasileiou","Ashwin Kumar","William Yeoh","Tran Cao Son","Francesca Toni"],"pdf_url":"https://arxiv.org/pdf/2306.14694v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02148v2","updated":"2024-08-08T16:16:06Z","published":"2024-08-04T21:27:36Z","title":"Environment Complexity and Nash Equilibria in a Sequential Social\n  Dilemma","summary":"  Multi-agent reinforcement learning (MARL) methods, while effective in\nzero-sum or positive-sum games, often yield suboptimal outcomes in general-sum\ngames where cooperation is essential for achieving globally optimal outcomes.\nMatrix game social dilemmas, which abstract key aspects of general-sum\ninteractions, such as cooperation, risk, and trust, fail to model the temporal\nand spatial dynamics characteristic of real-world scenarios. In response, our\nstudy extends matrix game social dilemmas into more complex, higher-dimensional\nMARL environments. We adapt a gridworld implementation of the Stag Hunt dilemma\nto more closely match the decision-space of a one-shot matrix game while also\nintroducing variable environment complexity. Our findings indicate that as\ncomplexity increases, MARL agents trained in these environments converge to\nsuboptimal strategies, consistent with the risk-dominant Nash equilibria\nstrategies found in matrix games. Our work highlights the impact of environment\ncomplexity on achieving optimal outcomes in higher-dimensional game-theoretic\nMARL environments.\n","authors":["Mustafa Yasir","Andrew Howes","Vasilios Mavroudis","Chris Hicks"],"pdf_url":"https://arxiv.org/pdf/2408.02148v2.pdf","comment":"Accepted to the 17th European Workshop on Reinforcement Learning\n  (EWRL)"},{"id":"http://arxiv.org/abs/2208.03561v2","updated":"2024-08-08T16:12:51Z","published":"2022-08-06T18:30:53Z","title":"Study of detecting behavioral signatures within DeepFake videos","summary":"  There is strong interest in the generation of synthetic video imagery of\npeople talking for various purposes, including entertainment, communication,\ntraining, and advertisement. With the development of deep fake generation\nmodels, synthetic video imagery will soon be visually indistinguishable to the\nnaked eye from a naturally capture video. In addition, many methods are\ncontinuing to improve to avoid more careful, forensic visual analysis. Some\ndeep fake videos are produced through the use of facial puppetry, which\ndirectly controls the head and face of the synthetic image through the\nmovements of the actor, allow the actor to 'puppet' the image of another. In\nthis paper, we address the question of whether one person's movements can be\ndistinguished from the original speaker by controlling the visual appearance of\nthe speaker but transferring the behavior signals from another source. We\nconduct a study by comparing synthetic imagery that: 1) originates from a\ndifferent person speaking a different utterance, 2) originates from the same\nperson speaking a different utterance, and 3) originates from a different\nperson speaking the same utterance. Our study shows that synthetic videos in\nall three cases are seen as less real and less engaging than the original\nsource video. Our results indicate that there could be a behavioral signature\nthat is detectable from a person's movements that is separate from their visual\nappearance, and that this behavioral signature could be used to distinguish a\ndeep fake from a properly captured video.\n","authors":["Qiaomu Miao","Sinhwa Kang","Stacy Marsella","Steve DiPaola","Chao Wang","Ari Shapiro"],"pdf_url":"https://arxiv.org/pdf/2208.03561v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2406.01561v3","updated":"2024-08-08T16:00:01Z","published":"2024-06-03T17:44:11Z","title":"Long and Short Guidance in Score identity Distillation for One-Step\n  Text-to-Image Generation","summary":"  Diffusion-based text-to-image generation models trained on extensive\ntext-image pairs have shown the capacity to generate photorealistic images\nconsistent with textual descriptions. However, a significant limitation of\nthese models is their slow sample generation, which requires iterative\nrefinement through the same network. In this paper, we enhance Score identity\nDistillation (SiD) by developing long and short classifier-free guidance (LSG)\nto efficiently distill pretrained Stable Diffusion models without using real\ntraining data. SiD aims to optimize a model-based explicit score matching loss,\nutilizing a score-identity-based approximation alongside the proposed LSG for\npractical computation. By training exclusively with fake images synthesized\nwith its one-step generator, SiD equipped with LSG rapidly improves FID and\nCLIP scores, achieving state-of-the-art FID performance while maintaining a\ncompetitive CLIP score. Specifically, its data-free distillation of Stable\nDiffusion 1.5 achieves a record low FID of 8.15 on the COCO-2014 validation\nset, with a CLIP score of 0.304 at an LSG scale of 1.5, and an FID of 9.56 with\na CLIP score of 0.313 at an LSG scale of 2. Our code and distilled one-step\ntext-to-image generators are available at\nhttps://github.com/mingyuanzhou/SiD-LSG.\n","authors":["Mingyuan Zhou","Zhendong Wang","Huangjie Zheng","Hai Huang"],"pdf_url":"https://arxiv.org/pdf/2406.01561v3.pdf","comment":"Code and model checkpoints available at\n  https://github.com/mingyuanzhou/SiD-LSG"},{"id":"http://arxiv.org/abs/2309.11984v3","updated":"2024-08-08T15:46:41Z","published":"2023-09-21T11:41:22Z","title":"State Representations as Incentives for Reinforcement Learning Agents: A\n  Sim2Real Analysis on Robotic Grasping","summary":"  Choosing an appropriate representation of the environment for the underlying\ndecision-making process of the reinforcement learning agent is not always\nstraightforward. The state representation should be inclusive enough to allow\nthe agent to informatively decide on its actions and disentangled enough to\nsimplify policy training and the corresponding sim2real transfer. Given this\noutlook, this work examines the effect of various representations in\nincentivizing the agent to solve a specific robotic task: antipodal and planar\nobject grasping. A continuum of state representations is defined, starting from\nhand-crafted numerical states to encoded image-based representations, with\ndecreasing levels of induced task-specific knowledge. The effects of each\nrepresentation on the ability of the agent to solve the task in simulation and\nthe transferability of the learned policy to the real robot are examined and\ncompared against a model-based approach with complete system knowledge. The\nresults show that reinforcement learning agents using numerical states can\nperform on par with non-learning baselines. Furthermore, we find that agents\nusing image-based representations from pre-trained environment embedding\nvectors perform better than end-to-end trained agents, and hypothesize that\nseparation of representation learning from reinforcement learning can benefit\nsim2real transfer. Finally, we conclude that incentivizing the state\nrepresentation with task-specific knowledge facilitates faster convergence for\nagent training and increases success rates in sim2real robot control.\n","authors":["Panagiotis Petropoulakis","Ludwig Gräf","Mohammadhossein Malmir","Josip Josifovski","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2309.11984v3.pdf","comment":"Accepted to IEEE International Conference on Systems, Man, and\n  Cybernetics (SMC) 2024"},{"id":"http://arxiv.org/abs/2408.04535v1","updated":"2024-08-08T15:42:00Z","published":"2024-08-08T15:42:00Z","title":"Synchronous Multi-modal Semantic CommunicationSystem with Packet-level\n  Coding","summary":"  Although the semantic communication with joint semantic-channel coding design\nhas shown promising performance in transmitting data of different modalities\nover physical layer channels, the synchronization and packet-level forward\nerror correction of multimodal semantics have not been well studied. Due to the\nindependent design of semantic encoders, synchronizing multimodal features in\nboth the semantic and time domains is a challenging problem. In this paper, we\ntake the facial video and speech transmission as an example and propose a\nSynchronous Multimodal Semantic Communication System (SyncSC) with Packet-Level\nCoding. To achieve semantic and time synchronization, 3D Morphable Mode (3DMM)\ncoefficients and text are transmitted as semantics, and we propose a semantic\ncodec that achieves similar quality of reconstruction and synchronization with\nlower bandwidth, compared to traditional methods. To protect semantic packets\nunder the erasure channel, we propose a packet-Level Forward Error Correction\n(FEC) method, called PacSC, that maintains a certain visual quality performance\neven at high packet loss rates. Particularly, for text packets, a text packet\nloss concealment module, called TextPC, based on Bidirectional Encoder\nRepresentations from Transformers (BERT) is proposed, which significantly\nimproves the performance of traditional FEC methods. The simulation results\nshow that our proposed SyncSC reduce transmission overhead and achieve\nhigh-quality synchronous transmission of video and speech over the packet loss\nnetwork.\n","authors":["Yun Tian","Jingkai Ying","Zhijin Qin","Ye Jin","Xiaoming Tao"],"pdf_url":"https://arxiv.org/pdf/2408.04535v1.pdf","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2407.21024v2","updated":"2024-08-08T15:32:43Z","published":"2024-07-13T14:23:57Z","title":"An Autonomous GIS Agent Framework for Geospatial Data Retrieval","summary":"  Powered by the emerging large language models (LLMs), autonomous geographic\ninformation systems (GIS) agents have the potential to accomplish spatial\nanalyses and cartographic tasks. However, a research gap exists to support\nfully autonomous GIS agents: how to enable agents to discover and download the\nnecessary data for geospatial analyses. This study proposes an autonomous GIS\nagent framework capable of retrieving required geospatial data by generating,\nexecuting, and debugging programs. The framework utilizes the LLM as the\ndecision-maker, selects the appropriate data source (s) from a pre-defined\nsource list, and fetches the data from the chosen source. Each data source has\na handbook that records the metadata and technical details for data retrieval.\nThe proposed framework is designed in a plug-and-play style to ensure\nflexibility and extensibility. Human users or autonomous data scrawlers can add\nnew data sources by adding new handbooks. We developed a prototype agent based\non the framework, released as a QGIS plugin (GeoData Retrieve Agent) and a\nPython program. Experiment results demonstrate its capability of retrieving\ndata from various sources including OpenStreetMap, administrative boundaries\nand demographic data from the US Census Bureau, satellite basemaps from ESRI\nWorld Imagery, global digital elevation model (DEM) from OpenTopography.org,\nweather data from a commercial provider, the COVID-19 cases from the NYTimes\nGitHub. Our study is among the first attempts to develop an autonomous\ngeospatial data retrieval agent.\n","authors":["Huan Ning","Zhenlong Li","Temitope Akinboyewa","M. Naser Lessani"],"pdf_url":"https://arxiv.org/pdf/2407.21024v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04528v1","updated":"2024-08-08T15:27:22Z","published":"2024-08-08T15:27:22Z","title":"Reasoning about Study Regulations in Answer Set Programming","summary":"  We are interested in automating reasoning with and about study regulations,\ncatering to various stakeholders, ranging from administrators, over faculty, to\nstudents at different stages. Our work builds on an extensive analysis of\nvarious study programs at the University of Potsdam. The conceptualization of\nthe underlying principles provides us with a formal account of study\nregulations. In particular, the formalization reveals the properties of\nadmissible study plans. With these at end, we propose an encoding of study\nregulations in Answer Set Programming that produces corresponding study plans.\nFinally, we show how this approach can be extended to a generic user interface\nfor exploring study plans.\n","authors":["Susana Hahn","Cedric Martens","Amade Nemes","Henry Otunuya","Javier Romero","Torsten Schaub","Sebastian Schellhorn"],"pdf_url":"https://arxiv.org/pdf/2408.04528v1.pdf","comment":"To appear in Theory and Practise of Logic Programming"},{"id":"http://arxiv.org/abs/2309.10092v4","updated":"2024-08-08T14:56:23Z","published":"2023-09-18T19:05:25Z","title":"Conformal Temporal Logic Planning using Large Language Models","summary":"  This paper addresses planning problems for mobile robots. We consider\nmissions that require accomplishing multiple high-level sub-tasks, expressed in\nnatural language (NL), in a temporal and logical order. To formally define the\nmission, we treat these sub-tasks as atomic predicates in a Linear Temporal\nLogic (LTL) formula. We refer to this task specification framework as LTL-NL.\nOur goal is to design plans, defined as sequences of robot actions,\naccomplishing LTL-NL tasks. This action planning problem cannot be solved\ndirectly by existing LTL planners because of the NL nature of atomic\npredicates. To address it, we propose HERACLEs, a hierarchical neuro-symbolic\nplanner that relies on a novel integration of (i) existing symbolic planners\ngenerating high-level task plans determining the order at which the NL\nsub-tasks should be accomplished; (ii) pre-trained Large Language Models (LLMs)\nto design sequences of robot actions based on these task plans; and (iii)\nconformal prediction acting as a formal interface between (i) and (ii) and\nmanaging uncertainties due to LLM imperfections. We show, both theoretically\nand empirically, that HERACLEs can achieve user-defined mission success rates.\nFinally, we provide comparative experiments demonstrating that HERACLEs\noutperforms LLM-based planners that require the mission to be defined solely\nusing NL. Additionally, we present examples demonstrating that our approach\nenhances user-friendliness compared to conventional symbolic approaches.\n","authors":["Jun Wang","Jiaming Tong","Kaiyuan Tan","Yevgeniy Vorobeychik","Yiannis Kantaros"],"pdf_url":"https://arxiv.org/pdf/2309.10092v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04491v1","updated":"2024-08-08T14:41:32Z","published":"2024-08-08T14:41:32Z","title":"Towards Synergistic Deep Learning Models for Volumetric Cirrhotic Liver\n  Segmentation in MRIs","summary":"  Liver cirrhosis, a leading cause of global mortality, requires precise\nsegmentation of ROIs for effective disease monitoring and treatment planning.\nExisting segmentation models often fail to capture complex feature interactions\nand generalize across diverse datasets. To address these limitations, we\npropose a novel synergistic theory that leverages complementary latent spaces\nfor enhanced feature interaction modeling. Our proposed architecture,\nnnSynergyNet3D integrates continuous and discrete latent spaces for 3D volumes\nand features auto-configured training. This approach captures both fine-grained\nand coarse features, enabling effective modeling of intricate feature\ninteractions. We empirically validated nnSynergyNet3D on a private dataset of\n628 high-resolution T1 abdominal MRI scans from 339 patients. Our model\noutperformed the baseline nnUNet3D by approximately 2%. Additionally, zero-shot\ntesting on healthy liver CT scans from the public LiTS dataset demonstrated\nsuperior cross-modal generalization capabilities. These results highlight the\npotential of synergistic latent space models to improve segmentation accuracy\nand robustness, thereby enhancing clinical workflows by ensuring consistency\nacross CT and MRI modalities.\n","authors":["Vandan Gorade","Onkar Susladkar","Gorkem Durak","Elif Keles","Ertugrul Aktas","Timurhan Cebeci","Alpay Medetalibeyoglu","Daniela Ladner","Debesh Jha","Ulas Bagci"],"pdf_url":"https://arxiv.org/pdf/2408.04491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04484v1","updated":"2024-08-08T14:23:06Z","published":"2024-08-08T14:23:06Z","title":"Statistical Framework for Clustering MU-MIMO Wireless via Second Order\n  Statistics","summary":"  This work explores the clustering of wireless users by examining the\ndistances between their channel covariance matrices, which reside on the\nRiemannian manifold of positive definite matrices. Specifically, we consider an\nestimator of the Log-Euclidean distance between multiple sample covariance\nmatrices (SCMs) consistent when the number of samples and the observation size\ngrow unbounded at the same rate. Within the context of multi-user MIMO\n(MU-MIMO) wireless communication systems, we develop a statistical framework\nthat allows to accurate predictions of the clustering algorithm's performance\nunder realistic conditions. Specifically, we present a central limit theorem\nthat establishes the asymptotic Gaussianity of the consistent estimator of the\nlog-Euclidean distance computed over two sample covariance matrices.\n","authors":["Roberto Pereira","Xavier Mestre"],"pdf_url":"https://arxiv.org/pdf/2408.04484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04482v1","updated":"2024-08-08T14:19:11Z","published":"2024-08-08T14:19:11Z","title":"SegXAL: Explainable Active Learning for Semantic Segmentation in Driving\n  Scene Scenarios","summary":"  Most of the sophisticated AI models utilize huge amounts of annotated data\nand heavy training to achieve high-end performance. However, there are certain\nchallenges that hinder the deployment of AI models \"in-the-wild\" scenarios,\ni.e., inefficient use of unlabeled data, lack of incorporation of human\nexpertise, and lack of interpretation of the results. To mitigate these\nchallenges, we propose a novel Explainable Active Learning (XAL) model,\nXAL-based semantic segmentation model \"SegXAL\", that can (i) effectively\nutilize the unlabeled data, (ii) facilitate the \"Human-in-the-loop\" paradigm,\nand (iii) augment the model decisions in an interpretable way. In particular,\nwe investigate the application of the SegXAL model for semantic segmentation in\ndriving scene scenarios. The SegXAL model proposes the image regions that\nrequire labeling assistance from Oracle by dint of explainable AI (XAI) and\nuncertainty measures in a weakly-supervised manner. Specifically, we propose a\nnovel Proximity-aware Explainable-AI (PAE) module and Entropy-based Uncertainty\n(EBU) module to get an Explainable Error Mask, which enables the machine\nteachers/human experts to provide intuitive reasoning behind the results and to\nsolicit feedback to the AI system via an active learning strategy. Such a\nmechanism bridges the semantic gap between man and machine through\ncollaborative intelligence, where humans and AI actively enhance each other's\ncomplementary strengths. A novel high-confidence sample selection technique\nbased on the DICE similarity coefficient is also presented within the SegXAL\nframework. Extensive quantitative and qualitative analyses are carried out in\nthe benchmarking Cityscape dataset. Results show the outperformance of our\nproposed SegXAL against other state-of-the-art models.\n","authors":["Sriram Mandalika","Athira Nambiar"],"pdf_url":"https://arxiv.org/pdf/2408.04482v1.pdf","comment":"17 pages, 7 figures. To appear in the proceedings of the 27th\n  International Conference on Pattern Recognition (ICPR), 01-05 December, 2024,\n  Kolkata, India"},{"id":"http://arxiv.org/abs/2405.02850v3","updated":"2024-08-08T14:06:14Z","published":"2024-05-05T08:43:07Z","title":"Halfway Escape Optimization: A Quantum-Inspired Solution for Complex\n  Optimization Problems","summary":"  This paper first proposes the Halfway Escape Optimization (HEO) algorithm, a\nnovel quantum-inspired metaheuristic designed to address complex optimization\nproblems characterized by rugged landscapes and high-dimensionality with an\nefficient convergence rate. The study presents a comprehensive comparative\nevaluation of HEO's performance against established optimization algorithms,\nincluding Particle Swarm Optimization (PSO), Genetic Algorithm (GA), Artificial\nFish Swarm Algorithm (AFSA), Grey Wolf Optimizer (GWO), and Quantum behaved\nParticle Swarm Optimization (QPSO). The primary analysis encompasses 14\nbenchmark functions with dimension 30, demonstrating HEO's effectiveness and\nadaptability in navigating complex optimization landscapes and providing\nvaluable insights into its performance. The simple test of HEO in Traveling\nSalesman Problem (TSP), Pressure Vessel Design and Tubular Column Design infers\nits feasibility and potential weakness in real-time applications.\n","authors":["Jiawen Li","Anwar PP Abdul Majeed","Pascal Lefevre"],"pdf_url":"https://arxiv.org/pdf/2405.02850v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13000v2","updated":"2024-08-08T13:33:12Z","published":"2024-03-12T16:25:38Z","title":"Duwak: Dual Watermarks in Large Language Models","summary":"  As large language models (LLM) are increasingly used for text generation\ntasks, it is critical to audit their usages, govern their applications, and\nmitigate their potential harms. Existing watermark techniques are shown\neffective in embedding single human-imperceptible and machine-detectable\npatterns without significantly affecting generated text quality and semantics.\nHowever, the efficiency in detecting watermarks, i.e., the minimum number of\ntokens required to assert detection with significance and robustness against\npost-editing, is still debatable. In this paper, we propose, Duwak, to\nfundamentally enhance the efficiency and quality of watermarking by embedding\ndual secret patterns in both token probability distribution and sampling\nschemes. To mitigate expression degradation caused by biasing toward certain\ntokens, we design a contrastive search to watermark the sampling scheme, which\nminimizes the token repetition and enhances the diversity. We theoretically\nexplain the interdependency of the two watermarks within Duwak. We evaluate\nDuwak extensively on Llama2 under various post-editing attacks, against four\nstate-of-the-art watermarking techniques and combinations of them. Our results\nshow that Duwak marked text achieves the highest watermarked text quality at\nthe lowest required token count for detection, up to 70% tokens less than\nexisting approaches, especially under post paraphrasing.\n","authors":["Chaoyi Zhu","Jeroen Galjaard","Pin-Yu Chen","Lydia Y. Chen"],"pdf_url":"https://arxiv.org/pdf/2403.13000v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05530v4","updated":"2024-08-08T13:25:56Z","published":"2024-03-08T18:54:20Z","title":"Gemini 1.5: Unlocking multimodal understanding across millions of tokens\n  of context","summary":"  In this report, we introduce the Gemini 1.5 family of models, representing\nthe next generation of highly compute-efficient multimodal models capable of\nrecalling and reasoning over fine-grained information from millions of tokens\nof context, including multiple long documents and hours of video and audio. The\nfamily includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds\nthe February version on the great majority of capabilities and benchmarks; (2)\nGemini 1.5 Flash, a more lightweight variant designed for efficiency with\nminimal regression in quality. Gemini 1.5 models achieve near-perfect recall on\nlong-context retrieval tasks across modalities, improve the state-of-the-art in\nlong-document QA, long-video QA and long-context ASR, and match or surpass\nGemini 1.0 Ultra's state-of-the-art performance across a broad set of\nbenchmarks. Studying the limits of Gemini 1.5's long-context ability, we find\ncontinued improvement in next-token prediction and near-perfect retrieval\n(>99%) up to at least 10M tokens, a generational leap over existing models such\nas Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world\nuse cases, such as Gemini 1.5 collaborating with professionals on completing\ntheir tasks achieving 26 to 75% time savings across 10 different job\ncategories, as well as surprising new capabilities of large language models at\nthe frontier; when given a grammar manual for Kalamang, a language with fewer\nthan 200 speakers worldwide, the model learns to translate English to Kalamang\nat a similar level to a person who learned from the same content.\n","authors":[" Gemini Team","Petko Georgiev","Ving Ian Lei","Ryan Burnell","Libin Bai","Anmol Gulati","Garrett Tanzer","Damien Vincent","Zhufeng Pan","Shibo Wang","Soroosh Mariooryad","Yifan Ding","Xinyang Geng","Fred Alcober","Roy Frostig","Mark Omernick","Lexi Walker","Cosmin Paduraru","Christina Sorokin","Andrea Tacchetti","Colin Gaffney","Samira Daruki","Olcan Sercinoglu","Zach Gleicher","Juliette Love","Paul Voigtlaender","Rohan Jain","Gabriela Surita","Kareem Mohamed","Rory Blevins","Junwhan Ahn","Tao Zhu","Kornraphop Kawintiranon","Orhan Firat","Yiming Gu","Yujing Zhang","Matthew Rahtz","Manaal Faruqui","Natalie Clay","Justin Gilmer","JD Co-Reyes","Ivo Penchev","Rui Zhu","Nobuyuki Morioka","Kevin Hui","Krishna Haridasan","Victor Campos","Mahdis Mahdieh","Mandy Guo","Samer Hassan","Kevin Kilgour","Arpi Vezer","Heng-Tze Cheng","Raoul de Liedekerke","Siddharth Goyal","Paul Barham","DJ Strouse","Seb Noury","Jonas Adler","Mukund Sundararajan","Sharad Vikram","Dmitry Lepikhin","Michela Paganini","Xavier Garcia","Fan Yang","Dasha Valter","Maja Trebacz","Kiran Vodrahalli","Chulayuth Asawaroengchai","Roman Ring","Norbert Kalb","Livio Baldini Soares","Siddhartha Brahma","David Steiner","Tianhe Yu","Fabian Mentzer","Antoine He","Lucas Gonzalez","Bibo Xu","Raphael Lopez Kaufman","Laurent El Shafey","Junhyuk Oh","Tom Hennigan","George van den Driessche","Seth Odoom","Mario Lucic","Becca Roelofs","Sid Lall","Amit Marathe","Betty Chan","Santiago Ontanon","Luheng He","Denis Teplyashin","Jonathan Lai","Phil Crone","Bogdan Damoc","Lewis Ho","Sebastian Riedel","Karel Lenc","Chih-Kuan Yeh","Aakanksha Chowdhery","Yang Xu","Mehran Kazemi","Ehsan Amid","Anastasia Petrushkina","Kevin Swersky","Ali Khodaei","Gowoon Chen","Chris Larkin","Mario Pinto","Geng Yan","Adria Puigdomenech Badia","Piyush Patil","Steven Hansen","Dave Orr","Sebastien M. R. Arnold","Jordan Grimstad","Andrew Dai","Sholto Douglas","Rishika Sinha","Vikas Yadav","Xi Chen","Elena Gribovskaya","Jacob Austin","Jeffrey Zhao","Kaushal Patel","Paul Komarek","Sophia Austin","Sebastian Borgeaud","Linda Friso","Abhimanyu Goyal","Ben Caine","Kris Cao","Da-Woon Chung","Matthew Lamm","Gabe Barth-Maron","Thais Kagohara","Kate Olszewska","Mia Chen","Kaushik Shivakumar","Rishabh Agarwal","Harshal Godhia","Ravi Rajwar","Javier Snaider","Xerxes Dotiwalla","Yuan Liu","Aditya Barua","Victor Ungureanu","Yuan Zhang","Bat-Orgil Batsaikhan","Mateo Wirth","James Qin","Ivo Danihelka","Tulsee Doshi","Martin Chadwick","Jilin Chen","Sanil Jain","Quoc Le","Arjun Kar","Madhu Gurumurthy","Cheng Li","Ruoxin Sang","Fangyu Liu","Lampros Lamprou","Rich Munoz","Nathan Lintz","Harsh Mehta","Heidi Howard","Malcolm Reynolds","Lora Aroyo","Quan Wang","Lorenzo Blanco","Albin Cassirer","Jordan Griffith","Dipanjan Das","Stephan Lee","Jakub Sygnowski","Zach Fisher","James Besley","Richard Powell","Zafarali Ahmed","Dominik Paulus","David Reitter","Zalan Borsos","Rishabh Joshi","Aedan Pope","Steven Hand","Vittorio Selo","Vihan Jain","Nikhil Sethi","Megha Goel","Takaki Makino","Rhys May","Zhen Yang","Johan Schalkwyk","Christina Butterfield","Anja Hauth","Alex Goldin","Will Hawkins","Evan Senter","Sergey Brin","Oliver Woodman","Marvin Ritter","Eric Noland","Minh Giang","Vijay Bolina","Lisa Lee","Tim Blyth","Ian Mackinnon","Machel Reid","Obaid Sarvana","David Silver","Alexander Chen","Lily Wang","Loren Maggiore","Oscar Chang","Nithya Attaluri","Gregory Thornton","Chung-Cheng Chiu","Oskar Bunyan","Nir Levine","Timothy Chung","Evgenii Eltyshev","Xiance Si","Timothy Lillicrap","Demetra Brady","Vaibhav Aggarwal","Boxi Wu","Yuanzhong Xu","Ross McIlroy","Kartikeya Badola","Paramjit Sandhu","Erica Moreira","Wojciech Stokowiec","Ross Hemsley","Dong Li","Alex Tudor","Pranav Shyam","Elahe Rahimtoroghi","Salem Haykal","Pablo Sprechmann","Xiang Zhou","Diana Mincu","Yujia Li","Ravi Addanki","Kalpesh Krishna","Xiao Wu","Alexandre Frechette","Matan Eyal","Allan Dafoe","Dave Lacey","Jay Whang","Thi Avrahami","Ye Zhang","Emanuel Taropa","Hanzhao Lin","Daniel Toyama","Eliza Rutherford","Motoki Sano","HyunJeong Choe","Alex Tomala","Chalence Safranek-Shrader","Nora Kassner","Mantas Pajarskas","Matt Harvey","Sean Sechrist","Meire Fortunato","Christina Lyu","Gamaleldin Elsayed","Chenkai Kuang","James Lottes","Eric Chu","Chao Jia","Chih-Wei Chen","Peter Humphreys","Kate Baumli","Connie Tao","Rajkumar Samuel","Cicero Nogueira dos Santos","Anders Andreassen","Nemanja Rakićević","Dominik Grewe","Aviral Kumar","Stephanie Winkler","Jonathan Caton","Andrew Brock","Sid Dalmia","Hannah Sheahan","Iain Barr","Yingjie Miao","Paul Natsev","Jacob Devlin","Feryal Behbahani","Flavien Prost","Yanhua Sun","Artiom Myaskovsky","Thanumalayan Sankaranarayana Pillai","Dan Hurt","Angeliki Lazaridou","Xi Xiong","Ce Zheng","Fabio Pardo","Xiaowei Li","Dan Horgan","Joe Stanton","Moran Ambar","Fei Xia","Alejandro Lince","Mingqiu Wang","Basil Mustafa","Albert Webson","Hyo Lee","Rohan Anil","Martin Wicke","Timothy Dozat","Abhishek Sinha","Enrique Piqueras","Elahe Dabir","Shyam Upadhyay","Anudhyan Boral","Lisa Anne Hendricks","Corey Fry","Josip Djolonga","Yi Su","Jake Walker","Jane Labanowski","Ronny Huang","Vedant Misra","Jeremy Chen","RJ Skerry-Ryan","Avi Singh","Shruti Rijhwani","Dian Yu","Alex Castro-Ros","Beer Changpinyo","Romina Datta","Sumit Bagri","Arnar Mar Hrafnkelsson","Marcello Maggioni","Daniel Zheng","Yury Sulsky","Shaobo Hou","Tom Le Paine","Antoine Yang","Jason Riesa","Dominika Rogozinska","Dror Marcus","Dalia El Badawy","Qiao Zhang","Luyu Wang","Helen Miller","Jeremy Greer","Lars Lowe Sjos","Azade Nova","Heiga Zen","Rahma Chaabouni","Mihaela Rosca","Jiepu Jiang","Charlie Chen","Ruibo Liu","Tara Sainath","Maxim Krikun","Alex Polozov","Jean-Baptiste Lespiau","Josh Newlan","Zeyncep Cankara","Soo Kwak","Yunhan Xu","Phil Chen","Andy Coenen","Clemens Meyer","Katerina Tsihlas","Ada Ma","Juraj Gottweis","Jinwei Xing","Chenjie Gu","Jin Miao","Christian Frank","Zeynep Cankara","Sanjay Ganapathy","Ishita Dasgupta","Steph Hughes-Fitt","Heng Chen","David Reid","Keran Rong","Hongmin Fan","Joost van Amersfoort","Vincent Zhuang","Aaron Cohen","Shixiang Shane Gu","Anhad Mohananey","Anastasija Ilic","Taylor Tobin","John Wieting","Anna Bortsova","Phoebe Thacker","Emma Wang","Emily Caveness","Justin Chiu","Eren Sezener","Alex Kaskasoli","Steven Baker","Katie Millican","Mohamed Elhawaty","Kostas Aisopos","Carl Lebsack","Nathan Byrd","Hanjun Dai","Wenhao Jia","Matthew Wiethoff","Elnaz Davoodi","Albert Weston","Lakshman Yagati","Arun Ahuja","Isabel Gao","Golan Pundak","Susan Zhang","Michael Azzam","Khe Chai Sim","Sergi Caelles","James Keeling","Abhanshu Sharma","Andy Swing","YaGuang Li","Chenxi Liu","Carrie Grimes Bostock","Yamini Bansal","Zachary Nado","Ankesh Anand","Josh Lipschultz","Abhijit Karmarkar","Lev Proleev","Abe Ittycheriah","Soheil Hassas Yeganeh","George Polovets","Aleksandra Faust","Jiao Sun","Alban Rrustemi","Pen Li","Rakesh Shivanna","Jeremiah Liu","Chris Welty","Federico Lebron","Anirudh Baddepudi","Sebastian Krause","Emilio Parisotto","Radu Soricut","Zheng Xu","Dawn Bloxwich","Melvin Johnson","Behnam Neyshabur","Justin Mao-Jones","Renshen Wang","Vinay Ramasesh","Zaheer Abbas","Arthur Guez","Constant Segal","Duc Dung Nguyen","James Svensson","Le Hou","Sarah York","Kieran Milan","Sophie Bridgers","Wiktor Gworek","Marco Tagliasacchi","James Lee-Thorp","Michael Chang","Alexey Guseynov","Ale Jakse Hartman","Michael Kwong","Ruizhe Zhao","Sheleem Kashem","Elizabeth Cole","Antoine Miech","Richard Tanburn","Mary Phuong","Filip Pavetic","Sebastien Cevey","Ramona Comanescu","Richard Ives","Sherry Yang","Cosmo Du","Bo Li","Zizhao Zhang","Mariko Iinuma","Clara Huiyi Hu","Aurko Roy","Shaan Bijwadia","Zhenkai Zhu","Danilo Martins","Rachel Saputro","Anita Gergely","Steven Zheng","Dawei Jia","Ioannis Antonoglou","Adam Sadovsky","Shane Gu","Yingying Bi","Alek Andreev","Sina Samangooei","Mina Khan","Tomas Kocisky","Angelos Filos","Chintu Kumar","Colton Bishop","Adams Yu","Sarah Hodkinson","Sid Mittal","Premal Shah","Alexandre Moufarek","Yong Cheng","Adam Bloniarz","Jaehoon Lee","Pedram Pejman","Paul Michel","Stephen Spencer","Vladimir Feinberg","Xuehan Xiong","Nikolay Savinov","Charlotte Smith","Siamak Shakeri","Dustin Tran","Mary Chesus","Bernd Bohnet","George Tucker","Tamara von Glehn","Carrie Muir","Yiran Mao","Hideto Kazawa","Ambrose Slone","Kedar Soparkar","Disha Shrivastava","James Cobon-Kerr","Michael Sharman","Jay Pavagadhi","Carlos Araya","Karolis Misiunas","Nimesh Ghelani","Michael Laskin","David Barker","Qiujia Li","Anton Briukhov","Neil Houlsby","Mia Glaese","Balaji Lakshminarayanan","Nathan Schucher","Yunhao Tang","Eli Collins","Hyeontaek Lim","Fangxiaoyu Feng","Adria Recasens","Guangda Lai","Alberto Magni","Nicola De Cao","Aditya Siddhant","Zoe Ashwood","Jordi Orbay","Mostafa Dehghani","Jenny Brennan","Yifan He","Kelvin Xu","Yang Gao","Carl Saroufim","James Molloy","Xinyi Wu","Seb Arnold","Solomon Chang","Julian Schrittwieser","Elena Buchatskaya","Soroush Radpour","Martin Polacek","Skye Giordano","Ankur Bapna","Simon Tokumine","Vincent Hellendoorn","Thibault Sottiaux","Sarah Cogan","Aliaksei Severyn","Mohammad Saleh","Shantanu Thakoor","Laurent Shefey","Siyuan Qiao","Meenu Gaba","Shuo-yiin Chang","Craig Swanson","Biao Zhang","Benjamin Lee","Paul Kishan Rubenstein","Gan Song","Tom Kwiatkowski","Anna Koop","Ajay Kannan","David Kao","Parker Schuh","Axel Stjerngren","Golnaz Ghiasi","Gena Gibson","Luke Vilnis","Ye Yuan","Felipe Tiengo Ferreira","Aishwarya Kamath","Ted Klimenko","Ken Franko","Kefan Xiao","Indro Bhattacharya","Miteyan Patel","Rui Wang","Alex Morris","Robin Strudel","Vivek Sharma","Peter Choy","Sayed Hadi Hashemi","Jessica Landon","Mara Finkelstein","Priya Jhakra","Justin Frye","Megan Barnes","Matthew Mauger","Dennis Daun","Khuslen Baatarsukh","Matthew Tung","Wael Farhan","Henryk Michalewski","Fabio Viola","Felix de Chaumont Quitry","Charline Le Lan","Tom Hudson","Qingze Wang","Felix Fischer","Ivy Zheng","Elspeth White","Anca Dragan","Jean-baptiste Alayrac","Eric Ni","Alexander Pritzel","Adam Iwanicki","Michael Isard","Anna Bulanova","Lukas Zilka","Ethan Dyer","Devendra Sachan","Srivatsan Srinivasan","Hannah Muckenhirn","Honglong Cai","Amol Mandhane","Mukarram Tariq","Jack W. Rae","Gary Wang","Kareem Ayoub","Nicholas FitzGerald","Yao Zhao","Woohyun Han","Chris Alberti","Dan Garrette","Kashyap Krishnakumar","Mai Gimenez","Anselm Levskaya","Daniel Sohn","Josip Matak","Inaki Iturrate","Michael B. Chang","Jackie Xiang","Yuan Cao","Nishant Ranka","Geoff Brown","Adrian Hutter","Vahab Mirrokni","Nanxin Chen","Kaisheng Yao","Zoltan Egyed","Francois Galilee","Tyler Liechty","Praveen Kallakuri","Evan Palmer","Sanjay Ghemawat","Jasmine Liu","David Tao","Chloe Thornton","Tim Green","Mimi Jasarevic","Sharon Lin","Victor Cotruta","Yi-Xuan Tan","Noah Fiedel","Hongkun Yu","Ed Chi","Alexander Neitz","Jens Heitkaemper","Anu Sinha","Denny Zhou","Yi Sun","Charbel Kaed","Brice Hulse","Swaroop Mishra","Maria Georgaki","Sneha Kudugunta","Clement Farabet","Izhak Shafran","Daniel Vlasic","Anton Tsitsulin","Rajagopal Ananthanarayanan","Alen Carin","Guolong Su","Pei Sun","Shashank V","Gabriel Carvajal","Josef Broder","Iulia Comsa","Alena Repina","William Wong","Warren Weilun Chen","Peter Hawkins","Egor Filonov","Lucia Loher","Christoph Hirnschall","Weiyi Wang","Jingchen Ye","Andrea Burns","Hardie Cate","Diana Gage Wright","Federico Piccinini","Lei Zhang","Chu-Cheng Lin","Ionel Gog","Yana Kulizhskaya","Ashwin Sreevatsa","Shuang Song","Luis C. Cobo","Anand Iyer","Chetan Tekur","Guillermo Garrido","Zhuyun Xiao","Rupert Kemp","Huaixiu Steven Zheng","Hui Li","Ananth Agarwal","Christel Ngani","Kati Goshvadi","Rebeca Santamaria-Fernandez","Wojciech Fica","Xinyun Chen","Chris Gorgolewski","Sean Sun","Roopal Garg","Xinyu Ye","S. M. Ali Eslami","Nan Hua","Jon Simon","Pratik Joshi","Yelin Kim","Ian Tenney","Sahitya Potluri","Lam Nguyen Thiet","Quan Yuan","Florian Luisier","Alexandra Chronopoulou","Salvatore Scellato","Praveen Srinivasan","Minmin Chen","Vinod Koverkathu","Valentin Dalibard","Yaming Xu","Brennan Saeta","Keith Anderson","Thibault Sellam","Nick Fernando","Fantine Huot","Junehyuk Jung","Mani Varadarajan","Michael Quinn","Amit Raul","Maigo Le","Ruslan Habalov","Jon Clark","Komal Jalan","Kalesha Bullard","Achintya Singhal","Thang Luong","Boyu Wang","Sujeevan Rajayogam","Julian Eisenschlos","Johnson Jia","Daniel Finchelstein","Alex Yakubovich","Daniel Balle","Michael Fink","Sameer Agarwal","Jing Li","Dj Dvijotham","Shalini Pal","Kai Kang","Jaclyn Konzelmann","Jennifer Beattie","Olivier Dousse","Diane Wu","Remi Crocker","Chen Elkind","Siddhartha Reddy Jonnalagadda","Jong Lee","Dan Holtmann-Rice","Krystal Kallarackal","Rosanne Liu","Denis Vnukov","Neera Vats","Luca Invernizzi","Mohsen Jafari","Huanjie Zhou","Lilly Taylor","Jennifer Prendki","Marcus Wu","Tom Eccles","Tianqi Liu","Kavya Kopparapu","Francoise Beaufays","Christof Angermueller","Andreea Marzoca","Shourya Sarcar","Hilal Dib","Jeff Stanway","Frank Perbet","Nejc Trdin","Rachel Sterneck","Andrey Khorlin","Dinghua Li","Xihui Wu","Sonam Goenka","David Madras","Sasha Goldshtein","Willi Gierke","Tong Zhou","Yaxin Liu","Yannie Liang","Anais White","Yunjie Li","Shreya Singh","Sanaz Bahargam","Mark Epstein","Sujoy Basu","Li Lao","Adnan Ozturel","Carl Crous","Alex Zhai","Han Lu","Zora Tung","Neeraj Gaur","Alanna Walton","Lucas Dixon","Ming Zhang","Amir Globerson","Grant Uy","Andrew Bolt","Olivia Wiles","Milad Nasr","Ilia Shumailov","Marco Selvi","Francesco Piccinno","Ricardo Aguilar","Sara McCarthy","Misha Khalman","Mrinal Shukla","Vlado Galic","John Carpenter","Kevin Villela","Haibin Zhang","Harry Richardson","James Martens","Matko Bosnjak","Shreyas Rammohan Belle","Jeff Seibert","Mahmoud Alnahlawi","Brian McWilliams","Sankalp Singh","Annie Louis","Wen Ding","Dan Popovici","Lenin Simicich","Laura Knight","Pulkit Mehta","Nishesh Gupta","Chongyang Shi","Saaber Fatehi","Jovana Mitrovic","Alex Grills","Joseph Pagadora","Dessie Petrova","Danielle Eisenbud","Zhishuai Zhang","Damion Yates","Bhavishya Mittal","Nilesh Tripuraneni","Yannis Assael","Thomas Brovelli","Prateek Jain","Mihajlo Velimirovic","Canfer Akbulut","Jiaqi Mu","Wolfgang Macherey","Ravin Kumar","Jun Xu","Haroon Qureshi","Gheorghe Comanici","Jeremy Wiesner","Zhitao Gong","Anton Ruddock","Matthias Bauer","Nick Felt","Anirudh GP","Anurag Arnab","Dustin Zelle","Jonas Rothfuss","Bill Rosgen","Ashish Shenoy","Bryan Seybold","Xinjian Li","Jayaram Mudigonda","Goker Erdogan","Jiawei Xia","Jiri Simsa","Andrea Michi","Yi Yao","Christopher Yew","Steven Kan","Isaac Caswell","Carey Radebaugh","Andre Elisseeff","Pedro Valenzuela","Kay McKinney","Kim Paterson","Albert Cui","Eri Latorre-Chimoto","Solomon Kim","William Zeng","Ken Durden","Priya Ponnapalli","Tiberiu Sosea","Christopher A. Choquette-Choo","James Manyika","Brona Robenek","Harsha Vashisht","Sebastien Pereira","Hoi Lam","Marko Velic","Denese Owusu-Afriyie","Katherine Lee","Tolga Bolukbasi","Alicia Parrish","Shawn Lu","Jane Park","Balaji Venkatraman","Alice Talbert","Lambert Rosique","Yuchung Cheng","Andrei Sozanschi","Adam Paszke","Praveen Kumar","Jessica Austin","Lu Li","Khalid Salama","Wooyeol Kim","Nandita Dukkipati","Anthony Baryshnikov","Christos Kaplanis","XiangHai Sheng","Yuri Chervonyi","Caglar Unlu","Diego de Las Casas","Harry Askham","Kathryn Tunyasuvunakool","Felix Gimeno","Siim Poder","Chester Kwak","Matt Miecnikowski","Vahab Mirrokni","Alek Dimitriev","Aaron Parisi","Dangyi Liu","Tomy Tsai","Toby Shevlane","Christina Kouridi","Drew Garmon","Adrian Goedeckemeyer","Adam R. Brown","Anitha Vijayakumar","Ali Elqursh","Sadegh Jazayeri","Jin Huang","Sara Mc Carthy","Jay Hoover","Lucy Kim","Sandeep Kumar","Wei Chen","Courtney Biles","Garrett Bingham","Evan Rosen","Lisa Wang","Qijun Tan","David Engel","Francesco Pongetti","Dario de Cesare","Dongseong Hwang","Lily Yu","Jennifer Pullman","Srini Narayanan","Kyle Levin","Siddharth Gopal","Megan Li","Asaf Aharoni","Trieu Trinh","Jessica Lo","Norman Casagrande","Roopali Vij","Loic Matthey","Bramandia Ramadhana","Austin Matthews","CJ Carey","Matthew Johnson","Kremena Goranova","Rohin Shah","Shereen Ashraf","Kingshuk Dasgupta","Rasmus Larsen","Yicheng Wang","Manish Reddy Vuyyuru","Chong Jiang","Joana Ijazi","Kazuki Osawa","Celine Smith","Ramya Sree Boppana","Taylan Bilal","Yuma Koizumi","Ying Xu","Yasemin Altun","Nir Shabat","Ben Bariach","Alex Korchemniy","Kiam Choo","Olaf Ronneberger","Chimezie Iwuanyanwu","Shubin Zhao","David Soergel","Cho-Jui Hsieh","Irene Cai","Shariq Iqbal","Martin Sundermeyer","Zhe Chen","Elie Bursztein","Chaitanya Malaviya","Fadi Biadsy","Prakash Shroff","Inderjit Dhillon","Tejasi Latkar","Chris Dyer","Hannah Forbes","Massimo Nicosia","Vitaly Nikolaev","Somer Greene","Marin Georgiev","Pidong Wang","Nina Martin","Hanie Sedghi","John Zhang","Praseem Banzal","Doug Fritz","Vikram Rao","Xuezhi Wang","Jiageng Zhang","Viorica Patraucean","Dayou Du","Igor Mordatch","Ivan Jurin","Lewis Liu","Ayush Dubey","Abhi Mohan","Janek Nowakowski","Vlad-Doru Ion","Nan Wei","Reiko Tojo","Maria Abi Raad","Drew A. Hudson","Vaishakh Keshava","Shubham Agrawal","Kevin Ramirez","Zhichun Wu","Hoang Nguyen","Ji Liu","Madhavi Sewak","Bryce Petrini","DongHyun Choi","Ivan Philips","Ziyue Wang","Ioana Bica","Ankush Garg","Jarek Wilkiewicz","Priyanka Agrawal","Xiaowei Li","Danhao Guo","Emily Xue","Naseer Shaik","Andrew Leach","Sadh MNM Khan","Julia Wiesinger","Sammy Jerome","Abhishek Chakladar","Alek Wenjiao Wang","Tina Ornduff","Folake Abu","Alireza Ghaffarkhah","Marcus Wainwright","Mario Cortes","Frederick Liu","Joshua Maynez","Andreas Terzis","Pouya Samangouei","Riham Mansour","Tomasz Kępa","François-Xavier Aubet","Anton Algymr","Dan Banica","Agoston Weisz","Andras Orban","Alexandre Senges","Ewa Andrejczuk","Mark Geller","Niccolo Dal Santo","Valentin Anklin","Majd Al Merey","Martin Baeuml","Trevor Strohman","Junwen Bai","Slav Petrov","Yonghui Wu","Demis Hassabis","Koray Kavukcuoglu","Jeffrey Dean","Oriol Vinyals"],"pdf_url":"https://arxiv.org/pdf/2403.05530v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04449v1","updated":"2024-08-08T13:19:37Z","published":"2024-08-08T13:19:37Z","title":"RiskAwareBench: Towards Evaluating Physical Risk Awareness for\n  High-level Planning of LLM-based Embodied Agents","summary":"  The integration of large language models (LLMs) into robotics significantly\nenhances the capabilities of embodied agents in understanding and executing\ncomplex natural language instructions. However, the unmitigated deployment of\nLLM-based embodied systems in real-world environments may pose potential\nphysical risks, such as property damage and personal injury. Existing security\nbenchmarks for LLMs overlook risk awareness for LLM-based embodied agents. To\naddress this gap, we propose RiskAwareBench, an automated framework designed to\nassess physical risks awareness in LLM-based embodied agents. RiskAwareBench\nconsists of four modules: safety tips generation, risky scene generation, plan\ngeneration, and evaluation, enabling comprehensive risk assessment with minimal\nmanual intervention. Utilizing this framework, we compile the PhysicalRisk\ndataset, encompassing diverse scenarios with associated safety tips,\nobservations, and instructions. Extensive experiments reveal that most LLMs\nexhibit insufficient physical risk awareness, and baseline risk mitigation\nstrategies yield limited enhancement, which emphasizes the urgency and\ncruciality of improving risk awareness in LLM-based embodied agents in the\nfuture.\n","authors":["Zihao Zhu","Bingzhe Wu","Zhengyou Zhang","Baoyuan Wu"],"pdf_url":"https://arxiv.org/pdf/2408.04449v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04442v1","updated":"2024-08-08T13:14:19Z","published":"2024-08-08T13:14:19Z","title":"FedAD-Bench: A Unified Benchmark for Federated Unsupervised Anomaly\n  Detection in Tabular Data","summary":"  The emergence of federated learning (FL) presents a promising approach to\nleverage decentralized data while preserving privacy. Furthermore, the\ncombination of FL and anomaly detection is particularly compelling because it\nallows for detecting rare and critical anomalies (usually also rare in locally\ngathered data) in sensitive data from multiple sources, such as cybersecurity\nand healthcare. However, benchmarking the performance of anomaly detection\nmethods in FL environments remains an underexplored area. This paper introduces\nFedAD-Bench, a unified benchmark for evaluating unsupervised anomaly detection\nalgorithms within the context of FL. We systematically analyze and compare the\nperformance of recent deep learning anomaly detection models under federated\nsettings, which were typically assessed solely in centralized settings.\nFedAD-Bench encompasses diverse datasets and metrics to provide a holistic\nevaluation. Through extensive experiments, we identify key challenges such as\nmodel aggregation inefficiencies and metric unreliability. We present insights\ninto FL's regularization effects, revealing scenarios in which it outperforms\ncentralized approaches due to its inherent ability to mitigate overfitting. Our\nwork aims to establish a standardized benchmark to guide future research and\ndevelopment in federated anomaly detection, promoting reproducibility and fair\ncomparison across studies.\n","authors":["Ahmed Anwar","Brian Moser","Dayananda Herurkar","Federico Raue","Vinit Hegiste","Tatjana Legler","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2408.04442v1.pdf","comment":"8 pages, 1 figure"},{"id":"http://arxiv.org/abs/2205.07537v3","updated":"2024-08-08T13:09:25Z","published":"2022-05-16T09:33:00Z","title":"Decomposition Strategies and Multi-shot ASP Solving for Job-shop\n  Scheduling","summary":"  The Job-shop Scheduling Problem (JSP) is a well-known and challenging\ncombinatorial optimization problem in which tasks sharing a machine are to be\narranged in a sequence such that encompassing jobs can be completed as early as\npossible. In this paper, we investigate problem decomposition into time windows\nwhose operations can be successively scheduled and optimized by means of\nmulti-shot Answer Set Programming (ASP) solving. From a computational\nperspective, decomposition aims to split highly complex scheduling tasks into\nbetter manageable subproblems with a balanced number of operations such that\ngood-quality or even optimal partial solutions can be reliably found in a small\nfraction of runtime. We devise and investigate a variety of decomposition\nstrategies in terms of the number and size of time windows as well as\nheuristics for choosing their operations. Moreover, we incorporate time window\noverlapping and compression techniques into the iterative scheduling process to\ncounteract optimization limitations due to the restriction to window-wise\npartial schedules. Our experiments on different JSP benchmark sets show that\nsuccessive optimization by multi-shot ASP solving leads to substantially better\nschedules within tight runtime limits than single-shot optimization on the full\nproblem. In particular, we find that decomposing initial solutions obtained\nwith proficient heuristic methods into time windows leads to improved solution\nquality.\n","authors":["Mohammed M. S. El-Kholany","Martin Gebser","Konstantin Schekotihin"],"pdf_url":"https://arxiv.org/pdf/2205.07537v3.pdf","comment":"This paper is an extended version of our papers presented at the 38th\n  International Conference on Logic Programming (ICLP 2022) and the 24th\n  International Symposium on Practical Aspects of Declarative Languages (PADL\n  2022)"},{"id":"http://arxiv.org/abs/2406.08223v2","updated":"2024-08-08T13:07:21Z","published":"2024-06-12T13:52:38Z","title":"Research Trends for the Interplay between Large Language Models and\n  Knowledge Graphs","summary":"  This survey investigates the synergistic relationship between Large Language\nModels (LLMs) and Knowledge Graphs (KGs), which is crucial for advancing AI's\ncapabilities in understanding, reasoning, and language processing. It aims to\naddress gaps in current research by exploring areas such as KG Question\nAnswering, ontology generation, KG validation, and the enhancement of KG\naccuracy and consistency through LLMs. The paper further examines the roles of\nLLMs in generating descriptive texts and natural language queries for KGs.\nThrough a structured analysis that includes categorizing LLM-KG interactions,\nexamining methodologies, and investigating collaborative uses and potential\nbiases, this study seeks to provide new insights into the combined potential of\nLLMs and KGs. It highlights the importance of their interaction for improving\nAI applications and outlines future research directions.\n","authors":["Hanieh Khorashadizadeh","Fatima Zahra Amara","Morteza Ezzabady","Frédéric Ieng","Sanju Tiwari","Nandana Mihindukulasooriya","Jinghua Groppe","Soror Sahri","Farah Benamara","Sven Groppe"],"pdf_url":"https://arxiv.org/pdf/2406.08223v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04414v1","updated":"2024-08-08T12:42:43Z","published":"2024-08-08T12:42:43Z","title":"Enhancing Robustness of Retrieval-Augmented Language Models with\n  In-Context Learning","summary":"  Retrieval-Augmented Language Models (RALMs) have significantly improved\nperformance in open-domain question answering (QA) by leveraging external\nknowledge. However, RALMs still struggle with unanswerable queries, where the\nretrieved contexts do not contain the correct answer, and with conflicting\ninformation, where different sources provide contradictory answers due to\nimperfect retrieval. This study introduces an in-context learning-based\napproach to enhance the reasoning capabilities of RALMs, making them more\nrobust in imperfect retrieval scenarios. Our method incorporates Machine\nReading Comprehension (MRC) demonstrations, referred to as cases, to boost the\nmodel's capabilities to identify unanswerabilities and conflicts among the\nretrieved contexts. Experiments on two open-domain QA datasets show that our\napproach increases accuracy in identifying unanswerable and conflicting\nscenarios without requiring additional fine-tuning. This work demonstrates that\nin-context learning can effectively enhance the robustness of RALMs in\nopen-domain QA tasks.\n","authors":["Seong-Il Park","Seung-Woo Choi","Na-Hyun Kim","Jay-Yoon Lee"],"pdf_url":"https://arxiv.org/pdf/2408.04414v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2408.03354v2","updated":"2024-08-08T12:31:12Z","published":"2024-08-06T09:15:25Z","title":"The Use of Large Language Models (LLM) for Cyber Threat Intelligence\n  (CTI) in Cybercrime Forums","summary":"  Large language models (LLMs) can be used to analyze cyber threat intelligence\n(CTI) data from cybercrime forums, which contain extensive information and key\ndiscussions about emerging cyber threats. However, to date, the level of\naccuracy and efficiency of LLMs for such critical tasks has yet to be\nthoroughly evaluated. Hence, this study assesses the accuracy of an LLM system\nbuilt on the OpenAI GPT-3.5-turbo model [7] to extract CTI information. To do\nso, a random sample of 500 daily conversations from three cybercrime forums,\nXSS, Exploit_in, and RAMP, was extracted, and the LLM system was instructed to\nsummarize the conversations and code 10 key CTI variables, such as whether a\nlarge organization and/or a critical infrastructure is being targeted. Then,\ntwo coders reviewed each conversation and evaluated whether the information\nextracted by the LLM was accurate. The LLM system performed strikingly well,\nwith an average accuracy score of 98%. Various ways to enhance the model were\nuncovered, such as the need to help the LLM distinguish between stories and\npast events, as well as being careful with verb tenses in prompts.\nNevertheless, the results of this study highlight the efficiency and relevance\nof using LLMs for cyber threat intelligence.\n","authors":["Vanessa Clairoux-Trepanier","Isa-May Beauchamp","Estelle Ruellan","Masarah Paquet-Clouston","Serge-Olivier Paquette","Eric Clay"],"pdf_url":"https://arxiv.org/pdf/2408.03354v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01661v2","updated":"2024-08-08T12:17:56Z","published":"2024-06-03T17:55:02Z","title":"A Diffusion Model Framework for Unsupervised Neural Combinatorial\n  Optimization","summary":"  Learning to sample from intractable distributions over discrete sets without\nrelying on corresponding training data is a central problem in a wide range of\nfields, including Combinatorial Optimization. Currently, popular deep\nlearning-based approaches rely primarily on generative models that yield exact\nsample likelihoods. This work introduces a method that lifts this restriction\nand opens the possibility to employ highly expressive latent variable models\nlike diffusion models. Our approach is conceptually based on a loss that upper\nbounds the reverse Kullback-Leibler divergence and evades the requirement of\nexact sample likelihoods. We experimentally validate our approach in data-free\nCombinatorial Optimization and demonstrate that our method achieves a new\nstate-of-the-art on a wide range of benchmark problems.\n","authors":["Sebastian Sanokowski","Sepp Hochreiter","Sebastian Lehner"],"pdf_url":"https://arxiv.org/pdf/2406.01661v2.pdf","comment":"Accepted at ICML 2024"},{"id":"http://arxiv.org/abs/2408.04405v1","updated":"2024-08-08T12:14:17Z","published":"2024-08-08T12:14:17Z","title":"Probabilistic energy forecasting through quantile regression in\n  reproducing kernel Hilbert spaces","summary":"  Accurate energy demand forecasting is crucial for sustainable and resilient\nenergy development. To meet the Net Zero Representative Concentration Pathways\n(RCP) $4.5$ scenario in the DACH countries, increased renewable energy\nproduction, energy storage, and reduced commercial building consumption are\nneeded. This scenario's success depends on hydroelectric capacity and climatic\nfactors. Informed decisions require quantifying uncertainty in forecasts. This\nstudy explores a non-parametric method based on \\emph{reproducing kernel\nHilbert spaces (RKHS)}, known as kernel quantile regression, for energy\nprediction. Our experiments demonstrate its reliability and sharpness, and we\nbenchmark it against state-of-the-art methods in load and price forecasting for\nthe DACH region. We offer our implementation in conjunction with additional\nscripts to ensure the reproducibility of our research.\n","authors":["Luca Pernigo","Rohan Sen","Davide Baroli"],"pdf_url":"https://arxiv.org/pdf/2408.04405v1.pdf","comment":"12 pages, {Owner/Author | ACM} {2024}. This is the author's version\n  of the work. It is posted here for your personal use. Not for redistribution.\n  The definitive Version of Record will published in https://energy.acm.org/eir"},{"id":"http://arxiv.org/abs/2408.04403v1","updated":"2024-08-08T12:10:50Z","published":"2024-08-08T12:10:50Z","title":"Exploring Reasoning Biases in Large Language Models Through Syllogism:\n  Insights from the NeuBAROCO Dataset","summary":"  This paper explores the question of how accurately current large language\nmodels can perform logical reasoning in natural language, with an emphasis on\nwhether these models exhibit reasoning biases similar to humans. Specifically,\nour study focuses on syllogistic reasoning, a form of deductive reasoning\nextensively studied in cognitive science as a natural form of human reasoning.\nWe present a syllogism dataset called NeuBAROCO, which consists of syllogistic\nreasoning problems in English and Japanese. This dataset was originally\ndesigned for psychological experiments to assess human reasoning capabilities\nusing various forms of syllogisms. Our experiments with leading large language\nmodels indicate that these models exhibit reasoning biases similar to humans,\nalong with other error tendencies. Notably, there is significant room for\nimprovement in reasoning problems where the relationship between premises and\nhypotheses is neither entailment nor contradiction. We also present\nexperimental results and in-depth analysis using a new Chain-of-Thought\nprompting method, which asks LLMs to translate syllogisms into abstract logical\nexpressions and then explain their reasoning process. Our analysis using this\nmethod suggests that the primary limitations of LLMs lie in the reasoning\nprocess itself rather than the interpretation of syllogisms.\n","authors":["Kentaro Ozeki","Risako Ando","Takanobu Morishita","Hirohiko Abe","Koji Mineshima","Mitsuhiro Okada"],"pdf_url":"https://arxiv.org/pdf/2408.04403v1.pdf","comment":"To appear in Findings of the Association for Computational\n  Linguistics: ACL 2024"},{"id":"http://arxiv.org/abs/2408.04400v1","updated":"2024-08-08T12:08:55Z","published":"2024-08-08T12:08:55Z","title":"DIVE: Subgraph Disagreement for Graph Out-of-Distribution Generalization","summary":"  This paper addresses the challenge of out-of-distribution (OOD)\ngeneralization in graph machine learning, a field rapidly advancing yet\ngrappling with the discrepancy between source and target data distributions.\nTraditional graph learning algorithms, based on the assumption of uniform\ndistribution between training and test data, falter in real-world scenarios\nwhere this assumption fails, resulting in suboptimal performance. A principal\nfactor contributing to this suboptimal performance is the inherent simplicity\nbias of neural networks trained through Stochastic Gradient Descent (SGD),\nwhich prefer simpler features over more complex yet equally or more predictive\nones. This bias leads to a reliance on spurious correlations, adversely\naffecting OOD performance in various tasks such as image recognition, natural\nlanguage understanding, and graph classification. Current methodologies,\nincluding subgraph-mixup and information bottleneck approaches, have achieved\npartial success but struggle to overcome simplicity bias, often reinforcing\nspurious correlations. To tackle this, we propose DIVE, training a collection\nof models to focus on all label-predictive subgraphs by encouraging the models\nto foster divergence on the subgraph mask, which circumvents the limitation of\na model solely focusing on the subgraph corresponding to simple structural\npatterns. Specifically, we employs a regularizer to punish overlap in extracted\nsubgraphs across models, thereby encouraging different models to concentrate on\ndistinct structural patterns. Model selection for robust OOD performance is\nachieved through validation accuracy. Tested across four datasets from GOOD\nbenchmark and one dataset from DrugOOD benchmark, our approach demonstrates\nsignificant improvement over existing methods, effectively addressing the\nsimplicity bias and enhancing generalization in graph machine learning.\n","authors":["Xin Sun","Liang Wang","Qiang Liu","Shu Wu","Zilei Wang","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2408.04400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04394v1","updated":"2024-08-08T11:56:57Z","published":"2024-08-08T11:56:57Z","title":"Automated Educational Question Generation at Different Bloom's Skill\n  Levels using Large Language Models: Strategies and Evaluation","summary":"  Developing questions that are pedagogically sound, relevant, and promote\nlearning is a challenging and time-consuming task for educators. Modern-day\nlarge language models (LLMs) generate high-quality content across multiple\ndomains, potentially helping educators to develop high-quality questions.\nAutomated educational question generation (AEQG) is important in scaling online\neducation catering to a diverse student population. Past attempts at AEQG have\nshown limited abilities to generate questions at higher cognitive levels. In\nthis study, we examine the ability of five state-of-the-art LLMs of different\nsizes to generate diverse and high-quality questions of different cognitive\nlevels, as defined by Bloom's taxonomy. We use advanced prompting techniques\nwith varying complexity for AEQG. We conducted expert and LLM-based evaluations\nto assess the linguistic and pedagogical relevance and quality of the\nquestions. Our findings suggest that LLms can generate relevant and\nhigh-quality educational questions of different cognitive levels when prompted\nwith adequate information, although there is a significant variance in the\nperformance of the five LLms considered. We also show that automated evaluation\nis not on par with human evaluation.\n","authors":["Nicy Scaria","Suma Dharani Chenna","Deepak Subramani"],"pdf_url":"https://arxiv.org/pdf/2408.04394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08214v2","updated":"2024-08-08T11:51:15Z","published":"2024-03-13T03:23:50Z","title":"P2LHAP:Wearable sensor-based human activity recognition, segmentation\n  and forecast through Patch-to-Label Seq2Seq Transformer","summary":"  Traditional deep learning methods struggle to simultaneously segment,\nrecognize, and forecast human activities from sensor data. This limits their\nusefulness in many fields such as healthcare and assisted living, where\nreal-time understanding of ongoing and upcoming activities is crucial. This\npaper introduces P2LHAP, a novel Patch-to-Label Seq2Seq framework that tackles\nall three tasks in a efficient single-task model. P2LHAP divides sensor data\nstreams into a sequence of \"patches\", served as input tokens, and outputs a\nsequence of patch-level activity labels including the predicted future\nactivities. A unique smoothing technique based on surrounding patch labels, is\nproposed to identify activity boundaries accurately. Additionally, P2LHAP\nlearns patch-level representation by sensor signal channel-independent\nTransformer encoders and decoders. All channels share embedding and Transformer\nweights across all sequences. Evaluated on three public datasets, P2LHAP\nsignificantly outperforms the state-of-the-art in all three tasks,\ndemonstrating its effectiveness and potential for real-world applications.\n","authors":["Shuangjian Li","Tao Zhu","Mingxing Nie","Huansheng Ning","Zhenyu Liu","Liming Chen"],"pdf_url":"https://arxiv.org/pdf/2403.08214v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04388v1","updated":"2024-08-08T11:44:57Z","published":"2024-08-08T11:44:57Z","title":"MM-Forecast: A Multimodal Approach to Temporal Event Forecasting with\n  Large Language Models","summary":"  We study an emerging and intriguing problem of multimodal temporal event\nforecasting with large language models. Compared to using text or graph\nmodalities, the investigation of utilizing images for temporal event\nforecasting has not been fully explored, especially in the era of large\nlanguage models (LLMs). To bridge this gap, we are particularly interested in\ntwo key questions of: 1) why images will help in temporal event forecasting,\nand 2) how to integrate images into the LLM-based forecasting framework. To\nanswer these research questions, we propose to identify two essential functions\nthat images play in the scenario of temporal event forecasting, i.e.,\nhighlighting and complementary. Then, we develop a novel framework, named\nMM-Forecast. It employs an Image Function Identification module to recognize\nthese functions as verbal descriptions using multimodal large language models\n(MLLMs), and subsequently incorporates these function descriptions into\nLLM-based forecasting models. To evaluate our approach, we construct a new\nmultimodal dataset, MidEast-TE-mm, by extending an existing event dataset\nMidEast-TE-mini with images. Empirical studies demonstrate that our MM-Forecast\ncan correctly identify the image functions, and further more, incorporating\nthese verbal function descriptions significantly improves the forecasting\nperformance. The dataset, code, and prompts are available at\nhttps://github.com/LuminosityX/MM-Forecast.\n","authors":["Haoxuan Li","Zhengmao Yang","Yunshan Ma","Yi Bin","Yang Yang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2408.04388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04385v1","updated":"2024-08-08T11:41:04Z","published":"2024-08-08T11:41:04Z","title":"Non-maximizing policies that fulfill multi-criterion aspirations in\n  expectation","summary":"  In dynamic programming and reinforcement learning, the policy for the\nsequential decision making of an agent in a stochastic environment is usually\ndetermined by expressing the goal as a scalar reward function and seeking a\npolicy that maximizes the expected total reward. However, many goals that\nhumans care about naturally concern multiple aspects of the world, and it may\nnot be obvious how to condense those into a single reward function.\nFurthermore, maximization suffers from specification gaming, where the obtained\npolicy achieves a high expected total reward in an unintended way, often taking\nextreme or nonsensical actions.\n  Here we consider finite acyclic Markov Decision Processes with multiple\ndistinct evaluation metrics, which do not necessarily represent quantities that\nthe user wants to be maximized. We assume the task of the agent is to ensure\nthat the vector of expected totals of the evaluation metrics falls into some\ngiven convex set, called the aspiration set. Our algorithm guarantees that this\ntask is fulfilled by using simplices to approximate feasibility sets and\npropagate aspirations forward while ensuring they remain feasible. It has\ncomplexity linear in the number of possible state-action-successor triples and\npolynomial in the number of evaluation metrics. Moreover, the explicitly\nnon-maximizing nature of the chosen policy and goals yields additional degrees\nof freedom, which can be used to apply heuristic safety criteria to the choice\nof actions. We discuss several such safety criteria that aim to steer the agent\ntowards more conservative behavior.\n","authors":["Simon Dima","Simon Fischer","Jobst Heitzig","Joss Oliver"],"pdf_url":"https://arxiv.org/pdf/2408.04385v1.pdf","comment":"16 pages main text + 4 pages supplement. Accepted for Algorithmic\n  Decision Theory 2024"},{"id":"http://arxiv.org/abs/2408.04382v1","updated":"2024-08-08T11:37:32Z","published":"2024-08-08T11:37:32Z","title":"Judgment2vec: Apply Graph Analytics to Searching and Recommendation of\n  Similar Judgments","summary":"  In court practice, legal professionals rely on their training to provide\nopinions that resolve cases, one of the most crucial aspects being the ability\nto identify similar judgments from previous courts efficiently. However,\nfinding a similar case is challenging and often depends on experience, legal\ndomain knowledge, and extensive labor hours, making veteran lawyers or judges\nindispensable. This research aims to automate the analysis of judgment text\nsimilarity. We utilized a judgment dataset labeled as the \"golden standard\" by\nexperts, which includes human-verified features that can be converted into an\n\"expert similarity score.\" We then constructed a knowledge graph based on\n\"case-article\" relationships, ranking each case using natural language\nprocessing to derive a \"Node2vec similarity score.\" By evaluating these two\nsimilarity scores, we identified their discrepancies and relationships. The\nresults can significantly reduce the labor hours required for legal searches\nand recommendations, with potential applications extending to various fields of\ninformation retrieval.\n","authors":["Hsuan-Lei Shao"],"pdf_url":"https://arxiv.org/pdf/2408.04382v1.pdf","comment":"5 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2406.08959v2","updated":"2024-08-08T11:33:47Z","published":"2024-06-13T09:44:04Z","title":"Beyond Recommendations: From Backward to Forward AI Support of Pilots'\n  Decision-Making Process","summary":"  AI is anticipated to enhance human decision-making in high-stakes domains\nlike aviation, but adoption is often hindered by challenges such as\ninappropriate reliance and poor alignment with users' decision-making. Recent\nresearch suggests that a core underlying issue is the recommendation-centric\ndesign of many AI systems, i.e., they give end-to-end recommendations and\nignore the rest of the decision-making process. Alternative support paradigms\nare rare, and it remains unclear how the few that do exist compare to\nrecommendation-centric support. In this work, we aimed to empirically compare\nrecommendation-centric support to an alternative paradigm, continuous support,\nin the context of diversions in aviation. We conducted a mixed-methods study\nwith 32 professional pilots in a realistic setting. To ensure the quality of\nour study scenarios, we conducted a focus group with four additional pilots\nprior to the study. We found that continuous support can support pilots'\ndecision-making in a forward direction, allowing them to think more beyond the\nlimits of the system and make faster decisions when combined with\nrecommendations, though the forward support can be disrupted. Participants'\nstatements further suggest a shift in design goal away from providing\nrecommendations, to supporting quick information gathering. Our results show\nways to design more helpful and effective AI decision support that goes beyond\nend-to-end recommendations.\n","authors":["Zelun Tony Zhang","Sebastian S. Feger","Lucas Dullenkopf","Rulu Liao","Lukas Süsslin","Yuanting Liu","Andreas Butz"],"pdf_url":"https://arxiv.org/pdf/2406.08959v2.pdf","comment":"Accepted to CSCW 2024, to be published in PACM HCI Vol. 8, No. CSCW2"},{"id":"http://arxiv.org/abs/2408.04377v1","updated":"2024-08-08T11:22:52Z","published":"2024-08-08T11:22:52Z","title":"Anomaly Prediction: A Novel Approach with Explicit Delay and Horizon","summary":"  Detecting anomalies in time series data is a critical challenge across\nvarious domains. Traditional methods typically focus on identifying anomalies\nin immediate subsequent steps, often underestimating the significance of\ntemporal dynamics such as delay time and horizons of anomalies, which generally\nrequire extensive post-analysis. This paper introduces a novel approach for\ntime series anomaly prediction, incorporating temporal information directly\ninto the prediction results. We propose a new dataset specifically designed to\nevaluate this approach and conduct comprehensive experiments using several\nstate-of-the-art methods. results demonstrate the efficacy of our approach in\nproviding timely and accurate anomaly predictions, setting a new benchmark for\nfuture research in this field.\n","authors":["Jiang You","Arben Cela","René Natowicz","Jacob Ouanounou","Patrick Siarry"],"pdf_url":"https://arxiv.org/pdf/2408.04377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04349v1","updated":"2024-08-08T10:20:13Z","published":"2024-08-08T10:20:13Z","title":"Optimal Layout-Aware CNOT Circuit Synthesis with Qubit Permutation","summary":"  CNOT optimization plays a significant role in noise reduction for Quantum\nCircuits. Several heuristic and exact approaches exist for CNOT optimization.\nIn this paper, we investigate more complicated variations of optimal synthesis\nby allowing qubit permutations and handling layout restrictions. We encode such\nproblems into Planning, SAT, and QBF. We provide optimization for both CNOT\ngate count and circuit depth. For experimental evaluation, we consider standard\nT-gate optimized benchmarks and optimize CNOT sub-circuits. We show that\nallowing qubit permutations can further reduce up to 56% in CNOT count and 46%\nin circuit depth. In the case of optimally mapped circuits under layout\nrestrictions, we observe a reduction up to 17% CNOT count and 19% CNOT depth.\n","authors":["Irfansha Shaik","Jaco van de Pol"],"pdf_url":"https://arxiv.org/pdf/2408.04349v1.pdf","comment":"9 pages, 12 tables"},{"id":"http://arxiv.org/abs/2408.04342v1","updated":"2024-08-08T09:59:30Z","published":"2024-08-08T09:59:30Z","title":"Towards Explainable Network Intrusion Detection using Large Language\n  Models","summary":"  Large Language Models (LLMs) have revolutionised natural language processing\ntasks, particularly as chat agents. However, their applicability to threat\ndetection problems remains unclear. This paper examines the feasibility of\nemploying LLMs as a Network Intrusion Detection System (NIDS), despite their\nhigh computational requirements, primarily for the sake of explainability.\nFurthermore, considerable resources have been invested in developing LLMs, and\nthey may offer utility for NIDS. Current state-of-the-art NIDS rely on\nartificial benchmarking datasets, resulting in skewed performance when applied\nto real-world networking environments. Therefore, we compare the GPT-4 and\nLLama3 models against traditional architectures and transformer-based models to\nassess their ability to detect malicious NetFlows without depending on\nartificially skewed datasets, but solely on their vast pre-trained acquired\nknowledge. Our results reveal that, although LLMs struggle with precise attack\ndetection, they hold significant potential for a path towards explainable NIDS.\nOur preliminary exploration shows that LLMs are unfit for the detection of\nMalicious NetFlows. Most promisingly, however, these exhibit significant\npotential as complementary agents in NIDS, particularly in providing\nexplanations and aiding in threat response when integrated with Retrieval\nAugmented Generation (RAG) and function calling capabilities.\n","authors":["Paul R. B. Houssel","Priyanka Singh","Siamak Layeghy","Marius Portmann"],"pdf_url":"https://arxiv.org/pdf/2408.04342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04336v1","updated":"2024-08-08T09:43:54Z","published":"2024-08-08T09:43:54Z","title":"KnowPC: Knowledge-Driven Programmatic Reinforcement Learning for\n  Zero-shot Coordination","summary":"  Zero-shot coordination (ZSC) remains a major challenge in the cooperative AI\nfield, which aims to learn an agent to cooperate with an unseen partner in\ntraining environments or even novel environments. In recent years, a popular\nZSC solution paradigm has been deep reinforcement learning (DRL) combined with\nadvanced self-play or population-based methods to enhance the neural policy's\nability to handle unseen partners. Despite some success, these approaches\nusually rely on black-box neural networks as the policy function. However,\nneural networks typically lack interpretability and logic, making the learned\npolicies difficult for partners (e.g., humans) to understand and limiting their\ngeneralization ability. These shortcomings hinder the application of\nreinforcement learning methods in diverse cooperative scenarios.We suggest to\nrepresent the agent's policy with an interpretable program. Unlike neural\nnetworks, programs contain stable logic, but they are non-differentiable and\ndifficult to optimize.To automatically learn such programs, we introduce\nKnowledge-driven Programmatic reinforcement learning for zero-shot Coordination\n(KnowPC). We first define a foundational Domain-Specific Language (DSL),\nincluding program structures, conditional primitives, and action primitives. A\nsignificant challenge is the vast program search space, making it difficult to\nfind high-performing programs efficiently. To address this, KnowPC integrates\nan extractor and an reasoner. The extractor discovers environmental transition\nknowledge from multi-agent interaction trajectories, while the reasoner deduces\nthe preconditions of each action primitive based on the transition knowledge.\n","authors":["Yin Gu","Qi Liu","Zhi Li","Kai Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.04336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20183v3","updated":"2024-08-08T09:40:10Z","published":"2024-03-29T13:57:46Z","title":"HARMamba: Efficient and Lightweight Wearable Sensor Human Activity\n  Recognition Based on Bidirectional Mamba","summary":"  Wearable sensor-based human activity recognition (HAR) is a critical research\ndomain in activity perception. However, achieving high efficiency and long\nsequence recognition remains a challenge. Despite the extensive investigation\nof temporal deep learning models, such as CNNs, RNNs, and transformers, their\nextensive parameters often pose significant computational and memory\nconstraints, rendering them less suitable for resource-constrained mobile\nhealth applications. This study introduces HARMamba, an innovative light-weight\nand versatile HAR architecture that combines selective bidirectional State\nSpaces Model and hardware-aware design. To optimize real-time resource\nconsumption in practical scenarios, HARMamba employs linear recursive\nmechanisms and parameter discretization, allowing it to selectively focus on\nrelevant input sequences while efficiently fusing scan and recompute\noperations. The model employs independent channels to process sensor data\nstreams, dividing each channel into patches and appending classification tokens\nto the end of the sequence. It utilizes position embedding to represent the\nsequence order. The patch sequence is subsequently processed by HARMamba Block,\nand the classification head finally outputs the activity category. The HARMamba\nBlock serves as the fundamental component of the HARMamba architecture,\nenabling the effective capture of more discriminative activity sequence\nfeatures. HARMamba outperforms contemporary state-of-the-art frameworks,\ndelivering comparable or better accuracy with significantly reducing\ncomputational and memory demands. It's effectiveness has been extensively\nvalidated on 4 publically available datasets namely PAMAP2, WISDM, UNIMIB SHAR\nand UCI. The F1 scores of HARMamba on the four datasets are 99.74%, 99.20%,\n88.23% and 97.01%, respectively.\n","authors":["Shuangjian Li","Tao Zhu","Furong Duan","Liming Chen","Huansheng Ning","Christopher Nugent","Yaping Wan"],"pdf_url":"https://arxiv.org/pdf/2403.20183v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16010v2","updated":"2024-08-08T09:12:13Z","published":"2024-07-22T19:33:12Z","title":"AIDE: Antithetical, Intent-based, and Diverse Example-Based Explanations","summary":"  For many use-cases, it is often important to explain the prediction of a\nblack-box model by identifying the most influential training data samples.\nExisting approaches lack customization for user intent and often provide a\nhomogeneous set of explanation samples, failing to reveal the model's reasoning\nfrom different angles.\n  In this paper, we propose AIDE, an approach for providing antithetical (i.e.,\ncontrastive), intent-based, diverse explanations for opaque and complex models.\nAIDE distinguishes three types of explainability intents: interpreting a\ncorrect, investigating a wrong, and clarifying an ambiguous prediction. For\neach intent, AIDE selects an appropriate set of influential training samples\nthat support or oppose the prediction either directly or by contrast. To\nprovide a succinct summary, AIDE uses diversity-aware sampling to avoid\nredundancy and increase coverage of the training data.\n  We demonstrate the effectiveness of AIDE on image and text classification\ntasks, in three ways: quantitatively, assessing correctness and continuity;\nqualitatively, comparing anecdotal evidence from AIDE and other example-based\napproaches; and via a user study, evaluating multiple aspects of AIDE. The\nresults show that AIDE addresses the limitations of existing methods and\nexhibits desirable traits for an explainability method.\n","authors":["Ikhtiyor Nematov","Dimitris Sacharidis","Tomer Sagi","Katja Hose"],"pdf_url":"https://arxiv.org/pdf/2407.16010v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03710v2","updated":"2024-08-08T09:03:51Z","published":"2024-04-04T13:43:17Z","title":"Self-organized free-flight arrival for urban air mobility","summary":"  Urban air mobility is an innovative mode of transportation in which electric\nvertical takeoff and landing (eVTOL) vehicles operate between nodes called\nvertiports. We outline a self-organized vertiport arrival system based on deep\nreinforcement learning. The airspace around the vertiport is assumed to be\ncircular, and the vehicles can freely operate inside. Each aircraft is\nconsidered an individual agent and follows a shared policy, resulting in\ndecentralized actions that are based on local information. We investigate the\ndevelopment of the reinforcement learning policy during training and illustrate\nhow the algorithm moves from suboptimal local holding patterns to a safe and\nefficient final policy. The latter is validated in simulation-based scenarios,\nincluding robustness analyses against sensor noise and a changing distribution\nof inbound traffic. Lastly, we deploy the final policy on small-scale unmanned\naerial vehicles to showcase its real-world usability.\n","authors":["Martin Waltz","Ostap Okhrin","Michael Schultz"],"pdf_url":"https://arxiv.org/pdf/2404.03710v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11652v6","updated":"2024-08-08T08:44:29Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v6.pdf","comment":"I found critical errors in the manuscript affecting its validity. I\n  need to correct these before resubmitting. Major changes to methodology and\n  results are underway, significantly altering the content. I will resubmit the\n  revised version"},{"id":"http://arxiv.org/abs/2408.04304v1","updated":"2024-08-08T08:38:02Z","published":"2024-08-08T08:38:02Z","title":"Learning with Digital Agents: An Analysis based on the Activity Theory","summary":"  Digital agents are considered a general-purpose technology. They spread\nquickly in private and organizational contexts, including education. Yet,\nresearch lacks a conceptual framing to describe interaction with such agents in\na holistic manner. While focusing on the interaction with a pedagogical agent,\ni.e., a digital agent capable of natural-language interaction with a learner,\nwe propose a model of learning activity based on activity theory. We use this\nmodel and a review of prior research on digital agents in education to analyze\nhow various characteristics of the activity, including features of a\npedagogical agent or learner, influence learning outcomes. The analysis leads\nto identification of IS research directions and guidance for developers of\npedagogical agents and digital agents in general. We conclude by extending the\nactivity theory-based model beyond the context of education and show how it\nhelps designers and researchers ask the right questions when creating a digital\nagent.\n","authors":["Mateusz Dolata","Dzmitry Katsiuba","Natalie Wellnhammer","Gerhard Schwabe"],"pdf_url":"https://arxiv.org/pdf/2408.04304v1.pdf","comment":"Authors manuscript accepted for publication in Journal of Management\n  Information Systems"},{"id":"http://arxiv.org/abs/2402.12062v3","updated":"2024-08-08T08:36:03Z","published":"2024-02-19T11:30:00Z","title":"Causal Equal Protection as Algorithmic Fairness","summary":"  By combining the philosophical literature on statistical evidence and the\ninterdisciplinary literature on algorithmic fairness, we revisit recent\nobjections against classification parity in light of causal analyses of\nalgorithmic fairness and the distinction between predictive and diagnostic\nevidence. We focus on trial proceedings as a black-box classification algorithm\nin which defendants are sorted into two groups by convicting or acquitting\nthem. We defend a novel principle, causal equal protection, that combines\nclassification parity with the causal approach. In the do-calculus, causal\nequal protection requires that individuals should not be subject to uneven\nrisks of classification error because of their protected or socially salient\ncharacteristics. The explicit use of protected characteristics, however, may be\nrequired if it equalizes these risks.\n","authors":["Marcello Di Bello","Nicolò Cangiotti","Michele Loi"],"pdf_url":"https://arxiv.org/pdf/2402.12062v3.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.04301v1","updated":"2024-08-08T08:35:32Z","published":"2024-08-08T08:35:32Z","title":"Tackling Noisy Clients in Federated Learning with End-to-end Label\n  Correction","summary":"  Recently, federated learning (FL) has achieved wide successes for diverse\nprivacy-sensitive applications without sacrificing the sensitive private\ninformation of clients. However, the data quality of client datasets can not be\nguaranteed since corresponding annotations of different clients often contain\ncomplex label noise of varying degrees, which inevitably causes the performance\ndegradation. Intuitively, the performance degradation is dominated by clients\nwith higher noise rates since their trained models contain more misinformation\nfrom data, thus it is necessary to devise an effective optimization scheme to\nmitigate the negative impacts of these noisy clients. In this work, we propose\na two-stage framework FedELC to tackle this complicated label noise issue. The\nfirst stage aims to guide the detection of noisy clients with higher label\nnoise, while the second stage aims to correct the labels of noisy clients' data\nvia an end-to-end label correction framework which is achieved by learning\npossible ground-truth labels of noisy clients' datasets via back propagation.\nWe implement sixteen related methods and evaluate five datasets with three\ntypes of complicated label noise scenarios for a comprehensive comparison.\nExtensive experimental results demonstrate our proposed framework achieves\nsuperior performance than its counterparts for different scenarios.\nAdditionally, we effectively improve the data quality of detected noisy\nclients' local datasets with our label correction framework. The code is\navailable at https://github.com/Sprinter1999/FedELC.\n","authors":["Xuefeng Jiang","Sheng Sun","Jia Li","Jingjing Xue","Runhan Li","Zhiyuan Wu","Gang Xu","Yuwei Wang","Min Liu"],"pdf_url":"https://arxiv.org/pdf/2408.04301v1.pdf","comment":"To appear in ACM CIKM'24 full research paper track"},{"id":"http://arxiv.org/abs/2408.04295v1","updated":"2024-08-08T08:18:05Z","published":"2024-08-08T08:18:05Z","title":"Assigning Credit with Partial Reward Decoupling in Multi-Agent Proximal\n  Policy Optimization","summary":"  Multi-agent proximal policy optimization (MAPPO) has recently demonstrated\nstate-of-the-art performance on challenging multi-agent reinforcement learning\ntasks. However, MAPPO still struggles with the credit assignment problem,\nwherein the sheer difficulty in ascribing credit to individual agents' actions\nscales poorly with team size. In this paper, we propose a multi-agent\nreinforcement learning algorithm that adapts recent developments in credit\nassignment to improve upon MAPPO. Our approach leverages partial reward\ndecoupling (PRD), which uses a learned attention mechanism to estimate which of\na particular agent's teammates are relevant to its learning updates. We use\nthis estimate to dynamically decompose large groups of agents into smaller,\nmore manageable subgroups. We empirically demonstrate that our approach,\nPRD-MAPPO, decouples agents from teammates that do not influence their expected\nfuture reward, thereby streamlining credit assignment. We additionally show\nthat PRD-MAPPO yields significantly higher data efficiency and asymptotic\nperformance compared to both MAPPO and other state-of-the-art methods across\nseveral multi-agent tasks, including StarCraft II. Finally, we propose a\nversion of PRD-MAPPO that is applicable to \\textit{shared} reward settings,\nwhere PRD was previously not applicable, and empirically show that this also\nleads to performance improvements over MAPPO.\n","authors":["Aditya Kapoor","Benjamin Freed","Howie Choset","Jeff Schneider"],"pdf_url":"https://arxiv.org/pdf/2408.04295v1.pdf","comment":"20 pages, 5 figures, 12 tables, Reinforcement Learning Journal and\n  Reinforcement Learning Conference 2024"},{"id":"http://arxiv.org/abs/2407.21670v2","updated":"2024-08-08T07:59:50Z","published":"2024-07-31T15:13:39Z","title":"Universal Approximation Theory: Foundations for Parallelism in Neural\n  Networks","summary":"  Neural networks are increasingly evolving towards training large models with\nbig data, a method that has demonstrated superior performance across many\ntasks. However, this approach introduces an urgent problem: current deep\nlearning models are predominantly serial, meaning that as the number of network\nlayers increases, so do the training and inference times. This is unacceptable\nif deep learning is to continue advancing. Therefore, this paper proposes a\ndeep learning parallelization strategy based on the Universal Approximation\nTheorem (UAT). From this foundation, we designed a parallel network called\nPara-Former to test our theory. Unlike traditional serial models, the inference\ntime of Para-Former does not increase with the number of layers, significantly\naccelerating the inference speed of multi-layer networks. Experimental results\nvalidate the effectiveness of this network.\n","authors":["Wei Wang","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2407.21670v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04281v1","updated":"2024-08-08T07:39:23Z","published":"2024-08-08T07:39:23Z","title":"AI-Driven Chatbot for Intrusion Detection in Edge Networks: Enhancing\n  Cybersecurity with Ethical User Consent","summary":"  In today's contemporary digital landscape, chatbots have become indispensable\ntools across various sectors, streamlining customer service, providing personal\nassistance, automating routine tasks, and offering health advice. However,\ntheir potential remains underexplored in the realm of network security,\nparticularly for intrusion detection. To bridge this gap, we propose an\narchitecture chatbot specifically designed to enhance security within edge\nnetworks specifically for intrusion detection. Leveraging advanced machine\nlearning algorithms, this chatbot will monitor network traffic to identify and\nmitigate potential intrusions. By securing the network environment using an\nedge network managed by a Raspberry Pi module and ensuring ethical user consent\npromoting transparency and trust, this innovative solution aims to safeguard\nsensitive data and maintain a secure workplace, thereby addressing the growing\nneed for robust network security measures in the digital age.\n","authors":["Mugheez Asif","Abdul Manan","Abdul Moiz ur Rehman","Mamoona Naveed Asghar","Muhammad Umair"],"pdf_url":"https://arxiv.org/pdf/2408.04281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13988v6","updated":"2024-08-08T07:32:14Z","published":"2023-03-24T13:24:41Z","title":"Machine Psychology","summary":"  Large language models (LLMs) show increasingly advanced emergent capabilities\nand are being incorporated across various societal domains. Understanding their\nbehavior and reasoning abilities therefore holds significant importance. We\nargue that a fruitful direction for research is engaging LLMs in behavioral\nexperiments inspired by psychology that have traditionally been aimed at\nunderstanding human cognition and behavior. In this article, we highlight and\nsummarize theoretical perspectives, experimental paradigms, and computational\nanalysis techniques that this approach brings to the table. It paves the way\nfor a \"machine psychology\" for generative artificial intelligence (AI) that\ngoes beyond performance benchmarks and focuses instead on computational\ninsights that move us toward a better understanding and discovery of emergent\nabilities and behavioral patterns in LLMs. We review existing work taking this\napproach, synthesize best practices, and highlight promising future directions.\nWe also highlight the important caveats of applying methodologies designed for\nunderstanding humans to machines. We posit that leveraging tools from\nexperimental psychology to study AI will become increasingly valuable as models\nevolve to be more powerful, opaque, multi-modal, and integrated into complex\nreal-world settings.\n","authors":["Thilo Hagendorff","Ishita Dasgupta","Marcel Binz","Stephanie C. Y. Chan","Andrew Lampinen","Jane X. Wang","Zeynep Akata","Eric Schulz"],"pdf_url":"https://arxiv.org/pdf/2303.13988v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05589v2","updated":"2024-08-08T07:15:04Z","published":"2023-12-09T14:49:34Z","title":"A Review of Hybrid and Ensemble in Deep Learning for Natural Language\n  Processing","summary":"  This review presents a comprehensive exploration of hybrid and ensemble deep\nlearning models within Natural Language Processing (NLP), shedding light on\ntheir transformative potential across diverse tasks such as Sentiment Analysis,\nNamed Entity Recognition, Machine Translation, Question Answering, Text\nClassification, Generation, Speech Recognition, Summarization, and Language\nModeling. The paper systematically introduces each task, delineates key\narchitectures from Recurrent Neural Networks (RNNs) to Transformer-based models\nlike BERT, and evaluates their performance, challenges, and computational\ndemands. The adaptability of ensemble techniques is emphasized, highlighting\ntheir capacity to enhance various NLP applications. Challenges in\nimplementation, including computational overhead, overfitting, and model\ninterpretation complexities, are addressed alongside the trade-off between\ninterpretability and performance. Serving as a concise yet invaluable guide,\nthis review synthesizes insights into tasks, architectures, and challenges,\noffering a holistic perspective for researchers and practitioners aiming to\nadvance language-driven applications through ensemble deep learning in NLP.\n","authors":["Jianguo Jia","Wen Liang","Youzhi Liang"],"pdf_url":"https://arxiv.org/pdf/2312.05589v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15809v2","updated":"2024-08-08T07:05:46Z","published":"2024-02-24T13:13:04Z","title":"Empowering Large Language Model Agents through Action Learning","summary":"  Large Language Model (LLM) Agents have recently garnered increasing interest\nyet they are limited in their ability to learn from trial and error, a key\nelement of intelligent behavior. In this work, we argue that the capacity to\nlearn new actions from experience is fundamental to the advancement of learning\nin LLM agents. While humans naturally expand their action spaces and develop\nskills through experiential learning, LLM agents typically operate within fixed\naction spaces, limiting their potential for growth. To address these\nchallenges, our study explores open-action learning for language agents. We\nintroduce a framework LearnAct with an iterative learning strategy to create\nand improve actions in the form of Python functions. In each iteration, LLM\nrevises and updates the currently available actions based on the errors\nidentified in unsuccessful training tasks, thereby enhancing action\neffectiveness. Our experimental evaluations across Robotic Planning and\nAlfworld environments reveal that after learning on a few training task\ninstances, our approach to open-action learning markedly improves agent\nperformance for the type of task (by 32 percent in AlfWorld compared to\nReAct+Reflexion, for instance) highlighting the importance of experiential\naction learning in the development of more intelligent LLM agents.\n","authors":["Haiteng Zhao","Chang Ma","Guoyin Wang","Jing Su","Lingpeng Kong","Jingjing Xu","Zhi-Hong Deng","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2402.15809v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2408.02912v2","updated":"2024-08-08T07:02:20Z","published":"2024-08-06T02:53:55Z","title":"KOI: Accelerating Online Imitation Learning via Hybrid Key-state\n  Guidance","summary":"  Online Imitation Learning methods struggle with the gap between extensive\nonline exploration space and limited expert trajectories, which hinder\nefficient exploration due to inaccurate task-aware reward estimation. Inspired\nby the findings from cognitive neuroscience that task decomposition could\nfacilitate cognitive processing for efficient learning, we hypothesize that an\nagent could estimate precise task-aware imitation rewards for efficient online\nexploration by decomposing the target task into the objectives of \"what to do\"\nand the mechanisms of \"how to do\". In this work, we introduce the hybrid\nKey-state guided Online Imitation (KOI) learning approach, which leverages the\nintegration of semantic and motion key states as guidance for task-aware reward\nestimation. Initially, we utilize the visual-language models to segment the\nexpert trajectory into semantic key states, indicating the objectives of \"what\nto do\". Within the intervals between semantic key states, optical flow is\nemployed to capture motion key states to understand the process of \"how to do\".\nBy integrating a thorough grasp of both semantic and motion key states, we\nrefine the trajectory-matching reward computation, encouraging task-aware\nexploration for efficient online imitation learning. Our experiment results\nprove that our method is more sample efficient in the Meta-World and LIBERO\nenvironments. We also conduct real-world robotic manipulation experiments to\nvalidate the efficacy of our method, demonstrating the practical applicability\nof our KOI method.\n","authors":["Jingxian Lu","Wenke Xia","Dong Wang","Zhigang Wang","Bin Zhao","Di Hu","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.02912v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04261v1","updated":"2024-08-08T06:58:48Z","published":"2024-08-08T06:58:48Z","title":"Unveiling Hidden Visual Information: A Reconstruction Attack Against\n  Adversarial Visual Information Hiding","summary":"  This paper investigates the security vulnerabilities of\nadversarial-example-based image encryption by executing data reconstruction\n(DR) attacks on encrypted images. A representative image encryption method is\nthe adversarial visual information hiding (AVIH), which uses type-I adversarial\nexample training to protect gallery datasets used in image recognition tasks.\nIn the AVIH method, the type-I adversarial example approach creates images that\nappear completely different but are still recognized by machines as the\noriginal ones. Additionally, the AVIH method can restore encrypted images to\ntheir original forms using a predefined private key generative model. For the\nbest security, assigning a unique key to each image is recommended; however,\nstorage limitations may necessitate some images sharing the same key model.\nThis raises a crucial security question for AVIH: How many images can safely\nshare the same key model without being compromised by a DR attack? To address\nthis question, we introduce a dual-strategy DR attack against the AVIH\nencryption method by incorporating (1) generative-adversarial loss and (2)\naugmented identity loss, which prevent DR from overfitting -- an issue akin to\nthat in machine learning. Our numerical results validate this approach through\nimage recognition and re-identification benchmarks, demonstrating that our\nstrategy can significantly enhance the quality of reconstructed images, thereby\nrequiring fewer key-sharing encrypted images. Our source code to reproduce our\nresults will be available soon.\n","authors":["Jonggyu Jang","Hyeonsu Lyu","Seongjin Hwang","Hyun Jong Yang"],"pdf_url":"https://arxiv.org/pdf/2408.04261v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2408.04259v1","updated":"2024-08-08T06:57:49Z","published":"2024-08-08T06:57:49Z","title":"EfficientRAG: Efficient Retriever for Multi-Hop Question Answering","summary":"  Retrieval-augmented generation (RAG) methods encounter difficulties when\naddressing complex questions like multi-hop queries. While iterative retrieval\nmethods improve performance by gathering additional information, current\napproaches often rely on multiple calls of large language models (LLMs). In\nthis paper, we introduce EfficientRAG, an efficient retriever for multi-hop\nquestion answering. EfficientRAG iteratively generates new queries without the\nneed for LLM calls at each iteration and filters out irrelevant information.\nExperimental results demonstrate that EfficientRAG surpasses existing RAG\nmethods on three open-domain multi-hop question-answering datasets.\n","authors":["Ziyuan Zhuang","Zhiyang Zhang","Sitao Cheng","Fangkai Yang","Jia Liu","Shujian Huang","Qingwei Lin","Saravan Rajmohan","Dongmei Zhang","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.04259v1.pdf","comment":"20 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.19913v2","updated":"2024-08-08T06:38:31Z","published":"2024-03-29T01:53:24Z","title":"MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of\n  Large Language Models","summary":"  Large language models such as ChatGPT and GPT-4 have recently achieved\nastonishing performance on a variety of natural language processing tasks. In\nthis paper, we propose MANGO, a benchmark to evaluate their capabilities to\nperform text-based mapping and navigation. Our benchmark includes 53 mazes\ntaken from a suite of textgames: each maze is paired with a walkthrough that\nvisits every location but does not cover all possible paths. The task is\nquestion-answering: for each maze, a large language model reads the walkthrough\nand answers hundreds of mapping and navigation questions such as \"How should\nyou go to Attic from West of House?\" and \"Where are we if we go north and east\nfrom Cellar?\". Although these questions are easy to humans, it turns out that\neven GPT-4, the best-to-date language model, performs poorly at answering them.\nFurther, our experiments suggest that a strong mapping and navigation ability\nwould benefit large language models in performing relevant downstream tasks,\nsuch as playing textgames. Our MANGO benchmark will facilitate future research\non methods that improve the mapping and navigation capabilities of language\nmodels. We host our leaderboard, data, code, and evaluation program at\nhttps://mango.ttic.edu and https://github.com/oaklight/mango/.\n","authors":["Peng Ding","Jiading Fang","Peng Li","Kangrui Wang","Xiaochen Zhou","Mo Yu","Jing Li","Matthew R. Walter","Hongyuan Mei"],"pdf_url":"https://arxiv.org/pdf/2403.19913v2.pdf","comment":"COLM 2024 camera-ready"},{"id":"http://arxiv.org/abs/2408.04245v1","updated":"2024-08-08T06:17:13Z","published":"2024-08-08T06:17:13Z","title":"Scalable Transformer for High Dimensional Multivariate Time Series\n  Forecasting","summary":"  Deep models for Multivariate Time Series (MTS) forecasting have recently\ndemonstrated significant success. Channel-dependent models capture complex\ndependencies that channel-independent models cannot capture. However, the\nnumber of channels in real-world applications outpaces the capabilities of\nexisting channel-dependent models, and contrary to common expectations, some\nmodels underperform the channel-independent models in handling high-dimensional\ndata, which raises questions about the performance of channel-dependent models.\nTo address this, our study first investigates the reasons behind the suboptimal\nperformance of these channel-dependent models on high-dimensional MTS data. Our\nanalysis reveals that two primary issues lie in the introduced noise from\nunrelated series that increases the difficulty of capturing the crucial\ninter-channel dependencies, and challenges in training strategies due to\nhigh-dimensional data. To address these issues, we propose STHD, the Scalable\nTransformer for High-Dimensional Multivariate Time Series Forecasting. STHD has\nthree components: a) Relation Matrix Sparsity that limits the noise introduced\nand alleviates the memory issue; b) ReIndex applied as a training strategy to\nenable a more flexible batch size setting and increase the diversity of\ntraining data; and c) Transformer that handles 2-D inputs and captures channel\ndependencies. These components jointly enable STHD to manage the\nhigh-dimensional MTS while maintaining computational feasibility. Furthermore,\nexperimental results show STHD's considerable improvement on three\nhigh-dimensional datasets: Crime-Chicago, Wiki-People, and Traffic. The source\ncode and dataset are publicly available\nhttps://github.com/xinzzzhou/ScalableTransformer4HighDimensionMTSF.git.\n","authors":["Xin Zhou","Weiqing Wang","Wray Buntine","Shilin Qu","Abishek Sriramulu","Weicong Tan","Christoph Bergmeir"],"pdf_url":"https://arxiv.org/pdf/2408.04245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12393v4","updated":"2024-08-08T06:08:54Z","published":"2024-07-17T08:13:22Z","title":"PersLLM: A Personified Training Approach for Large Language Models","summary":"  Large language models exhibit aspects of human-level intelligence that\ncatalyze their application as human-like agents in domains such as social\nsimulations, human-machine interactions, and collaborative multi-agent systems.\nHowever, the absence of distinct personalities, such as displaying ingratiating\nbehaviors, inconsistent opinions, and uniform response patterns, diminish LLMs\nutility in practical applications. Addressing this, the development of\npersonality traits in LLMs emerges as a crucial area of research to unlock\ntheir latent potential. Existing methods to personify LLMs generally involve\nstrategies like employing stylized training data for instruction tuning or\nusing prompt engineering to simulate different personalities. These methods\nonly capture superficial linguistic styles instead of the core of personalities\nand are therefore not stable. In this study, we propose PersLLM, integrating\npsychology-grounded principles of personality: social practice, consistency,\nand dynamic development, into a comprehensive training methodology. We\nincorporate personality traits directly into the model parameters, enhancing\nthe model's resistance to induction, promoting consistency, and supporting the\ndynamic evolution of personality. Single-agent evaluation validates our\nmethod's superiority, as it produces responses more aligned with reference\npersonalities compared to other approaches. Case studies for multi-agent\ncommunication highlight its benefits in enhancing opinion consistency within\nindividual agents and fostering collaborative creativity among multiple agents\nin dialogue contexts, potentially benefiting human simulation and multi-agent\ncooperation. Additionally, human-agent interaction evaluations indicate that\nour personified models significantly enhance interactive experiences,\nunderscoring the practical implications of our research.\n","authors":["Zheni Zeng","Jiayi Chen","Huimin Chen","Yukun Yan","Yuxuan Chen","Zhenghao Liu","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2407.12393v4.pdf","comment":"10 pages for main text, 5 figures"},{"id":"http://arxiv.org/abs/2408.04242v1","updated":"2024-08-08T06:08:04Z","published":"2024-08-08T06:08:04Z","title":"The Ungrounded Alignment Problem","summary":"  Modern machine learning systems have demonstrated substantial abilities with\nmethods that either embrace or ignore human-provided knowledge, but combining\nbenefits of both styles remains a challenge. One particular challenge involves\ndesigning learning systems that exhibit built-in responses to specific abstract\nstimulus patterns, yet are still plastic enough to be agnostic about the\nmodality and exact form of their inputs. In this paper, we investigate what we\ncall The Ungrounded Alignment Problem, which asks How can we build in\npredefined knowledge in a system where we don't know how a given stimulus will\nbe grounded? This paper examines a simplified version of the general problem,\nwhere an unsupervised learner is presented with a sequence of images for the\ncharacters in a text corpus, and this learner is later evaluated on its ability\nto recognize specific (possibly rare) sequential patterns. Importantly, the\nlearner is given no labels during learning or evaluation, but must map images\nfrom an unknown font or permutation to its correct class label. That is, at no\npoint is our learner given labeled images, where an image vector is explicitly\nassociated with a class label. Despite ample work in unsupervised and\nself-supervised loss functions, all current methods require a labeled\nfine-tuning phase to map the learned representations to correct classes.\nFinding this mapping in the absence of labels may seem a fool's errand, but our\nmain result resolves this seeming paradox. We show that leveraging only letter\nbigram frequencies is sufficient for an unsupervised learner both to reliably\nassociate images to class labels and to reliably identify trigger words in the\nsequence of inputs. More generally, this method suggests an approach for\nencoding specific desired innate behaviour in modality-agnostic models.\n","authors":["Marc Pickett","Aakash Kumar Nain","Joseph Modayil","Llion Jones"],"pdf_url":"https://arxiv.org/pdf/2408.04242v1.pdf","comment":"7 pages, plus references and appendix"},{"id":"http://arxiv.org/abs/2402.09085v3","updated":"2024-08-08T05:58:30Z","published":"2024-02-14T11:02:04Z","title":"Polynomial Semantics of Tractable Probabilistic Circuits","summary":"  Probabilistic circuits compute multilinear polynomials that represent\nmultivariate probability distributions. They are tractable models that support\nefficient marginal inference. However, various polynomial semantics have been\nconsidered in the literature (e.g., network polynomials, likelihood\npolynomials, generating functions, and Fourier transforms). The relationships\nbetween circuit representations of these polynomial encodings of distributions\nis largely unknown. In this paper, we prove that for distributions over binary\nvariables, each of these probabilistic circuit models is equivalent in the\nsense that any circuit for one of them can be transformed into a circuit for\nany of the others with only a polynomial increase in size. They are therefore\nall tractable for marginal inference on the same class of distributions.\nFinally, we explore the natural extension of one such polynomial semantics,\ncalled probabilistic generating circuits, to categorical random variables, and\nestablish that inference becomes #P-hard.\n","authors":["Oliver Broadrick","Honghua Zhang","Guy Van den Broeck"],"pdf_url":"https://arxiv.org/pdf/2402.09085v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.04095v2","updated":"2024-08-08T05:45:56Z","published":"2024-05-07T07:55:45Z","title":"DREAM: Combating Concept Drift with Explanatory Detection and Adaptation\n  in Malware Classification","summary":"  Deep learning-based malware classifiers face significant challenges due to\nconcept drift. The rapid evolution of malware, especially with new families,\ncan depress classification accuracy to near-random levels. Previous research\nhas primarily focused on detecting drift samples, relying on expert-led\nanalysis and labeling for model retraining. However, these methods often lack a\ncomprehensive understanding of malware concepts and provide limited guidance\nfor effective drift adaptation, leading to unstable detection performance and\nhigh human labeling costs. To address these limitations, we introduce DREAM, a\nnovel system designed to surpass the capabilities of existing drift detectors\nand to establish an explanatory drift adaptation process. DREAM enhances drift\ndetection through model sensitivity and data autonomy. The detector, trained in\na semi-supervised approach, proactively captures malware behavior concepts\nthrough classifier feedback. During testing, it utilizes samples generated by\nthe detector itself, eliminating reliance on extensive training data. For drift\nadaptation, DREAM enlarges human intervention, enabling revisions of malware\nlabels and concept explanations embedded within the detector's latent space. To\nensure a comprehensive response to concept drift, it facilitates a coordinated\nupdate process for both the classifier and the detector. Our evaluation shows\nthat DREAM can effectively improve the drift detection accuracy and reduce the\nexpert analysis effort in adaptation across different malware datasets and\nclassifiers.\n","authors":["Yiling He","Junchi Lei","Zhan Qin","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2405.04095v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04236v1","updated":"2024-08-08T05:43:20Z","published":"2024-08-08T05:43:20Z","title":"Cluster-Wide Task Slowdown Detection in Cloud System","summary":"  Slow task detection is a critical problem in cloud operation and maintenance\nsince it is highly related to user experience and can bring substantial\nliquidated damages. Most anomaly detection methods detect it from a single-task\naspect. However, considering millions of concurrent tasks in large-scale cloud\ncomputing clusters, it becomes impractical and inefficient. Moreover,\nsingle-task slowdowns are very common and do not necessarily indicate a\nmalfunction of a cluster due to its violent fluctuation nature in a virtual\nenvironment. Thus, we shift our attention to cluster-wide task slowdowns by\nutilizing the duration time distribution of tasks across a cluster, so that the\ncomputation complexity is not relevant to the number of tasks.\n  The task duration time distribution often exhibits compound periodicity and\nlocal exceptional fluctuations over time. Though transformer-based methods are\none of the most powerful methods to capture these time series normal variation\npatterns, we empirically find and theoretically explain the flaw of the\nstandard attention mechanism in reconstructing subperiods with low amplitude\nwhen dealing with compound periodicity.\n  To tackle these challenges, we propose SORN (i.e., Skimming Off subperiods in\ndescending amplitude order and Reconstructing Non-slowing fluctuation), which\nconsists of a Skimming Attention mechanism to reconstruct the compound\nperiodicity and a Neural Optimal Transport module to distinguish cluster-wide\nslowdowns from other exceptional fluctuations. Furthermore, since anomalies in\nthe training set are inevitable in a practical scenario, we propose a picky\nloss function, which adaptively assigns higher weights to reliable time slots\nin the training set. Extensive experiments demonstrate that SORN outperforms\nstate-of-the-art methods on multiple real-world industrial datasets.\n","authors":["Feiyi Chen","Yingying Zhang","Lunting Fan","Yuxuan Liang","Guansong Pang","Qingsong Wen","Shuiguang Deng"],"pdf_url":"https://arxiv.org/pdf/2408.04236v1.pdf","comment":"This paper has been accepted by KDD2024"},{"id":"http://arxiv.org/abs/2408.04229v1","updated":"2024-08-08T05:33:21Z","published":"2024-08-08T05:33:21Z","title":"Probabilistic Circuits for Cumulative Distribution Functions","summary":"  A probabilistic circuit (PC) succinctly expresses a function that represents\na multivariate probability distribution and, given sufficient structural\nproperties of the circuit, supports efficient probabilistic inference.\nTypically a PC computes the probability mass (or density) function (PMF or PDF)\nof the distribution. We consider PCs instead computing the cumulative\ndistribution function (CDF). We show that for distributions over binary random\nvariables these representations (PMF and CDF) are essentially equivalent, in\nthe sense that one can be transformed to the other in polynomial time. We then\nshow how a similar equivalence holds for distributions over finite discrete\nvariables using a modification of the standard encoding with binary variables\nthat aligns with the CDF semantics. Finally we show that for continuous\nvariables, smooth, decomposable PCs computing PDFs and CDFs can be efficiently\ntransformed to each other by modifying only the leaves of the circuit.\n","authors":["Oliver Broadrick","William Cao","Benjie Wang","Martin Trapp","Guy Van den Broeck"],"pdf_url":"https://arxiv.org/pdf/2408.04229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15143v2","updated":"2024-08-08T05:15:07Z","published":"2024-07-21T12:32:00Z","title":"Rethinking Feature Backbone Fine-tuning for Remote Sensing Object\n  Detection","summary":"  Recently, numerous methods have achieved impressive performance in remote\nsensing object detection, relying on convolution or transformer architectures.\nSuch detectors typically have a feature backbone to extract useful features\nfrom raw input images. For the remote sensing domain, a common practice among\ncurrent detectors is to initialize the backbone with pre-training on ImageNet\nconsisting of natural scenes. Fine-tuning the backbone is then typically\nrequired to generate features suitable for remote-sensing images. However, this\ncould hinder the extraction of basic visual features in long-term training,\nthus restricting performance improvement. To mitigate this issue, we propose a\nnovel method named DBF (Dynamic Backbone Freezing) for feature backbone\nfine-tuning on remote sensing object detection. Our method aims to handle the\ndilemma of whether the backbone should extract low-level generic features or\npossess specific knowledge of the remote sensing domain, by introducing a\nmodule called 'Freezing Scheduler' to dynamically manage the update of backbone\nfeatures during training. Extensive experiments on DOTA and DIOR-R show that\nour approach enables more accurate model learning while substantially reducing\ncomputational costs. Our method can be seamlessly adopted without additional\neffort due to its straightforward design.\n","authors":["Yechan Kim","JongHyun Park","SooYeon Kim","Moongu Jeon"],"pdf_url":"https://arxiv.org/pdf/2407.15143v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2408.04223v1","updated":"2024-08-08T05:14:07Z","published":"2024-08-08T05:14:07Z","title":"VideoQA in the Era of LLMs: An Empirical Study","summary":"  Video Large Language Models (Video-LLMs) are flourishing and has advanced\nmany video-language tasks. As a golden testbed, Video Question Answering\n(VideoQA) plays pivotal role in Video-LLM developing. This work conducts a\ntimely and comprehensive study of Video-LLMs' behavior in VideoQA, aiming to\nelucidate their success and failure modes, and provide insights towards more\nhuman-like video understanding and question answering. Our analyses demonstrate\nthat Video-LLMs excel in VideoQA; they can correlate contextual cues and\ngenerate plausible responses to questions about varied video contents. However,\nmodels falter in handling video temporality, both in reasoning about temporal\ncontent ordering and grounding QA-relevant temporal moments. Moreover, the\nmodels behave unintuitively - they are unresponsive to adversarial video\nperturbations while being sensitive to simple variations of candidate answers\nand questions. Also, they do not necessarily generalize better. The findings\ndemonstrate Video-LLMs' QA capability in standard condition yet highlight their\nsevere deficiency in robustness and interpretability, suggesting the urgent\nneed on rationales in Video-LLM developing.\n","authors":["Junbin Xiao","Nanxin Huang","Hangyu Qin","Dongyang Li","Yicong Li","Fengbin Zhu","Zhulin Tao","Jianxing Yu","Liang Lin","Tat-Seng Chua","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2408.04223v1.pdf","comment":"Preprint. Under Review"},{"id":"http://arxiv.org/abs/2408.04221v1","updated":"2024-08-08T05:09:02Z","published":"2024-08-08T05:09:02Z","title":"Connective Viewpoints of Signal-to-Noise Diffusion Models","summary":"  Diffusion models (DM) have become fundamental components of generative\nmodels, excelling across various domains such as image creation, audio\ngeneration, and complex data interpolation. Signal-to-Noise diffusion models\nconstitute a diverse family covering most state-of-the-art diffusion models.\nWhile there have been several attempts to study Signal-to-Noise (S2N) diffusion\nmodels from various perspectives, there remains a need for a comprehensive\nstudy connecting different viewpoints and exploring new perspectives. In this\nstudy, we offer a comprehensive perspective on noise schedulers, examining\ntheir role through the lens of the signal-to-noise ratio (SNR) and its\nconnections to information theory. Building upon this framework, we have\ndeveloped a generalized backward equation to enhance the performance of the\ninference process.\n","authors":["Khanh Doan","Long Tung Vuong","Tuan Nguyen","Anh Tuan Bui","Quyen Tran","Thanh-Toan Do","Dinh Phung","Trung Le"],"pdf_url":"https://arxiv.org/pdf/2408.04221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04216v1","updated":"2024-08-08T04:52:10Z","published":"2024-08-08T04:52:10Z","title":"Attention Mechanism and Context Modeling System for Text Mining Machine\n  Translation","summary":"  This paper advances a novel architectural schema anchored upon the\nTransformer paradigm and innovatively amalgamates the K-means categorization\nalgorithm to augment the contextual apprehension capabilities of the schema.\nThe transformer model performs well in machine translation tasks due to its\nparallel computing power and multi-head attention mechanism. However, it may\nencounter contextual ambiguity or ignore local features when dealing with\nhighly complex language structures. To circumvent this constraint, this\nexposition incorporates the K-Means algorithm, which is used to stratify the\nlexis and idioms of the input textual matter, thereby facilitating superior\nidentification and preservation of the local structure and contextual\nintelligence of the language. The advantage of this combination is that K-Means\ncan automatically discover the topic or concept regions in the text, which may\nbe directly related to translation quality. Consequently, the schema contrived\nherein enlists K-Means as a preparatory phase antecedent to the Transformer and\nrecalibrates the multi-head attention weights to assist in the discrimination\nof lexis and idioms bearing analogous semantics or functionalities. This\nensures the schema accords heightened regard to the contextual intelligence\nembodied by these clusters during the training phase, rather than merely\nfocusing on locational intelligence.\n","authors":["Shi Bo","Yuwei Zhang","Junming Huang","Sitong Liu","Zexi Chen","Zizheng Li"],"pdf_url":"https://arxiv.org/pdf/2408.04216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.06373v4","updated":"2024-08-08T04:47:20Z","published":"2024-05-10T10:19:14Z","title":"LLM Discussion: Enhancing the Creativity of Large Language Models via\n  Discussion Framework and Role-Play","summary":"  Large language models (LLMs) have shown exceptional proficiency in natural\nlanguage processing but often fall short of generating creative and original\nresponses to open-ended questions. To enhance LLM creativity, our key insight\nis to emulate the human process of inducing collective creativity through\nengaging discussions with participants from diverse backgrounds and\nperspectives. To this end, we propose LLM Discussion, a three-phase discussion\nframework that facilitates vigorous and diverging idea exchanges and ensures\nconvergence to creative answers. Moreover, we adopt a role-playing technique by\nassigning distinct roles to LLMs to combat the homogeneity of LLMs. We evaluate\nthe efficacy of the proposed framework with the Alternative Uses Test,\nSimilarities Test, Instances Test, and Scientific Creativity Test through both\nLLM evaluation and human study. The results show that our proposed framework\noutperforms single-LLM approaches and existing multi-LLM frameworks across\nvarious creativity metrics. The code is available at\nhttps://github.com/lawraa/LLM-Discussion.\n","authors":["Li-Chun Lu","Shou-Jen Chen","Tsung-Min Pai","Chan-Hung Yu","Hung-yi Lee","Shao-Hua Sun"],"pdf_url":"https://arxiv.org/pdf/2405.06373v4.pdf","comment":"40 pages, 9 figures, COLM 2024"},{"id":"http://arxiv.org/abs/2408.03541v2","updated":"2024-08-08T04:35:23Z","published":"2024-08-07T04:38:38Z","title":"EXAONE 3.0 7.8B Instruction Tuned Language Model","summary":"  We introduce EXAONE 3.0 instruction-tuned language model, the first open\nmodel in the family of Large Language Models (LLMs) developed by LG AI\nResearch. Among different model sizes, we publicly release the 7.8B\ninstruction-tuned model to promote open research and innovations. Through\nextensive evaluations across a wide range of public and in-house benchmarks,\nEXAONE 3.0 demonstrates highly competitive real-world performance with\ninstruction-following capability against other state-of-the-art open models of\nsimilar size. Our comparative analysis shows that EXAONE 3.0 excels\nparticularly in Korean, while achieving compelling performance across general\ntasks and complex reasoning. With its strong real-world effectiveness and\nbilingual proficiency, we hope that EXAONE keeps contributing to advancements\nin Expert AI. Our EXAONE 3.0 instruction-tuned model is available at\nhttps://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\n","authors":["LG AI Research"," :","Soyoung An","Kyunghoon Bae","Eunbi Choi","Stanley Jungkyu Choi","Yemuk Choi","Seokhee Hong","Yeonjung Hong","Junwon Hwang","Hyojin Jeon","Gerrard Jeongwon Jo","Hyunjik Jo","Jiyeon Jung","Yountae Jung","Euisoon Kim","Hyosang Kim","Joonkee Kim","Seonghwan Kim","Soyeon Kim","Sunkyoung Kim","Yireun Kim","Youchul Kim","Edward Hwayoung Lee","Haeju Lee","Honglak Lee","Jinsik Lee","Kyungmin Lee","Moontae Lee","Seungjun Lee","Woohyung Lim","Sangha Park","Sooyoun Park","Yongmin Park","Boseong Seo","Sihoon Yang","Heuiyeen Yeen","Kyungjae Yoo","Hyeongu Yun"],"pdf_url":"https://arxiv.org/pdf/2408.03541v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.11528v3","updated":"2024-08-08T04:18:34Z","published":"2021-06-22T03:44:03Z","title":"Recent Deep Semi-supervised Learning Approaches and Related Works","summary":"  This work proposes an overview of the recent semi-supervised learning\napproaches and related works. Despite the remarkable success of neural networks\nin various applications, there exist a few formidable constraints, including\nthe need for a large amount of labeled data. Therefore, semi-supervised\nlearning, which is a learning scheme in which scarce labels and a larger amount\nof unlabeled data are utilized to train models (e.g., deep neural networks), is\ngetting more important. Based on the key assumptions of semi-supervised\nlearning, which are the manifold assumption, cluster assumption, and continuity\nassumption, the work reviews the recent semi-supervised learning approaches. In\nparticular, the methods in regard to using deep neural networks in a\nsemi-supervised learning setting are primarily discussed. In addition, the\nexisting works are first classified based on the underlying idea and explained,\nthen the holistic approaches that unify the aforementioned ideas are detailed.\n","authors":["Gyeongho Kim"],"pdf_url":"https://arxiv.org/pdf/2106.11528v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04353v5","updated":"2024-08-08T04:15:31Z","published":"2023-10-06T16:21:22Z","title":"An In-Context Learning Agent for Formal Theorem-Proving","summary":"  We present an in-context learning agent for formal theorem-proving in\nenvironments like Lean and Coq. Current state-of-the-art models for the problem\nare finetuned on environment-specific proof data. By contrast, our approach,\ncalled COPRA, repeatedly asks a high-capacity, general-purpose large language\nmodel (GPT-4) to propose tactic applications from within a stateful\nbacktracking search. Proposed tactics are executed in the underlying proof\nenvironment. Feedback from the execution is used to build the prompt for the\nnext model query, along with selected information from the search history and\nlemmas retrieved from an external database. We evaluate our implementation of\nCOPRA on the miniF2F benchmark for Lean and a set of Coq tasks from the\nCompCert project. On these benchmarks, COPRA significantly outperforms few-shot\ninvocations of GPT-4. It also compares favorably against finetuning-based\napproaches, outperforming ReProver, a state-of-the-art finetuned approach for\nLean, in terms of the pass@1 metric. Our code and data are available at\nhttps://github.com/trishullab/copra.\n","authors":["Amitayush Thakur","George Tsoukalas","Yeming Wen","Jimmy Xin","Swarat Chaudhuri"],"pdf_url":"https://arxiv.org/pdf/2310.04353v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04203v1","updated":"2024-08-08T03:57:20Z","published":"2024-08-08T03:57:20Z","title":"MMRole: A Comprehensive Framework for Developing and Evaluating\n  Multimodal Role-Playing Agents","summary":"  Recently, Role-Playing Agents (RPAs) have garnered increasing attention for\ntheir potential to deliver emotional value and facilitate sociological\nresearch. However, existing studies are primarily confined to the textual\nmodality, unable to simulate humans' multimodal perceptual capabilities. To\nbridge this gap, we introduce the concept of Multimodal Role-Playing Agents\n(MRPAs), and propose a comprehensive framework, MMRole, for their development\nand evaluation, which comprises a personalized multimodal dataset and a robust\nevaluation method. Specifically, we construct a large-scale, high-quality\ndataset, MMRole-Data, consisting of 85 characters, 11K images, and 14K single\nor multi-turn dialogues. Additionally, we present a robust evaluation method,\nMMRole-Eval, encompassing eight metrics across three dimensions, where a reward\nmodel is trained to score MRPAs with the constructed ground-truth data for\ncomparison. Moreover, we develop the first specialized MRPA, MMRole-Agent.\nExtensive evaluation results demonstrate the improved performance of\nMMRole-Agent and highlight the primary challenges in developing MRPAs,\nemphasizing the need for enhanced multimodal understanding and role-playing\nconsistency. The data, code, and models will be available at\nhttps://github.com/YanqiDai/MMRole.\n","authors":["Yanqi Dai","Huanran Hu","Lei Wang","Shengjie Jin","Xu Chen","Zhiwu Lu"],"pdf_url":"https://arxiv.org/pdf/2408.04203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03528v2","updated":"2024-08-08T03:52:06Z","published":"2024-08-07T03:48:07Z","title":"Exploring the extent of similarities in software failures across\n  industries using LLMs","summary":"  The rapid evolution of software development necessitates enhanced safety\nmeasures. Extracting information about software failures from companies is\nbecoming increasingly more available through news articles.\n  This research utilizes the Failure Analysis Investigation with LLMs (FAIL)\nmodel to extract industry-specific information. Although the FAIL model's\ndatabase is rich in information, it could benefit from further categorization\nand industry-specific insights to further assist software engineers.\n  In previous work news articles were collected from reputable sources and\ncategorized by incidents inside a database. Prompt engineering and Large\nLanguage Models (LLMs) were then applied to extract relevant information\nregarding the software failure. This research extends these methods by\ncategorizing articles into specific domains and types of software failures. The\nresults are visually represented through graphs.\n  The analysis shows that throughout the database some software failures occur\nsignificantly more often in specific industries. This categorization provides a\nvaluable resource for software engineers and companies to identify and address\ncommon failures.\n  This research highlights the synergy between software engineering and Large\nLanguage Models (LLMs) to automate and enhance the analysis of software\nfailures. By transforming data from the database into an industry specific\nmodel, we provide a valuable resource that can be used to identify common\nvulnerabilities, predict potential risks, and implement proactive measures for\npreventing software failures. Leveraging the power of the current FAIL database\nand data visualization, we aim to provide an avenue for safer and more secure\nsoftware in the future.\n","authors":["Martin Detloff"],"pdf_url":"https://arxiv.org/pdf/2408.03528v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10385v4","updated":"2024-08-08T03:44:21Z","published":"2023-12-16T08:48:46Z","title":"Imitate the Good and Avoid the Bad: An Incremental Approach to Safe\n  Reinforcement Learning","summary":"  A popular framework for enforcing safe actions in Reinforcement Learning (RL)\nis Constrained RL, where trajectory based constraints on expected cost (or\nother cost measures) are employed to enforce safety and more importantly these\nconstraints are enforced while maximizing expected reward. Most recent\napproaches for solving Constrained RL convert the trajectory based cost\nconstraint into a surrogate problem that can be solved using minor\nmodifications to RL methods. A key drawback with such approaches is an over or\nunderestimation of the cost constraint at each state. Therefore, we provide an\napproach that does not modify the trajectory based cost constraint and instead\nimitates ``good'' trajectories and avoids ``bad'' trajectories generated from\nincrementally improving policies. We employ an oracle that utilizes a reward\nthreshold (which is varied with learning) and the overall cost constraint to\nlabel trajectories as ``good'' or ``bad''. A key advantage of our approach is\nthat we are able to work from any starting policy or set of trajectories and\nimprove on it. In an exhaustive set of experiments, we demonstrate that our\napproach is able to outperform top benchmark approaches for solving Constrained\nRL problems, with respect to expected cost, CVaR cost, or even unknown cost\nconstraints.\n","authors":["Huy Hoang","Tien Mai","Pradeep Varakantham"],"pdf_url":"https://arxiv.org/pdf/2312.10385v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04197v1","updated":"2024-08-08T03:35:35Z","published":"2024-08-08T03:35:35Z","title":"Pairwise Judgment Formulation for Semantic Embedding Model in Web Search","summary":"  Semantic Embedding Model (SEM), a neural network-based Siamese architecture,\nis gaining momentum in information retrieval and natural language processing.\nIn order to train SEM in a supervised fashion for Web search, the search engine\nquery log is typically utilized to automatically formulate pairwise judgments\nas training data. Despite the growing application of semantic embeddings in the\nsearch engine industry, little work has been done on formulating effective\npairwise judgments for training SEM. In this paper, we make the first in-depth\ninvestigation of a wide range of strategies for generating pairwise judgments\nfor SEM. An interesting (perhaps surprising) discovery reveals that the\nconventional pairwise judgment formulation strategy wildly used in the field of\npairwise Learning-to-Rank (LTR) is not necessarily effective for training SEM.\nThrough a large-scale empirical study based on query logs and click-through\nactivities from a major commercial search engine, we demonstrate the effective\nstrategies for SEM and highlight the advantages of a hybrid heuristic (i.e.,\nClicked > Non-Clicked) in comparison to the atomic heuristics (e.g., Clicked >\nSkipped) in LTR. We conclude with best practices for training SEM and offer\npromising insights for future research.\n","authors":["Mengze Hong","Chen Jason Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.04197v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04193v1","updated":"2024-08-08T03:25:41Z","published":"2024-08-08T03:25:41Z","title":"Uncertainty-Aware Crime Prediction With Spatial Temporal Multivariate\n  Graph Neural Networks","summary":"  Crime forecasting is a critical component of urban analysis and essential for\nstabilizing society today. Unlike other time series forecasting problems, crime\nincidents are sparse, particularly in small regions and within specific time\nperiods. Traditional spatial-temporal deep learning models often struggle with\nthis sparsity, as they typically cannot effectively handle the non-Gaussian\nnature of crime data, which is characterized by numerous zeros and\nover-dispersed patterns. To address these challenges, we introduce a novel\napproach termed Spatial Temporal Multivariate Zero-Inflated Negative Binomial\nGraph Neural Networks (STMGNN-ZINB). This framework leverages diffusion and\nconvolution networks to analyze spatial, temporal, and multivariate\ncorrelations, enabling the parameterization of probabilistic distributions of\ncrime incidents. By incorporating a Zero-Inflated Negative Binomial model,\nSTMGNN-ZINB effectively manages the sparse nature of crime data, enhancing\nprediction accuracy and the precision of confidence intervals. Our evaluation\non real-world datasets confirms that STMGNN-ZINB outperforms existing models,\nproviding a more reliable tool for predicting and understanding crime dynamics.\n","authors":["Zepu Wang","Xiaobo Ma","Huajie Yang","Weimin Lvu","Peng Sun","Sharath Chandra Guntuku"],"pdf_url":"https://arxiv.org/pdf/2408.04193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12539v3","updated":"2024-08-08T03:20:17Z","published":"2023-08-24T03:53:55Z","title":"CALM : A Multi-task Benchmark for Comprehensive Assessment of Language\n  Model Bias","summary":"  As language models (LMs) become increasingly powerful and widely used, it is\nimportant to quantify them for sociodemographic bias with potential for harm.\nPrior measures of bias are sensitive to perturbations in the templates designed\nto compare performance across social groups, due to factors such as low\ndiversity or limited number of templates. Also, most previous work considers\nonly one NLP task. We introduce Comprehensive Assessment of Language Models\n(CALM) for robust measurement of two types of universally relevant\nsociodemographic bias, gender and race. CALM integrates sixteen datasets for\nquestion-answering, sentiment analysis and natural language inference. Examples\nfrom each dataset are filtered to produce 224 templates with high diversity\n(e.g., length, vocabulary). We assemble 50 highly frequent person names for\neach of seven distinct demographic groups to generate 78,400 prompts covering\nthe three NLP tasks. Our empirical evaluation shows that CALM bias scores are\nmore robust and far less sensitive than previous bias measurements to\nperturbations in the templates, such as synonym substitution, or to random\nsubset selection of templates. We apply CALM to 20 large language models, and\nfind that for 2 language model series, larger parameter models tend to be more\nbiased than smaller ones. The T0 series is the least biased model families, of\nthe 20 LLMs investigated here. The code is available at\nhttps://github.com/vipulgupta1011/CALM.\n","authors":["Vipul Gupta","Pranav Narayanan Venkit","Hugo Laurençon","Shomir Wilson","Rebecca J. Passonneau"],"pdf_url":"https://arxiv.org/pdf/2308.12539v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06555v3","updated":"2024-08-08T03:20:05Z","published":"2023-11-11T12:05:01Z","title":"LLMs Learn Task Heuristics from Demonstrations: A Heuristic-Driven\n  Prompting Strategy for Document-Level Event Argument Extraction","summary":"  In this study, we investigate in-context learning (ICL) in document-level\nevent argument extraction (EAE) to alleviate the dependency on large-scale\nlabeled data for this task. We introduce the Heuristic-Driven Link-of-Analogy\n(HD-LoA) prompting to address the challenge of example selection and to develop\na prompting strategy tailored for EAE. Specifically, we hypothesize and\nvalidate that LLMs learn task-specific heuristics from demonstrations via ICL.\nBuilding upon this hypothesis, we introduce an explicit heuristic-driven\ndemonstration construction approach, which transforms the haphazard example\nselection process into a methodical method that emphasizes task heuristics.\nAdditionally, inspired by the analogical reasoning of human, we propose the\nlink-of-analogy prompting, which enables LLMs to process new situations by\ndrawing analogies to known situations, enhancing their performance on unseen\nclasses beyond limited ICL examples. Experiments show that our method\noutperforms existing prompting methods and few-shot supervised learning methods\non document-level EAE datasets. Additionally, the HD-LoA prompting shows\neffectiveness in diverse tasks like sentiment analysis and natural language\ninference, demonstrating its broad adaptability.\n","authors":["Hanzhang Zhou","Junlang Qian","Zijian Feng","Hui Lu","Zixiao Zhu","Kezhi Mao"],"pdf_url":"https://arxiv.org/pdf/2311.06555v3.pdf","comment":"Accepted to ACL 2024"},{"id":"http://arxiv.org/abs/2408.04190v1","updated":"2024-08-08T03:18:42Z","published":"2024-08-08T03:18:42Z","title":"Listwise Reward Estimation for Offline Preference-based Reinforcement\n  Learning","summary":"  In Reinforcement Learning (RL), designing precise reward functions remains to\nbe a challenge, particularly when aligning with human intent. Preference-based\nRL (PbRL) was introduced to address this problem by learning reward models from\nhuman feedback. However, existing PbRL methods have limitations as they often\noverlook the second-order preference that indicates the relative strength of\npreference. In this paper, we propose Listwise Reward Estimation (LiRE), a\nnovel approach for offline PbRL that leverages second-order preference\ninformation by constructing a Ranked List of Trajectories (RLT), which can be\nefficiently built by using the same ternary feedback type as traditional\nmethods. To validate the effectiveness of LiRE, we propose a new offline PbRL\ndataset that objectively reflects the effect of the estimated rewards. Our\nextensive experiments on the dataset demonstrate the superiority of LiRE, i.e.,\noutperforming state-of-the-art baselines even with modest feedback budgets and\nenjoying robustness with respect to the number of feedbacks and feedback noise.\nOur code is available at https://github.com/chwoong/LiRE\n","authors":["Heewoong Choi","Sangwon Jung","Hongjoon Ahn","Taesup Moon"],"pdf_url":"https://arxiv.org/pdf/2408.04190v1.pdf","comment":"21 pages, ICML 2024"},{"id":"http://arxiv.org/abs/2408.04181v1","updated":"2024-08-08T02:57:55Z","published":"2024-08-08T02:57:55Z","title":"EdgeShield: A Universal and Efficient Edge Computing Framework for\n  Robust AI","summary":"  The increasing prevalence of adversarial attacks on Artificial Intelligence\n(AI) systems has created a need for innovative security measures. However, the\ncurrent methods of defending against these attacks often come with a high\ncomputing cost and require back-end processing, making real-time defense\nchallenging. Fortunately, there have been remarkable advancements in\nedge-computing, which make it easier to deploy neural networks on edge devices.\nBuilding upon these advancements, we propose an edge framework design to enable\nuniversal and efficient detection of adversarial attacks. This framework\nincorporates an attention-based adversarial detection methodology and a\nlightweight detection network formation, making it suitable for a wide range of\nneural networks and can be deployed on edge devices. To assess the\neffectiveness of our proposed framework, we conducted evaluations on five\nneural networks. The results indicate an impressive 97.43% F-score can be\nachieved, demonstrating the framework's proficiency in detecting adversarial\nattacks. Moreover, our proposed framework also exhibits significantly reduced\ncomputing complexity and cost in comparison to previous detection methods. This\naspect is particularly beneficial as it ensures that the defense mechanism can\nbe efficiently implemented in real-time on-edge devices.\n","authors":["Duo Zhong","Bojing Li","Xiang Chen","Chenchen Liu"],"pdf_url":"https://arxiv.org/pdf/2408.04181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00833v2","updated":"2024-08-08T02:55:34Z","published":"2024-06-02T18:47:08Z","title":"Harvard Undergraduate Survey on Generative AI","summary":"  How has generative AI impacted the experiences of college students? We study\nthe influence of AI on the study habits, class choices, and career prospects of\nHarvard undergraduates (n=326), finding that almost 90% of students use\ngenerative AI. For roughly 25% of these students, AI has begun to substitute\nfor attending office hours and completing required readings. Half of students\nare concerned that AI will negatively impact their job prospects, and over half\nof students wish that Harvard had more classes on the future impacts of AI. We\nalso investigate students' outlook on the broader social implications of AI,\nfinding that half of students are worried that AI will increase economic\ninequality, and 40% believe that extinction risk from AI should be treated as a\nglobal priority with the same urgency as pandemics and nuclear war. Around half\nof students who have taken a class on AI expect AI to exceed human capabilities\non almost all tasks within 30 years. We make some recommendations to the\nHarvard community in light of these results.\n","authors":["Shikoh Hirabayashi","Rishab Jain","Nikola Jurković","Gabriel Wu"],"pdf_url":"https://arxiv.org/pdf/2406.00833v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04174v1","updated":"2024-08-08T02:36:04Z","published":"2024-08-08T02:36:04Z","title":"wav2graph: A Framework for Supervised Learning Knowledge Graph from\n  Speech","summary":"  Knowledge graphs (KGs) enhance the performance of large language models\n(LLMs) and search engines by providing structured, interconnected data that\nimproves reasoning and context-awareness. However, KGs only focus on text data,\nthereby neglecting other modalities such as speech. In this work, we introduce\nwav2graph, the first framework for supervised learning knowledge graph from\nspeech data. Our pipeline are straightforward: (1) constructing a KG based on\ntranscribed spoken utterances and a named entity database, (2) converting KG\ninto embedding vectors, and (3) training graph neural networks (GNNs) for node\nclassification and link prediction tasks. Through extensive experiments\nconducted in inductive and transductive learning contexts using\nstate-of-the-art GNN models, we provide baseline results and error analysis for\nnode classification and link prediction tasks on human transcripts and\nautomatic speech recognition (ASR) transcripts, including evaluations using\nboth encoder-based and decoder-based node embeddings, as well as monolingual\nand multilingual acoustic pre-trained models. All related code, data, and\nmodels are published online.\n","authors":["Khai Le-Duc","Quy-Anh Dang","Tan-Hanh Pham","Truong-Son Hy"],"pdf_url":"https://arxiv.org/pdf/2408.04174v1.pdf","comment":"Preprint, 32 pages"},{"id":"http://arxiv.org/abs/2408.04168v1","updated":"2024-08-08T02:28:43Z","published":"2024-08-08T02:28:43Z","title":"Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City\n  Navigation without Instructions","summary":"  This paper considers a scenario in city navigation: an AI agent is provided\nwith language descriptions of the goal location with respect to some well-known\nlandmarks; By only observing the scene around, including recognizing landmarks\nand road network connections, the agent has to make decisions to navigate to\nthe goal location without instructions. This problem is very challenging,\nbecause it requires agent to establish self-position and acquire spatial\nrepresentation of complex urban environment, where landmarks are often\ninvisible. In the absence of navigation instructions, such abilities are vital\nfor the agent to make high-quality decisions in long-range city navigation.\nWith the emergent reasoning ability of large language models (LLMs), a tempting\nbaseline is to prompt LLMs to \"react\" on each observation and make decisions\naccordingly. However, this baseline has very poor performance that the agent\noften repeatedly visits same locations and make short-sighted, inconsistent\ndecisions. To address these issues, this paper introduces a novel agentic\nworkflow featured by its abilities to perceive, reflect and plan. Specifically,\nwe find LLaVA-7B can be fine-tuned to perceive the direction and distance of\nlandmarks with sufficient accuracy for city navigation. Moreover, reflection is\nachieved through a memory mechanism, where past experiences are stored and can\nbe retrieved with current perception for effective decision argumentation.\nPlanning uses reflection results to produce long-term plans, which can avoid\nshort-sighted decisions in long-range navigation. We show the designed workflow\nsignificantly improves navigation ability of the LLM agent compared with the\nstate-of-the-art baselines.\n","authors":["Qingbin Zeng","Qinglong Yang","Shunan Dong","Heming Du","Liang Zheng","Fengli Xu","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2408.04168v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12668v2","updated":"2024-08-08T02:13:06Z","published":"2023-11-21T15:20:48Z","title":"From Concept to Manufacturing: Evaluating Vision-Language Models for\n  Engineering Design","summary":"  Engineering design is undergoing a transformative shift with the advent of\nAI, marking a new era in how we approach product, system, and service planning.\nLarge language models have demonstrated impressive capabilities in enabling\nthis shift. Yet, with text as their only input modality, they cannot leverage\nthe large body of visual artifacts that engineers have used for centuries and\nare accustomed to. This gap is addressed with the release of multimodal\nvision-language models (VLMs), such as GPT-4V, enabling AI to impact many more\ntypes of tasks. Our work presents a comprehensive evaluation of VLMs across a\nspectrum of engineering design tasks, categorized into four main areas:\nConceptual Design, System-Level and Detailed Design, Manufacturing and\nInspection, and Engineering Education Tasks. Specifically in this paper, we\nassess the capabilities of two VLMs, GPT-4V and LLaVA 1.6 34B, in design tasks\nsuch as sketch similarity analysis, CAD generation, topology optimization,\nmanufacturability assessment, and engineering textbook problems. Through this\nstructured evaluation, we not only explore VLMs' proficiency in handling\ncomplex design challenges but also identify their limitations in complex\nengineering design applications. Our research establishes a foundation for\nfuture assessments of vision language models. It also contributes a set of\nbenchmark testing datasets, with more than 1000 queries, for ongoing\nadvancements and applications in this field.\n","authors":["Cyril Picard","Kristen M. Edwards","Anna C. Doris","Brandon Man","Giorgio Giannone","Md Ferdous Alam","Faez Ahmed"],"pdf_url":"https://arxiv.org/pdf/2311.12668v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15633v3","updated":"2024-08-08T02:11:02Z","published":"2024-04-24T03:51:26Z","title":"Artificial Intelligence for Multi-Unit Auction design","summary":"  Understanding bidding behavior in multi-unit auctions remains an ongoing\nchallenge for researchers. Despite their widespread use, theoretical insights\ninto the bidding behavior, revenue ranking, and efficiency of commonly used\nmulti-unit auctions are limited. This paper utilizes artificial intelligence,\nspecifically reinforcement learning, as a model free learning approach to\nsimulate bidding in three prominent multi-unit auctions employed in practice.\nWe introduce six algorithms that are suitable for learning and bidding in\nmulti-unit auctions and compare them using an illustrative example. This paper\nunderscores the significance of using artificial intelligence in auction\ndesign, particularly in enhancing the design of multi-unit auctions.\n","authors":["Peyman Khezr","Kendall Taylor"],"pdf_url":"https://arxiv.org/pdf/2404.15633v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.07636v3","updated":"2024-08-08T01:48:32Z","published":"2023-07-14T21:27:00Z","title":"Dissenting Explanations: Leveraging Disagreement to Reduce Model\n  Overreliance","summary":"  While explainability is a desirable characteristic of increasingly complex\nblack-box models, modern explanation methods have been shown to be inconsistent\nand contradictory. The semantics of explanations is not always fully understood\n- to what extent do explanations \"explain\" a decision and to what extent do\nthey merely advocate for a decision? Can we help humans gain insights from\nexplanations accompanying correct predictions and not over-rely on incorrect\npredictions advocated for by explanations? With this perspective in mind, we\nintroduce the notion of dissenting explanations: conflicting predictions with\naccompanying explanations. We first explore the advantage of dissenting\nexplanations in the setting of model multiplicity, where multiple models with\nsimilar performance may have different predictions. In such cases, providing\ndissenting explanations could be done by invoking the explanations of\ndisagreeing models. Through a pilot study, we demonstrate that dissenting\nexplanations reduce overreliance on model predictions, without reducing overall\naccuracy. Motivated by the utility of dissenting explanations we present both\nglobal and local methods for their generation.\n","authors":["Omer Reingold","Judy Hanwen Shen","Aditi Talati"],"pdf_url":"https://arxiv.org/pdf/2307.07636v3.pdf","comment":"V2: AAAI 2024 V1: AI & HCI Workshop at ICML 2023"},{"id":"http://arxiv.org/abs/2311.08817v2","updated":"2024-08-08T01:46:19Z","published":"2023-11-15T09:38:53Z","title":"MAP's not dead yet: Uncovering true language model modes by conditioning\n  away degeneracy","summary":"  It has been widely observed that exact or approximate MAP (mode-seeking)\ndecoding from natural language generation (NLG) models consistently leads to\ndegenerate outputs (Holtzman et al., 2019; Stahlberg and Byrne, 2019). Prior\nwork has attributed this behavior to either a fundamental and unavoidable\ninadequacy of modes in probabilistic models or weaknesses in language modeling.\nContrastingly, we argue that degenerate modes can even occur in the absence of\nany modeling error, due to contamination of the training data. Specifically, we\nargue that mixing even a tiny amount of low-entropy noise with a population\ntext distribution can cause the data distribution's mode to become degenerate.\nWe therefore propose to apply MAP decoding to the model's true conditional\ndistribution where the conditioning variable explicitly avoids specific\ndegenerate behavior. Using exact search, we empirically verify that the\nlength-conditional modes of machine translation models and language models are\nindeed more fluent and topical than their unconditional modes. For the first\ntime, we also share many examples of exact modal sequences from these models,\nand from several variants of the LLaMA-7B model. Notably, we observe that\nvarious kinds of degenerate modes persist, even at the scale of LLaMA-7B.\nAlthough we cannot tractably address these degeneracies with exact search, we\nperform a classifier-based approximate search on LLaMA-7B, a model which was\nnot trained for instruction following, and find that we are able to elicit\nreasonable outputs without any finetuning.\n","authors":["Davis Yoshida","Kartik Goyal","Kevin Gimpel"],"pdf_url":"https://arxiv.org/pdf/2311.08817v2.pdf","comment":"52 pages, 5 figures, ACL version"},{"id":"http://arxiv.org/abs/2408.04154v1","updated":"2024-08-08T01:42:31Z","published":"2024-08-08T01:42:31Z","title":"The Data Addition Dilemma","summary":"  In many machine learning for healthcare tasks, standard datasets are\nconstructed by amassing data across many, often fundamentally dissimilar,\nsources. But when does adding more data help, and when does it hinder progress\non desired model outcomes in real-world settings? We identify this situation as\nthe \\textit{Data Addition Dilemma}, demonstrating that adding training data in\nthis multi-source scaling context can at times result in reduced overall\naccuracy, uncertain fairness outcomes, and reduced worst-subgroup performance.\nWe find that this possibly arises from an empirically observed trade-off\nbetween model performance improvements due to data scaling and model\ndeterioration from distribution shift. We thus establish baseline strategies\nfor navigating this dilemma, introducing distribution shift heuristics to guide\ndecision-making on which data sources to add in data scaling, in order to yield\nthe expected model performance improvements. We conclude with a discussion of\nthe required considerations for data collection and suggestions for studying\ndata composition and scale in the age of increasingly larger models.\n","authors":["Judy Hanwen Shen","Inioluwa Deborah Raji","Irene Y. Chen"],"pdf_url":"https://arxiv.org/pdf/2408.04154v1.pdf","comment":"Machine Learning For Health Care 2024 (MLHC)"},{"id":"http://arxiv.org/abs/2407.04831v2","updated":"2024-08-08T01:01:47Z","published":"2024-07-05T19:37:37Z","title":"Code Hallucination","summary":"  Generative models such as large language models are extensively used as code\ncopilots and for whole program generation. However, the programs they generate\noften have questionable correctness, authenticity and reliability in terms of\nintegration as they might not follow the user requirements, provide incorrect\nand/or nonsensical outputs, or even contain semantic/syntactic errors - overall\nknown as LLM hallucination. In this work, we present several types of code\nhallucination. We have generated such hallucinated code manually using large\nlanguage models. We also present a technique - HallTrigger, in order to\ndemonstrate efficient ways of generating arbitrary code hallucination. Our\nmethod leverages 3 different dynamic attributes of LLMs to craft prompts that\ncan successfully trigger hallucinations from models without the need to access\nmodel architecture or parameters. Results from popular blackbox models suggest\nthat HallTrigger is indeed effective and the pervasive LLM hallucination have\nsheer impact on software development.\n","authors":["Mirza Masfiqur Rahman","Ashish Kundu"],"pdf_url":"https://arxiv.org/pdf/2407.04831v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2408.04619v1","updated":"2024-08-08T17:49:07Z","published":"2024-08-08T17:49:07Z","title":"Transformer Explainer: Interactive Learning of Text-Generative Models","summary":"  Transformers have revolutionized machine learning, yet their inner workings\nremain opaque to many. We present Transformer Explainer, an interactive\nvisualization tool designed for non-experts to learn about Transformers through\nthe GPT-2 model. Our tool helps users understand complex Transformer concepts\nby integrating a model overview and enabling smooth transitions across\nabstraction levels of mathematical operations and model structures. It runs a\nlive GPT-2 instance locally in the user's browser, empowering users to\nexperiment with their own input and observe in real-time how the internal\ncomponents and parameters of the Transformer work together to predict the next\ntokens. Our tool requires no installation or special hardware, broadening the\npublic's education access to modern generative AI techniques. Our open-sourced\ntool is available at https://poloclub.github.io/transformer-explainer/. A video\ndemo is available at https://youtu.be/ECR4oAwocjs.\n","authors":["Aeree Cho","Grace C. Kim","Alexander Karpekov","Alec Helbling","Zijie J. Wang","Seongmin Lee","Benjamin Hoover","Duen Horng Chau"],"pdf_url":"https://arxiv.org/pdf/2408.04619v1.pdf","comment":"To be presented at IEEE VIS 2024"},{"id":"http://arxiv.org/abs/2408.04614v1","updated":"2024-08-08T17:42:32Z","published":"2024-08-08T17:42:32Z","title":"Better Alignment with Instruction Back-and-Forth Translation","summary":"  We propose a new method, instruction back-and-forth translation, to construct\nhigh-quality synthetic data grounded in world knowledge for aligning large\nlanguage models (LLMs). Given documents from a web corpus, we generate and\ncurate synthetic instructions using the backtranslation approach proposed by Li\net al.(2023a), and rewrite the responses to improve their quality further based\non the initial documents. Fine-tuning with the resulting (backtranslated\ninstruction, rewritten response) pairs yields higher win rates on AlpacaEval\nthan using other common instruction datasets such as Humpback, ShareGPT, Open\nOrca, Alpaca-GPT4 and Self-instruct. We also demonstrate that rewriting the\nresponses with an LLM outperforms direct distillation, and the two generated\ntext distributions exhibit significant distinction in embedding space. Further\nanalysis shows that our backtranslated instructions are of higher quality than\nother sources of synthetic instructions, while our responses are more diverse\nand complex than those obtained from distillation. Overall we find that\ninstruction back-and-forth translation combines the best of both worlds --\nmaking use of the information diversity and quantity found on the web, while\nensuring the quality of the responses which is necessary for effective\nalignment.\n","authors":["Thao Nguyen","Jeffrey Li","Sewoong Oh","Ludwig Schmidt","Jason Weston","Luke Zettlemoyer","Xian Li"],"pdf_url":"https://arxiv.org/pdf/2408.04614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04607v1","updated":"2024-08-08T17:27:29Z","published":"2024-08-08T17:27:29Z","title":"Risk and cross validation in ridge regression with correlated samples","summary":"  Recent years have seen substantial advances in our understanding of\nhigh-dimensional ridge regression, but existing theories assume that training\nexamples are independent. By leveraging recent techniques from random matrix\ntheory and free probability, we provide sharp asymptotics for the in- and\nout-of-sample risks of ridge regression when the data points have arbitrary\ncorrelations. We demonstrate that in this setting, the generalized cross\nvalidation estimator (GCV) fails to correctly predict the out-of-sample risk.\nHowever, in the case where the noise residuals have the same correlations as\nthe data points, one can modify the GCV to yield an efficiently-computable\nunbiased estimator that concentrates in the high-dimensional limit, which we\ndub CorrGCV. We further extend our asymptotic analysis to the case where the\ntest point has nontrivial correlations with the training set, a setting often\nencountered in time series forecasting. Assuming knowledge of the correlation\nstructure of the time series, this again yields an extension of the GCV\nestimator, and sharply characterizes the degree to which such test points yield\nan overly optimistic prediction of long-time risk. We validate the predictions\nof our theory across a variety of high dimensional data.\n","authors":["Alexander Atanasov","Jacob A. Zavatone-Veth","Cengiz Pehlevan"],"pdf_url":"https://arxiv.org/pdf/2408.04607v1.pdf","comment":"44 pages, 18 figures"},{"id":"http://arxiv.org/abs/2408.04595v1","updated":"2024-08-08T17:11:36Z","published":"2024-08-08T17:11:36Z","title":"Inference with the Upper Confidence Bound Algorithm","summary":"  In this paper, we discuss the asymptotic behavior of the Upper Confidence\nBound (UCB) algorithm in the context of multiarmed bandit problems and discuss\nits implication in downstream inferential tasks. While inferential tasks become\nchallenging when data is collected in a sequential manner, we argue that this\nproblem can be alleviated when the sequential algorithm at hand satisfies\ncertain stability property. This notion of stability is motivated from the\nseminal work of Lai and Wei (1982). Our first main result shows that such a\nstability property is always satisfied for the UCB algorithm, and as a result\nthe sample means for each arm are asymptotically normal. Next, we examine the\nstability properties of the UCB algorithm when the number of arms $K$ is\nallowed to grow with the number of arm pulls $T$. We show that in such a case\nthe arms are stable when $\\frac{\\log K}{\\log T} \\rightarrow 0$, and the number\nof near-optimal arms are large.\n","authors":["Koulik Khamaru","Cun-Hui Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.04595v1.pdf","comment":"17 pages, 1 figure"},{"id":"http://arxiv.org/abs/2408.04590v1","updated":"2024-08-08T17:01:26Z","published":"2024-08-08T17:01:26Z","title":"Learn To Learn More Precisely","summary":"  Meta-learning has been extensively applied in the domains of few-shot\nlearning and fast adaptation, achieving remarkable performance. While\nMeta-learning methods like Model-Agnostic Meta-Learning (MAML) and its variants\nprovide a good set of initial parameters for the model, the model still tends\nto learn shortcut features, which leads to poor generalization. In this paper,\nwe propose the formal conception of \"learn to learn more precisely\", which aims\nto make the model learn precise target knowledge from data and reduce the\neffect of noisy knowledge, such as background and noise. To achieve this\ntarget, we proposed a simple and effective meta-learning framework named Meta\nSelf-Distillation(MSD) to maximize the consistency of learned knowledge,\nenhancing the models' ability to learn precise target knowledge. In the inner\nloop, MSD uses different augmented views of the same support data to update the\nmodel respectively. Then in the outer loop, MSD utilizes the same query data to\noptimize the consistency of learned knowledge, enhancing the model's ability to\nlearn more precisely. Our experiment demonstrates that MSD exhibits remarkable\nperformance in few-shot classification tasks in both standard and augmented\nscenarios, effectively boosting the accuracy and consistency of knowledge\nlearned by the model.\n","authors":["Runxi Cheng","Yongxian Wei","Xianglong He","Wanyun Zhu","Songsong Huang","Fei Richard Yu","Fei Ma","Chun Yuan"],"pdf_url":"https://arxiv.org/pdf/2408.04590v1.pdf","comment":"10pages,4 figures, meta learning"},{"id":"http://arxiv.org/abs/2401.12731v2","updated":"2024-08-08T16:56:42Z","published":"2024-01-23T13:04:02Z","title":"The Distributional Uncertainty of the SHAP score in Explainable Machine\n  Learning","summary":"  Attribution scores reflect how important the feature values in an input\nentity are for the output of a machine learning model. One of the most popular\nattribution scores is the SHAP score, which is an instantiation of the general\nShapley value used in coalition game theory. The definition of this score\nrelies on a probability distribution on the entity population. Since the exact\ndistribution is generally unknown, it needs to be assigned subjectively or be\nestimated from data, which may lead to misleading feature scores. In this\npaper, we propose a principled framework for reasoning on SHAP scores under\nunknown entity population distributions. In our framework, we consider an\nuncertainty region that contains the potential distributions, and the SHAP\nscore of a feature becomes a function defined over this region. We study the\nbasic problems of finding maxima and minima of this function, which allows us\nto determine tight ranges for the SHAP scores of all features. In particular,\nwe pinpoint the complexity of these problems, and other related ones, showing\nthem to be NP-complete. Finally, we present experiments on a real-world\ndataset, showing that our framework may contribute to a more robust feature\nscoring.\n","authors":["Santiago Cifuentes","Leopoldo Bertossi","Nina Pardal","Sergio Abriola","Maria Vanina Martinez","Miguel Romero"],"pdf_url":"https://arxiv.org/pdf/2401.12731v2.pdf","comment":"In ECAI 2024 proceedings"},{"id":"http://arxiv.org/abs/2408.04586v1","updated":"2024-08-08T16:56:03Z","published":"2024-08-08T16:56:03Z","title":"Sampling for View Synthesis: From Local Light Field Fusion to Neural\n  Radiance Fields and Beyond","summary":"  Capturing and rendering novel views of complex real-world scenes is a\nlong-standing problem in computer graphics and vision, with applications in\naugmented and virtual reality, immersive experiences and 3D photography. The\nadvent of deep learning has enabled revolutionary advances in this area,\nclassically known as image-based rendering. However, previous approaches\nrequire intractably dense view sampling or provide little or no guidance for\nhow users should sample views of a scene to reliably render high-quality novel\nviews. Local light field fusion proposes an algorithm for practical view\nsynthesis from an irregular grid of sampled views that first expands each\nsampled view into a local light field via a multiplane image scene\nrepresentation, then renders novel views by blending adjacent local light\nfields. Crucially, we extend traditional plenoptic sampling theory to derive a\nbound that specifies precisely how densely users should sample views of a given\nscene when using our algorithm. We achieve the perceptual quality of Nyquist\nrate view sampling while using up to 4000x fewer views. Subsequent developments\nhave led to new scene representations for deep learning with view synthesis,\nnotably neural radiance fields, but the problem of sparse view synthesis from a\nsmall number of images has only grown in importance. We reprise some of the\nrecent results on sparse and even single image view synthesis, while posing the\nquestion of whether prescriptive sampling guidelines are feasible for the new\ngeneration of image-based rendering algorithms.\n","authors":["Ravi Ramamoorthi"],"pdf_url":"https://arxiv.org/pdf/2408.04586v1.pdf","comment":"Article written for Frontiers of Science Award, International\n  Congress on Basic Science, 2024"},{"id":"http://arxiv.org/abs/2408.04583v1","updated":"2024-08-08T16:48:33Z","published":"2024-08-08T16:48:33Z","title":"Unveiling the Power of Sparse Neural Networks for Feature Selection","summary":"  Sparse Neural Networks (SNNs) have emerged as powerful tools for efficient\nfeature selection. Leveraging the dynamic sparse training (DST) algorithms\nwithin SNNs has demonstrated promising feature selection capabilities while\ndrastically reducing computational overheads. Despite these advancements,\nseveral critical aspects remain insufficiently explored for feature selection.\nQuestions persist regarding the choice of the DST algorithm for network\ntraining, the choice of metric for ranking features/neurons, and the\ncomparative performance of these methods across diverse datasets when compared\nto dense networks. This paper addresses these gaps by presenting a\ncomprehensive systematic analysis of feature selection with sparse neural\nnetworks. Moreover, we introduce a novel metric considering sparse neural\nnetwork characteristics, which is designed to quantify feature importance\nwithin the context of SNNs. Our findings show that feature selection with SNNs\ntrained with DST algorithms can achieve, on average, more than $50\\%$ memory\nand $55\\%$ FLOPs reduction compared to the dense networks, while outperforming\nthem in terms of the quality of the selected features. Our code and the\nsupplementary material are available on GitHub\n(\\url{https://github.com/zahraatashgahi/Neuron-Attribution}).\n","authors":["Zahra Atashgahi","Tennison Liu","Mykola Pechenizkiy","Raymond Veldhuis","Decebal Constantin Mocanu","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2408.04583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00161v2","updated":"2024-08-08T16:31:05Z","published":"2024-07-31T21:12:21Z","title":"Automatic Generation of Behavioral Test Cases For Natural Language\n  Processing Using Clustering and Prompting","summary":"  Recent work in behavioral testing for natural language processing (NLP)\nmodels, such as Checklist, is inspired by related paradigms in software\nengineering testing. They allow evaluation of general linguistic capabilities\nand domain understanding, hence can help evaluate conceptual soundness and\nidentify model weaknesses. However, a major challenge is the creation of test\ncases. The current packages rely on semi-automated approach using manual\ndevelopment which requires domain expertise and can be time consuming. This\npaper introduces an automated approach to develop test cases by exploiting the\npower of large language models and statistical techniques. It clusters the text\nrepresentations to carefully construct meaningful groups and then apply\nprompting techniques to automatically generate Minimal Functionality Tests\n(MFT). The well-known Amazon Reviews corpus is used to demonstrate our\napproach. We analyze the behavioral test profiles across four different\nclassification algorithms and discuss the limitations and strengths of those\nmodels.\n","authors":["Ying Li","Rahul Singh","Tarun Joshi","Agus Sudjianto"],"pdf_url":"https://arxiv.org/pdf/2408.00161v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04570v1","updated":"2024-08-08T16:29:09Z","published":"2024-08-08T16:29:09Z","title":"Mathematical Programming For Adaptive Experiments","summary":"  Adaptive experimentation can significantly improve statistical power, but\nstandard algorithms overlook important practical issues including batched and\ndelayed feedback, personalization, non-stationarity, multiple objectives, and\nconstraints. To address these issues, the current algorithm design paradigm\ncrafts tailored methods for each problem instance. Since it is infeasible to\ndevise novel algorithms for every real-world instance, practitioners often have\nto resort to suboptimal approximations that do not address all of their\nchallenges. Moving away from developing bespoke algorithms for each setting, we\npresent a mathematical programming view of adaptive experimentation that can\nflexibly incorporate a wide range of objectives, constraints, and statistical\nprocedures. By formulating a dynamic program in the batched limit, our modeling\nframework enables the use of scalable optimization methods (e.g., SGD and\nauto-differentiation) to solve for treatment allocations. We evaluate our\nframework on benchmarks modeled after practical challenges such as\nnon-stationarity, personalization, multi-objectives, and constraints. Unlike\nbespoke algorithms such as modified variants of Thomson sampling, our\nmathematical programming approach provides remarkably robust performance across\ninstances.\n","authors":["Ethan Che","Daniel R. Jiang","Hongseok Namkoong","Jimmy Wang"],"pdf_url":"https://arxiv.org/pdf/2408.04570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04569v1","updated":"2024-08-08T16:28:56Z","published":"2024-08-08T16:28:56Z","title":"Activation thresholds and expressiveness of polynomial neural networks","summary":"  Polynomial neural networks have been implemented in a range of applications\nand present an advantageous framework for theoretical machine learning. A\npolynomial neural network of fixed architecture and activation degree gives an\nalgebraic map from the network's weights to a set of polynomials. The image of\nthis map is the space of functions representable by the network. Its Zariski\nclosure is an affine variety known as a neurovariety. The dimension of a\npolynomial neural network's neurovariety provides a measure of its\nexpressivity. In this work, we introduce the notion of the activation threshold\nof a network architecture which expresses when the dimension of a neurovariety\nachieves its theoretical maximum. In addition, we prove expressiveness results\nfor polynomial neural networks with equi-width~architectures.\n","authors":["Bella Finkel","Jose Israel Rodriguez","Chenxi Wu","Thomas Yahl"],"pdf_url":"https://arxiv.org/pdf/2408.04569v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2307.02694v3","updated":"2024-08-08T16:24:52Z","published":"2023-07-05T23:53:55Z","title":"Loss Functions and Metrics in Deep Learning","summary":"  When training or evaluating deep learning models, two essential parts are\npicking the proper loss function and deciding on performance metrics. In this\npaper, we provide a comprehensive overview of the most common loss functions\nand metrics used across many different types of deep learning tasks, from\ngeneral tasks such as regression and classification to more specific tasks in\nComputer Vision and Natural Language Processing. We introduce the formula for\neach loss and metric, discuss their strengths and limitations, and describe how\nthese methods can be applied to various problems within deep learning. We hope\nthis work serves as a reference for researchers and practitioners in the field,\nhelping them make informed decisions when selecting the most appropriate loss\nfunction and performance metrics for their deep learning projects.\n","authors":["Juan Terven","Diana M. Cordova-Esparza","Alfonso Ramirez-Pedraza","Edgar A. Chavez-Urbiola","Julio A. Romero-Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2307.02694v3.pdf","comment":"76 pages, 4 figures, 13 tables, 127 equations"},{"id":"http://arxiv.org/abs/2408.04556v1","updated":"2024-08-08T16:13:26Z","published":"2024-08-08T16:13:26Z","title":"Bias-Aware Low-Rank Adaptation: Mitigating Catastrophic Inheritance of\n  Large Language Models","summary":"  Large language models (LLMs) have exhibited remarkable proficiency across a\ndiverse array of natural language processing (NLP) tasks. However, adapting\nLLMs to downstream applications typically necessitates computationally\nintensive and memory-demanding fine-tuning procedures. To mitigate these\nburdens, parameter-efficient fine-tuning (PEFT) techniques have emerged as a\npromising approach to tailor LLMs with minimal computational overhead. While\nPEFT methods offer substantial advantages, they do not fully address the\npervasive issue of bias propagation from pre-training data. In this work, we\nintroduce Bias-Aware Low-Rank Adaptation (BA-LoRA), a novel PEFT method\ndesigned to counteract bias inheritance. BA-LoRA incorporates three distinct\nregularization terms: (1) consistency regularizer, (2) diversity regularizer,\nand (3) singular vector decomposition regularizer. These regularizers\ncollectively aim to improve the generative models' consistency, diversity, and\ngeneralization capabilities during the fine-tuning process. Through extensive\nexperiments on a variety of natural language understanding (NLU) and natural\nlanguage generation (NLG) tasks, employing prominent LLMs such as LLaMA,\nMistral, and Gemma, we demonstrate that BA-LoRA surpasses the performance of\nLoRA and its state-of-the-art variants. Moreover, our method effectively\nmitigates the deleterious effects of pre-training bias, leading to more\nreliable and robust model outputs. The code is available at\nhttps://github.com/cyp-jlu-ai/BA-LoRA.\n","authors":["Yupeng Chang","Yi Chang","Yuan Wu"],"pdf_url":"https://arxiv.org/pdf/2408.04556v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2405.01462v2","updated":"2024-08-08T16:11:33Z","published":"2024-05-02T16:50:47Z","title":"Uncertainty for Active Learning on Graphs","summary":"  Uncertainty Sampling is an Active Learning strategy that aims to improve the\ndata efficiency of machine learning models by iteratively acquiring labels of\ndata points with the highest uncertainty. While it has proven effective for\nindependent data its applicability to graphs remains under-explored. We propose\nthe first extensive study of Uncertainty Sampling for node classification: (1)\nWe benchmark Uncertainty Sampling beyond predictive uncertainty and highlight a\nsignificant performance gap to other Active Learning strategies. (2) We develop\nground-truth Bayesian uncertainty estimates in terms of the data generating\nprocess and prove their effectiveness in guiding Uncertainty Sampling toward\noptimal queries. We confirm our results on synthetic data and design an\napproximate approach that consistently outperforms other uncertainty estimators\non real datasets. (3) Based on this analysis, we relate pitfalls in modeling\nuncertainty to existing methods. Our analysis enables and informs the\ndevelopment of principled uncertainty estimation on graphs.\n","authors":["Dominik Fuchsgruber","Tom Wollschläger","Bertrand Charpentier","Antonio Oroz","Stephan Günnemann"],"pdf_url":"https://arxiv.org/pdf/2405.01462v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01561v3","updated":"2024-08-08T16:00:01Z","published":"2024-06-03T17:44:11Z","title":"Long and Short Guidance in Score identity Distillation for One-Step\n  Text-to-Image Generation","summary":"  Diffusion-based text-to-image generation models trained on extensive\ntext-image pairs have shown the capacity to generate photorealistic images\nconsistent with textual descriptions. However, a significant limitation of\nthese models is their slow sample generation, which requires iterative\nrefinement through the same network. In this paper, we enhance Score identity\nDistillation (SiD) by developing long and short classifier-free guidance (LSG)\nto efficiently distill pretrained Stable Diffusion models without using real\ntraining data. SiD aims to optimize a model-based explicit score matching loss,\nutilizing a score-identity-based approximation alongside the proposed LSG for\npractical computation. By training exclusively with fake images synthesized\nwith its one-step generator, SiD equipped with LSG rapidly improves FID and\nCLIP scores, achieving state-of-the-art FID performance while maintaining a\ncompetitive CLIP score. Specifically, its data-free distillation of Stable\nDiffusion 1.5 achieves a record low FID of 8.15 on the COCO-2014 validation\nset, with a CLIP score of 0.304 at an LSG scale of 1.5, and an FID of 9.56 with\na CLIP score of 0.313 at an LSG scale of 2. Our code and distilled one-step\ntext-to-image generators are available at\nhttps://github.com/mingyuanzhou/SiD-LSG.\n","authors":["Mingyuan Zhou","Zhendong Wang","Huangjie Zheng","Hai Huang"],"pdf_url":"https://arxiv.org/pdf/2406.01561v3.pdf","comment":"Code and model checkpoints available at\n  https://github.com/mingyuanzhou/SiD-LSG"},{"id":"http://arxiv.org/abs/2408.04543v1","updated":"2024-08-08T15:50:03Z","published":"2024-08-08T15:50:03Z","title":"Quantum Machine Learning: Performance and Security Implications in\n  Real-World Applications","summary":"  Quantum computing has garnered significant attention in recent years from\nboth academia and industry due to its potential to achieve a \"quantum\nadvantage\" over classical computers. The advent of quantum computing introduces\nnew challenges for security and privacy. This poster explores the performance\nand security implications of quantum computing through a case study of machine\nlearning in a real-world application. We compare the performance of quantum\nmachine learning (QML) algorithms to their classical counterparts using the\nAlzheimer's disease dataset. Our results indicate that QML algorithms show\npromising potential while they still have not surpassed classical algorithms in\nterms of learning capability and convergence difficulty, and running quantum\nalgorithms through simulations on classical computers requires significantly\nlarge memory space and CPU time. Our study also indicates that QMLs have\ninherited vulnerabilities from classical machine learning algorithms while also\nintroduce new attack vectors.\n","authors":["Zhengping Jay Luo","Tyler Stewart","Mourya Narasareddygari","Rui Duan","Shangqing Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.04543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03508v2","updated":"2024-08-08T15:37:19Z","published":"2024-08-07T02:19:17Z","title":"Autonomous, Self-driving Multi-Step Growth of Semiconductor\n  Heterostructures Guided by Machine Learning","summary":"  The semiconductor industry has prioritized automating repetitive tasks by\nclosed-loop, autonomous experimentation which enables accelerated optimization\nof complex multi-step processes. The emergence of machine learning (ML) has\nushered in automated process with minimal human intervention. In this work, we\ndevelop SemiEpi, a self-driving automation platform capable of executing\nmolecular beam epitaxy (MBE) growth with multi-steps, continuous in-situ\nmonitoring, and on-the-fly feedback control. By integrating standard hardware,\nhomemade software, curve fitting, and multiple ML models, SemiEpi operates\nautonomously, eliminating the need for extensive expertise in MBE processes to\nachieve optimal outcomes. The platform actively learns from previous\nexperimental results, identifying favorable conditions and proposing new\nexperiments to achieve the desired results. We standardize and optimize growth\nfor InAs/GaAs quantum dots (QDs) heterostructures to showcase the power of\nML-guided multi-step growth. A temperature calibration was implemented to get\nthe initial growth condition, and fine control of the process was executed\nusing ML. Leveraging RHEED movies acquired during the growth, SemiEpi\nsuccessfully identified and optimized a novel route for multi-step\nheterostructure growth. This work demonstrates the capabilities of closed-loop,\nML-guided systems in addressing challenges in multi-step growth for any device.\nOur method is critical to achieve repeatable materials growth using\ncommercially scalable tools. Our strategy facilitates the development of a\nhardware-independent process and enhancing process repeatability and stability,\neven without exhaustive knowledge of growth parameters.\n","authors":["Chao Shen","Wenkang Zhan","Hongyu Sun","Kaiyao Xin","Bo Xu","Zhanguo Wang","Chao Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.03508v2.pdf","comment":"5 figures"},{"id":"http://arxiv.org/abs/2408.04532v1","updated":"2024-08-08T15:33:02Z","published":"2024-08-08T15:33:02Z","title":"How Transformers Utilize Multi-Head Attention in In-Context Learning? A\n  Case Study on Sparse Linear Regression","summary":"  Despite the remarkable success of transformer-based models in various\nreal-world tasks, their underlying mechanisms remain poorly understood. Recent\nstudies have suggested that transformers can implement gradient descent as an\nin-context learner for linear regression problems and have developed various\ntheoretical analyses accordingly. However, these works mostly focus on the\nexpressive power of transformers by designing specific parameter constructions,\nlacking a comprehensive understanding of their inherent working mechanisms\npost-training. In this study, we consider a sparse linear regression problem\nand investigate how a trained multi-head transformer performs in-context\nlearning. We experimentally discover that the utilization of multi-heads\nexhibits different patterns across layers: multiple heads are utilized and\nessential in the first layer, while usually only a single head is sufficient\nfor subsequent layers. We provide a theoretical explanation for this\nobservation: the first layer preprocesses the context data, and the following\nlayers execute simple optimization steps based on the preprocessed context.\nMoreover, we demonstrate that such a preprocess-then-optimize algorithm can\nsignificantly outperform naive gradient descent and ridge regression\nalgorithms. Further experimental results support our explanations. Our findings\noffer insights into the benefits of multi-head attention and contribute to\nunderstanding the more intricate mechanisms hidden within trained transformers.\n","authors":["Xingwu Chen","Lei Zhao","Difan Zou"],"pdf_url":"https://arxiv.org/pdf/2408.04532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04531v1","updated":"2024-08-08T15:32:12Z","published":"2024-08-08T15:32:12Z","title":"AExGym: Benchmarks and Environments for Adaptive Experimentation","summary":"  Innovations across science and industry are evaluated using randomized trials\n(a.k.a. A/B tests). While simple and robust, such static designs are\ninefficient or infeasible for testing many hypotheses. Adaptive designs can\ngreatly improve statistical power in theory, but they have seen limited\nadoption due to their fragility in practice. We present a benchmark for\nadaptive experimentation based on real-world datasets, highlighting prominent\npractical challenges to operationalizing adaptivity: non-stationarity,\nbatched/delayed feedback, multiple outcomes and objectives, and external\nvalidity. Our benchmark aims to spur methodological development that puts\npractical performance (e.g., robustness) as a central concern, rather than\nmathematical guarantees on contrived instances. We release an open source\nlibrary, AExGym, which is designed with modularity and extensibility in mind to\nallow experimentation practitioners to develop custom environments and\nalgorithms.\n","authors":["Jimmy Wang","Ethan Che","Daniel R. Jiang","Hongseok Namkoong"],"pdf_url":"https://arxiv.org/pdf/2408.04531v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04526v1","updated":"2024-08-08T15:26:18Z","published":"2024-08-08T15:26:18Z","title":"Hybrid Reinforcement Learning Breaks Sample Size Barriers in Linear MDPs","summary":"  Hybrid Reinforcement Learning (RL), where an agent learns from both an\noffline dataset and online explorations in an unknown environment, has garnered\nsignificant recent interest. A crucial question posed by Xie et al. (2022) is\nwhether hybrid RL can improve upon the existing lower bounds established in\npurely offline and purely online RL without relying on the single-policy\nconcentrability assumption. While Li et al. (2023) provided an affirmative\nanswer to this question in the tabular PAC RL case, the question remains\nunsettled for both the regret-minimizing RL case and the non-tabular case.\n  In this work, building upon recent advancements in offline RL and\nreward-agnostic exploration, we develop computationally efficient algorithms\nfor both PAC and regret-minimizing RL with linear function approximation,\nwithout single-policy concentrability. We demonstrate that these algorithms\nachieve sharper error or regret bounds that are no worse than, and can improve\non, the optimal sample complexity in offline RL (the first algorithm, for PAC\nRL) and online RL (the second algorithm, for regret-minimizing RL) in linear\nMarkov decision processes (MDPs), regardless of the quality of the behavior\npolicy. To our knowledge, this work establishes the tightest theoretical\nguarantees currently available for hybrid RL in linear MDPs.\n","authors":["Kevin Tan","Wei Fan","Yuting Wei"],"pdf_url":"https://arxiv.org/pdf/2408.04526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04520v1","updated":"2024-08-08T15:21:07Z","published":"2024-08-08T15:21:07Z","title":"Advancing Molecular Machine (Learned) Representations with\n  Stereoelectronics-Infused Molecular Graphs","summary":"  Molecular representation is a foundational element in our understanding of\nthe physical world. Its importance ranges from the fundamentals of chemical\nreactions to the design of new therapies and materials. Previous molecular\nmachine learning models have employed strings, fingerprints, global features,\nand simple molecular graphs that are inherently information-sparse\nrepresentations. However, as the complexity of prediction tasks increases, the\nmolecular representation needs to encode higher fidelity information. This work\nintroduces a novel approach to infusing quantum-chemical-rich information into\nmolecular graphs via stereoelectronic effects. We show that the explicit\naddition of stereoelectronic interactions significantly improves the\nperformance of molecular machine learning models. Furthermore,\nstereoelectronics-infused representations can be learned and deployed with a\ntailored double graph neural network workflow, enabling its application to any\ndownstream molecular machine learning task. Finally, we show that the learned\nrepresentations allow for facile stereoelectronic evaluation of previously\nintractable systems, such as entire proteins, opening new avenues of molecular\ndesign.\n","authors":["Daniil A. Boiko","Thiago Reschützegger","Benjamin Sanchez-Lengeling","Samuel M. Blau","Gabe Gomes"],"pdf_url":"https://arxiv.org/pdf/2408.04520v1.pdf","comment":"23 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.16677v2","updated":"2024-08-08T15:02:13Z","published":"2024-03-25T12:14:48Z","title":"FOOL: Addressing the Downlink Bottleneck in Satellite Computing with\n  Neural Feature Compression","summary":"  Nanosatellite constellations equipped with sensors capturing large geographic\nregions provide unprecedented opportunities for Earth observation. As\nconstellation sizes increase, network contention poses a downlink bottleneck.\nOrbital Edge Computing (OEC) leverages limited onboard compute resources to\nreduce transfer costs by processing the raw captures at the source. However,\ncurrent solutions have limited practicability due to reliance on crude\nfiltering methods or over-prioritizing particular downstream tasks.\n  This work presents FOOL, an OEC-native and task-agnostic feature compression\nmethod that preserves prediction performance. FOOL partitions high-resolution\nsatellite imagery to maximize throughput. Further, it embeds context and\nleverages inter-tile dependencies to lower transfer costs with negligible\noverhead. While FOOL is a feature compressor, it can recover images with\ncompetitive scores on quality measures at lower bitrates. We extensively\nevaluate transfer cost reduction by including the peculiarity of intermittently\navailable network connections in low earth orbit. Lastly, we test the\nfeasibility of our system for standardized nanosatellite form factors. We\ndemonstrate that FOOL permits downlinking over 100x the data volume without\nrelying on prior information on the downstream tasks.\n","authors":["Alireza Furutanpey","Qiyang Zhang","Philipp Raith","Tobias Pfandzelter","Shangguang Wang","Schahram Dustdar"],"pdf_url":"https://arxiv.org/pdf/2403.16677v2.pdf","comment":"18 pages, double column, 19 figures, 7 tables, Revision 1"},{"id":"http://arxiv.org/abs/2408.04499v1","updated":"2024-08-08T14:50:48Z","published":"2024-08-08T14:50:48Z","title":"Knowledge-Aided Semantic Communication Leveraging Probabilistic\n  Graphical Modeling","summary":"  In this paper, we propose a semantic communication approach based on\nprobabilistic graphical model (PGM). The proposed approach involves\nconstructing a PGM from a training dataset, which is then shared as common\nknowledge between the transmitter and receiver. We evaluate the importance of\nvarious semantic features and present a PGM-based compression algorithm\ndesigned to eliminate predictable portions of semantic information.\nFurthermore, we introduce a technique to reconstruct the discarded semantic\ninformation at the receiver end, generating approximate results based on the\nPGM. Simulation results indicate a significant improvement in transmission\nefficiency over existing methods, while maintaining the quality of the\ntransmitted images.\n","authors":["Haowen Wan","Qianqian Yang","Jiancheng Tang","Zhiguo shi"],"pdf_url":"https://arxiv.org/pdf/2408.04499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04498v1","updated":"2024-08-08T14:46:01Z","published":"2024-08-08T14:46:01Z","title":"Model-Based Transfer Learning for Contextual Reinforcement Learning","summary":"  Deep reinforcement learning is a powerful approach to complex decision\nmaking. However, one issue that limits its practical application is its\nbrittleness, sometimes failing to train in the presence of small changes in the\nenvironment. This work is motivated by the empirical observation that directly\napplying an already trained model to a related task often works remarkably\nwell, also called zero-shot transfer. We take this practical trick one step\nfurther to consider how to systematically select good tasks to train,\nmaximizing overall performance across a range of tasks. Given the high cost of\ntraining, it is critical to choose a small set of training tasks. The key idea\nbehind our approach is to explicitly model the performance loss (generalization\ngap) incurred by transferring a trained model. We hence introduce Model-Based\nTransfer Learning (MBTL) for solving contextual RL problems. In this work, we\nmodel the performance loss as a simple linear function of task context\nsimilarity. Furthermore, we leverage Bayesian optimization techniques to\nefficiently model and estimate the unknown training performance of the task\nspace. We theoretically show that the method exhibits regret that is sublinear\nin the number of training tasks and discuss conditions to further tighten\nregret bounds. We experimentally validate our methods using urban traffic and\nstandard control benchmarks. Despite the conceptual simplicity, the\nexperimental results suggest that MBTL can achieve greater performance than\nstrong baselines, including exhaustive training on all tasks, multi-task\ntraining, and random selection of training tasks. This work lays the\nfoundations for investigating explicit modeling of generalization, thereby\nenabling principled yet effective methods for contextual RL.\n","authors":["Jung-Hoon Cho","Vindula Jayawardana","Sirui Li","Cathy Wu"],"pdf_url":"https://arxiv.org/pdf/2408.04498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13458v3","updated":"2024-08-08T14:26:23Z","published":"2023-03-23T17:26:12Z","title":"Optimization Dynamics of Equivariant and Augmented Neural Networks","summary":"  We investigate the optimization of neural networks on symmetric data, and\ncompare the strategy of constraining the architecture to be equivariant to that\nof using data augmentation. Our analysis reveals that that the relative\ngeometry of the admissible and the equivariant layers, respectively, plays a\nkey role. Under natural assumptions on the data, network, loss, and group of\nsymmetries, we show that compatibility of the spaces of admissible layers and\nequivariant layers, in the sense that the corresponding orthogonal projections\ncommute, implies that the sets of equivariant stationary points are identical\nfor the two strategies. If the linear layers of the network also are given a\nunitary parametrization, the set of equivariant layers is even invariant under\nthe gradient flow for augmented models. Our analysis however also reveals that\neven in the latter situation, stationary points may be unstable for augmented\ntraining although they are stable for the manifestly equivariant models.\n","authors":["Oskar Nordenfors","Fredrik Ohlsson Axel Flinth"],"pdf_url":"https://arxiv.org/pdf/2303.13458v3.pdf","comment":"v3: Completely revised manuscript: New framework for neural nets, new\n  main result (involving compability condition), new experiments, new author.\n  v2: Revised manuscript. Mostly small edits, apart from new experiments (see\n  Appendix E)"},{"id":"http://arxiv.org/abs/2408.04482v1","updated":"2024-08-08T14:19:11Z","published":"2024-08-08T14:19:11Z","title":"SegXAL: Explainable Active Learning for Semantic Segmentation in Driving\n  Scene Scenarios","summary":"  Most of the sophisticated AI models utilize huge amounts of annotated data\nand heavy training to achieve high-end performance. However, there are certain\nchallenges that hinder the deployment of AI models \"in-the-wild\" scenarios,\ni.e., inefficient use of unlabeled data, lack of incorporation of human\nexpertise, and lack of interpretation of the results. To mitigate these\nchallenges, we propose a novel Explainable Active Learning (XAL) model,\nXAL-based semantic segmentation model \"SegXAL\", that can (i) effectively\nutilize the unlabeled data, (ii) facilitate the \"Human-in-the-loop\" paradigm,\nand (iii) augment the model decisions in an interpretable way. In particular,\nwe investigate the application of the SegXAL model for semantic segmentation in\ndriving scene scenarios. The SegXAL model proposes the image regions that\nrequire labeling assistance from Oracle by dint of explainable AI (XAI) and\nuncertainty measures in a weakly-supervised manner. Specifically, we propose a\nnovel Proximity-aware Explainable-AI (PAE) module and Entropy-based Uncertainty\n(EBU) module to get an Explainable Error Mask, which enables the machine\nteachers/human experts to provide intuitive reasoning behind the results and to\nsolicit feedback to the AI system via an active learning strategy. Such a\nmechanism bridges the semantic gap between man and machine through\ncollaborative intelligence, where humans and AI actively enhance each other's\ncomplementary strengths. A novel high-confidence sample selection technique\nbased on the DICE similarity coefficient is also presented within the SegXAL\nframework. Extensive quantitative and qualitative analyses are carried out in\nthe benchmarking Cityscape dataset. Results show the outperformance of our\nproposed SegXAL against other state-of-the-art models.\n","authors":["Sriram Mandalika","Athira Nambiar"],"pdf_url":"https://arxiv.org/pdf/2408.04482v1.pdf","comment":"17 pages, 7 figures. To appear in the proceedings of the 27th\n  International Conference on Pattern Recognition (ICPR), 01-05 December, 2024,\n  Kolkata, India"},{"id":"http://arxiv.org/abs/2408.04478v1","updated":"2024-08-08T14:08:39Z","published":"2024-08-08T14:08:39Z","title":"NFDI4Health workflow and service for synthetic data generation,\n  assessment and risk management","summary":"  Individual health data is crucial for scientific advancements, particularly\nin developing Artificial Intelligence (AI); however, sharing real patient\ninformation is often restricted due to privacy concerns. A promising solution\nto this challenge is synthetic data generation. This technique creates entirely\nnew datasets that mimic the statistical properties of real data, while\npreserving confidential patient information. In this paper, we present the\nworkflow and different services developed in the context of Germany's National\nData Infrastructure project NFDI4Health. First, two state-of-the-art AI tools\n(namely, VAMBN and MultiNODEs) for generating synthetic health data are\noutlined. Further, we introduce SYNDAT (a public web-based tool) which allows\nusers to visualize and assess the quality and risk of synthetic data provided\nby desired generative models. Additionally, the utility of the proposed methods\nand the web-based tool is showcased using data from Alzheimer's Disease\nNeuroimaging Initiative (ADNI) and the Center for Cancer Registry Data of the\nRobert Koch Institute (RKI).\n","authors":["Sobhan Moazemi","Tim Adams","Hwei Geok NG","Lisa Kühnel","Julian Schneider","Anatol-Fiete Näher","Juliane Fluck","Holger Fröhlich"],"pdf_url":"https://arxiv.org/pdf/2408.04478v1.pdf","comment":"9 pages, 4 figures, accepted for publication in the proceedings of\n  the 69th Annual Conference of the Society for Medical Informatics, Biometry\n  and Epidemiology (GMDS)"},{"id":"http://arxiv.org/abs/2408.03685v2","updated":"2024-08-08T13:52:44Z","published":"2024-08-07T10:53:07Z","title":"RL-ADN: A High-Performance Deep Reinforcement Learning Environment for\n  Optimal Energy Storage Systems Dispatch in Active Distribution Networks","summary":"  Deep Reinforcement Learning (DRL) presents a promising avenue for optimizing\nEnergy Storage Systems (ESSs) dispatch in distribution networks. This paper\nintroduces RL-ADN, an innovative open-source library specifically designed for\nsolving the optimal ESSs dispatch in active distribution networks. RL-ADN\noffers unparalleled flexibility in modeling distribution networks, and ESSs,\naccommodating a wide range of research goals. A standout feature of RL-ADN is\nits data augmentation module, based on Gaussian Mixture Model and Copula (GMC)\nfunctions, which elevates the performance ceiling of DRL agents. Additionally,\nRL-ADN incorporates the Laurent power flow solver, significantly reducing the\ncomputational burden of power flow calculations during training without\nsacrificing accuracy. The effectiveness of RL-ADN is demonstrated using in\ndifferent sizes of distribution networks, showing marked performance\nimprovements in the adaptability of DRL algorithms for ESS dispatch tasks. This\nenhancement is particularly beneficial from the increased diversity of training\nscenarios. Furthermore, RL-ADN achieves a tenfold increase in computational\nefficiency during training, making it highly suitable for large-scale network\napplications. The library sets a new benchmark in DRL-based ESSs dispatch in\ndistribution networks and it is poised to advance DRL applications in\ndistribution network operations significantly. RL-ADN is available at:\nhttps://github.com/ShengrenHou/RL-ADN and\nhttps://github.com/distributionnetworksTUDelft/RL-ADN.\n","authors":["Shengren Hou","Shuyi Gao","Weijie Xia","Edgar Mauricio Salazar Duque","Peter Palensky","Pedro P. Vergara"],"pdf_url":"https://arxiv.org/pdf/2408.03685v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04461v1","updated":"2024-08-08T13:42:18Z","published":"2024-08-08T13:42:18Z","title":"Random Walk Diffusion for Efficient Large-Scale Graph Generation","summary":"  Graph generation addresses the problem of generating new graphs that have a\ndata distribution similar to real-world graphs. While previous diffusion-based\ngraph generation methods have shown promising results, they often struggle to\nscale to large graphs. In this work, we propose ARROW-Diff (AutoRegressive\nRandOm Walk Diffusion), a novel random walk-based diffusion approach for\nefficient large-scale graph generation. Our method encompasses two components\nin an iterative process of random walk sampling and graph pruning. We\ndemonstrate that ARROW-Diff can scale to large graphs efficiently, surpassing\nother baseline methods in terms of both generation time and multiple graph\nstatistics, reflecting the high quality of the generated graphs.\n","authors":["Tobias Bernecker","Ghalia Rehawi","Francesco Paolo Casale","Janine Knauer-Arloth","Annalisa Marsico"],"pdf_url":"https://arxiv.org/pdf/2408.04461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04460v1","updated":"2024-08-08T13:39:09Z","published":"2024-08-08T13:39:09Z","title":"An experimental comparative study of backpropagation and alternatives\n  for training binary neural networks for image classification","summary":"  Current artificial neural networks are trained with parameters encoded as\nfloating point numbers that occupy lots of memory space at inference time. Due\nto the increase in the size of deep learning models, it is becoming very\ndifficult to consider training and using artificial neural networks on edge\ndevices. Binary neural networks promise to reduce the size of deep neural\nnetwork models, as well as to increase inference speed while decreasing energy\nconsumption. Thus, they may allow the deployment of more powerful models on\nedge devices. However, binary neural networks are still proven to be difficult\nto train using the backpropagation-based gradient descent scheme. This paper\nextends the work of \\cite{crulis2023alternatives}, which proposed adapting to\nbinary neural networks two promising alternatives to backpropagation originally\ndesigned for continuous neural networks, and experimented with them on simple\nimage classification datasets. This paper proposes new experiments on the\nImageNette dataset, compares three different model architectures for image\nclassification, and adds two additional alternatives to backpropagation.\n","authors":["Ben Crulis","Barthelemy Serres","Cyril de Runz","Gilles Venturini"],"pdf_url":"https://arxiv.org/pdf/2408.04460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13000v2","updated":"2024-08-08T13:33:12Z","published":"2024-03-12T16:25:38Z","title":"Duwak: Dual Watermarks in Large Language Models","summary":"  As large language models (LLM) are increasingly used for text generation\ntasks, it is critical to audit their usages, govern their applications, and\nmitigate their potential harms. Existing watermark techniques are shown\neffective in embedding single human-imperceptible and machine-detectable\npatterns without significantly affecting generated text quality and semantics.\nHowever, the efficiency in detecting watermarks, i.e., the minimum number of\ntokens required to assert detection with significance and robustness against\npost-editing, is still debatable. In this paper, we propose, Duwak, to\nfundamentally enhance the efficiency and quality of watermarking by embedding\ndual secret patterns in both token probability distribution and sampling\nschemes. To mitigate expression degradation caused by biasing toward certain\ntokens, we design a contrastive search to watermark the sampling scheme, which\nminimizes the token repetition and enhances the diversity. We theoretically\nexplain the interdependency of the two watermarks within Duwak. We evaluate\nDuwak extensively on Llama2 under various post-editing attacks, against four\nstate-of-the-art watermarking techniques and combinations of them. Our results\nshow that Duwak marked text achieves the highest watermarked text quality at\nthe lowest required token count for detection, up to 70% tokens less than\nexisting approaches, especially under post paraphrasing.\n","authors":["Chaoyi Zhu","Jeroen Galjaard","Pin-Yu Chen","Lydia Y. Chen"],"pdf_url":"https://arxiv.org/pdf/2403.13000v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16449v2","updated":"2024-08-08T13:32:10Z","published":"2024-05-26T06:33:11Z","title":"Reinforcement Learning for Jump-Diffusions, with Financial Applications","summary":"  We study continuous-time reinforcement learning (RL) for stochastic control\nin which system dynamics are governed by jump-diffusion processes. We formulate\nan entropy-regularized exploratory control problem with stochastic policies to\ncapture the exploration--exploitation balance essential for RL. Unlike the pure\ndiffusion case initially studied by Wang et al. (2020), the derivation of the\nexploratory dynamics under jump-diffusions calls for a careful formulation of\nthe jump part. Through a theoretical analysis, we find that one can simply use\nthe same policy evaluation and $q$-learning algorithms in Jia and Zhou (2022a,\n2023), originally developed for controlled diffusions, without needing to check\na priori whether the underlying data come from a pure diffusion or a\njump-diffusion. However, we show that the presence of jumps ought to affect\nparameterizations of actors and critics in general. We investigate as an\napplication the mean--variance portfolio selection problem with stock price\nmodelled as a jump-diffusion, and show that both RL algorithms and\nparameterizations are invariant with respect to jumps. Finally, we present a\ndetailed study on applying the general theory to option hedging.\n","authors":["Xuefeng Gao","Lingfei Li","Xun Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2405.16449v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04442v1","updated":"2024-08-08T13:14:19Z","published":"2024-08-08T13:14:19Z","title":"FedAD-Bench: A Unified Benchmark for Federated Unsupervised Anomaly\n  Detection in Tabular Data","summary":"  The emergence of federated learning (FL) presents a promising approach to\nleverage decentralized data while preserving privacy. Furthermore, the\ncombination of FL and anomaly detection is particularly compelling because it\nallows for detecting rare and critical anomalies (usually also rare in locally\ngathered data) in sensitive data from multiple sources, such as cybersecurity\nand healthcare. However, benchmarking the performance of anomaly detection\nmethods in FL environments remains an underexplored area. This paper introduces\nFedAD-Bench, a unified benchmark for evaluating unsupervised anomaly detection\nalgorithms within the context of FL. We systematically analyze and compare the\nperformance of recent deep learning anomaly detection models under federated\nsettings, which were typically assessed solely in centralized settings.\nFedAD-Bench encompasses diverse datasets and metrics to provide a holistic\nevaluation. Through extensive experiments, we identify key challenges such as\nmodel aggregation inefficiencies and metric unreliability. We present insights\ninto FL's regularization effects, revealing scenarios in which it outperforms\ncentralized approaches due to its inherent ability to mitigate overfitting. Our\nwork aims to establish a standardized benchmark to guide future research and\ndevelopment in federated anomaly detection, promoting reproducibility and fair\ncomparison across studies.\n","authors":["Ahmed Anwar","Brian Moser","Dayananda Herurkar","Federico Raue","Vinit Hegiste","Tatjana Legler","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2408.04442v1.pdf","comment":"8 pages, 1 figure"},{"id":"http://arxiv.org/abs/2408.04439v1","updated":"2024-08-08T13:10:03Z","published":"2024-08-08T13:10:03Z","title":"Deep Learning for identifying systolic complexes in SCG traces: a\n  cross-dataset analysis","summary":"  The seismocardiographic signal is a promising alternative to the traditional\nECG in the analysis of the cardiac activity. In particular, the systolic\ncomplex is known to be the most informative part of the seismocardiogram, thus\nrequiring further analysis. State-of-art solutions to detect the systolic\ncomplex are based on Deep Learning models, which have been proven effective in\npioneering studies. However, these solutions have only been tested in a\ncontrolled scenario considering only clean signals acquired from users\nmaintained still in supine position. On top of that, all these studies consider\ndata coming from a single dataset, ignoring the benefits and challenges related\nto a cross-dataset scenario. In this work, a cross-dataset experimental\nanalysis was performed considering also data from a real-world scenario. Our\nfindings prove the effectiveness of a deep learning solution, while showing the\nimportance of a personalization step to contrast the domain shift, namely a\nchange in data distribution between training and testing data. Finally, we\ndemonstrate the benefits of a multi-channels approach, leveraging the\ninformation extracted from both accelerometers and gyroscopes data.\n","authors":["Michele Craighero","Sarah Solbiati","Federica Mozzini","Enrico Caiani","Giacomo Boracchi"],"pdf_url":"https://arxiv.org/pdf/2408.04439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08648v3","updated":"2024-08-08T13:09:51Z","published":"2022-12-16T18:48:54Z","title":"Connecting Permutation Equivariant Neural Networks and Partition\n  Diagrams","summary":"  Permutation equivariant neural networks are often constructed using tensor\npowers of $\\mathbb{R}^{n}$ as their layer spaces. We show that all of the\nweight matrices that appear in these neural networks can be obtained from\nSchur-Weyl duality between the symmetric group and the partition algebra. In\nparticular, we adapt Schur-Weyl duality to derive a simple, diagrammatic method\nfor calculating the weight matrices themselves.\n","authors":["Edward Pearce-Crump"],"pdf_url":"https://arxiv.org/pdf/2212.08648v3.pdf","comment":"ECAI 2024; 16 pages"},{"id":"http://arxiv.org/abs/2310.20609v2","updated":"2024-08-08T12:51:23Z","published":"2023-10-31T16:44:26Z","title":"Graph Matching via convex relaxation to the simplex","summary":"  This paper addresses the Graph Matching problem, which consists of finding\nthe best possible alignment between two input graphs, and has many applications\nin computer vision, network deanonymization and protein alignment. A common\napproach to tackle this problem is through convex relaxations of the NP-hard\n\\emph{Quadratic Assignment Problem} (QAP).\n  Here, we introduce a new convex relaxation onto the unit simplex and develop\nan efficient mirror descent scheme with closed-form iterations for solving this\nproblem. Under the correlated Gaussian Wigner model, we show that the simplex\nrelaxation admits a unique solution with high probability. In the noiseless\ncase, this is shown to imply exact recovery of the ground truth permutation.\nAdditionally, we establish a novel sufficiency condition for the input matrix\nin standard greedy rounding methods, which is less restrictive than the\ncommonly used `diagonal dominance' condition. We use this condition to show\nexact one-step recovery of the ground truth (holding almost surely) via the\nmirror descent scheme, in the noiseless setting. We also use this condition to\nobtain significantly improved conditions for the GRAMPA algorithm [Fan et al.\n2019] in the noiseless setting.\n","authors":["Ernesto Araya Valdivia","Hemant Tyagi"],"pdf_url":"https://arxiv.org/pdf/2310.20609v2.pdf","comment":"We fixed some typos and added Lemma 4. Reference to the published\n  version below"},{"id":"http://arxiv.org/abs/2408.04424v1","updated":"2024-08-08T12:48:54Z","published":"2024-08-08T12:48:54Z","title":"Detection of Animal Movement from Weather Radar using Self-Supervised\n  Learning","summary":"  Detecting flying animals (e.g., birds, bats, and insects) using weather radar\nhelps gain insights into animal movement and migration patterns, aids in\nmanagement efforts (such as biosecurity) and enhances our understanding of the\necosystem.The conventional approach to detecting animals in weather radar\ninvolves thresholding: defining and applying thresholds for the radar\nvariables, based on expert opinion. More recently, Deep Learning approaches\nhave been shown to provide improved performance in detection. However,\nobtaining sufficient labelled weather radar data for flying animals to build\nlearning-based models is time-consuming and labor-intensive. To address the\nchallenge of data labelling, we propose a self-supervised learning method for\ndetecting animal movement. In our proposed method, we pre-train our model on a\nlarge dataset with noisy labels produced by a threshold approach. The key\nadvantage is that the pre-trained dataset size is limited only by the number of\nradar images available. We then fine-tune the model on a small human-labelled\ndataset. Our experiments on Australian weather radar data for waterbird\nsegmentation show that the proposed method outperforms the current state-of-the\nart approach by 43.53% in the dice co-efficient statistic.\n","authors":["Mubin Ul Haque","Joel Janek Dabrowski","Rebecca M. Rogers","Hazel Parry"],"pdf_url":"https://arxiv.org/pdf/2408.04424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04413v1","updated":"2024-08-08T12:40:27Z","published":"2024-08-08T12:40:27Z","title":"Deeploy: Enabling Energy-Efficient Deployment of Small Language Models\n  On Heterogeneous Microcontrollers","summary":"  With the rise of Embodied Foundation Models (EFMs), most notably Small\nLanguage Models (SLMs), adapting Transformers for edge applications has become\na very active field of research. However, achieving end-to-end deployment of\nSLMs on microcontroller (MCU)-class chips without high-bandwidth off-chip main\nmemory access is still an open challenge. In this paper, we demonstrate\nhigh-efficiency end-to-end SLM deployment on a multicore RISC-V (RV32) MCU\naugmented with ML instruction extensions and a hardware neural processing unit\n(NPU). To automate the exploration of the constrained, multi-dimensional memory\nvs. computation tradeoffs involved in aggressive SLM deployment on\nheterogeneous (multicore+NPU) resources, we introduce Deeploy, a novel Deep\nNeural Network (DNN) compiler, which generates highly-optimized C code\nrequiring minimal runtime support. We demonstrate that Deeploy generates\nend-to-end code for executing SLMs, fully exploiting the RV32 cores'\ninstruction extensions and the NPU: We achieve leading-edge energy and\nthroughput of \\SI{490}{\\micro\\joule \\per Token}, at \\SI{340}{Token \\per\n\\second} for an SLM trained on the TinyStories dataset, running for the first\ntime on an MCU-class device without external memory.\n","authors":["Moritz Scherer","Luka Macan","Victor Jung","Philip Wiese","Luca Bompani","Alessio Burrello","Francesco Conti","Luca Benini"],"pdf_url":"https://arxiv.org/pdf/2408.04413v1.pdf","comment":"Accepted for publication at ESWEEK - CASES 2024"},{"id":"http://arxiv.org/abs/2406.01661v2","updated":"2024-08-08T12:17:56Z","published":"2024-06-03T17:55:02Z","title":"A Diffusion Model Framework for Unsupervised Neural Combinatorial\n  Optimization","summary":"  Learning to sample from intractable distributions over discrete sets without\nrelying on corresponding training data is a central problem in a wide range of\nfields, including Combinatorial Optimization. Currently, popular deep\nlearning-based approaches rely primarily on generative models that yield exact\nsample likelihoods. This work introduces a method that lifts this restriction\nand opens the possibility to employ highly expressive latent variable models\nlike diffusion models. Our approach is conceptually based on a loss that upper\nbounds the reverse Kullback-Leibler divergence and evades the requirement of\nexact sample likelihoods. We experimentally validate our approach in data-free\nCombinatorial Optimization and demonstrate that our method achieves a new\nstate-of-the-art on a wide range of benchmark problems.\n","authors":["Sebastian Sanokowski","Sepp Hochreiter","Sebastian Lehner"],"pdf_url":"https://arxiv.org/pdf/2406.01661v2.pdf","comment":"Accepted at ICML 2024"},{"id":"http://arxiv.org/abs/2408.04407v1","updated":"2024-08-08T12:16:14Z","published":"2024-08-08T12:16:14Z","title":"Clutter Classification Using Deep Learning in Multiple Stages","summary":"  Path loss prediction for wireless communications is highly dependent on the\nlocal environment. Propagation models including clutter information have been\nshown to significantly increase model accuracy. This paper explores the\napplication of deep learning to satellite imagery to identify environmental\nclutter types automatically. Recognizing these clutter types has numerous uses,\nbut our main application is to use clutter information to enhance propagation\nprediction models. Knowing the type of obstruction (tree, building, and further\nclassifications) can improve the prediction accuracy of key propagation metrics\nsuch as path loss.\n","authors":["Ryan Dempsey","Jonathan Ethier"],"pdf_url":"https://arxiv.org/pdf/2408.04407v1.pdf","comment":"SoutheastCon 2024"},{"id":"http://arxiv.org/abs/2408.04406v1","updated":"2024-08-08T12:15:45Z","published":"2024-08-08T12:15:45Z","title":"Finite sample learning of moving targets","summary":"  We consider a moving target that we seek to learn from samples. Our results\nextend randomized techniques developed in control and optimization for a\nconstant target to the case where the target is changing. We derive a novel\nbound on the number of samples that are required to construct a probably\napproximately correct (PAC) estimate of the target. Furthermore, when the\nmoving target is a convex polytope, we provide a constructive method of\ngenerating the PAC estimate using a mixed integer linear program (MILP). The\nproposed method is demonstrated on an application to autonomous emergency\nbraking.\n","authors":["Nikolaus Vertovec","Kostas Margellos","Maria Prandini"],"pdf_url":"https://arxiv.org/pdf/2408.04406v1.pdf","comment":"13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.18247v3","updated":"2024-08-08T12:15:18Z","published":"2023-10-27T16:34:00Z","title":"Guided Data Augmentation for Offline Reinforcement Learning and\n  Imitation Learning","summary":"  In offline reinforcement learning (RL), an RL agent learns to solve a task\nusing only a fixed dataset of previously collected data. While offline RL has\nbeen successful in learning real-world robot control policies, it typically\nrequires large amounts of expert-quality data to learn effective policies that\ngeneralize to out-of-distribution states. Unfortunately, such data is often\ndifficult and expensive to acquire in real-world tasks. Several recent works\nhave leveraged data augmentation (DA) to inexpensively generate additional\ndata, but most DA works apply augmentations in a random fashion and ultimately\nproduce highly suboptimal augmented experience. In this work, we propose Guided\nData Augmentation (GuDA), a human-guided DA framework that generates\nexpert-quality augmented data. The key insight behind GuDA is that while it may\nbe difficult to demonstrate the sequence of actions required to produce expert\ndata, a user can often easily characterize when an augmented trajectory segment\nrepresents progress toward task completion. Thus, a user can restrict the space\nof possible augmentations to automatically reject suboptimal augmented data. To\nextract a policy from GuDA, we use off-the-shelf offline reinforcement learning\nand behavior cloning algorithms. We evaluate GuDA on a physical robot soccer\ntask as well as simulated D4RL navigation tasks, a simulated autonomous driving\ntask, and a simulated soccer task. Empirically, GuDA enables learning given a\nsmall initial dataset of potentially suboptimal experience and outperforms a\nrandom DA strategy as well as a model-based DA strategy.\n","authors":["Nicholas E. Corrado","Yuxiao Qu","John U. Balis","Adam Labiosa","Josiah P. Hanna"],"pdf_url":"https://arxiv.org/pdf/2310.18247v3.pdf","comment":"RLC 2024"},{"id":"http://arxiv.org/abs/2408.04405v1","updated":"2024-08-08T12:14:17Z","published":"2024-08-08T12:14:17Z","title":"Probabilistic energy forecasting through quantile regression in\n  reproducing kernel Hilbert spaces","summary":"  Accurate energy demand forecasting is crucial for sustainable and resilient\nenergy development. To meet the Net Zero Representative Concentration Pathways\n(RCP) $4.5$ scenario in the DACH countries, increased renewable energy\nproduction, energy storage, and reduced commercial building consumption are\nneeded. This scenario's success depends on hydroelectric capacity and climatic\nfactors. Informed decisions require quantifying uncertainty in forecasts. This\nstudy explores a non-parametric method based on \\emph{reproducing kernel\nHilbert spaces (RKHS)}, known as kernel quantile regression, for energy\nprediction. Our experiments demonstrate its reliability and sharpness, and we\nbenchmark it against state-of-the-art methods in load and price forecasting for\nthe DACH region. We offer our implementation in conjunction with additional\nscripts to ensure the reproducibility of our research.\n","authors":["Luca Pernigo","Rohan Sen","Davide Baroli"],"pdf_url":"https://arxiv.org/pdf/2408.04405v1.pdf","comment":"12 pages, {Owner/Author | ACM} {2024}. This is the author's version\n  of the work. It is posted here for your personal use. Not for redistribution.\n  The definitive Version of Record will published in https://energy.acm.org/eir"},{"id":"http://arxiv.org/abs/2408.04400v1","updated":"2024-08-08T12:08:55Z","published":"2024-08-08T12:08:55Z","title":"DIVE: Subgraph Disagreement for Graph Out-of-Distribution Generalization","summary":"  This paper addresses the challenge of out-of-distribution (OOD)\ngeneralization in graph machine learning, a field rapidly advancing yet\ngrappling with the discrepancy between source and target data distributions.\nTraditional graph learning algorithms, based on the assumption of uniform\ndistribution between training and test data, falter in real-world scenarios\nwhere this assumption fails, resulting in suboptimal performance. A principal\nfactor contributing to this suboptimal performance is the inherent simplicity\nbias of neural networks trained through Stochastic Gradient Descent (SGD),\nwhich prefer simpler features over more complex yet equally or more predictive\nones. This bias leads to a reliance on spurious correlations, adversely\naffecting OOD performance in various tasks such as image recognition, natural\nlanguage understanding, and graph classification. Current methodologies,\nincluding subgraph-mixup and information bottleneck approaches, have achieved\npartial success but struggle to overcome simplicity bias, often reinforcing\nspurious correlations. To tackle this, we propose DIVE, training a collection\nof models to focus on all label-predictive subgraphs by encouraging the models\nto foster divergence on the subgraph mask, which circumvents the limitation of\na model solely focusing on the subgraph corresponding to simple structural\npatterns. Specifically, we employs a regularizer to punish overlap in extracted\nsubgraphs across models, thereby encouraging different models to concentrate on\ndistinct structural patterns. Model selection for robust OOD performance is\nachieved through validation accuracy. Tested across four datasets from GOOD\nbenchmark and one dataset from DrugOOD benchmark, our approach demonstrates\nsignificant improvement over existing methods, effectively addressing the\nsimplicity bias and enhancing generalization in graph machine learning.\n","authors":["Xin Sun","Liang Wang","Qiang Liu","Shu Wu","Zilei Wang","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2408.04400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04396v1","updated":"2024-08-08T12:03:03Z","published":"2024-08-08T12:03:03Z","title":"Evaluating the Impact of Pulse Oximetry Bias in Machine Learning under\n  Counterfactual Thinking","summary":"  Algorithmic bias in healthcare mirrors existing data biases. However, the\nfactors driving unfairness are not always known. Medical devices capture\nsignificant amounts of data but are prone to errors; for instance, pulse\noximeters overestimate the arterial oxygen saturation of darker-skinned\nindividuals, leading to worse outcomes. The impact of this bias in machine\nlearning (ML) models remains unclear. This study addresses the technical\nchallenges of quantifying the impact of medical device bias in downstream ML.\nOur experiments compare a \"perfect world\", without pulse oximetry bias, using\nSaO2 (blood-gas), to the \"actual world\", with biased measurements, using SpO2\n(pulse oximetry). Under this counterfactual design, two models are trained with\nidentical data, features, and settings, except for the method of measuring\noxygen saturation: models using SaO2 are a \"control\" and models using SpO2 a\n\"treatment\". The blood-gas oximetry linked dataset was a suitable test-bed,\ncontaining 163,396 nearly-simultaneous SpO2 - SaO2 paired measurements, aligned\nwith a wide array of clinical features and outcomes. We studied three\nclassification tasks: in-hospital mortality, respiratory SOFA score in the next\n24 hours, and SOFA score increase by two points. Models using SaO2 instead of\nSpO2 generally showed better performance. Patients with overestimation of O2 by\npulse oximetry of > 3% had significant decreases in mortality prediction\nrecall, from 0.63 to 0.59, P < 0.001. This mirrors clinical processes where\nbiased pulse oximetry readings provide clinicians with false reassurance of\npatients' oxygen levels. A similar degradation happened in ML models, with\npulse oximetry biases leading to more false negatives in predicting adverse\noutcomes.\n","authors":["Inês Martins","João Matos","Tiago Gonçalves","Leo A. Celi","A. Ian Wong","Jaime S. Cardoso"],"pdf_url":"https://arxiv.org/pdf/2408.04396v1.pdf","comment":"10 pages; accepted at MICCAI's Third Workshop on Applications of\n  Medical AI (2024)"},{"id":"http://arxiv.org/abs/2408.04391v1","updated":"2024-08-08T11:51:34Z","published":"2024-08-08T11:51:34Z","title":"Robustness investigation of quality measures for the assessment of\n  machine learning models","summary":"  In this paper the accuracy and robustness of quality measures for the\nassessment of machine learning models are investigated. The prediction quality\nof a machine learning model is evaluated model-independent based on a\ncross-validation approach, where the approximation error is estimated for\nunknown data. The presented measures quantify the amount of explained variation\nin the model prediction. The reliability of these measures is assessed by means\nof several numerical examples, where an additional data set for the\nverification of the estimated prediction error is available. Furthermore, the\nconfidence bounds of the presented quality measures are estimated and local\nquality measures are derived from the prediction residuals obtained by the\ncross-validation approach.\n","authors":["Thomas Most","Lars Gräning","Sebastian Wolff"],"pdf_url":"https://arxiv.org/pdf/2408.04391v1.pdf","comment":"under review, submitted to the journal Engineering Modelling,\n  Analysis & Simulation (EMAS)"},{"id":"http://arxiv.org/abs/2408.04380v1","updated":"2024-08-08T11:34:31Z","published":"2024-08-08T11:34:31Z","title":"Deep Generative Models in Robotics: A Survey on Learning from Multimodal\n  Demonstrations","summary":"  Learning from Demonstrations, the field that proposes to learn robot behavior\nmodels from data, is gaining popularity with the emergence of deep generative\nmodels. Although the problem has been studied for years under names such as\nImitation Learning, Behavioral Cloning, or Inverse Reinforcement Learning,\nclassical methods have relied on models that don't capture complex data\ndistributions well or don't scale well to large numbers of demonstrations. In\nrecent years, the robot learning community has shown increasing interest in\nusing deep generative models to capture the complexity of large datasets. In\nthis survey, we aim to provide a unified and comprehensive review of the last\nyear's progress in the use of deep generative models in robotics. We present\nthe different types of models that the community has explored, such as\nenergy-based models, diffusion models, action value maps, or generative\nadversarial networks. We also present the different types of applications in\nwhich deep generative models have been used, from grasp generation to\ntrajectory generation or cost learning. One of the most important elements of\ngenerative models is the generalization out of distributions. In our survey, we\nreview the different decisions the community has made to improve the\ngeneralization of the learned models. Finally, we highlight the research\nchallenges and propose a number of future directions for learning deep\ngenerative models in robotics.\n","authors":["Julen Urain","Ajay Mandlekar","Yilun Du","Mahi Shafiullah","Danfei Xu","Katerina Fragkiadaki","Georgia Chalvatzaki","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2408.04380v1.pdf","comment":"20 pages, 11 figures, submitted to TRO"},{"id":"http://arxiv.org/abs/2210.04797v3","updated":"2024-08-08T11:26:01Z","published":"2022-09-23T16:13:47Z","title":"DeepVol: Volatility Forecasting from High-Frequency Data with Dilated\n  Causal Convolutions","summary":"  Volatility forecasts play a central role among equity risk measures. Besides\ntraditional statistical models, modern forecasting techniques based on machine\nlearning can be employed when treating volatility as a univariate, daily\ntime-series. Moreover, econometric studies have shown that increasing the\nnumber of daily observations with high-frequency intraday data helps to improve\nvolatility predictions. In this work, we propose DeepVol, a model based on\nDilated Causal Convolutions that uses high-frequency data to forecast day-ahead\nvolatility. Our empirical findings demonstrate that dilated convolutional\nfilters are highly effective at extracting relevant information from intraday\nfinancial time-series, proving that this architecture can effectively leverage\npredictive information present in high-frequency data that would otherwise be\nlost if realised measures were precomputed. Simultaneously, dilated\nconvolutional filters trained with intraday high-frequency data help us avoid\nthe limitations of models that use daily data, such as model misspecification\nor manually designed handcrafted features, whose devise involves optimising the\ntrade-off between accuracy and computational efficiency and makes models prone\nto lack of adaptation into changing circumstances. In our analysis, we use two\nyears of intraday data from NASDAQ-100 to evaluate the performance of DeepVol.\nOur empirical results suggest that the proposed deep learning-based approach\neffectively learns global features from high-frequency data, resulting in more\naccurate predictions compared to traditional methodologies and producing more\naccurate risk measures.\n","authors":["Fernando Moreno-Pino","Stefan Zohren"],"pdf_url":"https://arxiv.org/pdf/2210.04797v3.pdf","comment":"Updated version"},{"id":"http://arxiv.org/abs/2408.04377v1","updated":"2024-08-08T11:22:52Z","published":"2024-08-08T11:22:52Z","title":"Anomaly Prediction: A Novel Approach with Explicit Delay and Horizon","summary":"  Detecting anomalies in time series data is a critical challenge across\nvarious domains. Traditional methods typically focus on identifying anomalies\nin immediate subsequent steps, often underestimating the significance of\ntemporal dynamics such as delay time and horizons of anomalies, which generally\nrequire extensive post-analysis. This paper introduces a novel approach for\ntime series anomaly prediction, incorporating temporal information directly\ninto the prediction results. We propose a new dataset specifically designed to\nevaluate this approach and conduct comprehensive experiments using several\nstate-of-the-art methods. results demonstrate the efficacy of our approach in\nproviding timely and accurate anomaly predictions, setting a new benchmark for\nfuture research in this field.\n","authors":["Jiang You","Arben Cela","René Natowicz","Jacob Ouanounou","Patrick Siarry"],"pdf_url":"https://arxiv.org/pdf/2408.04377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04376v1","updated":"2024-08-08T11:18:40Z","published":"2024-08-08T11:18:40Z","title":"Deep Reinforcement Learning for the Design of Metamaterial Mechanisms\n  with Functional Compliance Control","summary":"  Metamaterial mechanisms are micro-architectured compliant structures that\noperate through the elastic deformation of specially designed flexible members.\nThis study develops an efficient design methodology for compliant mechanisms\nusing deep reinforcement learning (RL). For this purpose, design domains are\ndigitized into finite cells with various hinge connections, and finite element\nanalyses (FEAs) are conducted to evaluate the deformation behaviors of the\ncompliance mechanism with different cell combinations. The FEA data are learned\nthrough the RL method to obtain optimal compliant mechanisms for desired\nfunctional requirements. The RL algorithm is applied to the design of a\ncompliant door-latch mechanism, exploring the effect of human guidance and\ntiling direction. The optimal result is achieved with minimal human guidance\nand inward tiling, resulting in a threefold increase in the predefined reward\ncompared to human-designed mechanisms. The proposed approach is extended to the\ndesign of a soft gripper mechanism, where the effect of hinge connections is\nadditionally considered. The optimal design under hinge penalization reveals\nremarkably enhanced compliance, and its performance is validated by\nexperimental tests using an additively manufactured gripper. These findings\ndemonstrate that RL-optimized designs outperform those developed with human\ninsight, providing an efficient design methodology for cell-based compliant\nmechanisms in practical applications.\n","authors":["Yejun Choi","Yeoneung Kim","Keun Park"],"pdf_url":"https://arxiv.org/pdf/2408.04376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04369v1","updated":"2024-08-08T10:58:33Z","published":"2024-08-08T10:58:33Z","title":"Analyzing Consumer Reviews for Understanding Drivers of Hotels Ratings:\n  An Indian Perspective","summary":"  In the internet era, almost every business entity is trying to have its\ndigital footprint in digital media and other social media platforms. For these\nentities, word of mouse is also very important. Particularly, this is quite\ncrucial for the hospitality sector dealing with hotels, restaurants etc.\nConsumers do read other consumers reviews before making final decisions. This\nis where it becomes very important to understand which aspects are affecting\nmost in the minds of the consumers while giving their ratings. The current\nstudy focuses on the consumer reviews of Indian hotels to extract aspects\nimportant for final ratings. The study involves gathering data using web\nscraping methods, analyzing the texts using Latent Dirichlet Allocation for\ntopic extraction and sentiment analysis for aspect-specific sentiment mapping.\nFinally, it incorporates Random Forest to understand the importance of the\naspects in predicting the final rating of a user.\n","authors":["Subhasis Dasgupta","Soumya Roy","Jaydip Sen"],"pdf_url":"https://arxiv.org/pdf/2408.04369v1.pdf","comment":"This is the pre-print of the paper that was accepted for oral\n  presentation and publication in the proceedings of IEEE ICCCNT 2024 which was\n  organized as IIT Mandi, India from June 24 to 28, 2024. The paper is 5 pages\n  long and it contains 4 figures and 6 tables. The is not the final version of\n  the paper"},{"id":"http://arxiv.org/abs/2408.04360v1","updated":"2024-08-08T10:47:02Z","published":"2024-08-08T10:47:02Z","title":"Detecting Car Speed using Object Detection and Depth Estimation: A Deep\n  Learning Framework","summary":"  Road accidents are quite common in almost every part of the world, and, in\nmajority, fatal accidents are attributed to over speeding of vehicles. The\ntendency to over speeding is usually tried to be controlled using check points\nat various parts of the road but not all traffic police have the device to\ncheck speed with existing speed estimating devices such as LIDAR based, or\nRadar based guns. The current project tries to address the issue of vehicle\nspeed estimation with handheld devices such as mobile phones or wearable\ncameras with network connection to estimate the speed using deep learning\nframeworks.\n","authors":["Subhasis Dasgupta","Arshi Naaz","Jayeeta Choudhury","Nancy Lahiri"],"pdf_url":"https://arxiv.org/pdf/2408.04360v1.pdf","comment":"This is the pre-print of the paper which was accepted for oral\n  presentation and publication in the proceedings of IEEE CONIT 2024, organized\n  at Pune from June 21 to 23, 2024. The paper is 6 pages long and it contains\n  11 figures and 1 table. This is not the final version of the paper"},{"id":"http://arxiv.org/abs/2407.07454v3","updated":"2024-08-08T10:40:43Z","published":"2024-07-10T08:16:13Z","title":"CM-DQN: A Value-Based Deep Reinforcement Learning Model to Simulate\n  Confirmation Bias","summary":"  In human decision-making tasks, individuals learn through trials and\nprediction errors. When individuals learn the task, some are more influenced by\ngood outcomes, while others weigh bad outcomes more heavily. Such confirmation\nbias can lead to different learning effects. In this study, we propose a new\nalgorithm in Deep Reinforcement Learning, CM-DQN, which applies the idea of\ndifferent update strategies for positive or negative prediction errors, to\nsimulate the human decision-making process when the task's states are\ncontinuous while the actions are discrete. We test in Lunar Lander environment\nwith confirmatory, disconfirmatory bias and non-biased to observe the learning\neffects. Moreover, we apply the confirmation model in a multi-armed bandit\nproblem (environment in discrete states and discrete actions), which utilizes\nthe same idea as our proposed algorithm, as a contrast experiment to\nalgorithmically simulate the impact of different confirmation bias in\ndecision-making process. In both experiments, confirmatory bias indicates a\nbetter learning effect.\n","authors":["Jiacheng Shen","Lihan Feng"],"pdf_url":"https://arxiv.org/pdf/2407.07454v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16093v3","updated":"2024-08-08T10:20:02Z","published":"2023-11-27T18:58:34Z","title":"Visual cognition in multimodal large language models","summary":"  A chief goal of artificial intelligence is to build machines that think like\npeople. Yet it has been argued that deep neural network architectures fail to\naccomplish this. Researchers have asserted these models' limitations in the\ndomains of causal reasoning, intuitive physics, and intuitive psychology. Yet\nrecent advancements, namely the rise of large language models, particularly\nthose designed for visual processing, have rekindled interest in the potential\nto emulate human-like cognitive abilities. This paper evaluates the current\nstate of vision-based large language models in the domains of intuitive\nphysics, causal reasoning, and intuitive psychology. Through a series of\ncontrolled experiments, we investigate the extent to which these modern models\ngrasp complex physical interactions, causal relationships, and intuitive\nunderstanding of others' preferences. Our findings reveal that, while some of\nthese models demonstrate a notable proficiency in processing and interpreting\nvisual data, they still fall short of human capabilities in these areas. Our\nresults emphasize the need for integrating more robust mechanisms for\nunderstanding causality, physical dynamics, and social cognition into\nmodern-day, vision-based language models, and point out the importance of\ncognitively-inspired benchmarks.\n","authors":["Luca M. Schulze Buschoff","Elif Akata","Matthias Bethge","Eric Schulz"],"pdf_url":"https://arxiv.org/pdf/2311.16093v3.pdf","comment":"Updated manuscript"},{"id":"http://arxiv.org/abs/2408.04339v1","updated":"2024-08-08T09:49:26Z","published":"2024-08-08T09:49:26Z","title":"Self-Supervised Contrastive Graph Clustering Network via Structural\n  Information Fusion","summary":"  Graph clustering, a classical task in graph learning, involves partitioning\nthe nodes of a graph into distinct clusters. This task has applications in\nvarious real-world scenarios, such as anomaly detection, social network\nanalysis, and community discovery. Current graph clustering methods commonly\nrely on module pre-training to obtain a reliable prior distribution for the\nmodel, which is then used as the optimization objective. However, these methods\noften overlook deeper supervised signals, leading to sub-optimal reliability of\nthe prior distribution. To address this issue, we propose a novel deep graph\nclustering method called CGCN. Our approach introduces contrastive signals and\ndeep structural information into the pre-training process. Specifically, CGCN\nutilizes a contrastive learning mechanism to foster information\ninteroperability among multiple modules and allows the model to adaptively\nadjust the degree of information aggregation for different order structures.\nOur CGCN method has been experimentally validated on multiple real-world graph\ndatasets, showcasing its ability to boost the dependability of prior clustering\ndistributions acquired through pre-training. As a result, we observed notable\nenhancements in the performance of the model.\n","authors":["Xiaoyang Ji","Yuchen Zhou","Haofu Yang","Shiyue Xu","Jiahao Li"],"pdf_url":"https://arxiv.org/pdf/2408.04339v1.pdf","comment":"6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2407.06704v2","updated":"2024-08-08T09:41:40Z","published":"2024-07-09T09:31:15Z","title":"Self-supervised visual learning from interactions with objects","summary":"  Self-supervised learning (SSL) has revolutionized visual representation\nlearning, but has not achieved the robustness of human vision. A reason for\nthis could be that SSL does not leverage all the data available to humans\nduring learning. When learning about an object, humans often purposefully turn\nor move around objects and research suggests that these interactions can\nsubstantially enhance their learning. Here we explore whether such\nobject-related actions can boost SSL. For this, we extract the actions\nperformed to change from one ego-centric view of an object to another in four\nvideo datasets. We then introduce a new loss function to learn visual and\naction embeddings by aligning the performed action with the representations of\ntwo images extracted from the same clip. This permits the performed actions to\nstructure the latent visual representation. Our experiments show that our\nmethod consistently outperforms previous methods on downstream category\nrecognition. In our analysis, we find that the observed improvement is\nassociated with a better viewpoint-wise alignment of different objects from the\nsame category. Overall, our work demonstrates that embodied interactions with\nobjects can improve SSL of object categories.\n","authors":["Arthur Aubret","Céline Teulière","Jochen Triesch"],"pdf_url":"https://arxiv.org/pdf/2407.06704v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16010v2","updated":"2024-08-08T09:12:13Z","published":"2024-07-22T19:33:12Z","title":"AIDE: Antithetical, Intent-based, and Diverse Example-Based Explanations","summary":"  For many use-cases, it is often important to explain the prediction of a\nblack-box model by identifying the most influential training data samples.\nExisting approaches lack customization for user intent and often provide a\nhomogeneous set of explanation samples, failing to reveal the model's reasoning\nfrom different angles.\n  In this paper, we propose AIDE, an approach for providing antithetical (i.e.,\ncontrastive), intent-based, diverse explanations for opaque and complex models.\nAIDE distinguishes three types of explainability intents: interpreting a\ncorrect, investigating a wrong, and clarifying an ambiguous prediction. For\neach intent, AIDE selects an appropriate set of influential training samples\nthat support or oppose the prediction either directly or by contrast. To\nprovide a succinct summary, AIDE uses diversity-aware sampling to avoid\nredundancy and increase coverage of the training data.\n  We demonstrate the effectiveness of AIDE on image and text classification\ntasks, in three ways: quantitatively, assessing correctness and continuity;\nqualitatively, comparing anecdotal evidence from AIDE and other example-based\napproaches; and via a user study, evaluating multiple aspects of AIDE. The\nresults show that AIDE addresses the limitations of existing methods and\nexhibits desirable traits for an explainability method.\n","authors":["Ikhtiyor Nematov","Dimitris Sacharidis","Tomer Sagi","Katja Hose"],"pdf_url":"https://arxiv.org/pdf/2407.16010v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02407v2","updated":"2024-08-08T09:06:50Z","published":"2024-08-05T12:01:42Z","title":"Terracorder: Sense Long and Prosper","summary":"  In-situ sensing devices need to be deployed in remote environments for long\nperiods of time; minimizing their power consumption is vital for maximising\nboth their operational lifetime and coverage. We introduce Terracorder -- a\nversatile multi-sensor device -- and showcase its exceptionally low power\nconsumption using an on-device reinforcement learning scheduler. We prototype a\nunique device setup for biodiversity monitoring and compare its battery life\nusing our scheduler against a number of fixed schedules; the scheduler captures\nmore than 80% of events at less than 50% of the number of activations of the\nbest-performing fixed schedule. We then explore how a collaborative scheduler\ncan maximise the useful operation of a network of devices, improving overall\nnetwork power consumption and robustness.\n","authors":["Josh Millar","Sarab Sethi","Hamed Haddadi","Anil Madhavapeddy"],"pdf_url":"https://arxiv.org/pdf/2408.02407v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2404.03710v2","updated":"2024-08-08T09:03:51Z","published":"2024-04-04T13:43:17Z","title":"Self-organized free-flight arrival for urban air mobility","summary":"  Urban air mobility is an innovative mode of transportation in which electric\nvertical takeoff and landing (eVTOL) vehicles operate between nodes called\nvertiports. We outline a self-organized vertiport arrival system based on deep\nreinforcement learning. The airspace around the vertiport is assumed to be\ncircular, and the vehicles can freely operate inside. Each aircraft is\nconsidered an individual agent and follows a shared policy, resulting in\ndecentralized actions that are based on local information. We investigate the\ndevelopment of the reinforcement learning policy during training and illustrate\nhow the algorithm moves from suboptimal local holding patterns to a safe and\nefficient final policy. The latter is validated in simulation-based scenarios,\nincluding robustness analyses against sensor noise and a changing distribution\nof inbound traffic. Lastly, we deploy the final policy on small-scale unmanned\naerial vehicles to showcase its real-world usability.\n","authors":["Martin Waltz","Ostap Okhrin","Michael Schultz"],"pdf_url":"https://arxiv.org/pdf/2404.03710v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04318v1","updated":"2024-08-08T08:52:29Z","published":"2024-08-08T08:52:29Z","title":"Deep Transfer Learning for Kidney Cancer Diagnosis","summary":"  Many incurable diseases prevalent across global societies stem from various\ninfluences, including lifestyle choices, economic conditions, social factors,\nand genetics. Research predominantly focuses on these diseases due to their\nwidespread nature, aiming to decrease mortality, enhance treatment options, and\nimprove healthcare standards. Among these, kidney disease stands out as a\nparticularly severe condition affecting men and women worldwide. Nonetheless,\nthere is a pressing need for continued research into innovative, early\ndiagnostic methods to develop more effective treatments for such diseases.\nRecently, automatic diagnosis of Kidney Cancer has become an important\nchallenge especially when using deep learning (DL) due to the importance of\ntraining medical datasets, which in most cases are difficult and expensive to\nobtain. Furthermore, in most cases, algorithms require data from the same\ndomain and a powerful computer with efficient storage capacity. To overcome\nthis issue, a new type of learning known as transfer learning (TL) has been\nproposed that can produce impressive results based on other different\npre-trained data. This paper presents, to the best of the authors' knowledge,\nthe first comprehensive survey of DL-based TL frameworks for kidney cancer\ndiagnosis. This is a strong contribution to help researchers understand the\ncurrent challenges and perspectives of this topic. Hence, the main limitations\nand advantages of each framework are identified and detailed critical analyses\nare provided. Looking ahead, the article identifies promising directions for\nfuture research. Moving on, the discussion is concluded by reflecting on the\npivotal role of TL in the development of precision medicine and its effects on\nclinical practice and research in oncology.\n","authors":["Yassine Habchi","Hamza Kheddar","Yassine Himeur","Abdelkrim Boukabou","Shadi Atalla","Wathiq Mansoor","Hussain Al-Ahmad"],"pdf_url":"https://arxiv.org/pdf/2408.04318v1.pdf","comment":"32 pages, 8 figures and 8 tables"},{"id":"http://arxiv.org/abs/2408.04315v1","updated":"2024-08-08T08:48:54Z","published":"2024-08-08T08:48:54Z","title":"Federated Cubic Regularized Newton Learning with\n  Sparsification-amplified Differential Privacy","summary":"  This paper investigates the use of the cubic-regularized Newton method within\na federated learning framework while addressing two major concerns that\ncommonly arise in federated learning: privacy leakage and communication\nbottleneck. We introduce a federated learning algorithm called Differentially\nPrivate Federated Cubic Regularized Newton (DP-FCRN). By leveraging\nsecond-order techniques, our algorithm achieves lower iteration complexity\ncompared to first-order methods. We also incorporate noise perturbation during\nlocal computations to ensure privacy. Furthermore, we employ sparsification in\nuplink transmission, which not only reduces the communication costs but also\namplifies the privacy guarantee. Specifically, this approach reduces the\nnecessary noise intensity without compromising privacy protection. We analyze\nthe convergence properties of our algorithm and establish the privacy\nguarantee. Finally, we validate the effectiveness of the proposed algorithm\nthrough experiments on a benchmark dataset.\n","authors":["Wei Huo","Changxin Liu","Kemi Ding","Karl Henrik Johansson","Ling Shi"],"pdf_url":"https://arxiv.org/pdf/2408.04315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04313v1","updated":"2024-08-08T08:47:20Z","published":"2024-08-08T08:47:20Z","title":"Better Locally Private Sparse Estimation Given Multiple Samples Per User","summary":"  Previous studies yielded discouraging results for item-level locally\ndifferentially private linear regression with $s^*$-sparsity assumption, where\nthe minimax rate for $nm$ samples is $\\mathcal{O}(s^{*}d / nm\\varepsilon^2)$.\nThis can be challenging for high-dimensional data, where the dimension $d$ is\nextremely large. In this work, we investigate user-level locally differentially\nprivate sparse linear regression. We show that with $n$ users each contributing\n$m$ samples, the linear dependency of dimension $d$ can be eliminated, yielding\nan error upper bound of $\\mathcal{O}(s^{*2} / nm\\varepsilon^2)$. We propose a\nframework that first selects candidate variables and then conducts estimation\nin the narrowed low-dimensional space, which is extendable to general sparse\nestimation problems with tight error bounds. Experiments on both synthetic and\nreal datasets demonstrate the superiority of the proposed methods. Both the\ntheoretical and empirical results suggest that, with the same number of\nsamples, locally private sparse estimation is better conducted when multiple\nsamples per user are available.\n","authors":["Yuheng Ma","Ke Jia","Hanfang Yang"],"pdf_url":"https://arxiv.org/pdf/2408.04313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11337v3","updated":"2024-08-08T08:44:52Z","published":"2023-12-18T16:41:30Z","title":"Challenges for Reinforcement Learning in Quantum Circuit Design","summary":"  Quantum computing (QC) in the current NISQ era is still limited in size and\nprecision. Hybrid applications mitigating those shortcomings are prevalent to\ngain early insight and advantages. Hybrid quantum machine learning (QML)\ncomprises both the application of QC to improve machine learning (ML) and ML to\nimprove QC architectures. This work considers the latter, leveraging\nreinforcement learning (RL) to improve quantum circuit design (QCD), which we\nformalize by a set of generic objectives. Furthermore, we propose qcd-gym, a\nconcrete framework formalized as a Markov decision process, to enable learning\npolicies capable of controlling a universal set of continuously parameterized\nquantum gates. Finally, we provide benchmark comparisons to assess the\nshortcomings and strengths of current state-of-the-art RL algorithms.\n","authors":["Philipp Altmann","Jonas Stein","Michael Kölle","Adelina Bärligea","Thomas Gabor","Thomy Phan","Sebastian Feld","Claudia Linnhoff-Popien"],"pdf_url":"https://arxiv.org/pdf/2312.11337v3.pdf","comment":"11 pages, 4 figures, accepted for publication at the 2024 IEEE\n  International Conference on Quantum Computing and Engineering (QCE)"},{"id":"http://arxiv.org/abs/2407.11652v6","updated":"2024-08-08T08:44:29Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v6.pdf","comment":"I found critical errors in the manuscript affecting its validity. I\n  need to correct these before resubmitting. Major changes to methodology and\n  results are underway, significantly altering the content. I will resubmit the\n  revised version"},{"id":"http://arxiv.org/abs/2408.04310v1","updated":"2024-08-08T08:42:47Z","published":"2024-08-08T08:42:47Z","title":"Constructing Adversarial Examples for Vertical Federated Learning:\n  Optimal Client Corruption through Multi-Armed Bandit","summary":"  Vertical federated learning (VFL), where each participating client holds a\nsubset of data features, has found numerous applications in finance,\nhealthcare, and IoT systems. However, adversarial attacks, particularly through\nthe injection of adversarial examples (AEs), pose serious challenges to the\nsecurity of VFL models. In this paper, we investigate such vulnerabilities\nthrough developing a novel attack to disrupt the VFL inference process, under a\npractical scenario where the adversary is able to adaptively corrupt a subset\nof clients. We formulate the problem of finding optimal attack strategies as an\nonline optimization problem, which is decomposed into an inner problem of\nadversarial example generation (AEG) and an outer problem of corruption pattern\nselection (CPS). Specifically, we establish the equivalence between the\nformulated CPS problem and a multi-armed bandit (MAB) problem, and propose the\nThompson sampling with Empirical maximum reward (E-TS) algorithm for the\nadversary to efficiently identify the optimal subset of clients for corruption.\nThe key idea of E-TS is to introduce an estimation of the expected maximum\nreward for each arm, which helps to specify a small set of competitive arms, on\nwhich the exploration for the optimal arm is performed. This significantly\nreduces the exploration space, which otherwise can quickly become prohibitively\nlarge as the number of clients increases. We analytically characterize the\nregret bound of E-TS, and empirically demonstrate its capability of efficiently\nrevealing the optimal corruption pattern with the highest attack success rate,\nunder various datasets of popular VFL tasks.\n","authors":["Duanyi Yao","Songze Li","Ye Xue","Jin Liu"],"pdf_url":"https://arxiv.org/pdf/2408.04310v1.pdf","comment":"Published on ICLR2024"},{"id":"http://arxiv.org/abs/2408.04309v1","updated":"2024-08-08T08:42:30Z","published":"2024-08-08T08:42:30Z","title":"TheGlueNote: Learned Representations for Robust and Flexible Note\n  Alignment","summary":"  Note alignment refers to the task of matching individual notes of two\nversions of the same symbolically encoded piece. Methods addressing this task\ncommonly rely on sequence alignment algorithms such as Hidden Markov Models or\nDynamic Time Warping (DTW) applied directly to note or onset sequences. While\nsuccessful in many cases, such methods struggle with large mismatches between\nthe versions. In this work, we learn note-wise representations from data\naugmented with various complex mismatch cases, e.g. repeats, skips, block\ninsertions, and long trills. At the heart of our approach lies a transformer\nencoder network - TheGlueNote - which predicts pairwise note similarities for\ntwo 512 note subsequences. We postprocess the predicted similarities using\nflavors of weightedDTW and pitch-separated onsetDTW to retrieve note matches\nfor two sequences of arbitrary length. Our approach performs on par with the\nstate of the art in terms of note alignment accuracy, is considerably more\nrobust to version mismatches, and works directly on any pair of MIDI files.\n","authors":["Silvan David Peter","Gerhard Widmer"],"pdf_url":"https://arxiv.org/pdf/2408.04309v1.pdf","comment":"to be published in Proceedings of the 25th International Society for\n  Music Information Retrieval Conference (ISMIR), 2024"},{"id":"http://arxiv.org/abs/2408.04307v1","updated":"2024-08-08T08:40:15Z","published":"2024-08-08T08:40:15Z","title":"Partial Experts Checkpoint: Efficient Fault Tolerance for Sparse\n  Mixture-of-Experts Model Training","summary":"  As large language models continue to scale up, the imperative for fault\ntolerance in distributed deep learning systems intensifies, becoming a focal\narea of AI infrastructure research. Checkpoint has emerged as the predominant\nfault tolerance strategy, with extensive studies dedicated to optimizing its\nefficiency. However, the advent of the sparse Mixture-of-Experts (MoE) model\npresents new challenges for traditional checkpoint techniques due to the\nsubstantial increase in model size, despite comparable computational demands to\ndense models. Breaking new ground in the realm of efficient fault tolerance for\nMoE model training, we introduce a novel Partial Experts Checkpoint (PEC)\nmechanism alongside a corresponding PEC fault-tolerant system. Our approach\nstrategically checkpoints a selected subset of experts, thereby significantly\nreducing the checkpoint size for MoE models to a level comparable with that of\ndense models. The empirical analysis on our 8-expert GPT-MoE model demonstrates\nthat the proposed PEC approach facilitates a substantial 54.2% decrease in the\nsize of non-redundant checkpoint (no data-parallel duplication), without\ncompromising the final model quality. Moreover, our PEC fault-tolerant system\nachieves a 76.9% reduction in checkpoint workload per data-parallel distributed\nrank, thereby correspondingly diminishing the checkpointing time and\nfacilitating complete overlap with the training process.\n","authors":["Weilin Cai","Le Qin","Jiayi Huang"],"pdf_url":"https://arxiv.org/pdf/2408.04307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04303v1","updated":"2024-08-08T08:37:28Z","published":"2024-08-08T08:37:28Z","title":"Trans-Tokenization and Cross-lingual Vocabulary Transfers: Language\n  Adaptation of LLMs for Low-Resource NLP","summary":"  The development of monolingual language models for low and mid-resource\nlanguages continues to be hindered by the difficulty in sourcing high-quality\ntraining data. In this study, we present a novel cross-lingual vocabulary\ntransfer strategy, trans-tokenization, designed to tackle this challenge and\nenable more efficient language adaptation. Our approach focuses on adapting a\nhigh-resource monolingual LLM to an unseen target language by initializing the\ntoken embeddings of the target language using a weighted average of\nsemantically similar token embeddings from the source language. For this, we\nleverage a translation resource covering both the source and target languages.\nWe validate our method with the Tweeties, a series of trans-tokenized LLMs, and\ndemonstrate their competitive performance on various downstream tasks across a\nsmall but diverse set of languages. Additionally, we introduce Hydra LLMs,\nmodels with multiple swappable language modeling heads and embedding tables,\nwhich further extend the capabilities of our trans-tokenization strategy. By\ndesigning a Hydra LLM based on the multilingual model TowerInstruct, we\ndeveloped a state-of-the-art machine translation model for Tatar, in a\nzero-shot manner, completely bypassing the need for high-quality parallel data.\nThis breakthrough is particularly significant for low-resource languages like\nTatar, where high-quality parallel data is hard to come by. By lowering the\ndata and time requirements for training high-quality models, our\ntrans-tokenization strategy allows for the development of LLMs for a wider\nrange of languages, especially those with limited resources. We hope that our\nwork will inspire further research and collaboration in the field of\ncross-lingual vocabulary transfer and contribute to the empowerment of\nlanguages on a global scale.\n","authors":["François Remy","Pieter Delobelle","Hayastan Avetisyan","Alfiya Khabibullina","Miryam de Lhoneux","Thomas Demeester"],"pdf_url":"https://arxiv.org/pdf/2408.04303v1.pdf","comment":"Accepted at COLM 2024"},{"id":"http://arxiv.org/abs/2402.12062v3","updated":"2024-08-08T08:36:03Z","published":"2024-02-19T11:30:00Z","title":"Causal Equal Protection as Algorithmic Fairness","summary":"  By combining the philosophical literature on statistical evidence and the\ninterdisciplinary literature on algorithmic fairness, we revisit recent\nobjections against classification parity in light of causal analyses of\nalgorithmic fairness and the distinction between predictive and diagnostic\nevidence. We focus on trial proceedings as a black-box classification algorithm\nin which defendants are sorted into two groups by convicting or acquitting\nthem. We defend a novel principle, causal equal protection, that combines\nclassification parity with the causal approach. In the do-calculus, causal\nequal protection requires that individuals should not be subject to uneven\nrisks of classification error because of their protected or socially salient\ncharacteristics. The explicit use of protected characteristics, however, may be\nrequired if it equalizes these risks.\n","authors":["Marcello Di Bello","Nicolò Cangiotti","Michele Loi"],"pdf_url":"https://arxiv.org/pdf/2402.12062v3.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.04301v1","updated":"2024-08-08T08:35:32Z","published":"2024-08-08T08:35:32Z","title":"Tackling Noisy Clients in Federated Learning with End-to-end Label\n  Correction","summary":"  Recently, federated learning (FL) has achieved wide successes for diverse\nprivacy-sensitive applications without sacrificing the sensitive private\ninformation of clients. However, the data quality of client datasets can not be\nguaranteed since corresponding annotations of different clients often contain\ncomplex label noise of varying degrees, which inevitably causes the performance\ndegradation. Intuitively, the performance degradation is dominated by clients\nwith higher noise rates since their trained models contain more misinformation\nfrom data, thus it is necessary to devise an effective optimization scheme to\nmitigate the negative impacts of these noisy clients. In this work, we propose\na two-stage framework FedELC to tackle this complicated label noise issue. The\nfirst stage aims to guide the detection of noisy clients with higher label\nnoise, while the second stage aims to correct the labels of noisy clients' data\nvia an end-to-end label correction framework which is achieved by learning\npossible ground-truth labels of noisy clients' datasets via back propagation.\nWe implement sixteen related methods and evaluate five datasets with three\ntypes of complicated label noise scenarios for a comprehensive comparison.\nExtensive experimental results demonstrate our proposed framework achieves\nsuperior performance than its counterparts for different scenarios.\nAdditionally, we effectively improve the data quality of detected noisy\nclients' local datasets with our label correction framework. The code is\navailable at https://github.com/Sprinter1999/FedELC.\n","authors":["Xuefeng Jiang","Sheng Sun","Jia Li","Jingjing Xue","Runhan Li","Zhiyuan Wu","Gang Xu","Yuwei Wang","Min Liu"],"pdf_url":"https://arxiv.org/pdf/2408.04301v1.pdf","comment":"To appear in ACM CIKM'24 full research paper track"},{"id":"http://arxiv.org/abs/2408.04295v1","updated":"2024-08-08T08:18:05Z","published":"2024-08-08T08:18:05Z","title":"Assigning Credit with Partial Reward Decoupling in Multi-Agent Proximal\n  Policy Optimization","summary":"  Multi-agent proximal policy optimization (MAPPO) has recently demonstrated\nstate-of-the-art performance on challenging multi-agent reinforcement learning\ntasks. However, MAPPO still struggles with the credit assignment problem,\nwherein the sheer difficulty in ascribing credit to individual agents' actions\nscales poorly with team size. In this paper, we propose a multi-agent\nreinforcement learning algorithm that adapts recent developments in credit\nassignment to improve upon MAPPO. Our approach leverages partial reward\ndecoupling (PRD), which uses a learned attention mechanism to estimate which of\na particular agent's teammates are relevant to its learning updates. We use\nthis estimate to dynamically decompose large groups of agents into smaller,\nmore manageable subgroups. We empirically demonstrate that our approach,\nPRD-MAPPO, decouples agents from teammates that do not influence their expected\nfuture reward, thereby streamlining credit assignment. We additionally show\nthat PRD-MAPPO yields significantly higher data efficiency and asymptotic\nperformance compared to both MAPPO and other state-of-the-art methods across\nseveral multi-agent tasks, including StarCraft II. Finally, we propose a\nversion of PRD-MAPPO that is applicable to \\textit{shared} reward settings,\nwhere PRD was previously not applicable, and empirically show that this also\nleads to performance improvements over MAPPO.\n","authors":["Aditya Kapoor","Benjamin Freed","Howie Choset","Jeff Schneider"],"pdf_url":"https://arxiv.org/pdf/2408.04295v1.pdf","comment":"20 pages, 5 figures, 12 tables, Reinforcement Learning Journal and\n  Reinforcement Learning Conference 2024"},{"id":"http://arxiv.org/abs/2408.04294v1","updated":"2024-08-08T08:17:50Z","published":"2024-08-08T08:17:50Z","title":"Dual-branch PolSAR Image Classification Based on GraphMAE and Local\n  Feature Extraction","summary":"  The annotation of polarimetric synthetic aperture radar (PolSAR) images is a\nlabor-intensive and time-consuming process. Therefore, classifying PolSAR\nimages with limited labels is a challenging task in remote sensing domain. In\nrecent years, self-supervised learning approaches have proven effective in\nPolSAR image classification with sparse labels. However, we observe a lack of\nresearch on generative selfsupervised learning in the studied task. Motivated\nby this, we propose a dual-branch classification model based on generative\nself-supervised learning in this paper. The first branch is a\nsuperpixel-branch, which learns superpixel-level polarimetric representations\nusing a generative self-supervised graph masked autoencoder. To acquire finer\nclassification results, a convolutional neural networks-based pixel-branch is\nfurther incorporated to learn pixel-level features. Classification with fused\ndual-branch features is finally performed to obtain the predictions.\nExperimental results on the benchmark Flevoland dataset demonstrate that our\napproach yields promising classification results.\n","authors":["Yuchen Wang","Ziyi Guo","Haixia Bi","Danfeng Hong","Chen Xu"],"pdf_url":"https://arxiv.org/pdf/2408.04294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04290v1","updated":"2024-08-08T08:06:42Z","published":"2024-08-08T08:06:42Z","title":"Efficient and Accurate Pneumonia Detection Using a Novel Multi-Scale\n  Transformer Approach","summary":"  Pneumonia, a severe respiratory disease, poses significant diagnostic\nchallenges, especially in underdeveloped regions. Traditional diagnostic\nmethods, such as chest X-rays, suffer from variability in interpretation among\nradiologists, necessitating reliable automated tools. In this study, we propose\na novel approach combining deep learning and transformer-based attention\nmechanisms to enhance pneumonia detection from chest X-rays. Our method begins\nwith lung segmentation using a TransUNet model that integrates our specialized\ntransformer module, which has fewer parameters compared to common transformers\nwhile maintaining performance. This model is trained on the \"Chest Xray Masks\nand Labels\" dataset and then applied to the Kermany and Cohen datasets to\nisolate lung regions, enhancing subsequent classification tasks. For\nclassification, we employ pre-trained ResNet models (ResNet-50 and ResNet-101)\nto extract multi-scale feature maps, processed through our modified transformer\nmodule. By employing our specialized transformer, we attain superior results\nwith significantly fewer parameters compared to common transformer models. Our\napproach achieves high accuracy rates of 92.79% on the Kermany dataset and\n95.11% on the Cohen dataset, ensuring robust and efficient performance suitable\nfor resource-constrained environments.\n\"https://github.com/amirrezafateh/Multi-Scale-Transformer-Pneumonia\"\n","authors":["Alireza Saber","Pouria Parhami","Alimihammad Siahkarzadeh","Amirreza Fateh"],"pdf_url":"https://arxiv.org/pdf/2408.04290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21670v2","updated":"2024-08-08T07:59:50Z","published":"2024-07-31T15:13:39Z","title":"Universal Approximation Theory: Foundations for Parallelism in Neural\n  Networks","summary":"  Neural networks are increasingly evolving towards training large models with\nbig data, a method that has demonstrated superior performance across many\ntasks. However, this approach introduces an urgent problem: current deep\nlearning models are predominantly serial, meaning that as the number of network\nlayers increases, so do the training and inference times. This is unacceptable\nif deep learning is to continue advancing. Therefore, this paper proposes a\ndeep learning parallelization strategy based on the Universal Approximation\nTheorem (UAT). From this foundation, we designed a parallel network called\nPara-Former to test our theory. Unlike traditional serial models, the inference\ntime of Para-Former does not increase with the number of layers, significantly\naccelerating the inference speed of multi-layer networks. Experimental results\nvalidate the effectiveness of this network.\n","authors":["Wei Wang","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2407.21670v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19429v3","updated":"2024-08-08T07:56:48Z","published":"2024-07-28T08:39:28Z","title":"FTF-ER: Feature-Topology Fusion-Based Experience Replay Method for\n  Continual Graph Learning","summary":"  Continual graph learning (CGL) is an important and challenging task that aims\nto extend static GNNs to dynamic task flow scenarios. As one of the mainstream\nCGL methods, the experience replay (ER) method receives widespread attention\ndue to its superior performance. However, existing ER methods focus on\nidentifying samples by feature significance or topological relevance, which\nlimits their utilization of comprehensive graph data. In addition, the\ntopology-based ER methods only consider local topological information and add\nneighboring nodes to the buffer, which ignores the global topological\ninformation and increases memory overhead. To bridge these gaps, we propose a\nnovel method called Feature-Topology Fusion-based Experience Replay (FTF-ER) to\neffectively mitigate the catastrophic forgetting issue with enhanced\nefficiency. Specifically, from an overall perspective to maximize the\nutilization of the entire graph data, we propose a highly complementary\napproach including both feature and global topological information, which can\nsignificantly improve the effectiveness of the sampled nodes. Moreover, to\nfurther utilize global topological information, we propose Hodge Potential\nScore (HPS) as a novel module to calculate the topological importance of nodes.\nHPS derives a global node ranking via Hodge decomposition on graphs, providing\nmore accurate global topological information compared to neighbor sampling. By\nexcluding neighbor sampling, HPS significantly reduces buffer storage costs for\nacquiring topological information and simultaneously decreases training time.\nCompared with state-of-the-art methods, FTF-ER achieves a significant\nimprovement of 3.6% in AA and 7.1% in AF on the OGB-Arxiv dataset,\ndemonstrating its superior performance in the class-incremental learning\nsetting.\n","authors":["Jinhui Pang","Changqing Lin","Xiaoshuai Hao","Rong Yin","Zixuan Wang","Zhihui Zhang","Jinglin He","Huang Tai Sheng"],"pdf_url":"https://arxiv.org/pdf/2407.19429v3.pdf","comment":"Accepted by ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2408.04283v1","updated":"2024-08-08T07:41:16Z","published":"2024-08-08T07:41:16Z","title":"Prompt-Assisted Semantic Interference Cancellation on Moderate\n  Interference Channels","summary":"  The performance of conventional interference management strategies degrades\nwhen interference power is comparable to signal power. We consider a new\nperspective on interference management using semantic communication.\nSpecifically, a multi-user semantic communication system is considered on\nmoderate interference channels (ICs), for which a novel framework of deep\nlearning-based prompt-assisted semantic interference cancellation (DeepPASIC)\nis proposed. Each transmitted signal is partitioned into common and private\nparts. The common parts of different users are transmitted simultaneously in a\nshared medium, resulting in superposition. The private part, on the other hand,\nserves as a prompt to assist in canceling the interference suffered by the\ncommon part at the semantic level. Simulation results demonstrate that the\nproposed DeepPASIC outperforms conventional interference management strategies\nunder moderate interference conditions.\n","authors":["Zian Meng","Qiang Li","Ashish Pandharipande","Xiaohu Ge"],"pdf_url":"https://arxiv.org/pdf/2408.04283v1.pdf","comment":"5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.04277v1","updated":"2024-08-08T07:31:22Z","published":"2024-08-08T07:31:22Z","title":"Stability Analysis of Equivariant Convolutional Representations Through\n  The Lens of Equivariant Multi-layered CKNs","summary":"  In this paper we construct and theoretically analyse group equivariant\nconvolutional kernel networks (CKNs) which are useful in understanding the\ngeometry of (equivariant) CNNs through the lens of reproducing kernel Hilbert\nspaces (RKHSs). We then proceed to study the stability analysis of such\nequiv-CKNs under the action of diffeomorphism and draw a connection with\nequiv-CNNs, where the goal is to analyse the geometry of inductive biases of\nequiv-CNNs through the lens of reproducing kernel Hilbert spaces (RKHSs).\nTraditional deep learning architectures, including CNNs, trained with\nsophisticated optimization algorithms is vulnerable to perturbations, including\n`adversarial examples'. Understanding the RKHS norm of such models through CKNs\nis useful in designing the appropriate architecture and can be useful in\ndesigning robust equivariant representation learning models.\n","authors":["Soutrik Roy Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2408.04277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04276v1","updated":"2024-08-08T07:24:28Z","published":"2024-08-08T07:24:28Z","title":"Early Risk Assessment Model for ICA Timing Strategy in Unstable Angina\n  Patients Using Multi-Modal Machine Learning","summary":"  Background: Invasive coronary arteriography (ICA) is recognized as the gold\nstandard for diagnosing cardiovascular diseases, including unstable angina\n(UA). The challenge lies in determining the optimal timing for ICA in UA\npatients, balancing the need for revascularization in high-risk patients\nagainst the potential complications in low-risk ones. Unlike myocardial\ninfarction, UA does not have specific indicators like ST-segment deviation or\ncardiac enzymes, making risk assessment complex. Objectives: Our study aims to\nenhance the early risk assessment for UA patients by utilizing machine learning\nalgorithms. These algorithms can potentially identify patients who would\nbenefit most from ICA by analyzing less specific yet related indicators that\nare challenging for human physicians to interpret. Methods: We collected data\nfrom 640 UA patients at Shanghai General Hospital, including medical history\nand electrocardiograms (ECG). Machine learning algorithms were trained using\nmulti-modal demographic characteristics including clinical risk factors,\nsymptoms, biomarker levels, and ECG features extracted by pre-trained neural\nnetworks. The goal was to stratify patients based on their revascularization\nrisk. Additionally, we translated our models into applicable and explainable\nlook-up tables through discretization for practical clinical use. Results: The\nstudy achieved an Area Under the Curve (AUC) of $0.719 \\pm 0.065$ in risk\nstratification, significantly surpassing the widely adopted GRACE score's AUC\nof $0.579 \\pm 0.044$. Conclusions: The results suggest that machine learning\ncan provide superior risk stratification for UA patients. This improved\nstratification could help in balancing the risks, costs, and complications\nassociated with ICA, indicating a potential shift in clinical assessment\npractices for unstable angina.\n","authors":["Candi Zheng","Kun Liu","Yang Wang","Shiyi Chen","Hongli Li"],"pdf_url":"https://arxiv.org/pdf/2408.04276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11427v2","updated":"2024-08-08T07:23:01Z","published":"2024-07-16T06:45:27Z","title":"Semi-Supervised Generative Models for Disease Trajectories: A Case Study\n  on Systemic Sclerosis","summary":"  We propose a deep generative approach using latent temporal processes for\nmodeling and holistically analyzing complex disease trajectories, with a\nparticular focus on Systemic Sclerosis (SSc). We aim to learn temporal latent\nrepresentations of the underlying generative process that explain the observed\npatient disease trajectories in an interpretable and comprehensive way. To\nenhance the interpretability of these latent temporal processes, we develop a\nsemi-supervised approach for disentangling the latent space using established\nmedical knowledge. By combining the generative approach with medical\ndefinitions of different characteristics of SSc, we facilitate the discovery of\nnew aspects of the disease. We show that the learned temporal latent processes\ncan be utilized for further data analysis and clinical hypothesis testing,\nincluding finding similar patients and clustering SSc patient trajectories into\nnovel sub-types. Moreover, our method enables personalized online monitoring\nand prediction of multivariate time series with uncertainty quantification.\n","authors":["Cécile Trottet","Manuel Schürch","Ahmed Allam","Imon Barua","Liubov Petelytska","David Launay","Paolo Airò","Radim Bečvář","Christopher Denton","Mislav Radic","Oliver Distler","Anna-Maria Hoffmann-Vold","Michael Krauthammer","the EUSTAR collaborators"],"pdf_url":"https://arxiv.org/pdf/2407.11427v2.pdf","comment":"Accepted at Machine Learning for Healthcare 2024. arXiv admin note:\n  substantial text overlap with arXiv:2311.08149"},{"id":"http://arxiv.org/abs/2407.06204v2","updated":"2024-08-08T07:13:37Z","published":"2024-06-26T16:34:33Z","title":"A Survey on Mixture of Experts","summary":"  Large language models (LLMs) have garnered unprecedented advancements across\ndiverse fields, ranging from natural language processing to computer vision and\nbeyond. The prowess of LLMs is underpinned by their substantial model size,\nextensive and diverse datasets, and the vast computational power harnessed\nduring training, all of which contribute to the emergent abilities of LLMs\n(e.g., in-context learning) that are not present in small models. Within this\ncontext, the mixture of experts (MoE) has emerged as an effective method for\nsubstantially scaling up model capacity with minimal computation overhead,\ngaining significant attention from academia and industry. Despite its growing\nprevalence, there lacks a systematic and comprehensive review of the literature\non MoE. This survey seeks to bridge that gap, serving as an essential resource\nfor researchers delving into the intricacies of MoE. We first briefly introduce\nthe structure of the MoE layer, followed by proposing a new taxonomy of MoE.\nNext, we overview the core designs for various MoE models including both\nalgorithmic and systemic aspects, alongside collections of available\nopen-source implementations, hyperparameter configurations and empirical\nevaluations. Furthermore, we delineate the multifaceted applications of MoE in\npractice, and outline some potential directions for future research. To\nfacilitate ongoing updates and the sharing of cutting-edge developments in MoE\nresearch, we have established a resource repository accessible at\nhttps://github.com/withinmiaov/A-Survey-on-Mixture-of-Experts.\n","authors":["Weilin Cai","Juyong Jiang","Fan Wang","Jing Tang","Sunghun Kim","Jiayi Huang"],"pdf_url":"https://arxiv.org/pdf/2407.06204v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.10373v2","updated":"2024-08-08T07:06:40Z","published":"2024-01-18T20:43:43Z","title":"Harmonized Spatial and Spectral Learning for Robust and Generalized\n  Medical Image Segmentation","summary":"  Deep learning has demonstrated remarkable achievements in medical image\nsegmentation. However, prevailing deep learning models struggle with poor\ngeneralization due to (i) intra-class variations, where the same class appears\ndifferently in different samples, and (ii) inter-class independence, resulting\nin difficulties capturing intricate relationships between distinct objects,\nleading to higher false negative cases. This paper presents a novel approach\nthat synergies spatial and spectral representations to enhance\ndomain-generalized medical image segmentation. We introduce the innovative\nSpectral Correlation Coefficient objective to improve the model's capacity to\ncapture middle-order features and contextual long-range dependencies. This\nobjective complements traditional spatial objectives by incorporating valuable\nspectral information. Extensive experiments reveal that optimizing this\nobjective with existing architectures like UNet and TransUNet significantly\nenhances generalization, interpretability, and noise robustness, producing more\nconfident predictions. For instance, in cardiac segmentation, we observe a 0.81\npp and 1.63 pp (pp = percentage point) improvement in DSC over UNet and\nTransUNet, respectively. Our interpretability study demonstrates that, in most\ntasks, objectives optimized with UNet outperform even TransUNet by introducing\nglobal contextual information alongside local details. These findings\nunderscore the versatility and effectiveness of our proposed method across\ndiverse imaging modalities and medical domains.\n","authors":["Vandan Gorade","Sparsh Mittal","Debesh Jha","Rekha Singhal","Ulas Bagci"],"pdf_url":"https://arxiv.org/pdf/2401.10373v2.pdf","comment":"Early Accepted at ICPR-2024 for Oral Presentation"},{"id":"http://arxiv.org/abs/2408.04254v1","updated":"2024-08-08T06:47:21Z","published":"2024-08-08T06:47:21Z","title":"Generating Fine-Grained Causality in Climate Time Series Data for\n  Forecasting and Anomaly Detection","summary":"  Understanding the causal interaction of time series variables can contribute\nto time series data analysis for many real-world applications, such as climate\nforecasting and extreme weather alerts. However, causal relationships are\ndifficult to be fully observed in real-world complex settings, such as\nspatial-temporal data from deployed sensor networks. Therefore, to capture\nfine-grained causal relations among spatial-temporal variables for further a\nmore accurate and reliable time series analysis, we first design a conceptual\nfine-grained causal model named TBN Granger Causality, which adds\ntime-respecting Bayesian Networks to the previous time-lagged Neural Granger\nCausality to offset the instantaneous effects. Second, we propose an end-to-end\ndeep generative model called TacSas, which discovers TBN Granger Causality in a\ngenerative manner to help forecast time series data and detect possible\nanomalies during the forecast. For evaluations, besides the causality discovery\nbenchmark Lorenz-96, we also test TacSas on climate benchmark ERA5 for climate\nforecasting and the extreme weather benchmark of NOAA for extreme weather\nalerts.\n","authors":["Dongqi Fu","Yada Zhu","Hanghang Tong","Kommy Weldemariam","Onkar Bhardwaj","Jingrui He"],"pdf_url":"https://arxiv.org/pdf/2408.04254v1.pdf","comment":"ICML 2024 AI for Science Workshop"},{"id":"http://arxiv.org/abs/2312.10308v4","updated":"2024-08-08T06:40:12Z","published":"2023-12-16T03:50:24Z","title":"Event-Based Contrastive Learning for Medical Time Series","summary":"  In clinical practice, one often needs to identify whether a patient is at\nhigh risk of adverse outcomes after some key medical event. For example,\nquantifying the risk of adverse outcomes after an acute cardiovascular event\nhelps healthcare providers identify those patients at the highest risk of poor\noutcomes; i.e., patients who benefit from invasive therapies that can lower\ntheir risk. Assessing the risk of adverse outcomes, however, is challenging due\nto the complexity, variability, and heterogeneity of longitudinal medical data,\nespecially for individuals suffering from chronic diseases like heart failure.\nIn this paper, we introduce Event-Based Contrastive Learning (EBCL) - a method\nfor learning embeddings of heterogeneous patient data that preserves temporal\ninformation before and after key index events. We demonstrate that EBCL can be\nused to construct models that yield improved performance on important\ndownstream tasks relative to other pretraining methods. We develop and test the\nmethod using a cohort of heart failure patients obtained from a large hospital\nnetwork and the publicly available MIMIC-IV dataset consisting of patients in\nan intensive care unit at a large tertiary care center. On both cohorts, EBCL\npretraining yields models that are performant with respect to a number of\ndownstream tasks, including mortality, hospital readmission, and length of\nstay. In addition, unsupervised EBCL embeddings effectively cluster heart\nfailure patients into subgroups with distinct outcomes, thereby providing\ninformation that helps identify new heart failure phenotypes. The contrastive\nframework around the index event can be adapted to a wide array of time-series\ndatasets and provides information that can be used to guide personalized care.\n","authors":["Hyewon Jeong","Nassim Oufattole","Matthew Mcdermott","Aparna Balagopalan","Bryan Jangeesingh","Marzyeh Ghassemi","Collin Stultz"],"pdf_url":"https://arxiv.org/pdf/2312.10308v4.pdf","comment":"Accepted at Unifying Representations in Neural Models Workshop in\n  NeurIPS 2023, MLHC 2024"},{"id":"http://arxiv.org/abs/2403.19913v2","updated":"2024-08-08T06:38:31Z","published":"2024-03-29T01:53:24Z","title":"MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of\n  Large Language Models","summary":"  Large language models such as ChatGPT and GPT-4 have recently achieved\nastonishing performance on a variety of natural language processing tasks. In\nthis paper, we propose MANGO, a benchmark to evaluate their capabilities to\nperform text-based mapping and navigation. Our benchmark includes 53 mazes\ntaken from a suite of textgames: each maze is paired with a walkthrough that\nvisits every location but does not cover all possible paths. The task is\nquestion-answering: for each maze, a large language model reads the walkthrough\nand answers hundreds of mapping and navigation questions such as \"How should\nyou go to Attic from West of House?\" and \"Where are we if we go north and east\nfrom Cellar?\". Although these questions are easy to humans, it turns out that\neven GPT-4, the best-to-date language model, performs poorly at answering them.\nFurther, our experiments suggest that a strong mapping and navigation ability\nwould benefit large language models in performing relevant downstream tasks,\nsuch as playing textgames. Our MANGO benchmark will facilitate future research\non methods that improve the mapping and navigation capabilities of language\nmodels. We host our leaderboard, data, code, and evaluation program at\nhttps://mango.ttic.edu and https://github.com/oaklight/mango/.\n","authors":["Peng Ding","Jiading Fang","Peng Li","Kangrui Wang","Xiaochen Zhou","Mo Yu","Jing Li","Matthew R. Walter","Hongyuan Mei"],"pdf_url":"https://arxiv.org/pdf/2403.19913v2.pdf","comment":"COLM 2024 camera-ready"},{"id":"http://arxiv.org/abs/2408.04251v1","updated":"2024-08-08T06:36:56Z","published":"2024-08-08T06:36:56Z","title":"Cooperative Multi-Agent Deep Reinforcement Learning in Content Ranking\n  Optimization","summary":"  In a typical e-commerce setting, Content Ranking Optimization (CRO)\nmechanisms are employed to surface content on the search page to fulfill\ncustomers' shopping missions. CRO commonly utilizes models such as contextual\ndeep bandits model to independently rank content at different positions, e.g.,\none optimizer dedicated to organic search results and another to sponsored\nresults. However, this regional optimization approach does not necessarily\ntranslate to whole page optimization, e.g., maximizing revenue at the top of\nthe page may inadvertently diminish the revenue of lower positions. In this\npaper, we propose a reinforcement learning based method for whole page ranking\nto jointly optimize across all positions by: 1) shifting from position level\noptimization to whole page level optimization to achieve an overall optimized\nranking; 2) applying reinforcement learning to optimize for the cumulative\nrewards instead of the instant reward. We formulate page level CRO as a\ncooperative Multi-agent Markov Decision Process , and address it with the novel\nMulti-Agent Deep Deterministic Policy Gradient (MADDPG) model. MADDPG supports\na flexible and scalable joint optimization framework by adopting a \"centralized\ntraining and decentralized execution\" approach. Extensive experiments\ndemonstrate that MADDPG scales to a 2.5 billion action space in the public\nMujoco environment, and outperforms the deep bandits modeling by 25.7% on the\noffline CRO data set from a leading e-commerce company. We foresee that this\nnovel multi-agent optimization is applicable to similar joint optimization\nproblems in the field of information retrieval.\n","authors":["Zhou Qin","Kai Yuan","Pratik Lahiri","Wenyang Liu"],"pdf_url":"https://arxiv.org/pdf/2408.04251v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2408.04245v1","updated":"2024-08-08T06:17:13Z","published":"2024-08-08T06:17:13Z","title":"Scalable Transformer for High Dimensional Multivariate Time Series\n  Forecasting","summary":"  Deep models for Multivariate Time Series (MTS) forecasting have recently\ndemonstrated significant success. Channel-dependent models capture complex\ndependencies that channel-independent models cannot capture. However, the\nnumber of channels in real-world applications outpaces the capabilities of\nexisting channel-dependent models, and contrary to common expectations, some\nmodels underperform the channel-independent models in handling high-dimensional\ndata, which raises questions about the performance of channel-dependent models.\nTo address this, our study first investigates the reasons behind the suboptimal\nperformance of these channel-dependent models on high-dimensional MTS data. Our\nanalysis reveals that two primary issues lie in the introduced noise from\nunrelated series that increases the difficulty of capturing the crucial\ninter-channel dependencies, and challenges in training strategies due to\nhigh-dimensional data. To address these issues, we propose STHD, the Scalable\nTransformer for High-Dimensional Multivariate Time Series Forecasting. STHD has\nthree components: a) Relation Matrix Sparsity that limits the noise introduced\nand alleviates the memory issue; b) ReIndex applied as a training strategy to\nenable a more flexible batch size setting and increase the diversity of\ntraining data; and c) Transformer that handles 2-D inputs and captures channel\ndependencies. These components jointly enable STHD to manage the\nhigh-dimensional MTS while maintaining computational feasibility. Furthermore,\nexperimental results show STHD's considerable improvement on three\nhigh-dimensional datasets: Crime-Chicago, Wiki-People, and Traffic. The source\ncode and dataset are publicly available\nhttps://github.com/xinzzzhou/ScalableTransformer4HighDimensionMTSF.git.\n","authors":["Xin Zhou","Weiqing Wang","Wray Buntine","Shilin Qu","Abishek Sriramulu","Weicong Tan","Christoph Bergmeir"],"pdf_url":"https://arxiv.org/pdf/2408.04245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04242v1","updated":"2024-08-08T06:08:04Z","published":"2024-08-08T06:08:04Z","title":"The Ungrounded Alignment Problem","summary":"  Modern machine learning systems have demonstrated substantial abilities with\nmethods that either embrace or ignore human-provided knowledge, but combining\nbenefits of both styles remains a challenge. One particular challenge involves\ndesigning learning systems that exhibit built-in responses to specific abstract\nstimulus patterns, yet are still plastic enough to be agnostic about the\nmodality and exact form of their inputs. In this paper, we investigate what we\ncall The Ungrounded Alignment Problem, which asks How can we build in\npredefined knowledge in a system where we don't know how a given stimulus will\nbe grounded? This paper examines a simplified version of the general problem,\nwhere an unsupervised learner is presented with a sequence of images for the\ncharacters in a text corpus, and this learner is later evaluated on its ability\nto recognize specific (possibly rare) sequential patterns. Importantly, the\nlearner is given no labels during learning or evaluation, but must map images\nfrom an unknown font or permutation to its correct class label. That is, at no\npoint is our learner given labeled images, where an image vector is explicitly\nassociated with a class label. Despite ample work in unsupervised and\nself-supervised loss functions, all current methods require a labeled\nfine-tuning phase to map the learned representations to correct classes.\nFinding this mapping in the absence of labels may seem a fool's errand, but our\nmain result resolves this seeming paradox. We show that leveraging only letter\nbigram frequencies is sufficient for an unsupervised learner both to reliably\nassociate images to class labels and to reliably identify trigger words in the\nsequence of inputs. More generally, this method suggests an approach for\nencoding specific desired innate behaviour in modality-agnostic models.\n","authors":["Marc Pickett","Aakash Kumar Nain","Joseph Modayil","Llion Jones"],"pdf_url":"https://arxiv.org/pdf/2408.04242v1.pdf","comment":"7 pages, plus references and appendix"},{"id":"http://arxiv.org/abs/2408.04236v1","updated":"2024-08-08T05:43:20Z","published":"2024-08-08T05:43:20Z","title":"Cluster-Wide Task Slowdown Detection in Cloud System","summary":"  Slow task detection is a critical problem in cloud operation and maintenance\nsince it is highly related to user experience and can bring substantial\nliquidated damages. Most anomaly detection methods detect it from a single-task\naspect. However, considering millions of concurrent tasks in large-scale cloud\ncomputing clusters, it becomes impractical and inefficient. Moreover,\nsingle-task slowdowns are very common and do not necessarily indicate a\nmalfunction of a cluster due to its violent fluctuation nature in a virtual\nenvironment. Thus, we shift our attention to cluster-wide task slowdowns by\nutilizing the duration time distribution of tasks across a cluster, so that the\ncomputation complexity is not relevant to the number of tasks.\n  The task duration time distribution often exhibits compound periodicity and\nlocal exceptional fluctuations over time. Though transformer-based methods are\none of the most powerful methods to capture these time series normal variation\npatterns, we empirically find and theoretically explain the flaw of the\nstandard attention mechanism in reconstructing subperiods with low amplitude\nwhen dealing with compound periodicity.\n  To tackle these challenges, we propose SORN (i.e., Skimming Off subperiods in\ndescending amplitude order and Reconstructing Non-slowing fluctuation), which\nconsists of a Skimming Attention mechanism to reconstruct the compound\nperiodicity and a Neural Optimal Transport module to distinguish cluster-wide\nslowdowns from other exceptional fluctuations. Furthermore, since anomalies in\nthe training set are inevitable in a practical scenario, we propose a picky\nloss function, which adaptively assigns higher weights to reliable time slots\nin the training set. Extensive experiments demonstrate that SORN outperforms\nstate-of-the-art methods on multiple real-world industrial datasets.\n","authors":["Feiyi Chen","Yingying Zhang","Lunting Fan","Yuxuan Liang","Guansong Pang","Qingsong Wen","Shuiguang Deng"],"pdf_url":"https://arxiv.org/pdf/2408.04236v1.pdf","comment":"This paper has been accepted by KDD2024"},{"id":"http://arxiv.org/abs/2408.04232v1","updated":"2024-08-08T05:37:17Z","published":"2024-08-08T05:37:17Z","title":"Enhanced Traffic Flow Prediction with Multi-Segment Fusion Tensor Graph\n  Convolutional Networks","summary":"  Accurate traffic Flow Prediction can assist in traffic management, route\nplanning, and congestion mitigation, which holds significant importance in\nenhancing the efficiency and reliability of intelligent transportation systems\n(ITS). However, existing traffic flow prediction models suffer from limitations\nin capturing the complex spatial-temporal dependencies within traffic networks.\nIn order to address this issue, this study proposes a multi-segment fusion\ntensor graph convolutional network (MS-FTGCN) for traffic flow prediction with\nthe following three-fold ideas: a) building a unified spatial-temporal graph\nconvolutional framework based on Tensor M-product, which capture the\nspatial-temporal patterns simultaneously; b) incorporating hourly, daily, and\nweekly components to model multi temporal properties of traffic flows,\nrespectively; c) fusing the outputs of the three components by attention\nmechanism to obtain the final traffic flow prediction results. The results of\nexperiments conducted on two traffic flow datasets demonstrate that the\nproposed MS-FTGCN outperforms the state-of-the-art models.\n","authors":["Wei Zhang","Peng Tang"],"pdf_url":"https://arxiv.org/pdf/2408.04232v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04229v1","updated":"2024-08-08T05:33:21Z","published":"2024-08-08T05:33:21Z","title":"Probabilistic Circuits for Cumulative Distribution Functions","summary":"  A probabilistic circuit (PC) succinctly expresses a function that represents\na multivariate probability distribution and, given sufficient structural\nproperties of the circuit, supports efficient probabilistic inference.\nTypically a PC computes the probability mass (or density) function (PMF or PDF)\nof the distribution. We consider PCs instead computing the cumulative\ndistribution function (CDF). We show that for distributions over binary random\nvariables these representations (PMF and CDF) are essentially equivalent, in\nthe sense that one can be transformed to the other in polynomial time. We then\nshow how a similar equivalence holds for distributions over finite discrete\nvariables using a modification of the standard encoding with binary variables\nthat aligns with the CDF semantics. Finally we show that for continuous\nvariables, smooth, decomposable PCs computing PDFs and CDFs can be efficiently\ntransformed to each other by modifying only the leaves of the circuit.\n","authors":["Oliver Broadrick","William Cao","Benjie Wang","Martin Trapp","Guy Van den Broeck"],"pdf_url":"https://arxiv.org/pdf/2408.04229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04221v1","updated":"2024-08-08T05:09:02Z","published":"2024-08-08T05:09:02Z","title":"Connective Viewpoints of Signal-to-Noise Diffusion Models","summary":"  Diffusion models (DM) have become fundamental components of generative\nmodels, excelling across various domains such as image creation, audio\ngeneration, and complex data interpolation. Signal-to-Noise diffusion models\nconstitute a diverse family covering most state-of-the-art diffusion models.\nWhile there have been several attempts to study Signal-to-Noise (S2N) diffusion\nmodels from various perspectives, there remains a need for a comprehensive\nstudy connecting different viewpoints and exploring new perspectives. In this\nstudy, we offer a comprehensive perspective on noise schedulers, examining\ntheir role through the lens of the signal-to-noise ratio (SNR) and its\nconnections to information theory. Building upon this framework, we have\ndeveloped a generalized backward equation to enhance the performance of the\ninference process.\n","authors":["Khanh Doan","Long Tung Vuong","Tuan Nguyen","Anh Tuan Bui","Quyen Tran","Thanh-Toan Do","Dinh Phung","Trung Le"],"pdf_url":"https://arxiv.org/pdf/2408.04221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04220v1","updated":"2024-08-08T05:06:22Z","published":"2024-08-08T05:06:22Z","title":"Diffusion Guided Language Modeling","summary":"  Current language models demonstrate remarkable proficiency in text\ngeneration. However, for many applications it is desirable to control\nattributes, such as sentiment, or toxicity, of the generated language --\nideally tailored towards each specific use case and target audience. For\nauto-regressive language models, existing guidance methods are prone to\ndecoding errors that cascade during generation and degrade performance. In\ncontrast, text diffusion models can easily be guided with, for example, a\nsimple linear sentiment classifier -- however they do suffer from significantly\nhigher perplexity than auto-regressive alternatives. In this paper we use a\nguided diffusion model to produce a latent proposal that steers an\nauto-regressive language model to generate text with desired properties. Our\nmodel inherits the unmatched fluency of the auto-regressive approach and the\nplug-and-play flexibility of diffusion. We show that it outperforms previous\nplug-and-play guidance methods across a wide range of benchmark data sets.\nFurther, controlling a new attribute in our framework is reduced to training a\nsingle logistic regression classifier.\n","authors":["Justin Lovelace","Varsha Kishore","Yiwei Chen","Kilian Q. Weinberger"],"pdf_url":"https://arxiv.org/pdf/2408.04220v1.pdf","comment":"ACL Findings 2024"},{"id":"http://arxiv.org/abs/2106.11528v3","updated":"2024-08-08T04:18:34Z","published":"2021-06-22T03:44:03Z","title":"Recent Deep Semi-supervised Learning Approaches and Related Works","summary":"  This work proposes an overview of the recent semi-supervised learning\napproaches and related works. Despite the remarkable success of neural networks\nin various applications, there exist a few formidable constraints, including\nthe need for a large amount of labeled data. Therefore, semi-supervised\nlearning, which is a learning scheme in which scarce labels and a larger amount\nof unlabeled data are utilized to train models (e.g., deep neural networks), is\ngetting more important. Based on the key assumptions of semi-supervised\nlearning, which are the manifold assumption, cluster assumption, and continuity\nassumption, the work reviews the recent semi-supervised learning approaches. In\nparticular, the methods in regard to using deep neural networks in a\nsemi-supervised learning setting are primarily discussed. In addition, the\nexisting works are first classified based on the underlying idea and explained,\nthen the holistic approaches that unify the aforementioned ideas are detailed.\n","authors":["Gyeongho Kim"],"pdf_url":"https://arxiv.org/pdf/2106.11528v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04353v5","updated":"2024-08-08T04:15:31Z","published":"2023-10-06T16:21:22Z","title":"An In-Context Learning Agent for Formal Theorem-Proving","summary":"  We present an in-context learning agent for formal theorem-proving in\nenvironments like Lean and Coq. Current state-of-the-art models for the problem\nare finetuned on environment-specific proof data. By contrast, our approach,\ncalled COPRA, repeatedly asks a high-capacity, general-purpose large language\nmodel (GPT-4) to propose tactic applications from within a stateful\nbacktracking search. Proposed tactics are executed in the underlying proof\nenvironment. Feedback from the execution is used to build the prompt for the\nnext model query, along with selected information from the search history and\nlemmas retrieved from an external database. We evaluate our implementation of\nCOPRA on the miniF2F benchmark for Lean and a set of Coq tasks from the\nCompCert project. On these benchmarks, COPRA significantly outperforms few-shot\ninvocations of GPT-4. It also compares favorably against finetuning-based\napproaches, outperforming ReProver, a state-of-the-art finetuned approach for\nLean, in terms of the pass@1 metric. Our code and data are available at\nhttps://github.com/trishullab/copra.\n","authors":["Amitayush Thakur","George Tsoukalas","Yeming Wen","Jimmy Xin","Swarat Chaudhuri"],"pdf_url":"https://arxiv.org/pdf/2310.04353v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04206v1","updated":"2024-08-08T04:05:50Z","published":"2024-08-08T04:05:50Z","title":"DC Algorithm for Estimation of Sparse Gaussian Graphical Models","summary":"  Sparse estimation for Gaussian graphical models is a crucial technique for\nmaking the relationships among numerous observed variables more interpretable\nand quantifiable. Various methods have been proposed, including graphical\nlasso, which utilizes the $\\ell_1$ norm as a regularization term, as well as\nmethods employing non-convex regularization terms. However, most of these\nmethods approximate the $\\ell_0$ norm with convex functions. To estimate more\naccurate solutions, it is desirable to treat the $\\ell_0$ norm directly as a\nregularization term. In this study, we formulate the sparse estimation problem\nfor Gaussian graphical models using the $\\ell_0$ norm and propose a method to\nsolve this problem using the Difference of Convex functions Algorithm (DCA).\nSpecifically, we convert the $\\ell_0$ norm constraint into an equivalent\nlargest-$K$ norm constraint, reformulate the constrained problem into a\npenalized form, and solve it using the DC algorithm (DCA). Furthermore, we\ndesigned an algorithm that efficiently computes using graphical lasso.\nExperimental results with synthetic data show that our method yields results\nthat are equivalent to or better than existing methods. Comparisons of model\nlearning through cross-validation confirm that our method is particularly\nadvantageous in selecting true edges.\n","authors":["Tomokaze Shiratori","Yuichi Takano"],"pdf_url":"https://arxiv.org/pdf/2408.04206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07887v5","updated":"2024-08-08T03:49:59Z","published":"2023-12-13T04:14:22Z","title":"Learn or Recall? Revisiting Incremental Learning with Pre-trained\n  Language Models","summary":"  Incremental Learning (IL) has been a long-standing problem in both vision and\nNatural Language Processing (NLP) communities. In recent years, as Pre-trained\nLanguage Models (PLMs) have achieved remarkable progress in various NLP\ndownstream tasks, utilizing PLMs as backbones has become a common practice in\nrecent research of IL in NLP. Most assume that catastrophic forgetting is the\nbiggest obstacle to achieving superior IL performance and propose various\ntechniques to overcome this issue. However, we find that this assumption is\nproblematic. Specifically, we revisit more than 20 methods on four\nclassification tasks (Text Classification, Intent Classification, Relation\nExtraction, and Named Entity Recognition) under the two most popular IL\nsettings (Class-Incremental and Task-Incremental) and reveal that most of them\nseverely underestimate the inherent anti-forgetting ability of PLMs. Based on\nthe observation, we propose a frustratingly easy method called SEQ* for IL with\nPLMs. The results show that SEQ* has competitive or superior performance\ncompared to state-of-the-art (SOTA) IL methods and requires considerably less\ntrainable parameters and training time. These findings urge us to revisit the\nIL with PLMs and encourage future studies to have a fundamental understanding\nof the catastrophic forgetting in PLMs. The data, code and scripts are publicly\navailable at\nhttps://github.com/zzz47zzz/codebase-for-incremental-learning-with-llm.\n","authors":["Junhao Zheng","Shengjie Qiu","Qianli Ma"],"pdf_url":"https://arxiv.org/pdf/2312.07887v5.pdf","comment":"ACL 2024 main conference (Oral)"},{"id":"http://arxiv.org/abs/2312.10385v4","updated":"2024-08-08T03:44:21Z","published":"2023-12-16T08:48:46Z","title":"Imitate the Good and Avoid the Bad: An Incremental Approach to Safe\n  Reinforcement Learning","summary":"  A popular framework for enforcing safe actions in Reinforcement Learning (RL)\nis Constrained RL, where trajectory based constraints on expected cost (or\nother cost measures) are employed to enforce safety and more importantly these\nconstraints are enforced while maximizing expected reward. Most recent\napproaches for solving Constrained RL convert the trajectory based cost\nconstraint into a surrogate problem that can be solved using minor\nmodifications to RL methods. A key drawback with such approaches is an over or\nunderestimation of the cost constraint at each state. Therefore, we provide an\napproach that does not modify the trajectory based cost constraint and instead\nimitates ``good'' trajectories and avoids ``bad'' trajectories generated from\nincrementally improving policies. We employ an oracle that utilizes a reward\nthreshold (which is varied with learning) and the overall cost constraint to\nlabel trajectories as ``good'' or ``bad''. A key advantage of our approach is\nthat we are able to work from any starting policy or set of trajectories and\nimprove on it. In an exhaustive set of experiments, we demonstrate that our\napproach is able to outperform top benchmark approaches for solving Constrained\nRL problems, with respect to expected cost, CVaR cost, or even unknown cost\nconstraints.\n","authors":["Huy Hoang","Tien Mai","Pradeep Varakantham"],"pdf_url":"https://arxiv.org/pdf/2312.10385v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04193v1","updated":"2024-08-08T03:25:41Z","published":"2024-08-08T03:25:41Z","title":"Uncertainty-Aware Crime Prediction With Spatial Temporal Multivariate\n  Graph Neural Networks","summary":"  Crime forecasting is a critical component of urban analysis and essential for\nstabilizing society today. Unlike other time series forecasting problems, crime\nincidents are sparse, particularly in small regions and within specific time\nperiods. Traditional spatial-temporal deep learning models often struggle with\nthis sparsity, as they typically cannot effectively handle the non-Gaussian\nnature of crime data, which is characterized by numerous zeros and\nover-dispersed patterns. To address these challenges, we introduce a novel\napproach termed Spatial Temporal Multivariate Zero-Inflated Negative Binomial\nGraph Neural Networks (STMGNN-ZINB). This framework leverages diffusion and\nconvolution networks to analyze spatial, temporal, and multivariate\ncorrelations, enabling the parameterization of probabilistic distributions of\ncrime incidents. By incorporating a Zero-Inflated Negative Binomial model,\nSTMGNN-ZINB effectively manages the sparse nature of crime data, enhancing\nprediction accuracy and the precision of confidence intervals. Our evaluation\non real-world datasets confirms that STMGNN-ZINB outperforms existing models,\nproviding a more reliable tool for predicting and understanding crime dynamics.\n","authors":["Zepu Wang","Xiaobo Ma","Huajie Yang","Weimin Lvu","Peng Sun","Sharath Chandra Guntuku"],"pdf_url":"https://arxiv.org/pdf/2408.04193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12539v3","updated":"2024-08-08T03:20:17Z","published":"2023-08-24T03:53:55Z","title":"CALM : A Multi-task Benchmark for Comprehensive Assessment of Language\n  Model Bias","summary":"  As language models (LMs) become increasingly powerful and widely used, it is\nimportant to quantify them for sociodemographic bias with potential for harm.\nPrior measures of bias are sensitive to perturbations in the templates designed\nto compare performance across social groups, due to factors such as low\ndiversity or limited number of templates. Also, most previous work considers\nonly one NLP task. We introduce Comprehensive Assessment of Language Models\n(CALM) for robust measurement of two types of universally relevant\nsociodemographic bias, gender and race. CALM integrates sixteen datasets for\nquestion-answering, sentiment analysis and natural language inference. Examples\nfrom each dataset are filtered to produce 224 templates with high diversity\n(e.g., length, vocabulary). We assemble 50 highly frequent person names for\neach of seven distinct demographic groups to generate 78,400 prompts covering\nthe three NLP tasks. Our empirical evaluation shows that CALM bias scores are\nmore robust and far less sensitive than previous bias measurements to\nperturbations in the templates, such as synonym substitution, or to random\nsubset selection of templates. We apply CALM to 20 large language models, and\nfind that for 2 language model series, larger parameter models tend to be more\nbiased than smaller ones. The T0 series is the least biased model families, of\nthe 20 LLMs investigated here. The code is available at\nhttps://github.com/vipulgupta1011/CALM.\n","authors":["Vipul Gupta","Pranav Narayanan Venkit","Hugo Laurençon","Shomir Wilson","Rebecca J. Passonneau"],"pdf_url":"https://arxiv.org/pdf/2308.12539v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04190v1","updated":"2024-08-08T03:18:42Z","published":"2024-08-08T03:18:42Z","title":"Listwise Reward Estimation for Offline Preference-based Reinforcement\n  Learning","summary":"  In Reinforcement Learning (RL), designing precise reward functions remains to\nbe a challenge, particularly when aligning with human intent. Preference-based\nRL (PbRL) was introduced to address this problem by learning reward models from\nhuman feedback. However, existing PbRL methods have limitations as they often\noverlook the second-order preference that indicates the relative strength of\npreference. In this paper, we propose Listwise Reward Estimation (LiRE), a\nnovel approach for offline PbRL that leverages second-order preference\ninformation by constructing a Ranked List of Trajectories (RLT), which can be\nefficiently built by using the same ternary feedback type as traditional\nmethods. To validate the effectiveness of LiRE, we propose a new offline PbRL\ndataset that objectively reflects the effect of the estimated rewards. Our\nextensive experiments on the dataset demonstrate the superiority of LiRE, i.e.,\noutperforming state-of-the-art baselines even with modest feedback budgets and\nenjoying robustness with respect to the number of feedbacks and feedback noise.\nOur code is available at https://github.com/chwoong/LiRE\n","authors":["Heewoong Choi","Sangwon Jung","Hongjoon Ahn","Taesup Moon"],"pdf_url":"https://arxiv.org/pdf/2408.04190v1.pdf","comment":"21 pages, ICML 2024"}]}}